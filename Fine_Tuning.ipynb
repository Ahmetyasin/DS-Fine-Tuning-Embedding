{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports & Initial Setup\n",
    "In this first cell, all required modules are imported. Import statements are combined and logically organized. We also handle potential SSL context issues that could arise with nltk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import statements\n",
    "import os\n",
    "import ssl\n",
    "import nltk\n",
    "import openai\n",
    "import PyPDF2\n",
    "from langchain_core.documents import Document\n",
    "from langchain_community.document_loaders.generic import GenericLoader\n",
    "from langchain_community.document_loaders.parsers import GrobidParser\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from llama_index.finetuning import (\n",
    "    generate_qa_embedding_pairs,\n",
    "    EmbeddingQAFinetuneDataset\n",
    ")\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.core.node_parser import SemanticSplitterNodeParser\n",
    "from llama_index.finetuning import SentenceTransformersFinetuneEngine\n",
    "from huggingface_hub import notebook_login\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Handling SSL certificate issue\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context\n",
    "\n",
    "# Download NLTK dependencies\n",
    "nltk.download()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Split PDF Files into Parts\n",
    "This cell defines functions for splitting large PDF files into smaller parts. The PDF files are split based on the number of pages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to split PDFs into smaller parts\n",
    "import os\n",
    "import ssl\n",
    "import nltk\n",
    "import openai\n",
    "import PyPDF2\n",
    "def split_pdf(file_path, parts, output_dir):\n",
    "    # Get the base name of the file without the extension\n",
    "    base_name = os.path.splitext(os.path.basename(file_path))[0]\n",
    "\n",
    "    # Open the existing PDF\n",
    "    with open(file_path, \"rb\") as file:\n",
    "        reader = PyPDF2.PdfReader(file)\n",
    "        total_pages = len(reader.pages)\n",
    "\n",
    "        # Calculate the number of pages per part\n",
    "        pages_per_part = total_pages // parts\n",
    "\n",
    "        for part in range(parts):\n",
    "            writer = PyPDF2.PdfWriter()\n",
    "            start_page = part * pages_per_part\n",
    "            # Include all remaining pages in the last part\n",
    "            end_page = (part + 1) * pages_per_part if part != parts - 1 else total_pages\n",
    "            \n",
    "            for page in range(start_page, end_page):\n",
    "                writer.add_page(reader.pages[page])\n",
    "            \n",
    "            # Save the split PDF file\n",
    "            part_path = os.path.join(output_dir, f\"{base_name}_part_{part + 1}.pdf\")\n",
    "            with open(part_path, \"wb\") as part_file:\n",
    "                writer.write(part_file)\n",
    "\n",
    "# Process a directory of PDFs and split them\n",
    "def process_directory(directory, destination_directory):\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith('.pdf'):\n",
    "            file_path = os.path.join(directory, filename)\n",
    "            with open(file_path, \"rb\") as file:\n",
    "                reader = PyPDF2.PdfReader(file)\n",
    "                total_pages = len(reader.pages)\n",
    "                parts = 0\n",
    "\n",
    "                # Determine number of parts based on total pages\n",
    "                if 500 <= total_pages <= 750:\n",
    "                    parts = 3\n",
    "                elif total_pages > 750:\n",
    "                    parts = 4\n",
    "                elif 500 > total_pages >= 250:\n",
    "                    parts = 2\n",
    "\n",
    "                # Split the PDF if required\n",
    "                if parts > 0:\n",
    "                    split_pdf(file_path, parts, destination_directory)\n",
    "\n",
    "# Example usage for processing PDFs\n",
    "directory = \"./books\"  # Path to input books directory\n",
    "destination_directory = \"./splitted_books\"  # Output directory for split PDFs\n",
    "process_directory(directory, destination_directory)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Extract Text from Split PDFs Using Grobid Parser\n",
    "In this cell, the split PDF files are processed using GrobidParser to extract text content and save it as .txt files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure the output directory for text files exists\n",
    "output_dir = './cleaned_docs'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Directory containing the split PDFs\n",
    "source_directory = './splitted_books'\n",
    "\n",
    "# Grobid parser configuration (without sentence segmentation)\n",
    "parser = GrobidParser(segment_sentences=False)\n",
    "\n",
    "# Process each PDF in the source directory\n",
    "for filename in os.listdir(source_directory):\n",
    "    if filename.endswith('.pdf'):\n",
    "        file_path = os.path.join(source_directory, filename)\n",
    "        \n",
    "        # Extract content using the GenericLoader\n",
    "        try:\n",
    "            content_list = GenericLoader.from_filesystem(file_path, parser=parser).load()\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to process {filename}: {e}\")\n",
    "            continue\n",
    "        \n",
    "        # Save extracted text to a .txt file\n",
    "        base_name = os.path.splitext(filename)[0]\n",
    "        output_text_file = os.path.join(output_dir, f\"{base_name}.txt\")\n",
    "        \n",
    "        with open(output_text_file, 'w', encoding='utf-8') as text_file:\n",
    "            for document in content_list:\n",
    "                text_file.write(document.page_content)\n",
    "\n",
    "print(\"All PDFs have been processed and text files are saved in 'cleaned_docs'.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load Documents for Semantic Splitting\n",
    "This cell loads the documents from the extracted text files and prepares them for semantic node parsing, which is used to prepare the dataset for fine-tuning.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set OpenAI API key\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"your-openai-api-key\"\n",
    "\n",
    "# Initialize OpenAI embedding model\n",
    "embed_model = OpenAIEmbedding()\n",
    "\n",
    "# Function to load documents and parse them into semantic nodes\n",
    "def load_corpus(docs, for_training=False, verbose=False):\n",
    "    parser = SemanticSplitterNodeParser(\n",
    "        embed_model=embed_model,\n",
    "        normalize=True,\n",
    "        breakpoint_percentile_threshold=95  # Default percentile threshold\n",
    "    )\n",
    "    \n",
    "    if for_training:\n",
    "        nodes = parser.build_semantic_nodes_from_documents(docs[:], show_progress=verbose)\n",
    "    \n",
    "    if verbose:\n",
    "        print(f'Parsed {len(nodes)} nodes')\n",
    "    \n",
    "    return nodes\n",
    "\n",
    "# Load documents from the 'cleaned_docs' folder\n",
    "books_folder_path = \"cleaned_docs\"\n",
    "filenames = [f for f in os.listdir(books_folder_path) if os.path.isfile(os.path.join(books_folder_path, f))]\n",
    "\n",
    "# Construct full file paths\n",
    "SEC_FILE = [os.path.join(books_folder_path, filename) for filename in filenames]\n",
    "print(f\"Loading files {SEC_FILE}\")\n",
    "\n",
    "# Read and load document data\n",
    "reader = SimpleDirectoryReader(input_files=SEC_FILE)\n",
    "docs = reader.load_data()\n",
    "print(f'Loaded {len(docs)} docs')\n",
    "\n",
    "# Parse the loaded documents into semantic nodes\n",
    "train_nodes = load_corpus(docs, for_training=True, verbose=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Fine-Tuning and Saving the Model\n",
    "This section involves generating a QA dataset from the semantic nodes and fine-tuning the model. The fine-tuned model is then uploaded to Hugging Face Hub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate QA pairs from semantic nodes\n",
    "train_dataset = generate_qa_embedding_pairs(\n",
    "    llm=OpenAI(model=\"gpt-4o-mini\"), \n",
    "    nodes=train_nodes\n",
    ")\n",
    "\n",
    "# Save the dataset to JSON\n",
    "train_dataset.save_json(\"train_dataset_17_book_grobid_semantics.json\")\n",
    "\n",
    "# Load the training dataset for fine-tuning\n",
    "train_dataset = EmbeddingQAFinetuneDataset.from_json(\"train_dataset_17_book_grobid_semantic.json\")\n",
    "\n",
    "# Initialize fine-tuning engine\n",
    "finetune_engine = SentenceTransformersFinetuneEngine(\n",
    "    train_dataset,\n",
    "    model_id=\"sentence-transformers/all-mpnet-base-v2\",\n",
    "    model_output_path=\"fine_tuned_model_17book_grobid_semantic\"\n",
    ")\n",
    "\n",
    "# Set environment variable for MPS (Apple Silicon support)\n",
    "os.environ['PYTORCH_MPS_HIGH_WATERMARK_RATIO'] = '0.0'\n",
    "\n",
    "# Start fine-tuning\n",
    "finetune_engine.finetune()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Upload Fine-Tuned Model to Hugging Face Hub\n",
    "Finally, this cell handles uploading the fine-tuned model to the Hugging Face Hub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log in to Hugging Face Hub\n",
    "notebook_login()\n",
    "\n",
    "# Load the fine-tuned model\n",
    "model = SentenceTransformer('fine_tuned_model_17book_grobid_semantic')\n",
    "\n",
    "# Save the model to Hugging Face Hub\n",
    "model.save_to_hub(\n",
    "    \"AhmetAytar/all-mpnet-base-v2-fine-tuned_17_textbook_grobid_semantic\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
