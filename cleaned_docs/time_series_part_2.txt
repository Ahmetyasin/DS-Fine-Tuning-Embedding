Example 3.1 The patient satisfaction data are in the sixth column of the array called patsat.data in which the second and third columns are the age and the severity.Note that we can use the "lm" function to fit the linear model.But as in the example, we will show how to obtain the regression coefficients using the matrix notation.nrow<-dim(patsat.data)[1]X<-cbind(matrix(1,nrow,1),patsat.data[,2:3]) y<-patsat.data[,6] beta<-solve(t(X)%*%X)%*%t(X)%*%y beta Example 3.4 We use the "lm" function again and obtain the linear model.Then we use "confint" function to obtain the confidence intervals of the model parameters.Note that the default confidence level is 95%.new <-data.frame(Age= 75, Severity=60) pred.sat1.clim<-predict(Satisfaction1.fit,newdata=new, se.fit = TRUE, interval = "confidence") pred.sat1.plim<-predict(Satisfaction1.fit,newdata=new, se.fit = TRUE, interval = "prediction") pred.sat1.clim$fitfit lwr upr 1 32.78074 26.99482 38.56666 pred.sat1.plim$fitfit lwr upr 1 32.78074 16.92615 48.63533 Example 3.7 The residual plots for Satisfaction1.fit can be obtained using the following commands: par(mfrow=c(2,2),oma=c(0,0,0,0)) qqnorm(Satisfaction1.fit$res,datax=TRUE,pch=16,xlab= 'Residual',main=") qqline(Satisfaction1.fit$res,datax=TRUE) plot(Satisfaction1.fit$fit,Satisfaction1.fit$res,pch=16, xlab='Fitted Value',ylab='Residual') abline(h=0) hist(Satisfaction1.fit$res,col="gray",xlab='Residual',main='')plot(Satisfaction1.fit$res,type="l",xlab='ObservationOrder',ylab='Residual') points(Satisfaction1.fit$res,pch=16,cex=.5) abline(h=0) Example 3.8 In R, one can do stepwise regression using step function which allows for stepwise selection of variables in forward, backward, or both directions.Note that step needs to be applied to a model with 4 input variables as indicated in the example.Therefore we first fit that model and apply the step function.Also note that the variable selection is done based on AIC and therefore we get in the forward selection slightly different results than the one provided in the textbook.In R, one can do best subset regression using leaps function from leaps package.We first upload the leaps package.As concluded in the example, adding the input variable Population seems to resolve the autocorrelation issue resulting in large p-value for the test for autocorrelation.Example 3.14 The Cochrane-Orcutt method can be found in package orcutt.The function to be used is "cochran.orcutt".The data are given in toothsales.datawhere the columns are Share and Price.The results are not exactly the same as the ones given in the example.This should be due to the difference in the approaches.Where the book uses a two-step procedure, the function Cochrane.Orcutt "estimates both autocorrelation and beta coefficients recursively until we reach the convergence (8th decimal)".Example 3.15 For this example we will use "gls" function in package nlme.From Example 3.14 we know that there is autocorrelation in the residuals of the linear model.We first assume that the first-order model will be sufficient to model the autocorrelation as shown in the book.23.31462 26.33218 29.34974 Price -27.10670 -23.59030 -20.07390 attr(,"label") [1] "Coefficients:" Correlation structure: lower est.upper Phi -0.04226294 0.4325871 0.7480172 attr (,"label") [1] "Correlation structure:" Residual standard error: lower est.upper 0.2805616 0.4074217 0.5916436 predict(tooth3.fit)The second-order autoregressive model for the errors can be fitted using, tooth4.fit<-gls(Share ~Price, data = toothsales.data,correlation=corARMA(p=2), method="ML") Example 3. 16 To create the lagged version of the variables and also adjust for the number of observations, we use the following commands:  Basin, Vol. 43, 1993) investigated the ozone levels in the South Coast Air Basin of California for the years 1976-1991.The author believes that the number of days the ozone levels exceeded 0.20 ppm (the response) depends on the seasonal meteorological index, which is the seasonal average 850-millibar Temperature (the predictor).Table E3  3.2 Montgomery, Peck, and Vining (2012) present data on the number of pounds of steam used per month at a plant.Steam usage is thought to be related to the average monthly ambient temperature.The past year's usages and temperatures are shown in Table E3.2.d. Plant management believes that an increase in average ambient temperature of one degree will increase average monthly steam consumption by 10,000 lb.Do the data support this statement?e. Construct a 99% prediction interval on steam usage in a month with average ambient temperature of 58 â€¢ F.On March 1, 1984, the Wall Street Journal published a survey of television advertisements conducted by Video Board Tests, Inc., a New York ad-testing company that interviewed 4000 adults.These people were regular product users who were asked to cite a commercial they had seen for that product category in the past week.In this case, the response is the number of millions of retained impressions per week.The predictor variable is the amount of money spent by the firm on advertising.The data are in Table E3.3.a. Fit the simple linear regression model to these data.b.Is there a significant relationship between the amount that a company spends on advertising and retained impressions?Justify your answer statistically.c.Analyze the residuals from this model.d.Construct the 95% confidence intervals on the regression coefficients.e. Give the 95% confidence and prediction intervals for the number of retained impressions for MCI.3.4 Suppose that we have fit the straight-line regression model Å· = Î²0 + Î²1 x 1 , but the response is affected by a second variable x 2 such that the true regression function isa. Is the least squares estimator of the slope in the original simple linear regression model unbiased?b.Show the bias in Î²1 .3.5 Suppose that we are fitting a straight line and wish to make the standard error of the slope as small as possible.Suppose that the "region of interest" for x is âˆ’1 â‰¤ x â‰¤ 1.Where should the observations x 1 , x 2 , â€¦ , x n be taken?Discuss the practical aspects of this data collection plan.Consider the simple linear regression modelwhere the intercept ð›½ 0 is known.a. Find the least squares estimator of ð›½ 1 for this model.Does this answer seem reasonable?b.What is the variance of the slope ( Î²1 ) for the least squares estimator found in part a? c.Find a 100(1 âˆ’ ð›¼) percent CI for ð›½ 1 .Is this interval narrower than the estimator for the case where both slope and intercept are unknown?The quality of Pinot Noir wine is thought to be related to the properties of clarity, aroma, body, flavor, and oakiness.Data for 38 wines are given in Table E3.4.values to the R 2 and adjusted R 2 for the linear regression model relating wine quality to only the predictors "Aroma" and "Flavor."Discuss your results.f.Find a 95% CI for the regression coefficient for "Flavor" for both models in part e. Discuss any differences.Reconsider the wine quality data in Table E3.4.The "Region" predictor refers to three distinct geographical regions where the wine was produced.Note that this is a categorical variable.a. Fit the model using the "Region" variable as it is given in Table E3.4.What potential difficulties could be introduced by including this variable in the regression model using the three levels shown in Table E3.4? b.An alternative way to include the categorical variable "Region" would be to introduce two indicator variables x 1 and x 2 as follows:Why is this approach better than just using the codes 1, 2, and 3? c.Rework Exercise 3.7 using the indicator variables defined in part b for "Region."3.9 Table B.6 in Appendix B contains data on the global mean surface air temperature anomaly and the global CO 2 concentration.Fit a regression model to these data, using the global CO 2 concentration as the predictor.Analyze the residuals from this model.Is there evidence of autocorrelation in these data?If so, use one iteration of the Cochrane-Orcutt method to estimate the parameters.B.13 in Appendix B contains hourly yield measurements from a chemical process and the process operating temperature.Fit a regression model to these data, using the temperature as the predictor.Analyze the residuals from this model.Is there evidence of autocorrelation in these data?The data in Table E3.5 give the percentage share of market of a particular brand of canned peaches (y t ) for the past 15 months and the relative selling price (x t ).a. Fit a simple linear regression model to these data.Plot the residuals versus time.Is there any indication of autocorrelation?b.Use the Durbin-Watson test to determine if there is positive autocorrelation in the errors.What are your conclusions?c.Use one iteration of the Cochrane-Orcutt procedure to estimate the regression coefficients.Find the standard errors of these regression coefficients.d.Is there positive autocorrelation remaining after the first iteration?Would you conclude that the iterative parameter estimation technique has been successful?The data in Table E3.6 give the monthly sales for a cosmetics manufacturer (y t ) and the corresponding monthly sales for the entire industry (x t ).The units of both variables are millions of dollars.a. Build a simple linear regression model relating company sales to industry sales.Plot the residuals against time.Is there any indication of autocorrelation?Compare the standard error of these regression coefficients with the standard error of the least squares estimates.d.Test for positive autocorrelation following the first iteration.Has the procedure been successful?Reconsider the data in Exercise 3.12.Define a new set of transformed variables as the first difference of the original variables, y â€² t = y t âˆ’ y tâˆ’1 and x â€² t = x t âˆ’ x tâˆ’1 .Regress y â€² t on x â€² t through the origin.Compare the estimate of the slope from this first-difference approach with the estimate obtained from the iterative method in Exercise 3.12.Show that an equivalent way to perform the test for significance of regression in multiple linear regression is to base the test on R 2 as follows.To test H 0 : ð›½ 1 = ð›½ 2 = â‹¯ = ð›½ k versus H 1 : at least one ð›½ j â‰  0, calculateand reject H 0 if the computed value of F 0 exceeds F a,k,nâˆ’p , where p = k + 1.Suppose that a linear regression model with k = 2 regressors has been fit to n = 25 observations and R 2 = 0.90.a. Test for significance of regression at ð›¼ = 0.05.Use the results of the Exercise 3.14.b.What is the smallest value of R 2 that would lead to the conclusion of a significant regression if ð›¼ = 0.05?Are you surprised at how small this value of R 2 is?3.16 Consider the simple linear regression model y t = ð›½ 0 + ð›½ 1 x + ðœ€ t , where the errors are generated by the second-order autoregressive processDiscuss how the Cochrane-Orcutt iterative procedure could be used in this situation.What transformations would be used on the variables y t and x t ?How would you estimate the parameters ðœŒ 1 and ðœŒ 2 ?3.17 Show that an alternate computing formula for the regression sum of squares in a linear regression model is3.18 An article in Quality Engineering (The Catapult Problem: Enhanced Engineering Modeling Using Experimental Design, Vol. 4, 1992) conducted an experiment with a catapult to determine the effects of hook (x 1 ), arm length (x 2 ), start angle (x 3 ), and stop angle (x 4 ) on the distance that the catapult throws a ball.They threw the ball three times for each setting of the factors.Table E3.7 summarizes the experimental results.a. Fit a regression model to the data and perform a residual analysis for the model.b.Use the sample variances as the basis for WLS estimation of the original data (not the sample means).c.Fit an appropriate model to the sample variances.Use this model to develop the appropriate weights and repeat part b.Consider the simple linear regression modelSuppose that we use the transformations y â€² = yâˆ•x and x â€² = 1âˆ•x.Is this a variance-stabilizing transformation?b.What are the relationships between the parameters in the original and transformed models?c.Suppose we use the method of WLS with w i = 1âˆ•x 2 i .Is this equivalent to the transformation introduced in part a? 3.20 Consider the WLS normal equations for the case of simple linear regression where time is the predictor variable, Eq. (3.62).Suppose that the variances of the errors are proportional to the index of time such that w t = 1âˆ•t.Simplify the normal equations for this situation.Solve for the estimates of the model parameters.Consider the simple linear regression model where time is the predictor variable.Assume that the errors are uncorrelated and have constant variance ðœŽ 2 .Show that the variances of the model parameter estimates areandAnalyze the regression model in Exercise 3.1 for leverage and influence.Discuss your results.Analyze the regression model in Exercise 3.2 for leverage and influence.Discuss your results.Analyze the regression model in Exercise 3.3 for leverage and influence.Discuss your results.Analyze the regression model for the wine quality data in Exercise 3.7 for leverage and influence.Discuss your results.Consider the wine quality data in Exercise 3.7.Use variable selection techniques to determine an appropriate regression model for these data.Consider the catapult data in Exercise 3.18.Use variable selection techniques to determine an appropriate regression model for these data.In determining the candidate variables, consider all of the two-factor cross-products of the original four variables.Consider the data in Table E3.5.Fit a time series regression model with autocorrected errors to these data.Compare this model with the results you obtained in Exercise 3.11 using the Cochrane-Orcutt procedure.Consider the data in Table E3.5.Fit the lagged variables regression models shown in Eqs.(3.119) and (3.120) to these data.Compare these models with the results you obtained in Exercise 3.11 using the Cochrane-Orcutt procedure, and with the time series regression model from Exercise 3.30.Consider the data in Table E3.5.Fit a time series regression model with autocorrected errors to these data.Compare this model with the results you obtained in Exercise 3.13 using the Cochrane-Orcutt procedure.If you have to forecast, forecast often.EDGAR R. FIEDLER, American economistWe can often think of a data set as consisting of two distinct components: signal and noise.Signal represents any pattern caused by the intrinsic dynamics of the process from which the data are collected.These patterns can take various forms from a simple constant process to a more complicated structure that cannot be extracted visually or with any basic statistical tools.The constant process, for example, is represented aswhere ðœ‡ represents the underlying constant level of system response and ðœ€ t is the noise at time t.The ðœ€ t is often assumed to be uncorrelated with mean 0 and constant variance ðœŽ 2 ðœ– .We have already discussed some basic data smoothers in Section 2.2.2.Smoothing can be seen as a technique to separate the signal and the noise as much as possible and in that a smoother acts as a filter to obtain an "estimate" for the signal.In Figure 4.1, we give various types of signals that with the help of a smoother can be "reconstructed" and the underlying pattern of the signal is to some extent recovered.The smoothers that we will discuss in this chapter achieve this by simply relating the current observation to the previous ones.For a given data set, one can devise forward and/or backward looking smoothers but in this chapter we will only consider backward looking smoothers.That is, at any given T, the observation y T will be replaced by a combination of observations at and before T. It does then intuitively make sense to use some sort of an "average" of the current and the previous observations to smooth the data.An obvious choice is to replace the current observation with the average of the observations at T, T âˆ’1, â€¦, 1.In fact this is the "best" choice in the least squares sense for a constant process given in Eq. (4.1).A constant process can be smoothed by replacing the current observation with the best estimate for ðœ‡.Using the least squares criterion, we define the error sum of squares, SS E , for the constant process as The least squares estimate of ðœ‡ can be found by setting the derivative of SS with respect to ðœ‡ to 0. This giveswhere Î¼ is the least squares estimate of ðœ‡.Equation (4.2) shows that the least squares estimate of ðœ‡ is indeed the average of observations up to time T. in the data and leads to the conclusion that during the 2-year period from June 1999 to June 2001, the Dow Jones Index was quite stable.As we can see, for the constant process the smoother in Eq. (4.2) is quite effective in providing a clear picture of the underlying pattern.What happens if the process is not constant but exhibits a more complicated pattern?Consider again, for example, the Dow Jones Index from June 1999 to June 2006 given in Figure 4.3 (the complete data set is in Table 4.1).It is clear that the data do not follow the behavior typical of a constant behavior during this period.In Figure 4.3, we can also see the pattern that the smoother in Eq. (4.2) extracts for the same period.As the process changes, this smoother is having trouble keeping up with the process.What could be the reason for the poor performance after June 2001?The answer is quite simple: the constant process assumption is no longer valid.However, as time goes on, the smoother in Eq. (4.2) accumulates more and more data points and gains some sort of "inertia".So when there is a change in the process, it becomes increasingly more difficult for this smoother to react to it.How often is the constant process assumption violated?The answer to this question is provided by the Second Law of Thermodynamics, which in the most simplistic way states that if left on its own (free of external influences) any system will deteriorate.Thus the constant process is not the norm but at best an exception.So what can we do to deal with this issue?Recall that the problem with the smoother in Eq. (4.2) was that it reacted too slowly to process changes because of its inertia.In fact, when there is a change in the process, earlier data no longer carry the information about the change in the process, yet they contribute to this inertia at an equal proportion compared to the more recent (and probably more useful) data.The most obvious choice is to somehow discount the older data.Also recall that in a simple average, as in Eq. (4.2), all the observations are weighted equally and hence have the same amount of influence on the average.Thus, if the weights of each observation are changed so that earlier observations are weighted less, a faster reacting smoother should be obtained.As mentioned in Section 2.2.2, a common solution is to use the simple moving average given in Eq. (2.3):The most crucial issue in simple moving averages is the choice of the span, N. A simple moving average will react faster to the changes if N is small.However, we know from Section 2.2.2 that the variance of the simple moving average with uncorrelated observations with variance ðœŽ 2 is given asThis means that as N gets small, the variance of the moving average gets bigger.This creates a dilemma in the choice of N. If the process is expected to be constant, a large N can be used whereas a small N is preferred if the process is changing.In Figure 4.4, we show the effect of going from a span of 10 observations to 5 observations.While the latter exhibits a more jittery behavior, it nevertheless follows the actual data more closely.A more thorough analysis on the choice of N can be performed based on the prediction error.We will explore this for exponential smoothers in Section 4.6.1,where we will discuss forecasting using exponential smoothing.A final note on the moving average is that even if the individual observations are independent, the moving averages will be autocorrelated as two successive moving averages contain the same Nâˆ’1 observations.In fact, J a n -0 6 M a y -0 5 S e p -0 4 J a n -0 4 M a y -0 3 S e p -0 2 J a n -0 2 M a y -0 1 S e p -0 0 J a n -0 0 J u n -9 9  the autocorrelation function (ACF) of the moving averages that are k-lags apart is given asAnother approach to obtain a smoother that will react to process changes faster is to give geometrically decreasing weights to the past observations.Hence an exponentially weighted smoother is obtained by introducing a discount factor ðœƒ asPlease note that if the past observations are to be discounted in a geometrically decreasing manner, then we should have |ðœƒ| < 1.However, the smoother in Eq. (4.3) is not an average as the sum of the weights isand hence does not necessarily add up to 1.For that we can adjust the smoother in Eq. ( 4.3) by multiplying it by (1âˆ’ðœƒ)/(1âˆ’ðœƒ T ).However, for large T values, ðœƒ T goes to zero and so the exponentially weighted average will have the following form:This is called a simple or first-order exponential smoother.There is an extensive literature on exponential smoothing.For example, see the books by Brown (1963), Abraham andLedolter (1983), andMontgomery et al. (1990), and the papers by Brown and Meyer (1961), Chatfield and Yar (1988), Cox (1961), Gardner (1985, Gardner andDannenbring (1980), andLedolter andAbraham (1984).An alternate expression in a recursive form for simple exponential smoothing is given byThe recursive form in Eq. (4.6) shows that first-order exponential smoothing can also be seen as the linear combination of the current observation and the smoothed observation at the previous time unit.As the latter contains the data from all previous observations, the smoothed observation at time T is in fact the linear combination of the current observation and the discounted sum of all previous observations.The simple exponential smoother is often represented in a different form by setting ðœ† = 1âˆ’ðœƒ,In this representation the discount factor, ðœ†, represents the weight put on the last observation and (1âˆ’ðœ†) represents the weight put on the smoothed value of the previous observations.Analogous to the size of the span in moving average smoothers, an important issue for the exponential smoothers is the choice of the discount factor, ðœ†.Moreover, from Eq. (4.7), we can see that the calculation of á»¹1 would require us to know á»¹0 .We will discuss these issues in the next two sections.Since á»¹0 is needed in the recursive calculations that start with á»¹1 = ðœ†y 1 + (1 âˆ’ ðœ†)á»¹ 0 , its value needs to be estimated.But from Eq. (4.7) we havewhich means that as T gets large and hence (1 âˆ’ ðœ†) T gets small, the contribution of á»¹0 to á»¹T becomes negligible.Thus for large data sets, the estimation of á»¹0 has little relevance.Nevertheless, two commonly used estimates for á»¹0 are the following.1. Set á»¹0 = y 1 .If the changes in the process are expected to occur early and fast, this choice for the starting value for á»¹T is reasonable.2. Take the average of the available data or a subset of the available data, á»¹, and set á»¹0 = È³.If the process is at least at the beginning locally constant, this starting value may be preferred.In Figures 4.5 and 4.6, respectively, we have two simple exponential smoothers for the Dow Jones Index data with ðœ† = 0.2 and ðœ† = 0.4.It can be seen that in the latter the smoothed values follow the original observations more closely.In general, as ðœ† gets closer to 1, and more emphasis is put on the last observation, the smoothed values will approach the original observations.Two extreme cases will be when ðœ† = 0 and ðœ† = 1.In the former, the smoothed values will all be equal to a constant, namely, y 0 .J a n -0 6 M a y -0 5 S e p -0 4 J a n -0 4 M a y -0 3 S e p -0 2 J a n -0 2 M a y -0 1 S e p -0 0 J a n -0 0 J u n -9 9  We can think of the constant line as the "smoothest" version of whatever pattern the actual time series follows.For ðœ† = 1, we have á»¹T = y T and this will represent the "least" smoothed (or unsmoothed) version of the original time series.We can accordingly expect the variance of the simple exponential smoother to vary between 0 and the variance of the original time series based on the choice of ðœ†.Note that under the independence and constant variance assumptions we haveVar(y T ).(4.8)Thus the question will be how much smoothing is needed.In the literature, ðœ† values between 0.1 and 0.4 are often recommended and do indeed perform well in practice.A more rigorous method of finding the right ðœ† value will be discussed in Section 4.6.1.Example 4.1 Consider the Dow Jones Index from June 1999 to June 2006 given in Figure 4.3.For first-order exponential smoothing we would need to address two issues as stated in the previous sections: how to pick the initial value y 0 and the smoothing constant ðœ†.Following the recommendation in Section 4.2.2, we will consider the smoothing constants 0.2 and 0.4.As for the initial value, we will consider the first recommendation in Section 4.2.1 and set á»¹0 = y 1 .Figures 4.5 and 4.6 show the smoothed and actual data obtained from Minitab with smoothing constants 0.2 and 0.4, respectively.Note that Minitab reports several measures of accuracy; MAPE, MAD, and MSD.Mean absolute percentage error (MAPE) is the average absolute percentage change between the predicted value that is á»¹tâˆ’1 for a one-stepahead forecast and the true value, given asMean absolute deviation (MAD) is the average absolute difference between the predicted and the true values, given asMean squared deviation (MSD) is the average squared difference between the predicted and the true values, given asIt should also be noted that the smoothed data with ðœ† = 0.4 follows the actual data closer.However, in both cases, when there is an apparent linear trend in the data (e.g., from February 2003 to February 2004) the smoothed values consistently underestimate the actual data.We will discuss this issue in greater detail in Section 4.3.As an alternative estimate for the initial value, we can also use the average of the data between June 1999 and June 2001, since during this period the time series data appear to be stable.Figures 4.7 and 4.8 show J a n -0 6 M a y -0 5 S e p -0 4 J a n -0 4 M a y -0 3 S e p -0 2 J a n -0 2 M a y -0 1 S e p -0 0 J a n -0 0 J u n -9 9 the single exponential smoothing with the initial value equal to the average of the first 25 observations corresponding to the period between June 1999 and June 2001.Note that the choice of the initial value has very little effect on the smoothed values as time goes on.In Section 4.1, we considered the constant process where the time series data are expected to vary around a constant level with random fluctuations, which are usually characterized by uncorrelated errors with mean 0 and constant variance ðœŽ 2 ðœ€ .In fact the constant process represents a very special case in a more general set of models often used in modeling time series data as a function of time.The general class of models can be represented aswhere ð›½ is the vector of unknown parameters and ðœ€ t represents the uncorrelated errors.Thus as a member of this general class of models, the constant process can be represented aswhere ð›½ 0 is equal to ðœ‡ in Eq. (4.1).We have seen in Chapter 3 how to estimate and make inferences about the regression coefficients.The same principles apply to the class of models in Eq. (4.9).However, we have seen in Section 4.1 that the least squares estimates for ð›½ 0 at any given time T will be very slow to react to changes in the level of the process.For that, we suggested to use either the moving average or simple exponential smoothing.As mentioned earlier, smoothing techniques are effective in illustrating the underlying pattern in the time series data.We have so far focused particularly on exponential smoothing techniques.For the class of models given in Eq. (4.9), we can find another use for the exponential smoothers: model estimation.Indeed for the constant process, we can see the simple exponential smoother as the estimate of the process level, or in regards to Eq. (4.10) an estimate of ð›½ 0 .To show this in greater detail we need to introduce the sum of weighted squared errors for the constant process.Remember that the sum of squared errors for the constant process is given byIf we argue that not all observations should have equal influence on the sum and decide to introduce a string of weights that are geometrically decreasing in time, the sum of squared errors becomes (4.11)where |ðœƒ| 1 < 1.To find the least squares estimate for ð›½ 0 , we take the derivative of Eq. (4.11) with respect to ð›½ 0 and set it to zero:The solution to Eq. (4.12), Î²0 , which is the least squares estimate of ð›½ 0 , is(4.13) From Eq. (4.4), we haveOnce again for large T, ðœƒ T goes to zero.We then haveWe can see from Eqs. (4.5) and (4.15) that ð›½ 0 = á»¹T .Thus the simple exponential smoothing procedure does in fact provide a weighted least squares estimate of ð›½ 0 in the constant process with weights that are exponentially decreasing in time.Now we return to our general class of models given in Eq. (4.9) and note that f (t; ð›½) can in fact be any function of t.For practical purposes it is usually more convenient to consider the polynomial family for nonseasonal time series.For seasonal time series, we will consider other forms of f (t; ð›½) that fit the data and exhibit a certain periodicity better.In the polynomial family, the constant process is indeed the simplest model we can consider.We will now consider the next obvious choice: the linear trend model.We will now return to our Dow Jones Index data but consider only the subset of the data from February 2003 to February 2004 as given in Figure 4.9.Evidently for that particular time period it was a bullish market and correspondingly the Dow Jones Index exhibits an upward linear trend as indicated with the dashed line.For this time period, an appropriate model in time from the polynomial family should be the linear trend model given as (4.16)where the ðœ€ t is once again assumed to be uncorrelated with mean 0 and constant variance ðœŽ 2 ðœ€ .Based on what we have learned so far, we may attempt to smooth/model this linear trend using the simple exponential smoothing procedure.The actual and fitted values for the simple exponential smoothing procedure are given in Figure 4.10.For the exponential smoother, without any loss of generality, we used á»¹0 = y 1 and ðœ† = 0.3.From Figure 4.10, we can see that while the simple exponential smoother was to some extent able to capture the slope of the linear trend, it also exhibits some bias.That is, the fitted values based on the exponential smoother are consistently underestimating the actual data.More interestingly, the amount of underestimation is more or less constant for all observations.In fact similar behavior for the simple exponential smoother can be observed in Figure 4.5 for the entire data from June 1999 to June 2006.Whenever the data exhibit a linear trend, the simple exponential smoother seems to over-or underestimates the actual data consistently.To further explore this, we will consider the expected value of á»¹T ,For the linear trend model in Eq. (4.16), E (y t ) = ð›½ 0 + ð›½ 1 t.So we haveBut for the infinite sums we haveHence the expected value of the simple exponential smoother for the linear trend model is (4.17)This means that the simple exponential smoother is a biased estimator for the linear trend model and the amount of bias is âˆ’[(1 âˆ’ ðœ†)âˆ•ðœ†]ð›½ 1 .This indeed explains the underestimation in Figure 4.10.One solution will be to use a large ðœ† value since (1 âˆ’ ðœ†)/ðœ† â†’ 0 as ðœ† â†’ 1.In Figure 4.11, we show two simple exponential smoothers with ðœ† = 0.3 and ðœ† = 0.99.It can be J a n -0 6 M a y -0 5 S e p -0 4 J a n -0 4 M a y -0 3 S e p -0 2 J a n -0 2 M a y -0 1 S e p -0 0 J a n -0 0 J u n -9 9 seen that the latter does a better job in capturing the linear trend.However, it should also be noted that as the smoother with ðœ† = 0.99 follows the actual observations very closely, it fails to smooth out the constant pattern during the first 2 years of the data.A method based on adaptive updating of the discount factor, ðœ†, following the changes in the process is given in Section 4.6.4.In this section to model a linear trend model we will instead introduce the second-order exponential smoothing by applying simple exponential smoothing on á»¹T aswhere á»¹(1) T and á»¹(2) T denote the first-and second-order smoothed exponentials, respectively.Of course, in Eq. (4.18) we can use a different ðœ† than in Eq. (4.7).However, for the derivations that follow, we will assume that the same ðœ† is used in the calculations of both á»¹(1)T and á»¹(2) T .From Eq. (4.17), we can see that the first-order exponential smoother introduces bias in estimating a linear trend.It can also be seen in Figure 4.7 that the first-order exponential smoother for the linear trend model exhibits a linear trend as well.Hence the second-order smoother-that is, a first-order exponential smoother of the original first-order exponential smoother-should also have a bias.We can represent this asFrom Eq. (4.19), an estimate for ð›½ 1 at time T isand for an estimate of ð›½ 0 at time T, we have from Eq. (4.17)In terms of the first-and second-order exponential smoothers, we haveFinally, combining Eq. (4.20) and (4.22), we have a predictor for y T as(4.23)It can easily be shown that Å·T is an unbiased predictor of y T .In Figure 4.12, we use Eq.(4.23) to estimate the Dow Jones Index from February 2003 to February 2004. From Figures 4.10 and4.12,we can clearly see that the second-order exponential smoother is doing a much better job in modeling the linear trend compared to the simple exponential smoother.As in the simple exponential smoothing, we have the same two issues to deal with: initial values for the smoothers and the discount factors.The  latter will be discussed in Section 4.6.1.For the former we will combine Eqs.(4.17) and (4.19) as the following:(4.24)The initial estimates of the model parameters are usually obtained by fitting the linear trend model to the entire or a subset of the available data.The least squares estimates of the parameter estimates are then used for Î²0,0 and Î²1,0 .Example 4.2 Consider the US Consumer Price Index (CPI) from January 1995 to December 2004 in Table 4.2. Figure 4.13 clearly shows that the data exhibits a linear trend.To smooth the data, following the recommendation in Section 4.2, we can use single exponential smoothing with ðœ† = 0.3 as given in Figure 4.14.As we expected, the exponential smoother does a very good job in capturing the general trend in the data and provides a less jittery (smooth) version of it.However, we also notice that the smoothed values are  consistently below the actual values.Hence there is an apparent bias in our smoothing.To fix this problem we have two choices: use a bigger ðœ† or second-order exponential smoothing.The former will lead to less smooth estimates and hence defeat the purpose.For the latter, however, we can use ðœ† = 0.3 to calculate and á»¹(1)T and á»¹(2) T as given in Table 4.3.Figure 4.15 shows the second-order exponential smoothing of the CPI.As we can see, the second-order exponential smoothing not only captures the trend in the data but also does not exhibit any bias.The calculations for the second-order smoothing for the CPI data are performed using Minitab.We first obtained the first-order exponential smoother for the CPI, á»¹(1)T , using ðœ† = 0.3 and á»¹(1) 0 = y 1 .Then we obtained á»¹(2) T by taking the first-order exponential smoother á»¹(1)T using ðœ† = 0.3 and á»¹(2) 0 = á»¹(1) 1 .Then using Eq.(4.23) we have Å·T = 2á»¹ (1) T âˆ’ á»¹(2) T .The "Double Exponential Smoothing" option available in Minitab is a slightly different approach based on Holt's method (Holt, 1957).This method divides the time series data into two components: the level, L t , and the trend, T t .These two components can be calculated fromHence for a given set of ð›¼ and ð›¾, these two components are calculated and L t is used to obtain the double exponential smoothing of the data at time t.Furthermore, the sum of the level and trend components at time t can be used as the one-step-ahead (t + 1) forecast.Figure 4.16 shows the actual and smoothed data using the double exponential smoothing option in Minitab with ð›¼ = 0.3 and ð›¾ = 0.3.In general, the initial values for the level and the trend terms can be obtained by fitting a linear regression model to the CPI data with time as   the regressor.Then the intercept and the slope can be used as the initial values of L t and T t respectively.Example 4.3 For the Dow Jones Index data, we observed that first-order exponential smoothing with low values of ðœ† showed some bias when there were linear trends in the data.We may therefore decide to use the secondorder exponential smoothing approach for this data as shown in Figure 4.17.Note that the bias present with first-order exponential smoothing has been eliminated.The calculations for second-order exponential smoothing for the Dow Jones Index are given in Table 4.4.So far we have discussed the use of exponential smoothers in estimating the constant and linear trend models.For the former we employed the simple or first-order exponential smoother and for the latter the secondorder exponential smoother.It can further be shown that for the general nth-degree polynomial model of the formJ a n -0 6 M a y -0 5 S e p -0 4 J a n -0 4 M a y -0 3 S e p -0 2 J a n -0 2 M a y -0 1 S e p -0 0 J a n -0 0 J u n -9 9 12,000 11,000 10,000 9000 8000Variable FIGURE 4.17 The second-order exponential smoothing of the Dow Jones Index (with ðœ† = 0.3, á»¹(1) 0 = y 1 , and á»¹(2)where the ðœ€ t is assumed to be independent with mean 0 and constant variance ðœŽ ðœ€ 2 , we employ (n + 1)-order exponential smoothersT to estimate the model parameters.For even the quadratic model (seconddegree polynomial), the calculations get quite complicated.Refer to Montgomery et al. (1990), Brown (1963), and Abraham and Ledolter (1983 for the solutions to higher-order exponential smoothing problems.If a highorder polynomial does seem to be required for the time series, the autoregressive integrated moving average (ARIMA) models and techniques discussed in Chapter 5 can instead be considered.We have so far considered exponential smoothing techniques as either visual aids to point out the underlying patterns in the time series data or to estimate the model parameters for the class of models given in Eq. (4.9).The latter brings up yet another use of exponential smoothing-forecasting future observations.At time T, we may wish to forecast the observation in the next time unit, T + 1, or further into the future.For that, we will denote the ðœ-step-ahead forecast made at time T as Å·T+ðœ (T).In the next two sections and without any loss of generality, we will once again consider first-and second-order exponential smoothers as examples for forecasting time series data from the constant and linear trend processes.In Section 4.2 we discussed first-order exponential smoothing for the constant process in Eq. (4.1) asIn Section 4.3 we further showed that the constant level in Eq. (4.1), ð›½ 0 , can be estimated by á»¹T .Since the constant model consists of two parts-ð›½ 0 that can be estimated by the first-order exponential smoother and the random error that cannot be predicted-our forecast for the future observation is simply equal to the current value of the exponential smoother Å·T+ðœ (T) = á»¹T = á»¹T .(4.26)Please note that, for the constant process, the forecast in Eq. (4.26) is the same for all future values.Since there may be changes in the level of the constant process, forecasting all future observations with the same value will most likely be misleading.However, as we start accumulating more observations, we can update our forecast.For example, if the data at T + 1 become available, our forecast for the future observations becomesWe can rewrite Eq. (4.27) for ðœ = 1 aswhere e T+1 (1) = y T+1 âˆ’ Å·T+1 (T) is called the one-step-ahead forecast or prediction error.The interpretation of Eq. (4.28) makes it easier to understand the forecasting process using exponential smoothing: our forecast for the next observation is simply our previous forecast for the current observation plus a fraction of the forecast error we made in forecasting the current observation.The fraction in this summation is determined by ðœ†.Hence how fast our forecast will react to the forecast error depends on the discount factor.A large discount factor will lead to fast reaction to the forecast error but it may also make our forecast react fast to random fluctuations.This once again brings up the issue of the choice of the discount factor.Choice of ð€ We will define the sum of the squared one-step-ahead forecast errors asFor a given historic data, we can in general calculate SS E values for various values of ðœ† and pick the value of ðœ† that gives the smallest sum of the squared forecast errors.Another issue in forecasting is the uncertainty associated with it.That is, we may be interested not only in the "point estimates" but also in the quantification of the prediction uncertainty.This is usually achieved by providing the prediction intervals that are expected at a specific confidence level to contain the future observations.Calculations of the prediction intervals will require the estimation of the variance of the forecast errors.We will discuss two different techniques in estimating prediction error variance in Section 4.6.3.For the constant process, the 100 (1 âˆ’ ð›¼âˆ•2) percent prediction intervals for any lead time ðœ are given aswhere á»¹T is the first-order exponential smoother, Z ð›¼âˆ•2 is the 100(1 âˆ’ ð›¼âˆ•2) percentile of the standard normal distribution, and Ïƒe is the estimate of the standard deviation of the forecast errors.It should be noted that the prediction interval is constant for all lead times.This of course can be (and probably is in most cases) quite unrealistic.As it will be more likely that the process goes through some changes as time goes on, we would correspondingly expect to be less and less "sure" about our predictions for large lead times (or large ðœ values).Hence we would anticipate prediction intervals that are getting wider and wider for increasing lead times.We propose a remedy for this in Section 4.6.3.We will discuss this issue further in Chapter 6.We are interested in the average speed on a specific stretch of a highway during nonrush hours.For the past year and a half (78 weeks), we have available weekly averages of the average speed in miles/hour between 10 AM and 3 PM.The data are given in Table 4.5.Figure 4.18 shows that the time series data follow a constant process.To smooth out the excessive variation, however, first-order exponential smoothing can be used.The "best" smoothing constant can be determined by finding the smoothing constant value that minimizes the sum of the squared one-stepahead prediction errors.The sum of the squared one-step-ahead prediction errors for various ðœ† values is given in Table 4.6.Furthermore, Figure 4.19 shows that the minimum SS E is obtained for ðœ† = 0.4.Let us assume that we are also asked to make forecasts for the next 12 weeks at week 78. Figure 4.20 shows the smoothed values for the first 78 weeks together with the forecasts for weeks 79-90 with prediction intervals.It also shows the actual weekly speed during that period.Note that since the constant process is assumed, the forecasts for the next 12 weeks are the same.Similarly, the prediction intervals are constant for that period.The t-step-ahead forecast for the linear trend model is given by In terms of the exponential smoothers, we can rewrite Eq. (4.30) as(4.31)It should be noted that the predictions for the trend model depend on the lead time and, as opposed to the constant model, will be different for different lead times.As we collect more data, we can improve our forecasts by updating our parameter estimates usingSubsequently, we can update our ðœ-step-ahead forecasts based on Eq. (4.32).As in the constant process, the discount factor, ðœ†, can be estimated by minimizing the sum of the squared one-step-ahead forecast errors given in Eq. (4.29).In this case, the 100(1 âˆ’ ð›¼/2) percent prediction interval for any lead time ðœ iswhereExample 4.5 Consider the CPI data in Example 4.2.Assume that we are currently in December 2003 and would like to make predictions of the CPI for the following year.Although the data from January 1995 to December 2003 clearly exhibit a linear trend, we may still like to consider first-order exponential smoothing first.We will then calculate the "best" ðœ† value that minimizes the sum of the squared one-step-ahead prediction errors.The predictions and prediction errors for various ðœ† values are given in Table 4.7.We notice that the SS E keeps on getting smaller as ðœ† gets bigger.This suggests that the data are highly autocorrelated.This can be clearly seen in the ACF plot in Figure 4.22.In fact if the "best" ðœ† value (i.e., ðœ† value that minimizes SS E ) turns out to be high, it may indeed be better to switch to a higher-order smoothing or use an ARIMA model as discussed in Chapter 5.  Since the first-order exponential smoothing is deemed inadequate, we will now try the second-order exponential smoothing to forecast next year's monthly CPI values.Usually we have two options:1. On December 2003, make forecasts for the entire 2004 year; that is, 1-step-ahead, 2-step-ahead, â€¦ , 12-step-ahead forecasts.For that we can use Eq.(4.30) or equivalently Eq. (4.31).Using the double exponential smoothing option in Minitab with ðœ† = 0.3, we obtain the forecasts given in Figure 4.23.Note that the forecasts further in the future (for the later part of 2004) are quite a bit off.To remedy this we may instead use the following strategy.2. In December 2003, make the one-step-ahead forecast for January 2004.When the data for January 2004 becomes available, then make the one-step-ahead forecast for February 2004, and so on.We can see from Figure 4.24 that forecasts when only one-step-ahead forecasts are used and adjusted as actual data becomes available perform better than in the previous case where, for December 2003, forecasts are made for the entire following year.The JMP software package also has an excellent forecasting capability.Table 4.8 shows output from JMP for the CPI data for double  exponential smoothing.JMP uses the double smoothing procedure that employs a single smoothing constant.The JMP output shows the time series plot and summary statistics including the sample ACF.It also provides a sample partial ACF, which we will discuss in Chapter 5. Then an optimal smoothing constant is chosen by finding the value of ðœ† that     minimizes the error sum of squares.The value selected is ðœ† = 0.814.This relatively large value is not unexpected, because there is a very strong linear trend in the data and considerable autocorrelation.Values of the forecast for the next 12 periods at origin December 2004 and the associated prediction interval are also shown.Finally, the residuals from the model fit are shown along with the sample ACF and sample partial ACF plots of the residuals.The sample ACF indicates that there may be a small amount of structure in the residuals, but it is not enough to cause concern.In the estimation of the variance of the forecast errors, ðœŽ 2 e , it is often assumed that the model (e.g., constant, linear trend) is correct and constant in time.With these assumptions, we have two different ways of estimating ðœŽ 2 e :1. We already defined the one-step-ahead forecast error as e T (1) = y T âˆ’ Å·T (T âˆ’ 1).The idea is to apply the model to the historic data and obtain the forecast errors to calculate:It should be noted that in the variance calculations the mean adjustment was not needed, since for the correct model the forecasts are unbiased; that is, the expected value of the forecast errors is 0.As more data are collected, the variance of the forecast errors can be updated as) .(4.34)As discussed in Section 4.6.1, it may be counterintuitive to have a constant forecast error variance for all lead times.We can instead define ðœŽ 2 e (ðœ) as the ðœ-step-ahead forecast error variance and estimate it byHence the estimate in Eq. (4.35) can instead be used in the calculations of the prediction interval for the ðœ-step-ahead forecast.2. For the second method of estimating ðœŽ 2 e we will first define the mean absolute deviation Î” asand, assuming that the model is correct, calculate its estimate byThen the estimate of the ðœŽ 2 e is given byFor further details, see Montgomery et al. (1990).In the previous sections we discussed estimation of the "best" discount factor, Î», by minimizing the sum of the squared one-step-ahead forecasts errors.However, as we have seen with the Dow Jones Index data, changes in the underlying time series model will make it difficult for the exponential smoother with fixed discount factor to follow these changes.Hence a need for monitoring and, if necessary, modifying the discount factor arises.By doing so, the discount factor will adapt to the changes in the time series model.For that we will employ the procedure originally described by Trigg and Leach (1967) for single discount factor.As an example we will consider the first-order exponential smoother and modify it asPlease note that in Eq. (4.39), the discount factor ðœ† T is given as a function of time and hence it is allowed to adapt to changes in the time series model.We also define the smoothed error aswhere ð›¿ is a smoothing parameter.FORECASTINGFinally, we define the tracking signal aswhere Î”T is given in Eq. (4.37).This ratio is expected to be close to 0 when the forecasting system performs well and to approach Â±1 as it starts to fail.In fact, Trigg and Leach (1967) suggest setting the discount factor toEquation ( 4.42) will allow for automatic updating of the discount factor.Example 4.6 Consider the Dow Jones Index from June 1999 to June 2006 given in Table 4.1.Figure 4.2 shows that the data do not exhibit a single regime of constant or linear trend behavior.Hence a single exponential smoother with adaptive discount factor as given in Eq. ( 4.42) can be used.Figure 4.25 shows two simple exponential smoothers for the Dow Jones Index: one with fixed ðœ† = 0.3 and another one with adaptive updating based on the Trigg-Leach method given in Eq. (4.42).J a n -0 6 M a y -0 5 S e p -0 4 J a n -0 4 M a y -0 3 S e p -0 2 J a n -0 2 M a y -0 1 S e p -0 0 J a n -0 0 J u n -9 9  This plot shows that a better smoother can be obtained by making automatic updates to the discount factor.The calculations for the Trigg-Leach smoother are given in Table 4.9.The adaptive smoothing procedure suggested by Trigg and Leach is a useful technique.For other approaches to adaptive adjustment of exponential smoothing parameters, see Chow (1965), Roberts andReed (1969), andMontgomery (1970).If the forecast model performs as expected, the forecast errors should not exhibit any pattern or structure; that is, they should be uncorrelated.Therefore it is always a good idea to verify this.As noted in Chapter 2, we can do so by calculating the sample ACF of the forecast errors fromwhereIf the one-step-ahead forecast errors are indeed uncorrelated, the sample autocorrelations for any lag k should be around 0 with a standard error 1âˆ•T. Hence a sample autocorrelation for any lag k that lies outside the Â±2âˆ•T limits will require further investigation of the model.EXPONENTIAL SMOOTHING FOR SEASONAL DATA 277 4.7 EXPONENTIAL SMOOTHING FOR SEASONAL DATA Some time series data exhibit cyclical or seasonal patterns that cannot be effectively modeled using the polynomial model in Eq. (4.25).Several approaches are available for the analysis of such data.In this chapter we will discuss exponential smoothing techniques that can be used in modeling seasonal time series.The methodology we will focus on was originally introduced by Holt (1957) and Winters (1960) and is generally known as Winters' method, where a seasonal adjustment is made to the linear trend model.Two types of adjustments are suggested-additive and multiplicative.Consider the US clothing sales data given in Figure 4.26.Clearly, for certain months of every year we have high (or low) sales.Hence we can conclude that the data exhibit seasonality.The data also exhibit a linear trend as the sales tend to get higher for the same month as time goes on.As the final observation, we note that the amplitude of the seasonal pattern, that is, the range of the periodic behavior within a year, remains more or less constant in time and remains independent of the average level within a year.We will for this case assume that the seasonal time series can be represented by the following model: In the model given in Eq. (4.44), for forecasting the future observations, we will employ first-order exponential smoothers with different discount factors.The procedure for updating the parameter estimates once the current observation y T is obtained is as follows.Step 1. Update the estimate of L T usingwhere 0 < ðœ† 1 < 1.It should be noted that in Eq. (4.46), the first part can be seen as the "current" value for L T and the second part as the forecast of L T based on the estimates at T âˆ’ 1.Step 2. Update the estimate of ð›½ 1 usingwhere 0 < ðœ† 2 < 1.As in Step 1, the estimate of ð›½ 1 in Eq. (4.47) can be seen as the linear combination of the "current" value of ð›½ 1 and its "forecast" at T âˆ’ 1.Step 3. Update the estimate of S t usingwhere 0 < ðœ† 3 < 1.Step 4. Finally, the ðœ-step-ahead forecast, Å·T+ðœ (T), isAs before, estimating the initial values of the exponential smoothers is important.For a given set of historic data with n seasons (hence ns observations), we can use the least squares estimates of the following model:whereThe least squares estimates of the parameters in Eq. ( 4.50) are used to obtain the initial values asThese are initial values of the model parameters at the original origin of time, t = 0. To make forecasts from the correct origin of time the permanent component must be shifted to time T by computing LT = L0 + ns Î²1 .Alternatively, one could smooth the parameters using equations (4.46)-(4.48)for time periods t = 1, 2,â€¦,T.As in the nonseasonal smoothing case, the calculations of the prediction intervals would require an estimate for the prediction error variance.The most common approach is to use the relationship between the exponential smoothing techniques and the ARIMA models of Chapter 5 as discussed in Section 4.8, and estimate the prediction error variance accordingly.It can be shown that the seasonal exponential smoothing using the three parameter Holt-Winters method is optimal for an ARIMA (0, 1, s + 1) Ã— (0, 1, 0) s , process, where s represents the length of the period of the seasonal cycles.For further details, see Yar and Chatfield (1990) and McKenzie (1986).An alternate approach is to recognize that the additive seasonal model is just a linear regression model and to use the ordinary least squares (OLS) regression procedure for constructing prediction intervals as discussed in Chapter 3. If the errors are correlated, the regression methods for autocorrelated errors could be used instead of OLS.Example 4.7 Consider the clothing sales data given in Table 4.10.To obtain the smoothed version of this data, we can use the Winters' method option in Minitab.Since the amplitude of the seasonal pattern is constant over time, we decide to use the additive model.Two issues we have encountered in previous exponential smoothers have to be addressed in this case as well-initial values and the choice of smoothing constants.Similar recommendations as in the previous exponential smoothing options can also be made in this case.Of course, the choice of the smoothing constant, in particular, is a bit more concerning since it involves the estimation of three smoothing constants.In this example, we follow our usual recommendation and choose smoothing constants that are all equal to 0.2.For more complicated cases, we recommend seasonal ARIMA models, which we will discuss in Chapter 5.Figure 4.27 shows the smoothed version of the seasonal clothing sales data.To use this model for forecasting, let us assume that we are currently in December 2002 and we are asked to make forecasts for the following year.Figure 4.28 shows the forecasted sales for 2003 together with the actual data and the 95% prediction limits.Note that the forecast for December 2003 is the 12-step-ahead forecast made in December 2002.Even though the forecast is made further in the future, it still performs well since in the "seasonal" sense it is in fact a one-step-ahead forecast.If the amplitude of the seasonal pattern is proportional to the average level of the seasonal time series, as in the liquor store sales data given in Figure 4.29, the following multiplicative seasonal model will be more appropriate:     As in the additive model, we will employ three exponential smoothers to estimate the parameters in Eq. (4.52).Step 1. Update the estimate of L T usingwhere 0 <ðœ† 1 < 1.Similar interpretation as in the additive model can be made for the exponential smoother in Eq. (4.54).Step 2. Update the estimate of ð›½ 1 usingwhere 0 < ðœ† 2 < 1.EXPONENTIAL SMOOTHING FOR SEASONAL DATAStep 3. Update the estimate of S t usingwhere 0 <ðœ† 3 < 1.Step 4. The ðœ-step-ahead forecast, Å·T+ðœ (T), is(4.57)It will almost be necessary to obtain starting values of the model parameters.Suppose that a record consisting of n seasons of data is available.From this set of historical data, the initial values, Î²0,0 , Î²1,0 , and Åœ0 , can be calculated aswhereFor further details, please see Montgomery et al. (1990) and Abraham and Ledolter (1983).Prediction Intervals Constructing prediction intervals for the multiplicative model is much harder than the additive model as the former is nonlinear.Several authors have considered this problem, including Chatfield and Yar (1991), Sweet (1985), and Gardner (1988.Chatfield and Yar (1991) propose an empirical method in which the length of the prediction interval depends on the point of origin of the forecast and may decrease in length near the low points of the seasonal cycle.They also discuss the case where the error is assumed to be proportional to the seasonal effect rather than constant, which is the standard assumption in Winters' method.Another approach would be to obtain a "linearized" version of Winters' model by expanding it in a first-order Taylor series and use this to find an approximate variance of the predicted value (statisticians call this the delta method).Then this prediction variance could be used to construct prediction intervals much as is done in the linear regression model case.Example 4.8 Consider the liquor store data given in As for forecasting using the multiplicative model, we can assume as usual that we are currently in December 2003 and are asked to forecast the sales in 2004.Figure 4.32 shows the forecasts together with the actual values and the prediction intervals.Bioterrorism is the use of biological agents in a campaign of aggression.The use of biological agents in warfare is not new; many centuries ago plague and other contagious diseases were employed as weapons.Their use today is potentially catastrophic, so medical and public health officials are designing and implementing biosurveillance systems to monitor populations for potential disease outbreaks.For example, public health officials collect syndrome data from sources such as hospital emergency rooms, outpatient clinics, and over-the-counter medication sales to detect disease outbreaks, such as the onset of the flu season.For an excellent and highly readable introduction to statistical techniques for biosurveillance and syndromic surveillance, see Fricker (2013).Monitoring of syndromic data is also a type of epidemiologic surveillance in a biosurveillance process,    where significantly higher than anticipated counts of influenza-like illness might signal a potential bioterrorism attack.As an example of such syndromic data, Fricker (2013) describes daily counts of respiratory and gastrointestinal complaints for more than 2 1 âˆ• 2 years at several hospitals in a large metropolitan area.Table 4.12 presents the respiratory count data from one of these hospitals.There are 980 observations.Fifty observations were missing from the original data set.The missing values were replaced with the last value that was observed on the same day of the week.This type of data imputation is a variation of "Hot Deck Imputation" discussed in Section 1. 4.3 and in Fricker (2013).It is also sometimes called last observation (or Value) carried forward (LOCF).For additional discussion see the web site: http://missingdata.lshtm.ac.uk/.Figure 4.33 is a time series plot of the respiratory syndrome count data in Table 4.12.This plot was constructed using the Graph Builder feature in JMP.This software package overlays a smoothed curve on the data.The curve is fitted using locally weighted regression, often called loess.This is a variation of kernel regression that uses a weighted average of the data in a local neighborhood around a specific location to determine the value to plot at that location.Loess usually uses either first-order linear regression or a quadratic regression model for the weighted least squares fit.For more information on kernel regression and loess see Montgomery, et al. (2012).Over the 2 1 âˆ• 2 year period, the daily counts of the respiratory syndrome appear to follow a weak seasonal pattern, with the highest peak in November-December (late fall), a secondary peak in March-April, and then decreasing to the lowest counts in June-August (summer).The amplitude, or range within a year, seems to vary, but counts do not appear to be increasing or decreasing over time.Not immediately evident from the time series plots is a potential day effect.The box plots of the residuals from the loess smoothed line in Figure 4.33 are plotted in Figure 4.34 versus day of the week.These plots exhibit variation that indicates slightly higher-than-expected counts on Monday and slightly lower-than-expected counts on Thursday, Friday, and Saturday.The exponential smoothing procedure in JMP was applied to the respiratory syndrome data.The results of first-order or simple exponential smoothing are summarized in Table 4. prediction error and the mean absolute, although there is very little difference between the optimal value of ðœ† = 0.21 and the values ðœ† = 0.1 and ðœ† = 0.4.The results of using second-order exponential smoothing are summarized in Table 4.14 and illustrated graphically for the last 100 observations in Figure 4.36.There is not a lot of difference between the two procedures, although the optimal first-order smoother does perform slightly better and the larger smoothing parameters in the double smoother perform more poorly.Single and double exponential smoothing do not account for the apparent mild seasonality observed in the original time series plot of the data.We used JMP to fit Winters' additive seasonal model to the respiratory syndrome count data.Because the seasonal patterns are not strong, we investigated seasons of length 3, 7, and 12 periods.The results are summarized in Table 4.15 and illustrated graphically for the last 100 observations in Figure 4.37.The 7-period season works best, probably reflecting the daily seasonal pattern that we observed in Figure 4.34.This is also the best smoother of all the techniques that were investigated.The values of ðœ† = 0 for the trend and seasonal components in this model are an indication that there is not a significant linear trend in the data and that the seasonal pattern is relatively stable over the period of available data.The first-order exponential smoother presented in Section 4.2 is a very effective model in forecasting.The discount factor, ðœ†, makes this smoother fairly flexible in handling time series data with various characteristics.The first-order exponential smoother is particularly good in forecasting time series data with certain specific characteristics.Recall that the first-order exponential smoother is given asand the forecast error is defined as  We will see in Chapter 5 that the model in Eq. (4.63) is called the integrated moving average model denoted as IMA(1,1), for the backshift operator is used only once on y T and only once on the error.It can be shown that if the process exhibits the dynamics defined in Eq. (4.63), that is an IMA(1,1) process, the first-order exponential smoother provides minimum mean squared error (MMSE) forecasts (see Muth (1960), Box andLuceno (1997), andBox, Jenkins, andReinsel (1994)).For more discussion of the equivalence between exponential smoothing techniques and the ARIMA models, see Abraham andLedolter (1983), Cogger (1974), Goodman (1974), Pandit andWu (1974), andMcKenzie (1984).Example 4.1 The Dow Jones index data are in the second column of the array called dji.data in which the first column is the month of the year.We can use the following simple function to obtain the first-order exponential smoothingNote that this function uses the first observation as the starting value by default.One can change this by providing a specific start value when calling the function.We can then obtain the smoothed version of the data for a specified lambda value and plot the fitted value as the following: For the first-order exponential smoothing, measures of accuracy such as MAPE, MAD, and MSD can be obtained from the following function: Note that alternatively we could use the Holt-Winters function from the stats package.The function requires three parameters (alpha, beta, and gamma) to be defined.Providing a specific value for alpha and setting beta and gamma to "FALSE" give the first-order exponential as the following dji1.fit<-HoltWinters(dji.data[,2],alpha=.4,beta=FALSE,gamma=FALSE)Beta corresponds to the second-order smoothing (or the trend term) and gamma is for the seasonal effect.To find the "best" smoothing constant, we will use the firstsmooth function for various lambda values and obtain the sum of squared one-step-ahead prediction error (SS E ) for each.The lambda value that minimizes the sum of squared prediction errors is deemed the "best" lambda.The obvious option is to apply firstsmooth function in a for loop to obtain SS E for various lambda values.Even though in this case this may not be an issue, in many cases for loops can slow down the computations in R and are to be avoided if possible.We will do that using sapply function.Note that we can also use Holt-Winters function to find the "best" value for the smoothing constant by not specifying the appropriate parameter as the following:Example 4.5 We will first try to find the best lambda for the CPI data using first-order exponential smoothing.We will also plot ACF of the data.Note that we will use the data up to December 2003.We now use the second-order exponential smoothing with lambda of 0.3.We calculate the forecasts using Eq.(4.31) for the two options suggested in the Example 4.5.Option 1: On December 2003, make the forecasts for the entire 2004 (1-to 12-step-ahead forecasts).Option 2: On December 2003, make the forecast for January 2004.Then when January 2004 data are available, make the forecast for February 2004 (only one-step-ahead forecasts).lcpi<-0.3T<-108 tau<-12 alpha.lev<-.05cpi.forecast<-rep(0,tau) cl<-rep(0,tau) cpi.smooth1<-rep(0,T+tau) cpi.smooth2<-rep(0,T+tau) #Initialize the vectors Qt<-vector() Dt<-vector() y.tilde<-vector() lambda<-vector() err<-vector() #Set the starting values for the vectors lambda  ,85,12),1]) lines(out.tl.dji[,1]) lines(dji.smooth1,col="grey40")legend(60,8000,c("Dow Jones","TL Smoother","Exponential Smoother"), pch=c(16, NA, NA),lwd=c(NA,.5,.5),cex=.55,col=c("black","black","grey40"))Example 4.7 The clothing sales data are in the second column of the array called closales.data in which the first column is the month of the year.We will use the data up to December 2002 to fit the model and make forecasts for the coming year ( 2003).We will use Holt-Winters function given in stats package.The model is additive seasonal model with all parameters equal to 0.2.Example 4.8 The liquor store sales data are in the second column of the array called liqsales.data in which the first column is the month of the year.We will first fit additive and multiplicative seasonal models to the entire data to see the difference in the fits.Then we will use the data up to December 2003 to fit the multiplicative model and make forecasts for the coming year (2004).We will once again use Holt-Winters function given in stats package.In all cases we set all parameters to 0.2.4.9 Reconsider the linear trend data in Table E4.4.Take the first difference of this data and plot the time series of first differences.Has differencing removed the trend?Use exponential smoothing on the first 11 differences.Instead of forecasting the original data, forecast the first differences for the remaining data using exponential smoothing and use these forecasts of the first differences to obtain forecasts for the original data.Does the forecasting procedure seem to be working satisfactorily?4.17 Reconsider the blue and gorgonzola cheese data in Table B.4 and Exercise 4.16.Take the first difference of this data and plot the time series of first differences.Has differencing removed the trend?Use exponential smoothing on the first differences.Instead of forecasting the original data, develop a procedure for forecasting the first differences and explain how you would use these forecasts of the first differences to obtain forecasts for the original data.B.5 shows data for US beverage manufacturer product shipments.Develop an appropriate exponential smoothing procedure for forecasting these data.Using the results of Exercise 4.46, determine the number of periods that it will take following the impulse for the expected value of the exponential smoothing statistic to return to within 0.10 ð›¿ of the original time series level ðœ‡.Plot the number of periods as a function of the smoothing constant.What conclusions can you draw?All models are wrong, some are useful.GEORGE E. P. BOX, British statisticianIn the previous chapter, we discussed forecasting techniques that, in general, were based on some variant of exponential smoothing.The general assumption for these models was that any time series data can be represented as the sum of two distinct components: deterministic and stochastic (random).The former is modeled as a function of time whereas for the latter we assumed that some random noise that is added on to the deterministic signal generates the stochastic behavior of the time series.One very important assumption is that the random noise is generated through independent shocks to the process.In practice, however, this assumption is often violated.That is, usually successive observations show serial dependence.Under these circumstances, forecasting methods based on exponential smoothing may be inefficient and sometimes inappropriate because they do not take advantage of the serial dependence in the observations in the most effective way.To formally incorporate this dependent structure, in this chapter we will explore a general class of models called autoregressive integrated moving average (MA) models or ARIMA models (also known as Box-Jenkins models).In statistical modeling, we are often engaged in an endless pursuit of finding the ever elusive true relationship between certain inputs and the output.As cleverly put by the quote of this chapter, these efforts usually result in models that are nothing but approximations of the "true" relationship.This is generally due to the choices the analyst makes along the way to ease the modeling efforts.A major assumption that often provides relief in modeling efforts is the linearity assumption.A linear filter, for example, is a linear operation from one time series x t to another time series y t ,with t = â€¦ , âˆ’1, 0, 1, â€¦.In that regard the linear filter can be seen as a "process" that converts the input, x t , into an output, y t , and that conversion is not instantaneous but involves all (present, past, and future) values of the input in the form of a summation with different "weights", { ðœ“ i } , on each x t .Furthermore, the linear filter in Eq. (5.1) is said to have the following properties:1. Time-invariant as the coefficients { ðœ“ i } do not depend on time.2. Physically realizable if ðœ“ i = 0 for i < 0; that is, the output y t is a linear function of the current and past values of the input:In linear filters, under certain conditions, some properties such as stationarity of the input time series are also reflected in the output.We discussed stationarity previously in Chapter 2. We will now give a more formal description of it before proceeding further with linear models for time series.The stationarity of a time series is related to its statistical properties in time.That is, in the more strict sense, a stationary time series exhibits similar "statistical behavior" in time and this is often characterized as a constant probability distribution in time.However, it is usually satisfactory to consider the first two moments of the time series and define stationarity (or weak stationarity) as follows: (1) the expected value of the time series does not depend on time and (2) the autocovariance function defined as Cov(y t , y t+k ) for any lag k is only a function of k and not time; that is, ð›¾ y (k) = Cov(y t , y t+k ).In a crude way, the stationarity of a time series can be determined by taking arbitrary "snapshots" of the process at different points in time and observing the general behavior of the time series.If it exhibits "similar" behavior, one can then proceed with the modeling efforts under the assumption of stationarity.Further preliminary tests also involve observing the behavior of the autocorrelation function.A strong and slowly dying ACF will also suggest deviations from stationarity.Better and more methodological tests of stationarity also exist and we will discuss some of them later in this chapter.Figure 5.1 shows examples of stationary and nonstationary time series data.For a time-invariant and stable linear filter and a stationary input time series x t with ðœ‡ x = E(x t ) and ð›¾ x (k) = Cov(x t , x t+k ), the output time series y t given in Eq. (5.1) is also a stationary time series withIt is then easy to show that the following stable linear process with white noise time series, ðœ€ t , is also stationary: with E(ðœ€ t ) = 0, andSo for the autocovariance function of y t , we haveWe can rewrite the linear process in Eq. ( 5.2) in terms of the backshift operator, B, as(5.4)This is called the infinite moving average and serves as a general class of models for any stationary time series.This is due to a theorem by Wold (1938) and basically states that any nondeterministic weakly stationary time series y t can be represented as in Eq. ( 5.2), whereA more intuitive interpretation of this theorem is that a stationary time series can be seen as the weighted sum of the present and past random "disturbances."For further explanations see Yule (1927) and Bisgaard andKulahci (2005, 2011).The theorem by Wold requires that the random shocks in (5.4) to be white noise which we defined as uncorrelated random shocks with constant variance.Some textbooks discuss independent or strong white noise for random shocks.It should be noted that there is a difference between correlation and independence.Independent random variables are also uncorrelated but the opposite is not always true.Independence between two random variables refers their joint probability distribution function being equal to the product of the marginal distributions.That is, two random variables X and Y are said to be independent ifThis can also be loosely interpreted as if X and Y are independent, knowing the value of X for example does not provide any information about what the value of Y might be.For two uncorrelated random variables X and Y, we have their correlation and their covariance equal to zero.That is,Clearly if two random variables are independent, they are also uncorrelated since under independence we always haveAs we mentioned earlier, the opposite is not always true.To illustrate this with an example, consider X, a random variable with a symmetric probability density function around 0, i.e., E[X] = 0. Assume that the second variable Y is equal to |X|.Since knowing the value of X also determines the value of Y, these two variables are clearly not independent.However we can show thatThis shows that X and Y are uncorrected but not independent.Wold's decomposition theorem practically forms the foundation of the models we discuss in this chapter.This means that the strong assumption of independence is not necessarily needed except for the discussion on forecasting using ARIMA models in Section 5.8 where we assume the random shocks to be independent.It can also be seen from Eq. ( 5.3) that there is a direct relation between the weights { ðœ“ i } and the autocovariance function.In modeling a stationary time series as in Eq. ( 5.4), it is obviously impractical to attempt to estimate the infinitely many weights given in { ðœ“ i } .Although very powerful in providing a general representation of any stationary time series, the infinite moving average model given in Eq. ( 5.2) is useless in practice except for certain special cases:1. Finite order moving average (MA) models where, except for a finite number of the weights in { ðœ“ i } , they are set to 0.2. Finite order autoregressive (AR) models, where the weights in { ðœ“ i } are generated using only a finite number of parameters.3. A mixture of finite order autoregressive and moving average models (ARMA).We shall now discuss each of these classes of models in great detail.FINITE ORDER MOVING AVERAGE PROCESSES 333In finite order moving average or MA models, conventionally ðœ“ 0 is set to 1 and the weights that are not set to 0 are represented by the Greek letter ðœƒ with a minus sign in front.Hence a moving average process of order q (MA(q)) is given aswhere { ðœ€ t } is white noise.Since Eq. ( 5.5) is a special case of Eq. ( 5.4) with only finite weights, an MA(q) process is always stationary regardless of values of the weights.In terms of the backward shift operator, the MA(q) process isis white noise, the expected value of the MA(q) process is simply(5.7) = ðœ‡ and its variance isSimilarly, the autocovariance at lag k can be calculated fromFrom Eqs. (5.8) and (5.9), the autocorrelation function of the MA(q) process isThis feature of the ACF is very helpful in identifying the MA model and its appropriate order as it "cuts off" after lag q.In real life applications, however, the sample ACF, r (k), will not necessarily be equal to zero after lag q.It is expected to become very small in absolute value after lag q.For a data set of N observations, this is often tested against Â±2âˆ•N limits, where 1âˆ•N is the approximate value for the standard deviation of the ACF for any lag under the assumption ðœŒ(k) = 0 for all k's as discussed in Chapter 2.Note that a more accurate formula for the standard error of the kth sample autocorrelation coefficient is provided by Bartlett (1946) asA special case would be white noise data for which ðœŒ(j) = 0 for all j's.Hence for a white noise process (i.e., no autocorrelation), a reasonable interval for the sample autocorrelation coefficients to fall in would be Â±2âˆ•âˆš N and any indication otherwise may be considered as evidence for serial dependence in the process.The simplest finite order MA model is obtained when q = 1 in Eq. (5.5):For the first-order moving average or MA(1) model, we have the autocovariance function asSimilarly, we have the autocorrelation function asFrom Eq. ( 5.13), we can see that the first lag autocorrelation in MA( 1) is bounded asand the autocorrelation function cuts off after lag 1. Consider, for example, the following MA(1) model:A realization of this model with its sample ACF is given in Figure 5.2.A visual inspection reveals that the mean and variance remain stable while there are some short runs where successive observations tend to follow each other for very brief durations, suggesting that there is indeed some positive autocorrelation in the data as revealed in the sample ACF plot.We can also consider the following model:A realization of this model is given in Figure 5.3.We can see that observations tend to oscillate successively.This suggests a negative autocorrelation as confirmed by the sample ACF plot.Another useful finite order moving average process is MA(2), given asThe autocovariance and autocorrelation functions for the MA(2) model are given as(5.17)Note that the sample ACF cuts off after lag 2.As mentioned in Section 5.1, while it is quite powerful and important, Wold's decomposition theorem does not help us much in our modeling and forecasting efforts as it implicitly requires the estimation of the infinitely many weights, { ðœ“ i } .In Section 5.2 we discussed a special case of this decomposition of the time series by assuming that it can be adequately modeled by only estimating a finite number of weights and setting the rest equal to 0. Another interpretation of the finite order MA processes is that at any given time, of the infinitely many past disturbances, only a finite number of those disturbances "contribute" to the current value of the time series and that the time window of the contributors "moves" in time, making the "oldest" disturbance obsolete for the next observation.It is indeed not too far fetched to think that some processes might have these intrinsic dynamics.However, for some others, we may be required to consider the "lingering" contributions of the disturbances that happened back in the past.This will of course bring us back to square one in terms of our efforts in estimating infinitely many weights.Another solution to this problem is through the autoregressive models in which the infinitely many weights are assumed to follow a distinct pattern and can be successfully represented with only a handful of parameters.We shall now consider some special cases of autoregressive processes.Let us first consider again the time series given in Eq. ( 5.2):As in the finite order MA processes, one approach to modeling this time series is to assume that the contributions of the disturbances that are way in the past should be small compared to the more recent disturbances that the process has experienced.Since the disturbances are independently and identically distributed random variables, we can simply assume a set of infinitely many weights in descending magnitudes reflecting the diminishing magnitudes of contributions of the disturbances in the past.A simple, yet intuitive set of such weights can be created following an exponential decay pattern.For that we will set ðœ“ i = ðœ™ i , where |ðœ™| < 1 to guarantee the exponential "decay."In this notation, the weights on the disturbances starting from the current disturbance and going back in past will be 1, ðœ™, ðœ™ 2 , ðœ™ 3 , â€¦ Hence Eq. ( 5.2) can be written asFrom Eq. (5.18), we also haveWe can then combine Eqs.(5.18) and (5.19) aswhere ð›¿ = (1 âˆ’ ðœ™) ðœ‡.The process in Eq. (5.20) is called a first-order autoregressive process, AR(1), because Eq. ( 5.20) can be seen as a regression of y t on y tâˆ’1 and hence the term autoregressive process.The assumption of |ðœ™| < 1 results in the weights that decay exponentially in time and also guarantees thatThis means that an AR(1) process is stationary if |ðœ™| < 1.For |ðœ™| > 1, past disturbances will get exponentially increasing weights as time goes on and the resulting time series will be explosive.Box et al. (2008) argue that this type of processes are of little practical interest and therefore only consider cases where |ðœ™| = 1 and |ðœ™| < 1.The solution in (5.18) does indeed not converge for |ðœ™| > 1.We can however rewrite the AR(1) process for y t+1 y t+1 = ðœ™y t + a t+1(5.21)For y t , we then haveFor |ðœ™| > 1 we have |ðœ™ âˆ’1 | < 1 and therefore the solution for y t given in (5.22) is stationary.The only problem is that it involves future values of disturbances.This of course is impractical as this type of models requires knowledge about the future to make forecasts about it.These are called non-causal models.Therefore there exists a stationary solution for an AR(1) process when |ðœ™| > 1, however, it results in a non-causal model.Throughout the book when we discuss the stationary autoregressive models, we implicitly refer to the causal autoregressive models.We can in fact show that an AR(I) process is nonstationary if and only ifThe mean of a stationary AR(1) process isThe autocovariance function of a stationary AR(1) can be calculated from Eq. (5.18) asThe covariance is then given asCorrespondingly, the autocorrelation function for a stationary AR(1) process is given asHence the ACF for an AR(1) process has an exponential decay form.A realization of the following AR(1) model,is shown in Figure 5.5.As in the MA(1) model with ðœƒ = âˆ’0.8,we can observe some short runs during which observations tend to move in the upward or downward direction.As opposed to the MA(1) model, however, the duration of these runs tends to be longer and the trend tends to linger.This can also be observed in the sample ACF plot. Figure 5.6 shows a realization of the AR(1) model y t = 8 âˆ’ 0.8y tâˆ’1 + ðœ€ t .We observe that instead of lingering runs, the observations exhibit jittery up/down movements because of the negative ðœ™ value.In this section, we will first start with the obvious extension of Eq. (5.20) to include the observation y tâˆ’2 as(5.27)We will then show that Eq. ( 5.27) can be represented in the infinite MA form and provide the conditions of stationarity for y t in terms of ðœ™ 1 and ðœ™ 2 .For that we will rewrite Eq. (5.27) asFurthermore, applying Î¦(B) âˆ’1 to both sides, we obtainandWe can use Eq. ( 5.32) to obtain the weights in Eq. (5.30) in terms of ðœ™ 1 and ðœ™ 2 .For that, we will useSince on the right-hand side of the Eq.(5.34) there are no backshift operators, for Î¦(B) Î¨(B) = 1, we needThe equations in (5.35) can indeed be solved for each ðœ“ j in a futile attempt to estimate infinitely many parameters.However, it should be noted that the ðœ“ j in Eq. (5.35) satisfy the second-order linear difference equation and that they can be expressed as the solution to this equation in terms of the two roots m 1 and m 2 of the associated polynomialThe equations in (5.39) are called the Yule-Walker equations for ð›¾(k).Similarly, we can obtain the autocorrelation function by dividing Eq. (5.39) by ð›¾ (0):The Yule-Walker equations for ðœŒ(k) in Eq. ( 5.40) can be solved recursively asA general solution can be obtained through the roots m 1 and m 2 of the associated polynomial m 2 âˆ’ ðœ™ 1 m âˆ’ ðœ™ 2 = 0.There are three cases.Case 1.If m 1 and m 2 are distinct, real roots, we then havewhere c 1 and c 2 are particular constants and can, for example, be obtained from ðœŒ(0) and ðœŒ(1).Moreover, since for stationarity we havein this case, the autocorrelation function is a mixture of two exponential decay terms.Case 2. If m 1 and m 2 are complex conjugates in the form of a Â± ib, we then haveagain c 1 and c 2 are particular constants.The ACF in this case has the form of a damped sinusoid, with damping factor R and frequency ðœ†; that is, the period is 2ðœ‹âˆ•ðœ†.Case 3. If there is one real root m 0 , m 1 = m 2 = m 0 , we then haveIn this case, the ACF will exhibit an exponential decay pattern.In case 1, for example, an AR(2) model can be seen as an "adjusted" AR(1) model for which a single exponential decay expression as in the AR(1) model is not enough to describe the pattern in the ACF, and hence an additional exponential decay expression is "added" by introducing the second lag term, y tâˆ’2 .Figure 5.7 shows a realization of the AR(2) processNote that the roots of the associated polynomial of this model are real.Hence the ACF is a mixture of two exponential decay terms.Similarly, Figure 5.8 shows a realization of the following AR(2) processFor this process, the roots of the associated polynomial are complex conjugates.Therefore the ACF plot exhibits a damped sinusoid behavior.From the previous two sections, a general, pth-order AR model is given aswhere ðœ€ t is white noise.Another representation of Eq. ( 5.44) can be given aswhereThe AR(p) time series { y t } in Eq. (5.44) is causal and stationary if the roots of the associated polynomialare less than one in absolute value.Furthermore, under this condition, the AR(p) time series { y t } is also said to have an absolutely summable infinite MA representationwhereAs in AR(2), the weights of the random shocks in Eq. (5.47) can be obtained from Î¦(B) Î¨(B) = 1 asWe can easily show that, for the stationary AR(p) processThus we haveBy dividing Eq. (5.49) by ð›¾ (0) for k > 0, it can be observed that the ACF of an AR(p) process satisfies the Yule-Walker equationsThe equations in (5.52) are pth-order linear difference equations, implying that the ACF for an AR(p) model can be found through the p roots of the associated polynomial in Eq. (5.46).For example, if the roots are all distinct and real, we havewhere c 1 , c 2 , â€¦ , c p are particular constants.However, in general, the roots may not all be distinct or real.Thus the ACF of an AR(p) process can be a mixture of exponential decay and damped sinusoid expressions depending on the roots of Eq. (5.46).In Section 5.2, we saw that the ACF is an excellent tool in identifying the order of an MA(q) process, because it is expected to "cut off" after lag q.However, in the previous section, we pointed out that the ACF is not as useful in the identification of the order of an AR(p) process for which it will most likely have a mixture of exponential decay and damped sinusoid expressions.Hence such behavior, while indicating that the process might have an AR structure, fails to provide further information about the order of such structure.For that, we will define and employ the partial autocorrelation function (PACF) of the time series.But before that, we discuss the concept of partial correlation to make the interpretation of the PACF easier.Partial Correlation Consider three random variables X, Y, and Z. Then consider simple linear regression of X on Z and Y on Z asandThen the errors can be obtained fromandThen the partial correlation between X and Y after adjusting for Z is defined as the correlation between X * and YThat is, partial correlation can be seen as the correlation between two variables after being adjusted for a common factor that may be affecting them.The generalization is of course possible by allowing for adjustment for more than just one factor.Following the above definition, the PACF between y t and y tâˆ’k is the autocorrelation between y t and y tâˆ’k after adjusting for y tâˆ’1 , y tâˆ’2 , â€¦ , y tâˆ’k+1 .Hence for an AR(p) model the PACF between y t and y tâˆ’k for k > p should be equal to zero.A more formal definition can be found below.Consider a stationary time series model { y t } that is not necessarily an AR process.Further consider, for any fixed value of k, the Yule-Walker equations for the ACF of an AR(p) process given in Eq. (5.52) asorHence we can write the equations in (5.54) in matrix notation as(5.56)whereThus to solve for ðœ™ k , we haveFor any given k, k = 1, 2, â€¦ , the last coefficient ðœ™ kk is called the partial autocorrelation of the process at lag k.Note that for an AR(p) process ðœ™ kk = 0 for k > p. Hence we say that the PACF cuts off after lag p for an AR(p).This suggests that the PACF can be used in identifying the order of an AR process similar to how the ACF can be used for an MA process.For sample calculations, Ï†kk , the sample estimate of ðœ™ kk , is obtained by using the sample ACF, r(k).Furthermore, in a sample of N observations from an AR(p) process, Ï†kk for k > p is approximately normally distributed withHence the 95% limits to judge whether any Ï†kk is statistically significantly different from zero are given by Â±2âˆ• âˆš N.For further detail see Quenouille (1949), Jenkins (1954, 1956), and Daniels (1956.Figure 5.9 shows the sample PACFs of the models we have considered so far.In Figure 5.9a we have the sample PACF of the realization of the MA(1) model with ðœƒ = 0.8 given in Figure 5.3.It exhibits an exponential decay pattern.Figure 5.9b shows the sample PACF of the realization of the MA(2) model in Figure 5.4 and it also has an exponential decay pattern in absolute value since for this model the roots of the associated polynomial are real.Figures 5.9c and 5.9d show the sample PACFs of the realization of the AR(1) model with ðœ™ = 0.8 and ðœ™ = âˆ’0.8,respectively.In bothFIGURE 5.9 Partial autocorrelation functions for the realizations of (a) MA(1) process,cases the PACF "cuts off" after the first lag.That is, the only significant sample PACF value is at lag 1, suggesting that the AR(1) model is indeed appropriate to fit the data.Similarly, in Figures 5.9e and 5.9f, we have the sample PACFs of the realizations of the AR(2) model.Note that the sample PACF cuts off after lag 2.As we discussed in Section 5.3, finite order MA processes are stationary.On the other hand as in the causality concept we discussed for the autoregressive processes, we will impose some restrictions on the parameters of the MA models as well.Consider for example the MA(1) model in (5.11)(5.59)Note that for the sake of simplicity, in (5.59) we consider a centered process, i.e.E(y t ) = 0. We can then rewrite (5.59) asIt can be seen from ( 5.60) that for |ðœƒ 1 | < 1, ðœ€ t is a convergent series of current and past observations and the process is called an invertible moving average process.Similar to the causality argument, for |ðœƒ 1 | > 1, ðœ€ t can be written as a convergent series of future observations and is called noninvertible.When |ðœƒ 1 | = 1, the MA(1) process is considered noninvertible in a more restricted sense (Brockwell and Davis (1991)).The direct implication of invertibility becomes apparent in model identification.Consider the MA(1) process as an example.The first lag autocorrelation for that process is given asThis allows for the calculation of ðœƒ 1 for a given ðœŒ(1) by rearranging (5.61) asand solving for ðœƒ 1 .Except for the case of a repeated root, this equation has two solutions.Consider for example ðœŒ(1) = 0.4 for which both ðœƒ 1 = 0.5 and ðœƒ 1 = 2 are the solutions for (5.62).Following the above argument, only ðœƒ 1 = 0.5 yields the invertible MA(1) process.It can be shown that when there are multiple solutions for possible values of MA parameters, there is only one solution that will satisfy the invertibility condition (Box et al. (2008), Section 6.4.1).Consider the MA(q) processAfter multiplying both sides with Î˜(B) âˆ’1 , we haveHence the infinite AR representation of an MA(q) process is given asThe ðœ‹ i can be determined fromwhich in turn yieldswith ðœ‹ 0 = âˆ’1 and ðœ‹ j = 0 for j < 0. Hence as in the previous arguments for the stationarity of AR(p) models, the ðœ‹ i are the solutions to the qth-order linear difference equations and therefore the condition for the invertibility of an MA(q) process turns out to be very similar to the stationarity condition of an AR(p) process: the roots of the associated polynomial given in Eq. (5.66) should be less than 1 in absolute value,(5.67)An invertible MA(q) process can then be written as an infinite AR process.Correspondingly, for such a process, adjusting for y tâˆ’1 , y tâˆ’2 , â€¦ , y tâˆ’k+1 does not necessarily eliminate the correlation between y t and y tâˆ’k and therefore its PACF will never "cut off."In general, the PACF of an MA(q) process is a mixture of exponential decay and damped sinusoid expressions.The ACF and the PACF do have very distinct and indicative properties for MA and AR models, respectively.Therefore, in model identification, we strongly recommend the use of both the sample ACF and the sample PACF simultaneously.In the previous sections we have considered special cases of Wold's decomposition of a stationary time series represented as a weighted sum of infinite random shocks.In an AR(1) process, for example, the weights in the infinite sum are forced to follow an exponential decay form with ðœ™ as the rate of decay.Since there are no restrictions apart from âˆ‘ âˆž i=0 ðœ“ 2 i < âˆž on the weights (ðœ“ i ), it may not be possible to approximate them by an exponential decay pattern.For that, we will need to increase the order of the AR model to approximate any pattern that these weights may in fact be exhibiting.On some occasions, however, it is possible to make simple adjustments to the exponential decay pattern by adding only a few terms and hence to have a more parsimonious model.Consider, for example, that the weights ðœ“ i do indeed exhibit an exponential decay pattern with a constant rate except for the fact that ðœ“ 1 is not equal to this rate of decay as it would be in the case of an AR(1) process.Hence instead of increasing the order of the AR model to accommodate for this "anomaly," we can add an MA(1) term that will simply adjust ðœ“ 1 while having no effect on the rate of exponential decay pattern of the rest of the weights.This results in a mixed autoregressive moving average or ARMA(1,1) model.In general, an ARMA(p, q) model is given aswhere ðœ€ t is a white noise process.The stationarity of an ARMA process is related to the AR component in the model and can be checked through the roots of the associated polynomialIf all the roots of Eq. (5.70) are less than one in absolute value, then ARMA(p, q) is stationary.This also implies that, under this condition, ARMA(p, q) has an infinite MA representation asThe coefficients in Î¨(B) can be found fromand ðœ“ 0 = 1.Similar to the stationarity condition, the invertibility of an ARMA process is related to the MA component and can be checked through the roots of the associated polynomialIf all the roots of Eq. (5.71) are less than one in absolute value, then ARMA(p, q) is said to be invertible and has an infinite AR representation,where ð›¼ = Î˜(B) âˆ’1 ð›¿ and Î (B) = Î˜(B) âˆ’1 Î¦(B).The coefficients in Î (B) can be found fromand ðœ‹ 0 = âˆ’1.In Figure 5.10 we provide realizations of two ARMA(1,1) models:Note that the sample ACFs and PACFs exhibit exponential decay behavior (sometimes in absolute value depending on the signs of the AR and MA coefficients).As in the stationarity and invertibility conditions, the ACF and PACF of an ARMA process are determined by the AR and MA components, respectively.It can therefore be shown that the ACF and PACF of an ARMA(p, q) both exhibit exponential decay and/or damped sinusoid patterns, which makes the identification of the order of the ARMA(p, q) model relatively more difficult.For that, additional sample functions such as the Extended Sample ACF (ESACF), the Generalized Sample PACF (GPACF), the Inverse ACF (IACF), and canonical correlations can be used.For further information see Box, Jenkins, andReinsel (2008), Wei (2006), Tiao and Box (1981), Tsay andTiao (1984), andAbraham andLedolter (1984).However, the availability of sophisticated statistical software packages such as Minitab JMP and SAS makes it possible for the practitioner to consider several different models with various orders and compare them based on the model selection criteria such as AIC, AICC, and BIC as described in Chapter 2 and residual analysis.The theoretical values of the ACF and PACF for stationary time series are summarized in Table 5.1.The summary of the sample ACFs and PACFs of the realizations of some of the models we have covered in this chapter are given in(with 5% significance limits for the partial autocorrelations)It is often the case that while the processes may not have a constant level, they exhibit homogeneous behavior over time.Consider, for example, the linear trend process given in Figure 5.1c.It can be seen that different snapshots taken in time do exhibit similar behavior except for the mean level of the process.Similarly, processes may show nonstationarity in the slope as well.We will call a time series, y t , homogeneous nonstationary if it is not stationary but its first difference, that is, w t = y t âˆ’ y tâˆ’1 = (1 âˆ’ B) y t , or higher-order differences, w t = (1 âˆ’ B) d y t , produce a stationary time series.We will further call y t an autoregressive integrated moving average (ARIMA) process of orders p, d, and q-that is, ARIMA(p, d, q)-if its dth difference, denoted by w t = (1 âˆ’ B) d y t , produces a stationary ARMA(p, q) process.The term integrated is used since, for d = 1, for example, we can write y t as the sum (or "integral") of the w t process asHence an ARIMA(p, d, q) can be written as(5.77)Thus once the differencing is performed and a stationary time series w t = (1 âˆ’ B) d y t is obtained, the methods provided in the previous sections can be used to obtain the full model.In most applications first differencing (d = 1) and occasionally second differencing (d = 2) would be enough to achieve stationarity.However, sometimes transformations other than differencing are useful in reducing a nonstationary time series to a stationary one.For example, in many economic time series the variability of the observations increases as the average level of the process increases; however, the percentage of change in the observations is relatively independent of level.Therefore taking the logarithm of the original series will be useful in achieving stationarity.The random walk process, ARIMA(0, 1, 0) is the simplest nonstationary model.It is given by (1 âˆ’ B)y t = ð›¿ + ðœ€ t (5.78)  suggesting that first differencing eliminates all serial dependence and yields a white noise process.Consider the process y t = 20 + y tâˆ’1 + ðœ€ t .A realization of this process together with its sample ACF and PACF are given in Figure 5.11a-c.We can see that the sample ACF dies out very slowly, while the sample PACF is only significant at the first lag.Also note that the PACF value at the first lag is very close to one.All this evidence suggests that the process is not stationary.The first difference, w t = y t âˆ’ y tâˆ’1 , and its sample ACF and PACF are shown in Figure 5.11d-f.The time series plot of w t implies that the first difference is stationary.In fact, the sample ACF and PACF do not show any significant values.This further suggests that differencing the original data once "clears out" the autocorrelation.Hence the data can be modeled using the random walk model given in Eq. (5.78).The ARIMA(0, 1, 1) process is given byThe infinite AR representation of Eq. ( 5.79) can be obtained from Eq. (5.75)with ðœ‹ 0 = âˆ’1.Thus we haveThis suggests that an ARIMA(0, 1, 1) (a.k.a.IMA(1, 1)) can be written as an exponentially weighted moving average (EWMA) of all past values.Consider the time series data in Figure 5.12a.It looks like the mean of the process is changing (moving upwards) in time.Yet the change in the mean (i.e., nonstationarity) is not as obvious as in the previous example.The sample ACF plot of the data in Figure 5.12b dies down relatively slowly and the sample PACF of the data in Figure 5.12c shows two significant values at lags 1 and 2. Hence we might be tempted to model this data using an AR(2) model because of the exponentially decaying ACF and significant PACF at the first two lags.Indeed, we might even have a good fit using an AR(2) model.We should nevertheless check the roots of the associated polynomial given in Eq. (5.36) to make sure that its roots are less than 1 in absolute value.Also note that a technically stationary process will behave more and more nonstationary as the roots of the associated polynomial approach unity.For that, observe the realization of the near nonstationary process, y t = 2 + 0.95y tâˆ’1 + ðœ€ t , given in Figure 5.1b.Based on the visual inspection, however, we may deem the process nonstationary and proceed with taking the first difference of the data.This is because the ðœ™ value of the AR(1) model is close to 1.Under these circumstances, where the nonstationarity  Lag FIGURE 5.12 A realization of the ARIMA(0, 1, 1) model, y t , its first difference, w t , and their sample ACFs and PACFs. of the process is dubious, we strongly recommend that the analyst refer back to basic underlying process knowledge.If, for example, the process mean is expected to wander off as in some financial data, assuming that the process is nonstationary and proceeding with differencing the data would be more appropriate.For the data given in Figure 5.12a, its first difference given in Figure 5.12d looks stationary.Furthermore, its sample ACF and PACF given in Figures 5.12e and 5.12f, respectively, suggest that an MA(1) model would be appropriate for the first difference since its ACF cuts off after the first lag and the PACF exhibits an exponential decay pattern.Hence the ARIMA(0, 1, 1) model given in Eq. (5.79) can be used for this data.A three-step iterative procedure is used to build an ARIMA model.First, a tentative model of the ARIMA class is identified through analysis of historical data.Second, the unknown parameters of the model are estimated.Third, through residual analysis, diagnostic checks are performed to determine the adequacy of the model, or to indicate potential improvements.We shall now discuss each of these steps in more detail.Model identification efforts should start with preliminary efforts in understanding the type of process from which the data is coming and how it is collected.The process' perceived characteristics and sampling frequency often provide valuable information in this preliminary stage of model identification.In today's data rich environments, it is often expected that the practitioners would be presented with "enough" data to be able to generate reliable models.It would nevertheless be recommended that 50 or preferably more observations should be initially considered.Before engaging in rigorous statistical model-building efforts, we also strongly recommend the use of "creative" plotting of the data, such as the simple time series plot and scatter plots of the time series data y t versus y tâˆ’1 , y tâˆ’2 , and so on.For the y t versus y tâˆ’1 scatter plot, for example, this can be achieved in a data set of N observations by plotting the first N âˆ’ 1 observations versus the last N âˆ’ 1. Simple time series plots should be used as the preliminary assessment tool for stationarity.The visual inspection of these plots should later be confirmed as described earlier in this chapter.If nonstationarity is suspected, the time series plot of the first (or dth) difference should also be considered.The unit root test by Dickey and Fuller (1979) can also be performed to make sure that the differencing is indeed needed.Once the stationarity of the time series can be presumed, the sample ACF and PACF of the time series of the original time series (or its dth difference if necessary) should be obtained.Depending on the nature of the autocorrelation, the first 20-25 sample autocorrelations and partial autocorrelations should be sufficient.More care should be taken of course if the process exhibits strong autocorrelation and/or seasonality, as we will discuss in the following sections.Table 5.1 together with the Â±2âˆ•N limits can be used as a guide for identifying AR or MA models.As discussed earlier, the identification of ARMA models would require more care, as both the ACF and PACF will exhibit exponential decay and/or damped sinusoid behavior.We have already discussed that the differenced series { w t } may have a nonzero mean, say, ðœ‡ w .At the identification stage we may obtain an indication of whether or not a nonzero value of ðœ‡ w is needed by comparing the sample mean of the differenced series, say, w =, with its approximate standard error.Box, Jenkins, and Reinsel (2008) give the approximate standard error of w for several useful ARIMA(p, d, q) models.Identification of the appropriate ARIMA model requires skills obtained by experience.Several excellent examples of the identification process are given in Box et al. (2008, Chap. 6), Montgomery et al. (1990), and Bisgaard and Kulahci (2011.There are several methods such as the methods of moments, maximum likelihood, and least squares that can be employed to estimate the parameters in the tentatively identified model.However, unlike the regression models of Chapter 2, most ARIMA models are nonlinear models and require the use of a nonlinear model fitting procedure.This is usually automatically performed by sophisticated software packages such as Minitab JMP, and SAS.In some software packages, the user may have the choice of estimation method and can accordingly choose the most appropriate method based on the problem specifications.After a tentative model has been fit to the data, we must examine its adequacy and, if necessary, suggest potential improvements.This is done through residual analysis.The residuals for an ARMA(p, q) process can be obtained fromIf the specified model is adequate and hence the appropriate orders p and q are identified, it should transform the observations to a white noise process.Thus the residuals in Eq. (5.82) should behave like white noise.Let the sample autocorrelation function of the residuals be denoted by { r e (k) } .If the model is appropriate, then the residual sample autocorrelation function should have no structure to identify.That is, the autocorrelation should not differ significantly from zero for all lags greater than one.If the form of the model were correct and if we knew the true parameter values, then the standard error of the residual autocorrelations would be N âˆ’1âˆ•2 .Rather than considering the r e (k) terms individually, we may obtain an indication of whether the first K residual autocorrelations considered together indicate adequacy of the model.This indication may be obtained through an approximate chi-square test of model adequacy.The test statistic iswhich is approximately distributed as chi-square with K âˆ’ p âˆ’ q degrees of freedom if the model is appropriate.If the model is inadequate, the calculated value of Q will be too large.Thus we should reject the hypothesis of model adequacy if Q exceeds an approximate small upper tail point of the chi-square distribution with K âˆ’ p âˆ’ q degrees of freedom.Further details of this test are in Chapter 2 and in the original reference by Box and Pierce (1970).The modification of this test by Ljung and Box (1978) presented in Chapter 2 is also useful in assessing model adequacy.In this section we shall present two examples of the identification, estimation, and diagnostic checking process.One example presents the analysis for a stationary time series, while the other is an example of modeling a nonstationary series.Example 5.1 Table 5.5 shows the weekly total number of loan applications in a local branch of a national bank for the last 2 years.It is suspected that there should be some relationship (i.e., autocorrelation) between the number of applications in the current week and the number of loan applications in the previous weeks.Modeling that relationship will help the management to proactively plan for the coming weeks through reliable forecasts.As always, we start our analysis with the time series plot of the data, shown in Figure 5.13. Figure 5.13 shows that the weekly data tend to have short runs and that the data seem to be indeed autocorrelated.Next, we visually inspect the stationarity.Although there might be a slight drop in the mean for the second year (weeks 53-104), in general, it seems to be safe to assume stationarity.We now look at the sample ACF and PACF plots in Figure 5.14.Here are possible interpretations of the ACF plot:1.It cuts off after lag 2 (or maybe even 3), suggesting an MA(2) (or MA(3)) model.2. It has an (or a mixture of) exponential decay(s) pattern suggesting an AR(p) model.To resolve the conflict, consider the sample PACF plot.For that, we have only one interpretation; it cuts off after lag 2. Hence we use the second interpretation of the sample ACF plot and assume that the appropriate model to fit is the AR(2) model.Table 5.6 shows the Minitab output for the AR(2) model.The parameter estimates are Ï†1 = 0.27 and Ï†2 = 0.42, and they turn out to be significant (see the P-values).MSE is calculated to be 39.35.The modified Box-Pierce test suggests that there is no autocorrelation left in the residuals.We can also see this in the ACF and PACF plots of the residuals in Figure 5    the methodology presented in this chapter has a very sound theoretical foundation.However, as in any modeling effort, we should also keep in mind the subjective component of model identification.In fact, as we mentioned earlier, time series model fitting can be seen as a mixture of science and art and can best be learned by practice and experience.The next example will illustrate this point further.Example 5.2 Consider the Dow Jones Index data from Chapter 4. A time series plot of the data is given in Figure 5.18.The process shows signs of nonstationarity with changing mean and possibly variance.Similarly, the slowly decreasing sample ACF and sample PACF with significant value at lag 1, which is close to 1 in Figure 5.19, confirm that indeed the process can be deemed nonstationary.On the other hand, one might argue that the significant sample PACF value at lag 1 suggests that the AR(1) model might also fit the data well.We will consider this interpretation first and fit an AR(1) model to the Dow Jones Index data.Table 5.7 shows the Minitab output for the AR(1) model.Although it is close to 1, the AR(1) model coefficient estimate Ï† = 0.9045 turns out to be quite significant and the modified Box-Pierce test suggests that there is no autocorrelation left in the residuals.This is also confirmed by the sample ACF and PACF plots of the residuals given in Figure 5.20.The only concern in the residual plots in Figure 5.21 is in the changing variance observed in the time series plot of the residuals.This is indeed a very important issue since it violates the constant variance assumption.We will discuss this issue further in Section 7.3 but for illustration purposes we will ignore it in this example.Overall it can be argued that an AR(1) model provides a decent fit to the data.However, we will now consider the earlier interpretation and assume that the Dow Jones Index data comes from a nonstationary process.We then take the first difference of the data as shown in Figure 5.22.While there are once again some serious concerns about changing variance, the level of the first difference remains the same.If we ignore the changing variance and look at the sample ACF and PACF plots given in Figure 5.23, we may conclude that the first difference is in fact white noise.That is, since these plots do not show any sign of significant autocorrelation, a model we may consider for the Dow Jones Index data would be the random walk model, ARIMA(0, 1, 0).Now the analyst has to decide between the two models: AR(1) and ARIMA(0, 1, 0).One can certainly use some of the criteria we discussed in Section 2.6.2 to choose one of these models.Since these two models are fundamentally quite different, we strongly recommend that the analyst use the subject matter/process knowledge as much as possible.Do we expect a financial index such as the Dow Jones Index to wander about a fixed mean as implied by the AR(1)?In most cases involving financial data, the answer would be no.Hence a model such as ARIMA(0, 1, 0) that takes into account the inherent nonstationarity of the process should be preferred.However, we do have a problem with the proposed model.A random walk model means that the price changes are random and cannot be predicted.If we have a higher price today compared to yesterday, that would have no bearing on the forecasts tomorrow.That is, tomorrow's price can be higher or lower than today's and we would have no way to forecast it effectively.This further suggests that the best forecast for tomorrow's price is in fact the price we have today.This is obviously not a reliable and effective forecasting model.This very same issue of the random walk models for financial data has been discussed in great detail in the literature.We simply used this data to illustrate that in time series model fitting we can end up with fundamentally different models that will fit the data equally well.At this point, process knowledge can provide the needed guidance in picking the "right" model.It should be noted that, in this example, we tried to keep the models simple for illustration purposes.Indeed, a more thorough analysis would (and should) pay close attention to the changing variance issue.In fact, this is a very common concern particularly when dealing with financial data.For that, we once again refer the reader to Section 7.3.Once an appropriate time series model has been fit, it may be used to generate forecasts of future observations.If we denote the current time by T, the forecast for y T+ðœ is called the ðœ-period-ahead forecast and denoted by Å·T+ðœ (T).The standard criterion to use in obtaining the best forecast is the mean squared error for which the expected value of the squared forecast errors, E[(y T+ðœ âˆ’ Å·T+ðœ (T)) 2 ] = E[e T (ðœ) 2 ], is minimized.It can be shown that the best forecast in the mean square sense is the conditional expectation of y T+ðœ given current and previous observations, that is, y T , y Tâˆ’1 , â€¦:(5.84)Consider, for example, an ARIMA(p, d, q) process at time T + ðœ (i.e., ðœ period in the future):Further consider its infinite MA representation,We can partition Eq. (5.86) asIn this partition, we can clearly see that the âˆ‘ ðœâˆ’1 i=0 ðœ“ i ðœ€ T+ðœâˆ’i component involves the future errors, whereas the âˆ‘ âˆž i=ðœ ðœ“ i ðœ€ T+ðœâˆ’i component involves the present and past errors.From the relationship between the current and past observations and the corresponding random shocks as well as the fact that the random shocks are assumed to have mean zero and to be independent, we can show that the best forecast in the mean square sense isSince the forecast error in Eq. (5.89) is a linear combination of random shocks, we haveIt should be noted that the variance of the forecast error gets bigger with increasing forecast lead times ðœ.This intuitively makes sense as we should expect more uncertainty in our forecasts further into the future.Moreover, if the random shocks are assumed to be normally distributed, N(0, ðœŽ 2 ), then the forecast errors will also be normally distributed with N(0, ðœŽ 2 (ðœ)).We can then obtain the 100(1 âˆ’ ð›¼) percent prediction intervals for the future observations fromwhere z ð›¼âˆ•2 is the upper ð›¼âˆ•2 percentile of the standard normal distribution, N (0, 1).Hence the 100(1 âˆ’ ð›¼) percent prediction interval for y T+ðœ is Å·T+ðœ (T) Â± z ð›¼âˆ•2 ðœŽ (ðœ) (5.93)There are two issues with the forecast equation in (5.88).First, it involves infinitely many terms in the past.However, in practice, we will only have a finite amount of data.For a sufficiently large data set, this can be overlooked.Second, Eq. (5.88) requires knowledge of the magnitude of random shocks in the past, which is unrealistic.A solution to this problem is to "estimate" the past random shocks through one-step-ahead forecasts.For the ARIMA model we can calculaterecursively by setting the initial values of the random shocks to zero for t < p + d + 1.For more accurate results, these initial values together with the y t for t â‰¤ 0 can also be obtained using back-forecasting.For further details, see Box, Jenkins, and Reinsel (2008).As an illustration consider forecasting the ARIMA(1, 1, 1) processWe will consider two of the most commonly used approaches:1.As discussed earlier, this approach involves the infinite MA representation of the model in Eq. (5.95), also known as the random shock form of the model:Hence the ðœ-step-ahead forecast can be calculated from Å·T+ðœ (T) = ðœ“ ðœ ðœ€ T + ðœ“ ðœ+1 ðœ€ Tâˆ’1 + â‹¯(5.97)The weights ðœ“ i can be calculated fromand the random shocks can be estimated using the one-step-ahead forecast error; for example, ðœ€ T can be replaced by e Tâˆ’1 (1) = y T âˆ’ Å·T (T âˆ’ 1). 2. Another approach that is often employed in practice is to use difference equations as given byFor ðœ = 1, the best forecast in the mean squared error sense isWe can further show that for lead times ðœ > 2, the forecast isPrediction intervals for forecasts of future observations at time period T + ðœ are found using equation 5.87.However, in using Equation 5.87 the ðœ“ weights must be found in order to compute the variance (or standard deviation) of the ðœ-step ahead forecast error.The ðœ“ weights for the general ARIMA(p, d, q) model may be obtained by equating like powers of B in the expansion ofand solving for the ðœ“ weights.We now illustrate this with three examples.Example 5.3 The ARMA(1, 1) Model For the ARMA(1, 1) model the product of the required polynomials isEquating like power of B we find thatIn general, we can show for the ARMA(1,1) model that ðœ“ j = ðœ™ jâˆ’1 (ðœ™ âˆ’ ðœƒ).Example 5.4 The AR(2) Model For the AR(2) model the product of the required polynomials isEquating like power of B, we find thatIn general, we can show for the AR(2) model that ðœ“ j = ðœ™ 1 ðœ“ jâˆ’1 + ðœ™ 2 ðœ“ jâˆ’2 .Example 5.5 The ARIMA(0, 1, 1) or IMA(1,1) Model Now consider a nonstationary model, the IMA(1, 1) model.The product of the required polynomials for this model isIt is straightforward to show that the ðœ“ weights for this model areNotice that the prediction intervals will increase in length rapidly as the forecast lead time increases.This is typical of nonstationary ARIMA models.It implies that these models may not be very effective in forecasting more than a few periods ahead.Example 5.6 Consider the loan applications data given in Table 5.5.Now assume that the manager wants to make forecasts for the next 3 months (12 weeks) using the AR(2) model from Example 5.1.Hence at the 104th week we need to make 1-step, 2-step, â€¦ , 12-step-ahead predictions, which are obtained and plotted using Minitab in Figure 5.24 together with the 95% prediction interval.Table 5.8 shows the output from JMP for fitting an AR(2) model to the weekly loan application data.In addition to the sample ACF and PACF, JMP provides the model fitting information including the estimates of the model parameters, the forecasts for 10 periods into the future and the associated prediction intervals, and the residual autocorrelation and PACF.The AR(2) model is an excellent fit to the data.Time series data may sometimes exhibit strong periodic patterns.This is often referred to as the time series having a seasonal behavior.This mostly occurs when data is taken in specific intervals-monthly, weekly, and so on.One way to represent such data is through an additive model where the process is assumed to be composed of two parts,where S t is the deterministic component with periodicity s and N t is the stochastic component that may be modeled as an ARMA process.In that, y t can be seen as a process with predictable periodic behavior with some noise sprinkled on top of it.Since the S t is deterministic and has periodicity s, we have S t = S t+s orApplying the (1 âˆ’ B s ) operator to Eq. (5.102), we have(5.104)     where ðœ€ t is white noise.We can also consider S t as a stochastic process.We will further assume that after seasonal differencing, (1 âˆ’ B s ), (1 âˆ’ B s ) y t = w t becomes stationary.This, however, may not eliminate all seasonal features in the process.That is, the seasonally differenced data may still show strong autocorrelation at lags s, 2s, â€¦ .So the seasonal ARMA model is In practice, although it is case specific, it is not expected to have P, D, and Q greater than 1.The results for regular ARIMA processes that we discussed in previous sections apply to the seasonal models given in Eq. (5.107).As in the nonseasonal ARIMA models, the forecasts for the seasonal ARIMA models can be obtained from the difference equations as illustrated for example in Eq. (5.101) for a nonseasonal ARIMA(1,1,1) process.Similarly the weights in the random shock form given in Eq. (5.96) can be estimated as in Eq. (5.98) to obtain the estimate for the variance of the forecast errors as well as the prediction intervals given in Eqs.(5.91) and (5.92), respectively.Example 5.7 The ARIMA (0, 1, 1) Ã— (0, 1, 1) model with s = 12 isFor this process, the autocovariances are calculated as  4.9.The data obviously exhibit some seasonality and upward linear trend.The sample ACF and PACF plots given in Figure 5.25 indicate a monthly seasonality, s = 12, as ACF values at lags 12, 24, 36 are significant and slowly decreasing, and there is a significant PACF value at lag 12 that is close to 1.Moreover, the slowly decreasing ACF in general, also indicates a nonstationarity that can be remedied by taking the first difference.Hence we would now consider w t = (1 âˆ’ B) (1 âˆ’ B 12 )y t .Figure 5.26 shows that first difference together with seasonal differencing-that is, w t = (1 âˆ’ B)(1 âˆ’ B 12 )y t -helps in terms of stationarity and eliminating the seasonality, which is also confirmed by sample ACF and PACF plots given in Figure 5.27.Moreover, the sample ACF with a significant value at lag 1 and the sample PACF with exponentially  decaying values at the first 8 lags suggest that a nonseasonal MA(1) model should be used.The interpretation of the remaining seasonality is a bit more difficult.For that we should focus on the sample ACF and PACF values at lags 12, 24, 36, and so on.The sample ACF at lag 12 seems to be significant and the sample PACF at lags 12, 24, 36 (albeit not significant) seems to be alternating in sign.That suggests that a seasonal MA(1) model can be used as well.Hence an ARIMA(0, 1, 1) Ã— (0, 1, 1) 12 model is used to model the data, y t .The output from Minitab is given in Table 5.9.Both MA(1) and seasonal MA(1) coefficient estimates are significant.As we can see from the sample ACF and PACF plots in Figure 5.28, while there are still some  small significant values, as indicated by the modified Box pierce statistic most of the autocorrelation is now modeled out.The residual plots in Figure 5.29 provided by Minitab seem to be acceptable as well.Finally, the time series plot of the actual and fitted values in Figure 5.30 suggests that the ARIMA(0, 1, 1) Ã— (0, 1, 1) 12 model provides a reasonable fit to this highly seasonal and nonstationary time series data.In Section 4.8 we introduced the daily counts of respiratory and gastrointestinal complaints for more than 2-1 âˆ• 2 years at several hospitals in a large metropolitan area from Fricker (2013).Table 4.12 presents the 980 observations from one of these hospitals.Section 4.8 described modeling the respiratory count data with exponential smoothing.We now present an ARIMA modeling approach.Figure 5.31 presents the sample ACF, PACF, and the variogram from JMP for these data.Examination of the original time series plot in Figure 4.35 and the ACF and variogram indicate that the daily respiratory syndrome counts may be nonstationary and that the data should be differenced to obtain a stationary time series for ARIMA modeling.The ACF for the differenced series (d = 1) shown in Figure 5.32 cuts off after lag 1 while the PACF appears to be a mixture of exponential decays.This suggests either an ARIMA(1, 1, 1) or ARIMA(2, 1, 1) model.The Time Series Modeling platform in JMP allows a group of ARIMA models to be fit by specifying ranges for the AR, difference, and MA terms.Table 5.10 summarizes the fits obtained for a constant difference (d = 1), and both AR (p) and MA (q) parameters ranging from 0 to 2.    In terms of the model summary statistics variance of the errors, AIC and mean absolute prediction error (MAPE) several models look potentially reasonable.For the ARIMA(1, 1, 1) we obtained the following results from JMP:   The lag 2 AR parameter is highly significant.Figure 5.35 presents the plots of the ACF, PACF, and variogram of the residuals from ARIMA(2, 1, 1).Other residual plots are shown in Figure 5.36.Based on the significant lag 2 AR parameter, this model is preferable to the ARIMA(1, 1, 1) model fit previously.Considering the variation in counts by day of week that was observed previously, a seasonal ARIMA model with a seasonal period of 7 days may be appropriate.The resulting model has an error variance of 50.9, smaller than for the ARIMA(1, 1, 1) and ARIMA(2, 1, 1) models.The AIC is also smaller.Notice that all of the model parameters are highly significant.The residual ACF, PACF, and variogram shown inARIMA models (a.k.a.Box-Jenkins models) present a very powerful and flexible class of models for time series analysis and forecasting.Over the years, they have been very successfully applied to many problems in research and practice.However, there might be certain situations where they may fall short on providing the "right" answers.For example, in ARIMA models, forecasting future observations primarily relies on the past data and implicitly assumes that the conditions at which the data is collected will remain the same in the future as well.In many situations this assumption may (and most likely will) not be appropriate.For those cases, the transfer function-noise models, where a set of input variables that may have an effect on the time series are added to the model, provide suitable options.We shall discuss these models in the next chapter.For an excellent discussion of this matter and of time series analysis and forecasting in general, see Jenkins (1979).We first fit an ARIMA(1,0,0) model to the data using arima function in the stats package.#4-in-1 plot of the residuals par(mfrow=c(2,2),oma=c(0,0,0,0)) qqnorm (res.dji.ar1,datax=TRUE,pch=16,xlab='Residual',main='') qqline(res.dji.ar1,datax=TRUE) plot(fit.dji.ar1,res.dji.ar1,pch=16,xlab='Fitted Value',ylab='Residual') abline(h=0) hist(res.dji.ar1,col="gray",xlab='Residual',main='')plot (res.dji.ar1,type="l",xlab='Observation Order',ylab='Residual') points(res.dji.ar1,pch=16,cex=.5)abline(h=0) We now consider the first difference of the Dow Jones index.wt.dji<-diff(dji.data[,2])plot (wt.dji,type="o",pch=16,cex=.5,xlab='Date',ylab='w(t) ', xaxt='n') axis (1, seq(1,85,12), dji.data[seq(1,85,12),1])Jun-01 Jun-03 Date Example 5.6 The loan applications data are in the second column of the array called loan.data in which the first column is the number of weeks.We use the AR(2) model to make the forecasts.forecasting for these data.Explain how prediction intervals would be computed.B.13 presents data on ice cream and frozen yogurt sales.Develop an appropriate ARIMA model and a procedure for forecasting for these data.Explain how prediction intervals would be computed.B.14 presents the CO 2 readings from Mauna Loa.Develop an appropriate ARIMA model and a procedure for forecasting for these data.Explain how prediction intervals would be computed.B.15 presents data on the occurrence of violent crimes.Develop an appropriate ARIMA model and a procedure for forecasting for these data.Explain how prediction intervals would be computed.B.16 presents data on the US gross domestic product (GDP).Develop an appropriate ARIMA model and a procedure for forecasting for these data.Explain how prediction intervals would be computed.Total annual energy consumption is shown in Table B.17. Develop an appropriate ARIMA model and a procedure for forecasting for these data.Explain how prediction intervals would be computed.B.18 contains data on coal production.Develop an appropriate ARIMA model and a procedure for forecasting for these data.Explain how prediction intervals would be computed.B.19 contains data on the number of children 0-4 years old who drowned in Arizona.Develop an appropriate ARIMA model and a procedure for forecasting for these data.Explain how prediction intervals would be computed.Data on tax refunds and population are shown in Table B.20.Develop an appropriate ARIMA model and a procedure for forecasting for these data.Explain how prediction intervals would be computed.CPI Jan-2003Jan-1999Jan-1995 If the roots obtained byHence if the roots m 1 and m 2 are both less than 1 in absolute value, then the AR(2) model is causal and stationary.Note that if the roots of Eq. (5.36) are complex conjugates of the form a Â± ib, the condition for stationarity is that, has an infinite MA representation as in Eq. (5.30).This implies that for the second-order autoregressive process to be stationary, the parameters ðœ™ 1 and ðœ™ 2 must satisfy.Now that we have established the conditions for the stationarity of an AR(2) time series, let us now consider its mean, autocovariance, and autocorrelation functions.From Eq. (5.27), we have(5.37)Note that for 1 âˆ’ ðœ™ 1 âˆ’ ðœ™ 2 = 0, m = 1 is one of the roots for the associated polynomial in Eq. (5.36) and hence the time series is deemed nonstationary.The autocovariance function is Example 5.2 The Dow Jones index data are in the second column of the array called dji.data in which the first column is the month of the year.We first plot the data as well as the ACF and PACF.plot (dji.data[,2],type="o",pch=16,cex=.5,xlab='Date',ylab='DJI',xaxt='n')axis (1, seq(1,85,12), dji.data[seq(1,85,12),1])Example 5.8 The clothing sales data are in the second column of the array called closales.data in which the first column is the month of the year.We first plot the data and its ACF and PACF.par(mfrow=c(1,2),oma=c(0,0,0,0)) acf(closales.data[,2],lag.max=50,type="correlation",main="ACF for the \n Clothing Sales") acf(closales.data[,2],lag.max=50,type="partial",main="PACF for the \n Clothing Sales") We now take the seasonal and non-seasonal difference of the data.par(mfrow=c(1,2),oma=c(0,0,0,0)) acf(wt.closales,lag.max=50,type="correlation",main="ACF for w(t)") acf(wt.closales,lag.max=50,type="partial",main="PACF for w(t)") We now fit a seasonal ARIMA(0,1,1) Ã— (0,1,1) 12 model to the data.We then plot the residuals plots including ACF and PACF of the residuals.In the end we plot the true and fitted values.smoothing with ðœ† = 0.2 to smooth the data, and to forecast the last 20 observations.Compare the ARIMA and exponential smoothing forecasts.Which forecasting method do you prefer?5.11 Table B.2 contains data on pharmaceutical product sales.a. Fit an ARIMA model to this time series, excluding the last 10 observations.Investigate model adequacy.Explain how this model would be used for forecasting.b.Forecast the last 10 observations.c.In Exercise 4.12, you were asked to use simple exponential smoothing with ðœ† = 0.1 to smooth the data, and to forecast the last 10 observations.Compare the ARIMA and exponential smoothing forecasts.Which forecasting method do you prefer?d.How would prediction intervals be obtained for the ARIMA forecasts?Reconsider the blue and gorgonzola cheese data in Table B.4 and Exercise 5.13.In Exercise 4.17 you were asked to take the first difference of this data and develop a forecasting procedure based on using exponential smoothing on the first differences.Compare this procedure with the ARIMA model of Exercise 5.13.B.5 shows US beverage manufacturer product shipments.Develop an appropriate ARIMA model and a procedure for forecasting for these data.Explain how prediction intervals would be computed.B.6 contains data on the global mean surface air temperature anomaly.Develop an appropriate ARIMA model and a procedure for forecasting for these data.Explain how prediction intervals would be computed.Reconsider the global mean surface air temperature anomaly data shown in Table B.6 and used in Exercise 5.16.In Exercise 4.20 you were asked to use simple exponential smoothing with the optimum value of ðœ† to smooth the data.Compare the results with those obtained with the ARIMA model in Exercise 5.16.Market.Develop an appropriate ARIMA model and a procedure for these data.Explain how prediction intervals would be computed.Compare the results with those obtained from the ARIMA model in Exercise 5.18.Unemployment rate data is given in Table B.8. Develop an appropriate ARIMA model and a procedure for forecasting for these data.Explain how prediction intervals would be computed.Reconsider the unemployment rate data shown in Table B.8 and used in Exercise 5.21.In Exercise 4.24 you used simple exponential smoothing with the optimum value of ðœ† to smooth the data.Compare the results with those obtained from the ARIMA model in Exercise 5.20.B.9 contains yearly data on the international sunspot numbers.Develop an appropriate ARIMA model and a procedure for forecasting for these data.Explain how prediction intervals would be computed.The following ARIMA model has been fit to a time series:Suppose that we are at the end of time period T = 100 and we know that the forecast for period 100 was 130 and the actual observed value was y 100 = 140.Determine forecasts for periods 101, 102, 103, â€¦ from this model at origin 100.b.What is the shape of the forecast function from this model?c.Suppose that the observation for time period 101 turns out to be y 101 = 132.Revise your forecasts for periods 102, 103, â€¦ using period 101 as the new origin of time.d.If your estimate Ïƒ2 = 1.5, find a 95% prediction interval on the forecast of period 101 made at the end of period 100.The following ARIMA model has been fit to a time series:a. Suppose that we are at the end of time period T = 100 and we know that the observed forecast error for period 100 was 0.5 and for period 99 we know that the observed forecast error was âˆ’0.8.Determine forecasts for periods 101, 102, 103, â€¦ from this model at origin 100.b.What is the shape of the forecast function that evolves from this model?c.Suppose that the observations for the next four time periods turn out to be 17.5, 21.25, 18.75, and 16.75.Revise your forecasts for periods 102, 103, â€¦ using a rolling horizon approach.d.If your estimate Ïƒ = 0.5, find a 95% prediction interval on the forecast of period 101 made at the end of period 100.EXERCISES 423The following ARIMA model has been fit to a time series:a. Suppose that we are at the end of time period T = 100 and we know that the observed forecast error for period 100 was 2. Determine forecasts for periods 101, 102, 103, â€¦ from this model at origin 100.b.What is the shape of the forecast function from this model?c.Suppose that the observations for the next four time periods turn out to be 53, 55, 46, and 50.Revise your forecasts for periods 102, 103, â€¦ using a rolling horizon approach.d.If your estimate Ïƒ = 1, find a 95% prediction interval on the forecast of period 101 made at the end of period 100.For each of the ARIMA models shown below, give the forecasting equation that evolves for lead times ðœ = 1, 2, â€¦ , L. In each case, explain the shape of the resulting forecast function over the forecast lead time.He uses statistics as a drunken man uses lamp posts -For support rather than illumination Andrew Lang, Scottish poetThe ARIMA models discussed in the previous chapter represent a general class of models that can be used very effectively in time series modeling and forecasting problems.An implicit assumption in these models is that the conditions under which the data for the time series process is collected remain the same.If, however, these conditions change over time, ARIMA models can be improved by introducing certain inputs reflecting these changes in the process conditions.This will lead to what is known as transfer function-noise models.These models can be seen as regression models in Chapter 3 with serially dependent response, inputs, and the error term.The identification and the estimation of these models can be challenging.Furthermore, not all standard statistical software packages possess the capability to fit such models.So far in this book, we have usedIn Section 5.2, we discussed the linear filter and defined it asFollowing the definition of a linear filter, Eq. (6.1) is:1. Time-invariant as the coefficients {v i } do not depend on time.2. Physically realizable if v i = 0 for i < 0; that is, the output y t is a linear function of the current and past values of the input:There are two interesting special cases for the input x t :Impulse Response Function.If x t is a unit impulse at time t = 0, that is,Therefore the coefficients v i in Eq. ( 6.2) are also called the impulse response function.Step Response Function.If x t is a unit step, that is,{ 0, t < 0 1, t â‰¥ 0 (6.5) then the output y t is (6.6) which is also called the step response function.A generalization of the step response function is obtained when Eq. (6.5) is modified so that x t is kept at a certain target value X after t â‰¥ 0; that is,{ 0, t < 0 X, t â‰¥ 0. (6.7)Hence we havewhere g is called the steady-state gain.A more realistic representation of the response is obtained by adding a noise or disturbance term to Eq. (6.2) to account for unanticipated and/or ignored factors that may have an effect on the response as well.Hence the "additive" model representation of the dynamic systems is given as y t = v(B)x t + N t , (6.9)where N t represents the unobservable noise process.In Eq. (6.9), x t and N t are assumed to be independent.The model representation in Eq. (6.9) is also called the transfer function-noise model.Since the noise process is unobservable, the predictions of the response can be made by estimating the impulse response function {v t } .Similar to our discussion about the estimation of the coefficients in Wold's decomposition theorem in Chapter 5, attempting to estimate the infinitely many coefficients in {v t } is a futile exercise.Therefore also parallel to the arguments we made in Chapter 5, we will make assumptions about these infinitely many coefficients to be able to represent them with only a handful of parameters.Following the derivations we had for the ARMA models, we will assume that the coefficients in {v t } have a structure and can be represented as(6.10)The interpretation of Eq. (6.10) is quite similar to the one we had for ARMA models; the denominator summarizes the infinitely many coefficients with a certain structure determined by {ð›¿ i } as in the AR part of the ARMA model and the numerator represents the adjustment we may like to make to the strictly structured infinitely many coefficients as in the MA part of the ARMA model.So the transfer function-noise model in Eq. (6.9) can be rewritten aswhere w(B)âˆ•ð›¿(B) = ð›¿(B) âˆ’1 w(B) = âˆ‘ +âˆž i=0 v i B i .For some processes, there may also be a delay before a change in the input x t shows its effect on the response y t .If we assume that there is b time units of delay between the response and the input, a more general representation for the transfer function-noise models can be obtained as  (6.11)CPI Jan-2003Jan-1999Jan-1995 If the roots obtained byHence if the roots m 1 and m 2 are both less than 1 in absolute value, then the AR(2) model is causal and stationary.Note that if the roots of Eq. (5.36) are complex conjugates of the form a Â± ib, the condition for stationarity is that, has an infinite MA representation as in Eq. (5.30).This implies that for the second-order autoregressive process to be stationary, the parameters ðœ™ 1 and ðœ™ 2 must satisfy.Now that we have established the conditions for the stationarity of an AR(2) time series, let us now consider its mean, autocovariance, and autocorrelation functions.From Eq. (5.27), we have(5.37)Note that for 1 âˆ’ ðœ™ 1 âˆ’ ðœ™ 2 = 0, m = 1 is one of the roots for the associated polynomial in Eq. (5.36) and hence the time series is deemed nonstationary.The autocovariance function is Example 5.2 The Dow Jones index data are in the second column of the array called dji.data in which the first column is the month of the year.We first plot the data as well as the ACF and PACF.plot (dji.data[,2],type="o",pch=16,cex=.5,xlab='Date',ylab='DJI',xaxt='n')axis (1, seq(1,85,12), dji.data[seq(1,85,12),1])Example 5.8 The clothing sales data are in the second column of the array called closales.data in which the first column is the month of the year.We first plot the data and its ACF and PACF.par(mfrow=c(1,2),oma=c(0,0,0,0)) acf(closales.data[,2],lag.max=50,type="correlation",main="ACF for the \n Clothing Sales") acf(closales.data[,2],lag.max=50,type="partial",main="PACF for the \n Clothing Sales") We now take the seasonal and non-seasonal difference of the data.par(mfrow=c(1,2),oma=c(0,0,0,0)) acf(wt.closales,lag.max=50,type="correlation",main="ACF for w(t)") acf(wt.closales,lag.max=50,type="partial",main="PACF for w(t)") We now fit a seasonal ARIMA(0,1,1) Ã— (0,1,1) 12 model to the data.We then plot the residuals plots including ACF and PACF of the residuals.In the end we plot the true and fitted values.smoothing with ðœ† = 0.2 to smooth the data, and to forecast the last 20 observations.Compare the ARIMA and exponential smoothing forecasts.Which forecasting method do you prefer?5.11 Table B.2 contains data on pharmaceutical product sales.a. Fit an ARIMA model to this time series, excluding the last 10 observations.Investigate model adequacy.Explain how this model would be used for forecasting.b.Forecast the last 10 observations.c.In Exercise 4.12, you were asked to use simple exponential smoothing with ðœ† = 0.1 to smooth the data, and to forecast the last 10 observations.Compare the ARIMA and exponential smoothing forecasts.Which forecasting method do you prefer?d.How would prediction intervals be obtained for the ARIMA forecasts?Reconsider the blue and gorgonzola cheese data in Table B.4 and Exercise 5.13.In Exercise 4.17 you were asked to take the first difference of this data and develop a forecasting procedure based on using exponential smoothing on the first differences.Compare this procedure with the ARIMA model of Exercise 5.13.B.5 shows US beverage manufacturer product shipments.Develop an appropriate ARIMA model and a procedure for forecasting for these data.Explain how prediction intervals would be computed.B.6 contains data on the global mean surface air temperature anomaly.Develop an appropriate ARIMA model and a procedure for forecasting for these data.Explain how prediction intervals would be computed.Reconsider the global mean surface air temperature anomaly data shown in Table B.6 and used in Exercise 5.16.In Exercise 4.20 you were asked to use simple exponential smoothing with the optimum value of ðœ† to smooth the data.Compare the results with those obtained with the ARIMA model in Exercise 5.16.Market.Develop an appropriate ARIMA model and a procedure for these data.Explain how prediction intervals would be computed.Compare the results with those obtained from the ARIMA model in Exercise 5.18.Unemployment rate data is given in Table B.8. Develop an appropriate ARIMA model and a procedure for forecasting for these data.Explain how prediction intervals would be computed.Reconsider the unemployment rate data shown in Table B.8 and used in Exercise 5.21.In Exercise 4.24 you used simple exponential smoothing with the optimum value of ðœ† to smooth the data.Compare the results with those obtained from the ARIMA model in Exercise 5.20.B.9 contains yearly data on the international sunspot numbers.Develop an appropriate ARIMA model and a procedure for forecasting for these data.Explain how prediction intervals would be computed.The following ARIMA model has been fit to a time series:Suppose that we are at the end of time period T = 100 and we know that the forecast for period 100 was 130 and the actual observed value was y 100 = 140.Determine forecasts for periods 101, 102, 103, â€¦ from this model at origin 100.b.What is the shape of the forecast function from this model?c.Suppose that the observation for time period 101 turns out to be y 101 = 132.Revise your forecasts for periods 102, 103, â€¦ using period 101 as the new origin of time.d.If your estimate Ïƒ2 = 1.5, find a 95% prediction interval on the forecast of period 101 made at the end of period 100.The following ARIMA model has been fit to a time series:a. Suppose that we are at the end of time period T = 100 and we know that the observed forecast error for period 100 was 0.5 and for period 99 we know that the observed forecast error was âˆ’0.8.Determine forecasts for periods 101, 102, 103, â€¦ from this model at origin 100.b.What is the shape of the forecast function that evolves from this model?c.Suppose that the observations for the next four time periods turn out to be 17.5, 21.25, 18.75, and 16.75.Revise your forecasts for periods 102, 103, â€¦ using a rolling horizon approach.d.If your estimate Ïƒ = 0.5, find a 95% prediction interval on the forecast of period 101 made at the end of period 100.EXERCISES 423The following ARIMA model has been fit to a time series:a. Suppose that we are at the end of time period T = 100 and we know that the observed forecast error for period 100 was 2. Determine forecasts for periods 101, 102, 103, â€¦ from this model at origin 100.b.What is the shape of the forecast function from this model?c.Suppose that the observations for the next four time periods turn out to be 53, 55, 46, and 50.Revise your forecasts for periods 102, 103, â€¦ using a rolling horizon approach.d.If your estimate Ïƒ = 1, find a 95% prediction interval on the forecast of period 101 made at the end of period 100.For each of the ARIMA models shown below, give the forecasting equation that evolves for lead times ðœ = 1, 2, â€¦ , L. In each case, explain the shape of the resulting forecast function over the forecast lead time.He uses statistics as a drunken man uses lamp posts -For support rather than illumination Andrew Lang, Scottish poetThe ARIMA models discussed in the previous chapter represent a general class of models that can be used very effectively in time series modeling and forecasting problems.An implicit assumption in these models is that the conditions under which the data for the time series process is collected remain the same.If, however, these conditions change over time, ARIMA models can be improved by introducing certain inputs reflecting these changes in the process conditions.This will lead to what is known as transfer function-noise models.These models can be seen as regression models in Chapter 3 with serially dependent response, inputs, and the error term.The identification and the estimation of these models can be challenging.Furthermore, not all standard statistical software packages possess the capability to fit such models.So far in this book, we have usedIn Section 5.2, we discussed the linear filter and defined it asFollowing the definition of a linear filter, Eq. (6.1) is:1. Time-invariant as the coefficients {v i } do not depend on time.2. Physically realizable if v i = 0 for i < 0; that is, the output y t is a linear function of the current and past values of the input:There are two interesting special cases for the input x t :Impulse Response Function.If x t is a unit impulse at time t = 0, that is,Therefore the coefficients v i in Eq. ( 6.2) are also called the impulse response function.Step Response Function.If x t is a unit step, that is,{ 0, t < 0 1, t â‰¥ 0 (6.5) then the output y t is (6.6) which is also called the step response function.A generalization of the step response function is obtained when Eq. (6.5) is modified so that x t is kept at a certain target value X after t â‰¥ 0; that is,{ 0, t < 0 X, t â‰¥ 0. (6.7)Hence we havewhere g is called the steady-state gain.A more realistic representation of the response is obtained by adding a noise or disturbance term to Eq. (6.2) to account for unanticipated and/or ignored factors that may have an effect on the response as well.Hence the "additive" model representation of the dynamic systems is given as y t = v(B)x t + N t , (6.9)where N t represents the unobservable noise process.In Eq. (6.9), x t and N t are assumed to be independent.The model representation in Eq. (6.9) is also called the transfer function-noise model.Since the noise process is unobservable, the predictions of the response can be made by estimating the impulse response function {v t } .Similar to our discussion about the estimation of the coefficients in Wold's decomposition theorem in Chapter 5, attempting to estimate the infinitely many coefficients in {v t } is a futile exercise.Therefore also parallel to the arguments we made in Chapter 5, we will make assumptions about these infinitely many coefficients to be able to represent them with only a handful of parameters.Following the derivations we had for the ARMA models, we will assume that the coefficients in {v t } have a structure and can be represented as(6.10)The interpretation of Eq. (6.10) is quite similar to the one we had for ARMA models; the denominator summarizes the infinitely many coefficients with a certain structure determined by {ð›¿ i } as in the AR part of the ARMA model and the numerator represents the adjustment we may like to make to the strictly structured infinitely many coefficients as in the MA part of the ARMA model.So the transfer function-noise model in Eq. (6.9) can be rewritten aswhere w(B)âˆ•ð›¿(B) = ð›¿(B) âˆ’1 w(B) = âˆ‘ +âˆž i=0 v i B i .For some processes, there may also be a delay before a change in the input x t shows its effect on the response y t .If we assume that there is b time units of delay between the response and the input, a more general representation for the transfer function-noise models can be obtained as  (6.11)