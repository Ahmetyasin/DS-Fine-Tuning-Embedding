It is a capital mistake to theorize before one has data.We are surrounded by data, but starved for insights.This book is based on lecture notes prepared for use in the 2023 ASU research-oriented course on Reinforcement Learning (RL) that I have offered in each of the last five years, as the field was rapidly evolving.The purpose of the book is to give an overview of the RL methodology, particularly as it relates to problems of optimal and suboptimal control, as well as discrete optimization.More broadly, we will aim to provide a framework for structured thinking about RL and its connections to the decision and control methodology, which is couched on, but is not dominated by mathematics.Generally, RL can be viewed as the art and science of sequential decision making for large and difficult problems, often in the presence of imprecisely known and changing environment conditions.Dynamic Programming (DP) is a broad and well-established algorithmic methodology for making optimal sequential decisions, and is the theoretical foundation upon which RL rests.This is unlikely to change in the future, despite the rapid pace of technological innovation.In fact, there are strong connections between sequential decision making and the new wave of technological change, generative technology, transformers, GPT applications, and natural language processing ideas, as we will aim to show in this book.In DP there are two principal objects to compute: the optimal value function that provides the optimal cost that can be attained starting from any given initial state, and the optimal policy that provides the optimal decision to apply at any given state and time.Unfortunately, the exact application of DP runs into formidable computational difficulties, commonly referred to as the curse of dimensionality.To address these, RL aims to approximate the optimal value function and policy, by using manageable off-line and/or on-line computation, which often involves neural networks (hence the alternative name Neuro-Dynamic Programming [BeT96]).Thus there are two major methodological approaches in RL: approximation in value space, where we approximate in some way the optimal value function, and approximation in policy space, whereby we construct a suboptimal policy by using some form of optimization over a suitably restricted class of policies.In some schemes these approximation approaches may be combined, aiming to capitalize on the advantages of both.Generally, approximation in value space is tied more closely to the central DP ideas of value and policy iteration than approximation in policy space, which in practice often relies on gradient-like descent, a more broadly applicable optimization methodology.The book focuses primarily on approximation in value space, with limited coverage of approximation in policy space.However, it is structured so that it can be easily supplemented by an instructor who wishes to go into approximation in policy space in greater detail, using any of a number of available sources.An important part of our line of development is a new conceptual framework, which aims to bridge the gaps between the artificial intelligence, control theory, and operations research views of our subject.This framework, the focus of the author's recent monograph "Lessons from Alp-haZero ...", [Ber22a], centers on approximate forms of DP that are inspired by some of the major successes of RL involving games.Primary examples are the recent (2017) AlphaZero program (which plays chess), and the similarly structured and earlier (1990s) TD-Gammon program (which plays backgammon).Our framework is couched on two general algorithms that are designed largely independently of each other and operate in synergy through the powerful mechanism of Newton's method, applied to the fundamental Bellman equation of DP.We call these the off-line training and the on-line play algorithms.In the AlphaZero and TD-Gammon game contexts, the off-line training algorithm is the method used to teach the program how to evaluate positions and to generate good moves at any given position, while the on-line play algorithm is the method used to play in real time against human or computer opponents.Our synergistic view of off-line training and on-line play is motivated by some striking empirical observations.In particular, both AlphaZero and TD-Gammon were trained off-line extensively using neural networks and an approximate version of the fundamental DP algorithm of policy iteration.Yet the AlphaZero player that was obtained off-line is not used directly during on-line play (it is too inaccurate due to approximation errors that are inherent in off-line neural network training).Instead, a separate on-line player is used to select moves, based on multistep lookahead minimization and a terminal position evaluator that was trained using experience with the off-line player.The on-line player performs a form of policy improvement, which is not degraded by neural network approximations.As a result, it greatly improves the performance of the off-line player.Similarly, TD-Gammon performs on-line a policy improvement step using one-step or two-step lookahead minimization, which is not degraded by neural network approximations.To this end, it uses an off-line neural network-trained terminal position evaluator, and importantly it also extends its on-line lookahead by rollout (simulation with the one-step looka-head player that is based on the position evaluator).Thus in summary:(a) The on-line player of AlphaZero plays much better than its extensively trained off-line player.This is due to the beneficial effect of exact policy improvement with long lookahead minimization, which corrects for the inevitable imperfections of the neural network-trained off-line player, and position evaluator/terminal cost approximation.(b) The TD-Gammon player that uses long rollout plays much better than TD-Gammon without rollout.This is due to the beneficial effect of the rollout, which serves as a supplement and substitute for long lookahead minimization.An important lesson from AlphaZero and TD-Gammon is that the performance of an off-line trained policy can be greatly improved by on-line approximation in value space, with long lookahead (involving minimization or rollout with the off-line policy, or both), and terminal cost approximation that is obtained off-line.This performance enhancement is often dramatic and is due to a simple fact, which is couched on algorithmic mathematics and is a focal point of our course: approximation in value space with onestep lookahead minimization amounts to a step of Newton's method for solving Bellman's equation, while the starting point for the Newton step is based on the results of off-line training, and may be enhanced by longer lookahead minimization and on-line rollout .Indeed the major determinant of the quality of the on-line policy is the Newton step that is performed on-line, while off-line training plays a secondary role by comparison.Significantly, the synergy between off-line training and on-line play also underlies Model Predictive Control (MPC), a major control system design methodology that has been extensively developed since the 1980s.This synergy is evident in the context of our narrative and helps to explain the all-important stability issues within the MPC context.An additional benefit of policy improvement by approximation in value space, not observed in the context of games (which have stable rules and environment), is that it works well with changing problem parameters and on-line replanning, similar to the methodology of indirect adaptive control.In particular, the Bellman equation is perturbed due to the parameter changes, but approximation in value space still operates as a Newton step.An essential requirement here is that a system model is estimated on-line through some identification method, and is used during the one-step or multistep lookahead minimization process.In this book, we will describe the basic RL methodologies, and we will aim to explain (often with visualization) their effectiveness (or lack thereof) in the context of the synergy between off-line training and on-line play.In the process, we will bring out the strong connections between the artificial intelligence view of RL, the control theory views of MPC and adaptive control, and the operations research view of discrete optimization algorithms.Moreover, we will describe a broad variety of algorithms (especially truncated rollout, but also other methods) that can be used for on-line play.In particular, we will aim to show that the methodology of approximation in value space and rollout applies very broadly to deterministic and stochastic optimal control problems, involving both discrete and continuous search spaces, as well as finite and infinite horizon.We will also show that in addition to MPC and adaptive control, our conceptual framework can be effectively integrated with other important methodologies such as multiagent systems and decentralized control, discrete and Bayesian optimization, and heuristic algorithms for discrete optimization.In this book, we will deemphasize mathematical proofs, and focus instead on visualizations and intuitive (but still rigorous) explanations.However, there is considerable related analysis, which supports our narrative, and can be found within a broad range of sources, which we describe next.The author's approximate DP/RL books [1] Bertsekas, D. P., 2019.Reinforcement Learning and Optimal Control, Athena Scientific, Belmont, MA.[2] Bertsekas, D. P., 2020.Rollout, Policy Iteration, and Distributed Reinforcement Learning, Athena Scientific, Belmont, MA.provide a far more detailed discussion of MPC, adaptive control, discrete optimization, and distributed computation topics, than the present book.Moreover, some other popular methods, such as temporal difference algorithms and Q-learning, are discussed in the books [1] and [2], but not in the present book.The author's two-volume DP book(a) To provide an overview of the exact dynamic programming (DP) methodology, with a view towards suboptimal solution methods.We will first discuss finite horizon problems, which involve a finite sequence of successive decisions, and are thus conceptually and analytically simpler.We will consider separately deterministic and stochastic finite horizon problems (Sections 1.2 and 1.3, respectively).The reason is that deterministic problems are simpler and have some favorable characteristics, which allow the application of a broader variety of methods.Significantly they include challenging discrete and combinatorial optimization problems, which can be fruitfully addressed with some of the reinforcement learning (RL) methods that are the main subject of the book.We will also discuss somewhat briefly the more intricate infinite horizon methodology (Section 1.4), and refer to the author's DP textbooks [Ber12], [Ber17a], the RL books [Ber19a],[Ber20a], and the neuro-dynamic programming monograph [BeT96] for a fuller presentation.(b) To discuss in summary the principal RL methodologies, with primary emphasis on approximation in value space.This is the architecture that underlies the AlphaZero, AlphaGo, TD-Gammon and other related programs, as well as the Model Predictive Control (MPC) methodology, one of the principal control system design methods.We will also argue later (Chapter 2) that approximation in value space provides the entry point for the use of RL methods for solving discrete optimization and integer programming problems.(c) To explain the major principles of approximation in value space, and its division into the off-line training and the on-line play algorithms.A key idea here is the connection of these two algorithms through the algorithmic methodology of Newton's method for solving the problem's Bellman equation.This viewpoint, recently developed in the author's "Rollout and Policy Iteration ..." book [Ber20a] and the visually oriented "Lessons from AlphaZero ..." monograph [Ber22a], underlies the entire course and is discussed for the simple, intuitive, and important class of linear quadratic problems in Section 1.5.(d) To overview the range of problem types where our RL methods apply, and to explain some of their major algorithmic ideas (Section 1.6).Included here are partial state observation problems (POMDP), multiagent problems, and problems with unknown model parameters, which can be addressed with adaptive control methods.We will also discuss selectively in this chapter some major algorithmic topics in approximate DP and RL, including rollout and policy iteration.A broader discussion of DP/RL may be found in the RL books [Ber19a], [Ber20a], the DP textbooks [Ber12], [Ber17a], the neuro-dynamic program-To understand the overall structure of AlphaZero, and its connection to our DP/RL methodology, it is useful to divide its design into two parts: off-line training, which is an algorithm that learns how to evaluate chess positions, and how to steer itself towards good positions with a default/base chess player, and on-line play, which is an algorithm that generates good moves in real time against a human or computer opponent, using the training it went through off-line.We will next briefly describe these algorithms, and relate them to DP concepts and principles.This is the part of the program that learns how to play through off-line self-training, and is illustrated in Fig. 1.1.1.The algorithm generates a sequence of chess players and position evaluators.A chess player assigns "probabilities" to all possible moves at any given chess position (these are the probabilities with which the player selects the possible moves at the given position).A position evaluator assigns a numerical score to any given chess position (akin to a "probability" of winning the game from that position), and thus predicts quantitatively the performance of a player starting from any position.The chess player and the position evaluator are represented by two neural networks, a policy network and a value network , which accept a chess position and generate a set of move probabilities and a position evaluation, respectively.†In the more conventional DP-oriented terms of this book, a position is the state of the game, a position evaluator is a cost function that gives (an estimate of) the optimal cost-to-go at a given state, and the chess player is a randomized policy for selecting actions/controls at a given state.‡ of Tetris, also based on the method of policy iteration, is described by Scherrer et al. [SGG15], who mention several related antecedent works.For a better understanding of the connections of AlphaZero and AlphaGo Zero with Tesauro's programs and the concepts developed here, the "Methods" section of the paper [SSS17] is recommended.† Here the neural networks play the role of function approximators; see Chapter 3. By viewing a player as a function that assigns move probabilities to a position, and a position evaluator as a function that assigns a numerical score to a position, the policy and value networks provide approximations to these functions based on training with data (training algorithms for neural networks and other approximation architectures are also discussed in the RL books [Ber19a], [Ber20a], and the neuro-dynamic programming book [BeT96]).‡ One more complication is that chess and Go are two-player games, while most of our development will involve single-player optimization.However, DP theory extends to two-player games, although we will not focus on this extension.Alternately, we can consider training a game program to play against a known fixed opponent; this is a one-player setting.Chap. 1Self-Learning/Policy Iteration Constraint Relaxation  The overall training algorithm is a form of policy iteration, a classical DP algorithm that will be of primary interest to us in this book.Starting from a given player, it repeatedly generates (approximately) improved players, and settles on a final player that is judged empirically to be "best" out of all the players generated.† Policy iteration may be separated conceptually in two stages (see Fig. 1.1.1).(a) Policy evaluation: Given the current player and a chess position, the outcome of a game played out from the position provides a single data point.Many data points are collected and used to train a value network, whose output serves as the position evaluator for that player.(b) Policy improvement : Given the current player and its position evaluator, trial move sequences are selected and evaluated for the remainder of the game starting from many positions.An improved player is then generated by adjusting the move probabilities of the current player towards the trial moves that have yielded the best results.In Alp- † Quoting from the paper [SSS17]: "The AlphaGo Zero selfplay algorithm can similarly be understood as an approximate policy iteration scheme in which MCTS is used for both policy improvement and policy evaluation.Policy improvement starts with a neural network policy, executes an MCTS based on that policy's recommendations, and then projects the (much stronger) search policy back into the function space of the neural network.Policy evaluation is applied to the (much stronger) search policy: the outcomes of selfplay games are also projected back into the function space of the neural network.These projection steps are achieved by training the neural network parameters to match the search probabilities and selfplay game outcome respectively."Note, however, that a twoperson game player, trained through selfplay, may fail against a particular human or computer player that can exploit training vulnerabilities.This is a theoretical but rare possibility; see our discussion in Section 2.12.AlphaZero, Off-Line Training, and On-Line Play 7 haZero this is done with a complicated algorithm called Monte Carlo Tree Search.However, policy improvement can also be done more simply.For example one could try all possible move sequences from a given position, extending forward to a given number of moves, and then evaluate the terminal position with the player's position evaluator.The move evaluations obtained in this way are used to nudge the move probabilities of the current player towards more successful moves, thereby obtaining data that is used to train a policy network that represents the new player.Tesauro's TD-Gammon algorithm [Tes94] program is similarly based on approximate policy iteration, but uses a different methodology for approximate policy evaluation [it is based on the TD(λ) algorithm]; see the book [BeT96], Section 8.6, for a detailed description.Moreover, it does not use a policy network and MCTS.It involves only a value network, which replicates the functionality of a policy network by generating moves on-line via a one-step or two-step lookahead minimization.Suppose that a "final" player has been obtained through the AlphaZero offline training process just described.It could then be used in principle to play chess against any human or computer opponent, since it is capable of generating move probabilities at each given chess position using its policy network.In particular, during on-line play, at a given position the player can simply choose the move of highest probability supplied by the off-line trained policy network.This player would play very fast on-line, but it would not play good enough chess to beat strong human opponents.The extraordinary strength of AlphaZero is attained only after the player and its position evaluator obtained from off-line training have been embedded into another algorithm, which we refer to as the "on-line player."Given the policy network/player obtained off-line and its value network/position evaluator, this algorithm plays as follows (see Fig. 1.1.2).At a given position, it generates a lookahead tree of all possible multiple move and countermove sequences, up to a given depth.It then runs the off-line obtained player for some more moves, and then evaluates the effect of the remaining moves by using the position evaluator of the off-line obtained value network.Actually the middle portion, called "truncated rollout," is not used in the published version of AlphaZero/chess [SHS17], [SHS17]; the first portion (multistep lookahead) is quite long and implemented efficiently, so that the rollout portion is not essential.Rollout is used in AlphaGo [SHM16], and plays a very important role the final version of Tesauro's backgammon program [TeG96].The reason is that in backgammon, long multistep lookahead is not possible because of rapid expansion of the lookahead tree with every move.  .At a given position, it generates a lookahead tree of multiple moves up to some depth, then runs the off-line obtained player for some more moves, and evaluates the effect of the remaining moves by using the position evaluator of the off-line player.We should note that the preceding description of AlphaZero and related games is oversimplified.We will be discussing refinements and details as the book progresses.However, DP ideas with cost function approximations, similar to the on-line player illustrated in Fig. 1.1.2,will be central.Moreover, the algorithmic division between off-line training and on-line policy implementation will be conceptually very important for our purposes in this book.Note that the off-line training and the on-line play algorithms may be decoupled and may be designed independently.For example the off-line training portion may be very simple, such as using a simple known policy for rollout without truncation, or without terminal cost approximation.Conversely, a sophisticated process may be used for off-line training of a terminal cost function approximation, which is used immediately following one-step or multistep lookahead in a value space approximation scheme.In control system design, similar architectures to the ones of Alp-haZero and TD-Gammon are employed in model predictive control (MPC).There, the number of steps in lookahead minimization is called the control interval , while the total number of steps in lookahead minimization and truncated rollout is called the prediction interval ; see e.g., Magni et al. [MDM01].† The benefit of truncated rollout in providing an economical substitute for longer lookahead minimization is well known within this † The Matlab toolbox for MPC design explicitly allows the user to set these two intervals.Starting from state x k , the next state under control u k is generated nonrandomly, according toand a stage cost g k (x k , u k ) is incurred.context.Dynamic programming frameworks with cost function approximations that are similar to the on-line player illustrated in Fig. 1.1.2,are also known as approximate dynamic programming, or neuro-dynamic programming, and will be central for our purposes.They will be generically referred to as approximation in value space in this book.†In all DP problems, the central object is a discrete-time dynamic system that generates a sequence of states under the influence of control.The system may evolve deterministically or randomly (under the additional influence of a random disturbance).In finite horizon problems the system evolves over a finite number N of time steps (also called stages).The state and control at time k of the system will be generally denoted by x k and u k , respectively.In deterministic systems, x k+1 is generated nonrandomly, i.e., it is determined solely by x k and u k ; † The names "approximate dynamic programming" and "neuro-dynamic programming" are often used as synonyms to RL.However, RL is generally thought to also subsume the methodology of approximation in policy space, which involves search for optimal parameters within a parametrized set of policies.The search is done with methods that are largely unrelated to DP, such as for example stochastic gradient or random search methods.Approximation in policy space may be used off-line to design a policy that can be used for on-line rollout.It will be discussed rather briefly here, but a fuller account that is consistent in terminology with the present book may be found in Chapter 5 of the RL book [Ber19a].Chap. 1 see Fig. 1.2.1.Thus, a deterministic DP problem involves a system of the form x k+1 = f k (x k , u k ), k= 0, 1, . . ., N − 1, (where k is the time index,x k is the state of the system, an element of some space, u k is the control or decision variable, to be selected at time k from some given set U k (x k ) that depends on x k , f k is a function of (x k , u k ) that describes the mechanism by which the state is updated from time k to time k + 1, N is the horizon, i.e., the number of times control is applied.In the case of a finite number of states, the system function f k may be represented by a table that gives the next state x k+1 for each possible value of the pair (x k , u k ).Otherwise a mathematical expression or a computer implementation is necessary to represent f k .The set of all possible x k is called the state space at time k.It can be any set and may depend on k.Similarly, the set of all possible u k is called the control space at time k.Again it can be any set and may depend on k.Similarly the system function f k can be arbitrary and may depend on k. †The problem also involves a cost function that is additive in the sense that the cost incurred at time k, denoted by g k (x k , u k ), accumulates over time.Formally, g k is a function of (x k , u k ) that takes scalar values, and may depend on k.For a given initial state x 0 , the total cost of a control sequence {u 0 , . . ., u N −1 } is J(x 0 ; u 0 , . . ., u N −1 ) = g N (x N ) + N −1 k=0 g k (x k , u k ), (1.2) † This generality is one of the great strengths of the DP methodology and guides the exposition style of this book, and the author's other DP works.By allowing general state and control spaces (discrete, continuous, or mixtures thereof), and a k-dependent choice of these spaces, we can focus attention on the truly essential algorithmic aspects of the DP approach, exclude extraneous assumptions and constraints from our model, and avoid duplication of analysis.The generality of our DP model is also partly responsible for our choice of notation.In the artificial intelligence and operations research communities, finite state models, often referred to as Markovian Decision Problems (MDP), are common and use a transition probability notation (see Section 1.7.2).Unfortunately, this notation is not well suited for deterministic models, and also for continuous spaces models, both of which are important for the purposes of this book.For the latter models, it involves transition probability distributions over continuous spaces, and leads to mathematics that are far more complex as well as less intuitive than those based on the use of the system function (1.1).where g N (x N ) is a terminal cost incurred at the end of the process.This is a well-defined scalar, since the control sequence {u 0 , . . ., u N −1 } together with x 0 determines exactly the state sequence {x 1 , . . ., x N } via the system equation (1.1).We want to minimize the cost (1.2) over all sequences {u 0 , . . ., u N −1 } that satisfy the control constraints, thereby obtaining the optimal value as a function of x 0 : †There are many situations where the state and control spaces are naturally discrete and consist of a finite number of elements.Such problems are often conveniently described with an acyclic graph specifying for each state x k the possible transitions to next states x k+1 .The nodes of the graph correspond to states x k and the arcs of the graph correspond to state-control pairs (x k , u k ).Each arc with start node x k corresponds to a choice of a single control u k ∈ U k (x k ) and has as end node the next stateTo handle the final stage, an artificial terminal node t is added.Each state x N at stage N is connected to the terminal node t with an arc having cost g N (x N ).Note that control sequences {u 0 , . . ., u N −1 } correspond to paths originating at the initial state (a node at stage 0) and terminating at one of the † Here and later we write "min" (rather than "inf") even if we are not sure that the minimum is attained; similarly we write "max" (rather than "sup") even if we are not sure that the maximum is attained.Chap. 1 nodes corresponding to the final stage N .If we view the cost of an arc as its length, we see that a deterministic finite-state finite-horizon problem is equivalent to finding a minimum-length (or shortest) path from the initial nodes of the graph (stage 0) to the terminal node t.Here, by the length of a path we mean the sum of the lengths of its arcs.† Generally, combinatorial optimization problems can be formulated as deterministic finite-state finite-horizon optimal control problems.The idea is to break down the solution into components, which can be computed sequentially.The following is an illustrative example.Suppose that to produce a certain product, four operations must be performed on a given machine.The operations are denoted by A, B, C, and D. We assume that operation B can be performed only after operation A has been performed, and operation D can be performed only after operation C has been performed.(Thus the sequence CDAB is allowable but the sequence † It turns out also that any shortest path problem (with a possibly nonacyclic graph) can be reformulated as a finite-state deterministic optimal control problem.See [Ber17a], Section 2.1, and [Ber91], [Ber98] for extensive accounts of shortest path methods, which connect with our discussion here.Deterministic Dynamic Programming 13 CDBA is not.)The setup cost Cmn for passing from any operation m to any other operation n is given.There is also an initial startup cost SA or SC for starting with operation A or C, respectively (cf.Fig. 1.2.3).The cost of a sequence is the sum of the setup costs associated with it; for example, the operation sequence ACDB has cost SA + CAC + CCD + CDB.We can view this problem as a sequence of three decisions, namely the choice of the first three operations to be performed (the last operation is determined from the preceding three).It is appropriate to consider as state the set of operations already performed, the initial state being an artificial state corresponding to the beginning of the decision process.The possible state transitions corresponding to the possible states and decisions for this problem are shown in Fig. 1.2.3.Here the problem is deterministic, i.e., at a given state, each choice of control leads to a uniquely determined state.For example, at state AC the decision to perform operation D leads to state ACD with certainty, and has cost CCD.Thus the problem can be conveniently represented with the transition graph of Fig. 1.2.3 (which in turn is a special case of the graph of Fig. 1.2.2).The optimal solution corresponds to the path that starts at the initial state and ends at some state at the terminal time and has minimum sum of arc costs plus the terminal cost.In this section we will state the DP algorithm and formally justify it.The algorithm rests on a simple idea, the principle of optimality, which roughly states the following; see Fig. 1.2.4.Let {u * 0 , . . ., u * N −1 } be an optimal control sequence, which together with x 0 determines the corresponding state sequence {x * 1 , . . ., x * N } via the system equation (1.1).Consider the subproblem whereby we start at x * k at time k and wish to minimize the "cost-to-go" from time k to time N ,Then the truncated optimal control sequence {u * k , . . ., u * N −1 } is optimal for this subproblem.The subproblem referred to above is called the tail subproblem that starts at x * k .Stated succinctly, the principle of optimality says that the tail of an optimal sequence is optimal for the tail subproblem.Its intuitive justification is simple.If the truncated control sequence {u * k , . . ., u * N −1 } were not optimal as stated, we would be able to reduce the cost further by switching to an optimal sequence for the subproblem once we reach x * (since the preceding choices of controls, u * 0 , . . ., u * k−1 , do not restrict our future choices).For an auto travel analogy, suppose that the fastest route from Phoenix to Boston passes through St Louis.The principle of optimality translates to the obvious fact that the St Louis to Boston portion of the route is also the fastest route for a trip that starts from St Louis and ends in Boston.†The principle of optimality suggests that the optimal cost function can be constructed in piecemeal fashion going backwards: first compute the optimal cost function for the "tail subproblem" involving the last stage, then solve the "tail subproblem" involving the last two stages, and continue in this manner until the optimal cost function for the entire problem is constructed.The DP algorithm is based on this idea: it proceeds sequentially by solving all the tail subproblems of a given time length, using the solution of the tail subproblems of shorter time length.We illustrate the algorithm with the scheduling problem of Example 1.2.1.The calculations are simple but tedious, and may be skipped without loss of continuity.However, they may be worth going over by a reader that has no prior experience in the use of DP.Let us consider the scheduling Example 1.2.1, and let us apply the principle of optimality to calculate the optimal schedule.We have to schedule optimally the four operations A, B, C, and D. There is a cost for a transition between two operations, and the numerical values of the transition costs are shown in Fig. 1.2.5 next to the corresponding arcs.According to the principle of optimality, the "tail" portion of an optimal schedule must be optimal.For example, suppose that the optimal schedule † In the words of Bellman [Bel57]: "An optimal trajectory has the property that at an intermediate point, no matter how it was reached, the rest of the trajectory must coincide with an optimal trajectory as computed from this intermediate point as the starting point."Next to each node/state we show the cost to optimally complete the schedule starting from that state.This is the optimal cost of the corresponding tail subproblem (cf. the principle of optimality).The optimal cost for the original problem is equal to 10, as shown next to the initial state.The optimal schedule corresponds to the thick-line arcs.is CABD.Then, having scheduled first C and then A, it must be optimal to complete the schedule with BD rather than with DB.With this in mind, we solve all possible tail subproblems of length two, then all tail subproblems of length three, and finally the original problem that has length four (the subproblems of length one are of course trivial because there is only one operation that is as yet unscheduled).As we will see shortly, the tail subproblems of length k + 1 are easily solved once we have solved the tail subproblems of length k, and this is the essence of the DP technique.Tail Subproblems of Length 2 : These subproblems are the ones that involve two unscheduled operations and correspond to the states AB, AC, CA, and CD (see Fig. 1State AB : Here it is only possible to schedule operation C as the next operation, so the optimal cost of this subproblem is 9 (the cost of scheduling C after B, which is 3, plus the cost of scheduling D after C, which is 6).State CD: Here it is only possible to schedule operation A as the next operation, so the optimal cost of this subproblem is 5.Tail Subproblems of Length 3 : These subproblems can now be solved using the optimal costs of the subproblems of length 2. Original Problem of Length 4 : The possibilities here are (a) start with operation A (cost 5) and then solve optimally the corresponding subproblem of length 3 (cost 8, as computed earlier), a total cost of 13, or (b) start with operation C (cost 3) and then solve optimally the corresponding subproblem of length 3 (cost 7, as computed earlier), a total cost of 10.The second possibility is optimal, and the corresponding optimal cost is 10, as shown next to the initial state node in Fig. 1.2.5.Note that having computed the optimal cost of the original problem through the solution of all the tail subproblems, we can construct the optimal schedule: we begin at the initial node and proceed forward, each time choosing the optimal operation, i.e., the one that starts the optimal schedule for the corresponding tail subproblem.In this way, by inspection of the graph and the computational results of Fig. 1.2.5, we determine that CABD is the optimal schedule.We now state the DP algorithm for deterministic finite horizon problems by translating into mathematical terms the heuristic argument underlying the principle of optimality.The algorithm constructs functions. ., J * 0 (x 0 ), sequentially, starting from J * N , and proceeding backwards to J * N −1 , J * N −2 , etc.We will show that the value J * k (x k ) represents the optimal cost of the tail subproblem that starts at state x k at time k.Illustration of the DP algorithm.The tail subproblem that starts at x k at time k minimizes over {u k , . . ., u N−1 } the "cost-to-go" from k to N ,To solve it, we choose u k to minimize the (1st stage cost + Optimal tail problem cost) orStart withand for k = 0, . . ., N − 1, let(1.5)The DP algorithm together with the construction of the functions J * k (x k ) are illustrated in Fig. 1.2.6.Note that at stage k, the calculation in Eq. (1.5) must be done for all states x k before proceeding to stage k − 1.The key fact about the DP algorithm is that for every initial state x 0 , the number J * 0 (x 0 ) obtained at the last step, is equal to the optimal cost J * (x 0 ).Indeed, a more general fact can be shown, namely that for allChap. 1 k = 0, 1, . . ., N − 1, and all states x k at time k, we havewhere J(x k ; u k , . . ., u N −1 ) is the cost generated by starting at x k and using subsequent controls u k , . . ., u N −1 :Thus, J * k (x k ) is the optimal cost for an (N − k)-stage tail subproblem that starts at state x k and time k, and ends at time N .† Based on the interpretation (1.6) of J * k (x k ), we call it the optimal cost-to-go from state x k at stage k, and refer to J * k as the optimal cost-to-go function or optimal cost function at time k.In maximization problems the DP algorithm (1.5) is written with maximization in place of minimization, and then J * k is referred to as the optimal value function at time k.Once the functions J * 0 , . . ., J * N have been obtained, we can use a forward algorithm to construct an optimal control sequence {u * 0 , . . ., u * N −1 } and corresponding state trajectory {x * 1 , . . ., x * N } for the given initial state x 0 .† We can prove this by induction.The assertion holds for k = N in view of the initial conditionTo show that it holds for all k, we use Eqs.(1.6) and (1.7) to writewhere for the last equality we use the induction hypothesis.A subtle mathematical point here is that, through the minimization operation, the cost-to-go functions J * k may take the value −∞ for some x k .Still the preceding induction argument is valid even if this is so.andThe same algorithm can be used to find an optimal control sequence for any tail subproblem.Figure 1.2.5 traces the calculations of the DP algorithm for the scheduling Example 1.2.1.The numbers next to the nodes, give the corresponding cost-to-go values, and the thick-line arcs give the construction of the optimal control sequence using the preceding algorithm.The following example deals with the classical traveling salesman problem involving N cities.Here, the number of states grows exponentially with N , and so does the corresponding amount of computation for exact DP.We will show later that with rollout, we can solve the problem approximately with computation that grows polynomially with N .Here we are given N cities and the travel time between each pair of cities.We wish to find a minimum time travel that visits each of the cities exactly once and returns to the start city.To convert this problem to a DP problem, we form a graph whose nodes are the sequences of k distinct cities, where k = 1, . . ., N. The k-city sequences correspond to the states of the kth stage.The initial state x0 consists of some city, taken as the start (city A in the example of Fig. 1.2.7).A k-city node/state leads to a (k + 1)-city node/state by adding a new city at a cost equal to the travel time between the last two of the k + 1 cities; see Fig. 1.2.7.Each sequence of N cities is connected to an artificial terminal node t with an arc of cost equal to the travel time from the last city of the sequence to the starting city, thus completing the transformation to a DP problem.The optimal costs-to-go from each node to the terminal state can be obtained by the DP algorithm and are shown next to the nodes.Note, however, that the number of nodes grows exponentially with the number of cities N .This makes the DP solution intractable for large N .As a result, large travel-Chap.1 The optimal costs-to-go are generated by DP starting from the terminal state and going backwards towards the initial state, and are shown next to the nodes.There is a unique optimal sequence here (ABDCA), and it is marked with thick lines.The optimal sequence can be obtained by forward minimization [cf.Eq. (1.8)], starting from the initial state x 0 .ing salesman and related scheduling problems are typically not addressed with exact DP, but rather with approximation methods.Some of these methods are based on DP and will be discussed later.An alternative (and equivalent) form of the DP algorithm (1.5), uses the optimal cost-to-go functions J * k indirectly.In particular, it generates theDeterministic Dynamic Programming 21 optimal Q-factors, defined for all pairs (x k , u k ) and k byThus the optimal Q-factors are simply the expressions that are minimized in the right-hand side of the DP equation (1.5).† Note that the optimal cost function J * k can be recovered from the optimal Q-factor Q * k by means of the minimizationMoreover, the DP algorithm (1.5) can be written in an essentially equivalent form that involves Q-factors only [cf.Eqs.(1.9)-(1.10)]:Exact and approximate forms of this and other related algorithms, including counterparts for stochastic optimal control problems, comprise an important class of RL methods known as Q-learning.The forward optimal control sequence construction of Eq. (1.8) is possible only after we have computed J * k (x k ) by DP for all x k and k.Unfortunately, in practice this is often prohibitively time-consuming, because the number of possible x k and k can be very large.However, a similar forward algorithmic process can be used if the optimal cost-to-go functions J * k are replaced by some approximations Jk .This is the basis for an idea that is central in RL: approximation in value space.‡ It constructs a suboptimal solution {ũ 0 , . . ., ũN−1 } in place of the optimal {u * 0 , . . ., u * N −1 }, based on using Jk in place of J * k in the DP algorithm (1.8).† The term "Q-factor" has been used in the booksand is adopted here as well.Another term used is "action value" (at a given state).The terms "state-action value" and "Q-value" are also common in the literature.The name "Q-factor" originated in reference to the notation used in an influential Ph.D. thesis [Wat89] that proposed the use of Q-factors in RL. ‡ Approximation in value space (sometimes called "search" or "tree search" in the AI literature) is a simple idea that has been used quite extensively for deterministic problems, well before the development of the modern RL methodology.For example it conceptually underlies the widely used A * method for computing approximate solutions to large scale shortest path problems.For a view of A * that is consistent with our approximate DP framework, the reader may consult the author's DP bookand set x1 = f 0 (x 0 , ũ0 ).Sequentially, going forward, for k = 1, 2, . . ., N − 1, set ũk ∈ arg minIn approximation in value space the calculation of the suboptimal sequence {ũ 0 , . . ., ũN−1 } is done by going forward (no backward calculation is needed once the approximate cost-to-go functions Jk are available).This is similar to the calculation of the optimal sequence {u * 0 , . . ., u * N −1 }, and is independent of how the functions Jk are computed.The motivation for approximation in value space for stochastic DP problems is vastly reduced computation relative to the exact DP algorithm (once Jk have been obtained): the minimization (1.11) needs to be performed only for the N states x 0 , x1 , . . ., xN−1 that are encountered during the on-line control of the system, and not for every state within the potentially enormous state space, as is the case for exact DP.The algorithm (1.11) is said to involve a one-step lookahead minimization, since it solves a one-stage DP problem for each k.In what follows we will also discuss the possibility of multistep lookahead , which involves the solution of an -step DP problem, where is an integer, 1 < < N − k, with a terminal cost function approximation Jk+ .Multistep lookahead typically (but not always) provides better performance over one-step lookahead in RL approximation schemes.For example in AlphaZero chess, long multistep lookahead is critical for good on-line performance.The intuitive reason is that with stages being treated "exactly" (by optimization), the effect of the approximation error Jk+ − J * k+ tends to become less significant as increases.However, the solution of the multistep lookahead optimization problem, instead of the one-step lookahead counterpart of Eq. (1.11), becomes more time consuming.A major issue in value space approximation is the construction of suitable approximate cost-to-go functions Jk .This can be done in many different ways, giving rise to some of the principal RL methods.For example, Jk may be constructed with a sophisticated off-line training method, as discussed in Section 1.1.Alternatively, Jk may be obtained on-line with rollout , which will be discussed in detail in this book.In rollout, the approximate values Jk (x k ) are obtained when needed by running a heuristic control scheme, called base heuristic or base policy, for a suitably large number of stages, starting from the state x k , and accumulating the costs incurred at these stages.The major theoretical property of rollout is cost improvement : the cost obtained by rollout using some base heuristic is less or equal to the corresponding cost of the base heuristic.This is true for any starting state, provided the base heuristic satisfies some simple conditions, which will be discussed in Chapter 2. † There are also several variants of rollout, including versions involving multiple heuristics, combinations with other forms of approximation in value space methods, multistep lookahead, and stochastic uncertainty.We will discuss such variants later.For the moment we will focus on a deterministic DP problem with a finite number of controls.Given a state x k at time k, this algorithm considers all the tail subproblems that start at every possible next state x k+1 , and solves them suboptimally by using some algorithm, referred to as base heuristic.Thus when at x k , rollout generates on-line the next states x k+1 that correspond to all u k ∈ U k (x k ), and uses the base heuristic to compute the sequence of states {x k+1 , . . ., x N } and controls {u k+1 , . . ., u N −1 } such thatand the corresponding costThe rollout algorithm then applies the control that minimizes over u k ∈ U k (x k ) the tail cost expression for stages k to N :). † For an intuitive justification of the cost improvement mechanism, note that the rollout control ũk is calculated from Eq. (1.11) to attain the minimum over u k over the sum of two terms: the first stage cost g k (x k , u k ) plus the cost of the remaining stages (k +1 to N ) using the heuristic controls.Thus rollout involves a first stage optimization (rather than just using the base heuristic), which accounts for the cost improvement.This reasoning also explains why multistep lookahead tends to provide better performance than one-step lookahead in rollout schemes.Chap. 1x 0 0 x 1 ) . . .and selects the control μk (x k ) with minimal Q-factor.Equivalently, and more succinctly, the rollout algorithm applies at state x k the control μk (x k ) given by the minimization μk (x k ) ∈ arg minwhere Qk (x k , u k ) is the approximate Q-factor defined bysee Fig. 1.2.9.Note that the rollout algorithm requires running the base heuristic for a number of times that is bounded by N n, where n is an upper bound on the number of control choices available at each state.Thus if n is small relative to N , it requires computation equal to a small multiple of N times the computation time for a single application of the base heuristic.Similarly, if n is bounded by a polynomial in N , the ratio of the rollout algorithm computation time to the base heuristic computation time is a polynomial in N .Let us consider the traveling salesman problem of Example 1.2.2, whereby a salesman wants to find a minimum cost tour that visits each of N given cities c = 0, . . ., N − 1 exactly once and returns to the city he started from.With each pair of distinct cities c, c , we associate a traversal cost g(c, c ).Note ) . . .that we assume that we can go directly from every city to every other city.There is no loss of generality in doing so because we can assign a very high cost g(c, c ) to any pair of cities (c, c ) that is precluded from participation in the solution.The problem is to find a visit order that goes through each city exactly once and whose sum of costs is minimum.There are many heuristic approaches for solving the traveling salesman problem.For illustration purposes, let us focus on the simple nearest neighbor heuristic, which starts with a partial tour, i.e., an ordered collection of distinct cities, and constructs a sequence of partial tours, adding to the each partial tour a new city that does not close a cycle and minimizes the cost of the enlargement.In particular, given a sequence {c0, c1, . . ., c k } (with k < N − 1) consisting of distinct cities, the nearest neighbor heuristic adds a city c k+1 that minimizes g(c k , c k+1 ) over all cities c k+1 = c0, . . ., c k , thereby forming the sequence {c0, c1, . . ., c k , c k+1 }.Continuing in this manner, the heuristic eventually forms a sequence of N cities, {c0, c1, . . ., cN−1}, thus yielding a complete tour with cost(1.14)We can formulate the traveling salesman problem as a DP problem as we discussed in Example 1.2.2.We choose a starting city, say c0, as the initial state x0.Each state x k corresponds to a partial tour (c0, c1, . . ., c k ) consisting of distinct cities.The states x k+1 , next to x k , are sequences of the form (c0, c1, . . ., c k , c k+1 ) that correspond to adding one more unvisited city c k+1 = c0, c1, . . ., c k (thus the unvisited cities are the feasible controls at a given partial tour/state).The terminal states xN are the complete tours of the form (c0, c1, . . ., cN−1, c0), and the cost of the corresponding sequence of city choices is the cost of the corresponding complete tour given by Eq. (1.14).Note that the number of states at stage k increases exponentially with k, and so does the computation required to solve the problem by exact DP.Let us now use as a base heuristic the nearest neighbor method.The corresponding rollout algorithm operates as follows: After k < N − 1 iterations, we have a state x k , i.e., a sequence {c0, . . ., c k } consisting of distinct cities.At the next iteration, we add one more city by running the  A (it has no other choice).The final tour T 2 generated by rollout turns out to be optimal in this example, while the tour T 0 generated by the base heuristic is suboptimal.This is suggestive of a general result: the rollout algorithm for deterministic problems generates a sequence of solutions of decreasing cost under some conditions on the base heuristic that we will discuss in Chapter 2, and which are satisfied by the nearest neighbor heuristic.nearest neighbor heuristic starting from each of the sequences of the form {c0, . . ., c k , c} where c = c0, . . ., c k .We then select as next city c k+1 the city c that yielded the minimum cost tour under the nearest neighbor heuristic; see Fig.We will now extend the DP algorithm and our discussion of approximation in value space to problems that involve stochastic uncertainty in their system equation and cost function.We will first discuss the finite horizon case, and the extension of the ideas underlying the principle of optimality and approximation in value space schemes.We will then consider the infinite horizon version of the problem, and provide an overview of the underlying theory and algorithmic methodology.The stochastic optimal control problem differs from its deterministic counterpart primarily in the nature of the discrete-time dynamic system that governs the evolution of the state x k .This system includes a random "disturbance" w k with a probability distribution P k (• | x k , u k ) that may depend explicitly on x k and u k , but not on values of prior disturbances w k−1 , . . ., w 0 .The system has the formwhere as earlier x k is an element of some state space, the control u k is an element of some control space.† The cost per stage is denoted by g k (x k , u k , w k ) and also depends on the random disturbance w k ; see Fig. 1.3.1.The control u k is constrained to take values in a given subset U k (x k ), which depends on the current state x k .Given an initial state x 0 and a policy π = {µ 0 , . . ., µ N −1 }, the future states x k and disturbances w k are random variables with distributions defined through the system equationThe discrete equation format and corresponding x-u-w notation is standard in the optimal control literature.For finite-state stochastic problems, also called Markovian Decision Problems (MDP), the system is often represented conveniently in terms of control-dependent transition probabilities.A common notation in the RL literature is p(s, a, s ) for transition probability from s to s under action a.This type of notation is not well suited for deterministic problems, which involve no probabilistic structure at all and are of major interest in this book.The transition probability notation is also cumbersome for problems with a continuous state space; see Sections 1.7.1 and 1.7.2 for further discussion.The reader should note, however, that mathematically the system equation and transition probabilities are equivalent, and any analysis that can be done in one notational system can be translated to the other notational system.Exact and Approximate Dynamic Programming Chap. 1 ... ... and the given distributions P k (• | x k , u k ).Thus, for given functions g k , k = 0, 1, . . ., N, the expected cost of π starting at x 0 iswhere the expected value operation E{•} is taken with respect to the joint distribution of all the random variables w k and x k .† An optimal policy π * is one that minimizes this cost; i.e.,where Π is the set of all policies.An important difference from the deterministic case is that we optimize not over control sequences {u 0 , . . ., u N −1 } [cf.Eq. (1.3)], but rather over policies (also called closed-loop control laws, or feedback policies) that consist of a sequence of functions π = {µ 0 , . . ., µ N −1 }, where µ k maps states x k into controls u k = µ k (x k ), and satisfies the control constraints, i.e., is such that µ k (x k ) ∈ U k (x k ) for all x k .Policies are more general objects than control sequences, and in the presence of stochastic uncertainty, they can result in improved cost, since they allow choices of controls u k that incorporate knowledge of the state x k .Without this knowledge, the controller cannot adapt appropriately to unexpected values of the state, and as a result the cost can be adversely affected.This is a fundamental distinction between deterministic and stochastic optimal control problems.† We assume an introductory probability background on the part of the reader.For an account that is consistent with our use of probability in this book, see the textbook by Bertsekas and Tsitsiklis [BeT08].Programming 29The optimal cost depends on x 0 and is denoted by J * (x 0 ); i.e.,We view J * as a function that assigns to each initial state x 0 the optimal cost J * (x 0 ), and call it the optimal cost function or optimal value function.The DP algorithm for the stochastic finite horizon optimal control problem has a similar form to its deterministic version, and shares several of its major characteristics:(a) Using tail subproblems to break down the minimization over multiple stages to single stage minimizations.(b) Generating backwards for all k and x k the values J * k (x k ), which give the optimal cost-to-go starting from state x k at stage k.(c) Obtaining an optimal policy by minimization in the DP equations.(d) A structure that is suitable for approximation in value space, whereby we replace J * k by approximations Jk , and obtain a suboptimal policy by the corresponding minimization.J * N (x N ) = g N (x N ), and for k = 0, . . ., N − 1, let(1.15)For each x k and k, define µ * k (x k ) = u * k where u * k attains the minimum in the right side of this equation.Then, the policy π * = {µ * 0 , . . ., µ * N −1 } is optimal.The key fact is that starting from any initial state x 0 , the optimal cost is equal to the number J * 0 (x 0 ), obtained at the last step of the above DP algorithm.This can be proved by induction similar to the deterministic case; we will omit the proof (which incidentally involves some mathematical fine points; see the discussion of Section 1.3 in the textbook [Ber17a]).Simultaneously with the off-line computation of the optimal costto-go functions J * 0 , . . ., J * N , we can compute and store an optimal policy π * = {µ * 0 , . . ., µ * N −1 } by minimization in Eq. (1.15).We can then use this policy on-line to retrieve from memory and apply the control µ * k (x k ) once we reach state x k .The alternative is to forego the storage of the policy π * and to calculate the control µ * k (x k ) by executing the minimization (1.15) on-line.There are a few favorable cases where the optimal cost-to-go functions J * k and the optimal policies µ * k can be computed analytically using the stochastic DP algorithm.A prominent such case involves a linear system and a quadratic cost function, which is a fundamental problem in control theory.We illustrate the scalar version of this problem next.The analysis can be generalized to multidimensional systems (see optimal control textbooks such as [Ber17a]).Example 1.3.1 (Linear Quadratic Optimal Control)Here the system is linear,and the state, control, and disturbance are scalars.The cost is quadratic of the form:where q and r are known positive weighting parameters.We assume no constraints on x k and u k (in reality such problems include constraints, but it is common to neglect the constraints initially, and check whether they are seriously violated later).As an illustration, consider a vehicle that moves on a straight-line road under the influence of a force u k and without friction.Our objective is to maintain the vehicle's velocity at a constant level v (as in an oversimplified cruise control system).The velocity v k at time k, after time discretization of its Newtonian dynamics and addition of stochastic noise, evolves according towhere w k is a stochastic disturbance with zero mean and given variance σ 2 .By introducing x k = v k − v, the deviation between the vehicle's velocity v k at time k from the desired level v, we obtain the system equationHere the coefficient b relates to a number of problem characteristics including the weight of the vehicle, the road conditions.The cost function expresses our desire to keep x k near zero with relatively little force.We will apply the DP algorithm, and derive the optimal cost-to-go functions J * k and optimal policy.We haveStochastic Exact and Approximate Dynamic Programming 31and by applying Eq. (1.15), we obtainand finally, using the assumptions E{wN−1} = 0, E{w 2 N−1 } = σ 2 , and bringing out of the minimization the terms that do not depend on uN−1,The expression minimized over uN−1 in the preceding equation is convex quadratic in uN−1, so by setting to zero its derivative with respect to uN−1,we obtain the optimal policy for the last stage:Substituting this expression into Eq.(1.17), we obtain with a straightforward calculationwhere KN−1 = a 2 rq r + b 2 q + q.We can now continue the DP algorithm to obtain J * N−2 from J * N−1 .An important observation is that J * N−1 is quadratic (plus an inconsequential constant term), so with a similar calculation we can derive µ * N−2 and J * N−2 in closed form, as a linear and a quadratic (plus constant) function of xN−2, respectively.This process can be continued going backwards, and it can be verified by induction that for all k, we obtain the optimal policy and optimal cost-to-go function in the formwhereExact and Approximate Dynamic Programming Chap. 1and the sequence {K k } is generated backwards by the equationstarting from the terminal condition KN = q.The process by which we obtained an analytical solution in this example is noteworthy.A little thought while tracing the steps of the algorithm will convince the reader that what simplifies the solution is the quadratic nature of the cost and the linearity of the system equation.Indeed, it can be shown in generality that when the system is linear and the cost is quadratic, the optimal policy and cost-to-go function are given by closed-form expressions, even for multi-dimensional linear systems (see [Ber17a], Section 3.1).The optimal policy is a linear function of the state, and the optimal cost function is a quadratic in the state plus a constant.Another remarkable feature of this example, which can also be extended to multi-dimensional systems, is that the optimal policy does not depend on the variance of w k , and remains unaffected when w k is replaced by its mean (which is zero in our example).This is known as certainty equivalence, and occurs in several types of problems involving a linear system and a quadratic cost; see [Ber17a], Sections 3.1 and 4.2.For example it holds even when w k has nonzero mean.For other problems, certainty equivalence can be used as a basis for problem approximation, e.g., assume that certainty equivalence holds (i.e., replace stochastic quantities by some typical values, such as their expected values) and apply exact DP to the resulting deterministic optimal control problem.This is an important part of the RL methodology, which we will discuss later in this chapter, and in more detail in Chapter 2.Note that the linear quadratic type of problem illustrated in the preceding example is exceptional in that it admits an elegant analytical solution.Most DP problems encountered in practice require a computational solution.Similar to the case of deterministic problems [cf.Eq. (1.9)], we can define optimal Q-factors for a stochastic problem, as the expressions that are minimized in the right-hand side of the stochastic DP equation (1.15).They are given byThe optimal cost-to-go functions J * k can be recovered from the optimal Q-factors Q * k by means ofStochastic Exact and Approximate Dynamic Programming 33and the DP algorithm can be written in terms of Q-factors asWe will later be interested in approximate Q-factors, where J * k+1 in Eq. (1.20) is replaced by an approximation Jk+1 .Again, the Q-factor corresponding to a state-control pair (x k , u k ) is the sum of the expected first stage cost using (x k , u k ), plus the expected cost of the remaining stages starting from the next state as estimated by the function Jk+1 .(1.21)The one-step lookahead minimization (1.21) needs to be performed only for the N states x 0 , . . ., x N −1 that are encountered during the on-line control of the system.By contrast, exact DP requires that this type of minimization be done for every state and stage.When designing approximation in value space schemes, one may consider several interesting simplification ideas, which are aimed at alleviating the computational overhead.Aside from cost function approximation (use Jk+1 in place of J * k+1 ), there are other possibilities.One of them is to simplify the lookahead minimization over u k ∈ U k (x k ) [cf.Eq. (1.15)] by replacing U k (x k ) with a suitably chosen subset of controls that are viewed as most promising based on some heuristic criterion.In Section 1.6.5, we will discuss a related idea for control space simplification for the multiagent case where the control consists of multiple components, u k = (u 1 k , . . ., u m k ).Then, a sequence of m single component minimizations can be used instead, with potentially enormous computational savings resulting.Another type of simplification relates to approximations in the computation of the expected value in Eq. (1.21) by using limited Monte Carlo simulation.The Monte Carlo Tree Search method, which will be discussed in Chapter 2, Section 2.7.4, is one possibility of this type.Still another type of expected value simplification is based on the certainty equivalence approach, which will be discussed in more detail in Chapter 2, Section 2.7.2.In this approach, at stage k, we replace the future random variables w k+1 , . . ., w k+m by some deterministic values w k+1 , . . ., w k+m , such as their expected values.We may also view this as a form of problem approximation, whereby for the purpose of computing Jk+1 (x k+1 ), we "pretend" that the problem is deterministic, with the future random quantities replaced by deterministic typical values.This is one of the most effective techniques to make approximation in value space for stochastic problems computationally tractable, particularly when it is also combined with multistep lookahead minimization, as we will discuss later.Figure 1.3.2illustrates the three approximations involved in approximation in value space for stochastic problems: cost-to-go approximation, simplified minimization, and expected value approximation.They may be designed largely independently of each other, and may be implemented with a variety of methods.Much of the discussion in this book will revolve around different ways to organize these three approximations for both cases of one-step and multistep lookahead.As indicated in Fig. 1.3.2,an important approach for cost-to-go approximation is problem approximation, whereby the functions Jk+1 in Eq. (1.21) are obtained as the optimal or nearly optimal cost functions of a simplified optimization problem, which is more convenient for computation.Simplifications may include exploiting decomposable structure, ignoring various types of uncertainties, and reducing the size of the state space.Several types of problem approximation approaches are discussed in the author's RL book [Ber19a].A major approach is aggregation, which will be discussed in Section 3.5.In this book, problem approximation will not receive much attention, despite the fact that it can often be combined very effectively with the approximation in value space methodology that is our main focus.Another important approach for on-line cost-to-go approximation is rollout, which we discuss next.This is similar to the rollout approach for deterministic problems, discussed in Section 1.2.In the rollout approach, we select Jk+1 in Eq. (1.21) to be the cost function of a suitable base policy (perhaps with some approximation).Note that Steps "Future"Steps "Future" First Step  2 Schematic illustration of approximation in value space for stochastic problems, and the three approximations involved in its design.Typically the approximations can be designed independently of each other, and with a variety of approaches.There are also multistep lookahead versions of approximation in value space, which will be discussed later.any policy can be used on-line as base policy, including policies obtained by a sophisticated off-line procedure, using for example neural networks and training data.The rollout algorithm has a cost improvement property, whereby it yields an improved cost relative to its underlying base policy.We will discuss this property and some conditions under which it is guaranteed to hold in Chapter 2.A major variant of rollout is truncated rollout, which combines the use of one-step optimization, simulation of the base policy for a certain number of steps m, and then adds an approximate cost Jk+m+1 (x k+m+1 ) to the cost of the simulation, which depends on the state x k+m+1 obtained at the end of the rollout.Note that if one foregoes the use of a base policy (i.e., m = 0), one recovers as a special case the general approximation in value space scheme (1.21); see Fig. 1.3.3.Thus rollout provides an extra layer of lookahead to the one-step minimization, but this lookahead need not extend to the end of the horizon.Note also that versions of truncated rollout with multistep lookahead minimization are possible.They will be discussed later.The terminal cost approximation is necessary in infinite horizon problems, since an infinite number of stages of the base policy rollout is impossible.However, even for finite horizon problems it may be necessary and/or beneficial to artificially truncate the rollout horizon.Generally, a large combined number of multistep lookahead minimization and rollout steps is likely to be beneficial.Similar to the deterministic case, Q-learning involves the calculation of either the optimal Q-factors (1.20)  approximate Q-factors may be obtained using approximation in value space schemes, and can be used to obtain approximately optimal policies through the Q-factor minimization μk (x k ) ∈ arg minSince it is possible to implement approximation in value space by using cost function approximations [cf.Eq. (1.21)] or by using Q-factor approximations [cf.Eq. (1.22)], the question arises which one to use in a given practical situation.One important consideration is the facility of obtaining suitable cost or Q-factor approximations.This depends largely on the problem and also on the availability of data on which the approximations can be based.However, there are some other major considerations.In particular, the cost function approximation scheme μk (x k ) ∈ arg min) has an important disadvantage: the expected value above needs to be computed on-line for all u k ∈ U k (x k ), and this may involve substantial computation.It also has an important advantage in situations where the system function f k , the cost per stage g k , or the control constraint set U k (x k ) can change as the system is operating.Assuming that the new f k , g k , or U k (x k ) become known to the controller at time k, on-line replanning may be used, and this may improve substantially the robustness of the approximation in value space scheme.By comparison, the Q-factor function approximation scheme (1.22) does not allow for on-line replanning.On the other hand, for problems where there is no need for on-line replanning, the Q-factor approximation scheme may not require the on-line computation of expected values and may allow a much faster on-line computation of the minimizing control μk (x k ) via Eq.(1.22).One more disadvantage of using Q-factors will emerge later, as we discuss the synergy between off-line training and on-line play based on Newton's method; see Section 1.5.In particular, we will interpret the cost function of the lookahead minimization policy {μ 0 , . . ., μN−1 } as the result of one step of Newton's method for solving the Bellman equation that underlies the DP problem, starting from the terminal cost function approximations { J1 , . . ., JN }.This synergy tends to be negatively affected when Q-factor (rather than cost) approximations are used.The major alternative to approximation in value space is approximation in policy space, whereby we select the policy from a suitably restricted class of policies, usually a parametric class of some form.In particular, we can introduce a parametric family of policies (or approximation architecture, as we will call it in Chapter 3), μk (x k , r k ), k= 0, . . ., N − 1, where r k is a parameter, and then estimate the parameters r k using some type of training process or optimization; cf.Fig. 1.3.4.Neural networks, described in Chapter 3, are often used to generate the parametric class of policies, in which case r k is the vector of weights/parameters of the neural network.In Chapter 3, we will also discuss methods for obtaining the training data required for obtaining the parameters r k , and we will consider several other classes of approximation architectures.A general scheme for parametric approximation in policy space is to somehow obtain a training set, consisting of a large number of sample state-control pairs (x s k , u s k ), s = 1, . . ., q, such that for each s, u s k is a "good" control at state x s k .We can then choose the parameter r k by solving the least squares/regression problem min   (possibly modified to add regularization).† In particular, we may determine u s k using a human or a software "expert" that can choose "near-optimal" controls at given states, so μk is trained to match the behavior of the expert.Methods of this type are commonly referred to as supervised learning in artificial intelligence.An important approach for generating the training set (x s k , u s k ), s = 1, . . ., q, for the least squares training problem (1.24) is based on approximation in value space.In particular, we may use a one-step lookahead minimization of the form, † Here • denotes the standard quadratic Euclidean norm.It is implicitly assumed here (and in similar situations later) that the controls are members of a Euclidean space (i.e., the space of finite dimensional vectors with real-valued components) so that the distance between two controls can be measured by their normed difference (randomized controls, i.e., probabilities that a particular action will be used, fall in this category).Regression problems of this type arise in the training of parametric classifiers based on data, including the use of neural networks (see Section 3.4).Assuming a finite control space, the classifier is trained using the data x s k , u s k , s = 1, . . ., q, which are viewed as state-category pairs, and then a state x k is classified as being of "category" μk (x k , r k ).Parametric approximation architectures, and their training through the use of classification and regression techniques are described in Chapter 3.An important modification is to use regularized regression where a quadratic regularization term is added to the least squares objective.This term is a positive multiple of the squared deviation r − r 2 of r from some initial guess r.Programmingwhere Jk+1 is a suitable (separately obtained) approximation in value space.Alternatively, we may use an approximate Q-factor based minimization u s k ∈ arg minwhere Qk is a (separately obtained) Q-factor approximation.We may view this as approximation in policy space built on top of approximation in value space.There is a significant advantage of the least squares training procedure of Eq. (1.24), and more generally approximation in policy space: once the parametrized policy μk is obtained, the computation of controlsduring on-line operation of the system is often much easier compared with the lookahead minimization (1.23).For this reason, one of the major uses of approximation in policy space is to provide an approximate implementation of a known policy (no matter how obtained) for the purpose of convenient on-line use.On the negative side, such an implementation is less well suited for on-line replanning.There are also alternative optimization-based approaches for policy space approximation.The main idea is that once we use a vector (r 0 , r 1 , . . ., r N −1 ) to parametrize the policies π, the expected cost J π (x 0 ) is parametrized as well, and can be viewed as a function of (r 0 , r 1 , . . ., r N −1 ).We can then optimize this cost by using a gradient-like or random search method.This is a widely used approach for optimization in policy space, which, however, will receive limited attention in this book (see Section 3.5, and the RL book [Ber19a], Section 5.7).An interesting feature of this approach is that in principle it does not require a mathematical model of the system and the cost function; a computer simulator (or availability of the real system for experimentation) suffices instead.This is sometimes called a model-free implementation.The advisability of implementations of this type, particularly when they rely exclusively on simulation (i.e., without the use of prior mathematical model knowledge), is a hotly debated and much contested issue; see for example the review paper by Alamir [Ala22].We finally note an important conceptual difference between approximation in value space and approximation in policy space.The former is primarily an on-line method (with off-line training used optionally to construct cost function approximations for one-step or multistep lookahead).The latter is primarily an off-line training method (which may be used without modification for on-line play or optionally to provide a policy for on-line rollout).Chap. 1When it comes to off-line constructed approximations, a major approach is based on the use of parametric approximation.Feature-based architectures and neural networks are very useful within our RL context, and will be discussed in Chapter 3, together with methods that can be used for training them.†Training Data Approximation Architecture Parameter Approximation Architecture Parameter J(x) Training Data x s , J(x s ) s = 1, . . ., q , . . ., q Parameter r r J(x, r)r Approximating Function Approximating Function The general structure for parametric cost approximation.We approximate the target cost function J(x) with a member from a parametric class J(x, r) that depend on a parameter vector r.We use training data x s , J(x s ) , s = 1, . . ., q, and a form of optimization that aims to find a parameter r that "minimizes" the size of the errors J(x s ) − J(x s , r), s = 1, . . ., q.A general structure for parametric cost function approximation is illustrated in Fig. 1.3.5.We have a target function J(x) that we want to approximate with a member of a parametric class of functions J(x, r) that depend on a parameter vector r (to simplify, we drop the time index, using J in place of J k ).To this end, we collect training data x s , J(x s ) , s = 1, . . ., q, which we use to determine a parameter r that leads to a good "fit" between the data J(x s ) and the predictions J(x s , r) of the parametrized function.This is usually done through some form of optimization that † The principal role of neural networks within the context of this book is to provide the means for approximating various target functions from input-output data.This includes cost functions and Q-factors of given policies, and optimal cost-to-go functions and Q-factors; in this case the neural network is referred to as a value network (sometimes the alternative term critic network is also used).In other cases the neural network represents a policy viewed as a function from state to control, in which case it is called a policy network (the alternative term actor network is also used).The training methods for constructing the cost function, Q-factor, and policy approximations themselves from data are mostly based on optimization and regression, and will be reviewed in Chapter 3. Further DPoriented discussions are found in many sources, including the RL books [Ber19a], [Ber20a], and the neuro-dynamic programming book [BeT96].Machine learning books, including those describing at length neural network architectures and training are also recommended; see e.g., the recent book by Bishop and Bishop [BiB24], and the references quoted therein.aims to minimize in some sense the size of the errors J(x s ) − J(x s , r), s = 1, . . ., q.The methodological ideas for parametric cost approximation can also be used for approximation of a target policy µ with a policy from a parametric class μ(x, r).The training data may be obtained, for example, from rollout control calculations, thus enabling the construction of both value and policy networks that can be combined for use in a perpetual rollout scheme.However, there is an important difference: the approximate cost values J (x, r) are real numbers, whereas the approximate policy values μ(x, r) are elements of a control space U .Thus if U consists of m dimensional vectors, μ(x, r) consists of m numerical components.In this case the parametric approximation problems for cost functions and for policies are fairly similar, and both involve continuous space approximations.On the other hand, the case where the control space is finite, U = {u 1 , . . ., u m }. is markedly different.In this case, for any x, μ(x, r) consists of one of the m possible controls u 1 , . . ., u m .This ushers a connection with traditional classification schemes, whereby objects x are classified as belonging to one of the categories u 1 , . . ., u m , so that µ(x) defines the category of x, and can be viewed as a classifier.Some of the most prominent classification schemes actually produce randomized outcomes, i.e., x is associated with a probability distributionwhich is a randomized policy in our policy approximation context; see Fig. 1.3.6.This is done usually for reasons of algorithmic convenience, since many optimization methods, including least squares regression, require that the optimization variables are continuous.In this case, the randomized policy (1.25) can be converted to a nonrandomized policy using a maximization operation: associate x with the control of maximum probability (cf.The use of classification methods for approximation in policy space will be discussed in Chapter 3 (Section 3.4).We will now provide an outline of infinite horizon stochastic DP with an emphasis on its aspects that relate to our RL/approximation methods.We will deal primarily with infinite horizon stochastic problems, where we aim to minimize the total cost over an infinite number of stages, given by ... ... Here, J π (x 0 ) denotes the cost associated with an initial state x 0 and a policy π = {µ 0 , µ 1 , . ..}, and α is a scalar in the interval (0, 1].The functions g and f that define the cost per stage and the system equationdo not change from one stage to the next.The stochastic disturbances, w 0 , w 1 , . .., have a common probability distributionWhen α is strictly less that 1, it has the meaning of a discount factor , and its effect is that future costs matter to us less than the same costs incurred at the present time.Among others, a discount factor guarantees that the limit defining J π (x 0 ) exists and is finite (assuming that the range of values of the stage cost g is bounded).This is a nice mathematical property that makes discounted problems analytically and algorithmically tractable.Thus, by definition, the infinite horizon cost of a policy is the limit of its finite horizon costs as the horizon tends to infinity.The three types of problems that we will focus on are:(a) Stochastic shortest path problems (SSP for short).Here, α = 1 but there is a special cost-free termination state; once the system reaches that state it remains there at no further cost.In some types of problems, the termination state may represent a goal state that we are trying to reach at minimum cost, while in others it may be a state that we are trying to avoid for as long as possible.We will mostly assume a problem structure such that termination is inevitable under all policies.Thus the horizon is in effect finite, but its length is random and may be affected by the policy being used.A significantly more complicated type of SSP problems, which we will discuss selectively, arises when termination can be guaranteed only for a subset of policies, which includes all optimal policies.Some common types of SSP belong to this category, including deterministic shortest path problems that involve graphs with cycles.(b) Discounted problems.Here, α < 1 and there need not be a termination state.However, we will see that a discounted problem with a finite number of states can be readily converted to an SSP problem.This can be done by introducing an artificial termination state to which the system moves with probability 1 − α at every state and stage, thus making termination inevitable.As a result, algorithms and analysis for SSP problems can be easily adapted to discounted problems; the DP textbook [Ber17a] provides a detailed account of this conversion, and an accessible introduction to discounted and SSP problems with a finite number of states.(c) Deterministic nonnegative cost problems.Here, the disturbance w k takes a single known value.Equivalently, there is no disturbance in the system equation and the cost expression, which now take the formandWe assume further that there is a cost-free and absorbing termination state t, and that we haveand g(t, u) = 0 for all u ∈ U (t).This type of structure expresses the objective to reach or approach t at minimum cost, a classical control problem.An extensive analysis of the undiscounted version of this problem was given in the author's paper [Ber17b].Discounted stochastic problems with a finite number of states [also referred to as discounted MDP (abbreviation for Markovian Decision Problem)] are very common in the DP/RL literature, particularly because of their benign analytical and computational nature.Moreover, there is a widespread belief that discounted MDP can be used as a universal model, i.e., that in practice any other kind of problem (e.g., undiscounted problems with a termination state and/or a continuous state space) can be painlessly Chap. 1 converted to a discounted MDP with a discount factor that is close enough to 1.This is questionable, however, for a number of reasons:(a) Deterministic models are common as well as natural in many practical contexts (including discrete optimization/integer programming problems), so to convert them to MDP does not make sense.(b) The conversion of a continuous-state problem to a finite-state problem through some kind of discretization involves mathematical subtleties that can lead to serious practical/algorithmic complications.In particular, the character of the optimal solution may be seriously distorted by converting to a discounted MDP through some form of discretization, regardless of how fine the discretization is.(c) For some practical shortest path contexts it is essential that the termination state is ultimately reached.However, when a discount factor α is introduced in such a problem, the character of the problem may be fundamentally altered.In particular, the threshold for an appropriate value of α may be very close to 1 and may be unknown in practice.For a simple example consider a shortest path problem with states 1 and 2 plus a termination state t.From state 1 we can go to state 2 at cost 0, from state 2 we can go to either state 1 at a small cost > 0 or to the termination state at a substantial cost C > 0. The optimal policy over an infinite horizon is to go from 1 to 2 and from 2 to t. Suppose now that we approximate the problem by introducing a discount factor α ∈ (0, 1).Then it can be shown that if α < 1 − /C, it is optimal to move indefinitely around the cycle 1 → 2 → 1 → 2 and never reach t, while for α > 1 − /C the shortest path 2 → 1 → t will be obtained.Thus the solution of the discounted problem varies discontinuously with α: it changes radically at some threshold, which in general may be unknown.An important class of problems that we will consider in some detail in this book is finite-state deterministic problems with a large number of states.Finite horizon versions of these problems include challenging discrete optimization problems, whose exact solution is practically impossible.An important fact to keep in mind is that we can transform such problems to infinite horizon SSP problems with a termination state at the end of the horizon, so that the conceptual framework of the present section applies.The approximate solution of discrete optimization problems by RL methods, and particularly by rollout, will be considered in Chapter 2, and has been discussed at length in the books [Ber19a] and [Ber20a].There are several analytical and computational issues regarding our infinite horizon problems.Many of them revolve around the relation between the optimal cost function J * of the infinite horizon problem and the optimal cost functions of the corresponding N -stage problems.In particular, let J N (x) denote the optimal cost of the problem involving N stages, initial state x, cost per stage g(x, u, w), and zero terminal cost.This cost is generated after N iterations of the algorithm(1.31) starting from J 0 (x) ≡ 0. † The algorithm (1.31) is known as the value iteration algorithm (VI for short).Since the infinite horizon cost of a given policy is, by definition, the limit of the corresponding N -stage costs as N → ∞, it is natural to speculate that:(a) The optimal infinite horizon cost is the limit of the corresponding N -stage optimal costs as N → ∞; i.e.,for all states x.(b) The following equation should hold for all states x,This is obtained by taking the limit as N → ∞ in the VI algorithm (1.31) using Eq.(1.32).The preceding equation, called Bellman's equation, is really a system of equations (one equation per state x), which has as solution the optimal costs-to-go of all the states.(c) If µ(x) attains the minimum in the right-hand side of the Bellman equation (1.33) for each x, then the policy {µ, µ, . ..} should be optimal.This type of policy is called stationary, and for simplicity it is denoted by µ.(d) The cost function J µ of a stationary policy µ satisfies(1.34) † This is just the finite horizon DP algorithm of Section 1.3.1,except that we have reversed the time indexing to suit our infinite horizon context.In particular, consider the N -stages problem and let V N−k (x) be the optimal cost-to-go starting at x with k stages to go, and with terminal cost equal to 0. Applying DP, we have for all x,We can view this as just the Bellman equation (1.33) for a different problem, where for each x, the control constraint set U (x) consists of just one control, namely µ(x).Moreover, we expect that J µ is obtained in the limit by the VI algorithm:where J µ,N is the N -stage cost function of µ generated bystarting from J µ,0 (x) ≡ 0 or some other initial condition; cf.Eqs.(1.31)-(1.32).All four of the preceding results can be shown to hold for finitestate discounted problems, and also for finite-state SSP problems under reasonable assumptions.The results also hold for infinite-state discounted problems, provided the cost per stage function g is bounded over the set of possible values of (x, u, w), in which case we additionally can show that J * is the unique solution of Bellman's equation.The VI algorithm is also valid under these conditions, in the sense that J k → J * , even if the initial function J 0 is nonzero.The motivation for a different choice of J 0 is faster convergence to J * ; generally the convergence is faster as J 0 is chosen closer to J * .The associated mathematical proofs can be found in several sources, e.g., [Ber12], Chapter 1, or [Ber19a], Chapter 4. † It is important to note that for infinite horizon problems, there are additional important algorithms that are amenable to approximation in value space.Approximate policy iteration, Q-learning, temporal difference methods, linear programming, and their variants are some of these; see the RL books [Ber19a], [Ber20a].For this reason, in the infinite horizon case, there is a richer set of algorithmic options for approximation in value space, despite the fact that the associated mathematical theory is more complex.In this book, we will only discuss approximate forms and variations of the policy iteration algorithm, which we describe next.A major infinite horizon algorithm is policy iteration (PI for short).We will argue that PI, together with its variations, forms the foundation for † For undiscounted problems and discounted problems with unbounded cost per stage, we may still adopt the four preceding results as a working hypothesis.However, we should also be aware that exceptional behavior is possible under unfavorable circumstances, including nonuniqueness of solution of Bellman's equation, and nonconvergence of the VI algorithm to J * from some initial conditions; see the books  self-learning in RL, i.e., learning from data that is self-generated (from the system itself as it operates) rather than from data supplied from an external source.Figure 1.4.2describes the method as repeated rollout, and indicates that each of its iterations consists of two phases:(a) Policy evaluation, which computes the cost function J µ of the current (or base) policy µ.One possibility is to solve the corresponding Bellman equationcf. Eq. (1.34).However, the value J µ (x) for any x can also be computed by Monte Carlo simulation, by averaging over many randomly generated trajectories the cost of the policy starting from x.(b) Policy improvement , which computes the "improved" (or rollout) policy μ using the one-step lookahead minimization μ(x) ∈ arg minWe call μ "improved policy" because we can generally prove thatThis cost improvement property will be shown in Chapter 2, Section 2.7, and can be used to show that PI produces an optimal policy in a finite number of iterations under favorable conditions (for example for finitestate discounted problems; see the DP booksThe rollout algorithm in its pure form is just a single iteration of the PI algorithm.It starts from a given base policy µ and produces the rollout policy μ.It may be viewed as approximation in value space with one-step lookahead that uses J µ as terminal cost function approximation.Chap. 1It has the advantage that it can be applied on-line by computing the needed values of J µ (x) by simulation.By contrast, approximate forms of PI for challenging problems, involving for example neural network training, can only be implemented off-line.The approximation in value space approach that we discussed in connection with finite horizon problems can be extended in a natural way to infinite horizon problems.Here in place of J * , we use an approximation J, and generate at any state x, a control μ(x) by the one-step lookahead minimizationThis minimization yields a stationary policy {μ, μ, . ..}, with cost function denoted J μ [i.e., J μ(x) is the total infinite horizon discounted cost obtained when using μ starting at state x]; see Fig. 1.4.3.Note that when J = J * , the one-step lookahead policy attains the minimum in the Bellman equation (1.33) and is expected to be optimal.This suggests that one should try to use J as close as possible to J * , which is generally true as we will argue later.Naturally an important goal to strive for is that J μ is close to J * in some sense.However, for classical control problems, which involve steering and maintaining the state near a desired reference state (e.g., problems with a cost-free and absorbing terminal state, and positive cost for all other states), stability of μ may be a principal objective.In this book, we will discuss stability issues primarily for this one class of problems, and we will consider the policy μ to be stable if J μ is real-valued , i.e.,for all states x.Selecting J so that μ is stable is a question of major interest for some application contexts, such as model predictive and adaptive control, and will be discussed in the next section within the limited context of linear quadratic problems.An important extension of one-step lookahead minimization is -step lookahead , whereby at a state x k we minimize the cost of the first > 1 stages with the future costs approximated by a function J (see the bottom halfFirst Step First Steps "Future" Steps "Future"Steps "Future"3 Schematic illustration of approximation in value space with one-step and -step lookahead minimization for infinite horizon problems.In the former case, the minimization yields at state x a control ũ, which defines the one-step lookahead policy μ via μ(x) = ũ.In the latter case, the minimization yields a control ũk policies μk+1 , . . ., μk+ −1 .The control ũk is applied at x k while the remaining sequence μk+1 , . . ., μk+ −1 is discarded.The control ũk defines the -step lookahead policy μ.of Fig. 1.4.3).† This minimization yields a control ũk and a sequence μk+1 , . . ., μk+ −1 .The control ũk is applied at x k , and defines the -step lookahead policy μ via μ(x k ) = ũk , while μk+1 , . . ., μk+ −1 are discarded.Actually, we may view -step lookahead minimization as the special case of its one-step counterpart where the lookahead function is the optimal cost function of an ( − 1)-stage DP problem with a terminal cost J(x k+ ) on the state x k+ obtained after − 1 stages.The motivation for -step lookahead minimization is that by increasing the value of , we may require a less accurate approximation J to obtain good performance.Otherwise expressed, for the same quality of cost function approximation, better performance may be obtained as becomes larger.This will be explained visually later, using the formalism of Newton's method in Section 1.5.In particular, for AlphaZero chess, long multistep lookahead is critical for good on-line performance.Another motivation for multistep lookahead is to enhance the stability properties of the generated on-line policy, as we will discuss later in Section 1.5.On the other † On-line play with multistep lookahead minimization (and possibly truncated rollout) is referred to by a number of different names in the RL literature, such as on-line search, predictive learning, learning from prediction, etc; in the model predictive control literature the combined interval of lookahead minimization and truncated rollout is referred as the prediction interval .Chap. 1Steps "Future"Steps "Future" First StepApproximation in value space with one-step lookahead for infinite horizon problems.There are three potential areas of approximation, which can be considered independently of each other: optimal cost approximation, expected value approximation, and minimization approximation.hand, solving the multistep lookahead minimization problem, instead of the one-step lookahead counterpart of Eq. (1.36), is more time consuming.There are three potential areas of approximation for infinite horizon problems: optimal cost approximation, expected value approximation, and minimization approximation; cf.Fig. 1.4.4.They are similar to their finite horizon counterparts that we discussed in Section 1.3.2.In particular, we have potentially:(a) A terminal cost approximation J of the optimal cost function J * :A major advantage of the infinite horizon context is that only one approximate cost function J is needed, rather than the N functions J1 , . . ., JN of the N -step horizon case.(b) An approximation of the expected value operation: This operation can be very time consuming.It may be simplified in various ways.For example some of the random quantities w k , w k+1 , . . ., w k+ −1 appearing in the -step lookahead minimization may be replaced by deterministic quantities; this is another example of the certainty equivalence approach, which we discussed in Section 1.3.2.(c) A simplification of the minimization operation: For example in multiagent problems the control consists of multiple components,with each component u i chosen by a different agent/decision maker.In this case the size of the control space can be enormous, but it can be simplified in ways that will be discussed later (e.g., choosingSec. 1.4Infinite Horizon Problems -An Overview 51 components sequentially, one-agent-at-a-time).This will form the core of our approach to multiagent problems; see Section 1.6.5 and Chapter 2, Section 2.9.We will next describe briefly various approaches for selecting the terminal cost function approximation.A major issue in value space approximation is the construction of a suitable approximate cost function J.This can be done in many different ways, giving rise to some of the principal RL methods.For example, J may be constructed with sophisticated off-line training methods.Alternatively, the approximate values J(x) may be obtained online as needed with truncated rollout, by running an off-line obtained policy for a suitably large number of steps, starting from x, and supplementing it with a suitable, perhaps primitive, terminal cost approximation.For orientation purposes, let us describe briefly four broad types of approximation.We will return to these approaches later, and we also refer to the RL and approximate DP literature for more detailed discussions.(a) Off-line problem approximation: Here the function J is computed offline as the optimal or nearly optimal cost function of a simplified optimization problem, which is more convenient for computation.Simplifications may include exploiting decomposable structure, reducing the size of the state space, neglecting some of the constraints, and ignoring various types of uncertainties.For example we may consider using as J the cost function of a related deterministic problem, obtained through some form of certainty equivalence approximation, thus allowing computation of J by gradient-based optimal control methods or shortest path-type methods.A major type of problem approximation method is aggregation, described in Section 3.6, and in the books [Ber12], [Ber19a] and papers [Ber18a], [Ber18b].Aggregation provides a systematic procedure to simplify a given problem by grouping states together into a relatively small number of subsets, called aggregate states.The optimal cost function of the simpler aggregate problem is computed by exact DP methods, possibly involving the use of simulation.This cost function is then used to provide an approximation J to the optimal cost function J * of the original problem, using some form of interpolation.(b) On-line simulation: This possibility arises in rollout algorithms for stochastic problems, where we use Monte-Carlo simulation and some suboptimal policy µ (the base policy) to compute (whenever needed) values J(x) that are exactly or approximately equal to J µ (x).The policy µ may be obtained by any method, e.g., one based on heuristic reasoning (such as in the case of the traveling salesman ExampleExact and Approximate Dynamic Programming Chap. 11.2.3), or off-line training based on a more principled approach, such as approximate policy iteration or approximation in policy space.Note that while simulation is time-consuming, it is uniquely wellsuited for the use of parallel computation.Moreover, it can be simplified through the use of certainty equivalence approximations.(c) On-line approximate optimization.This approach involves the solution of a suitably constructed shorter horizon version of the problem, with a simple terminal cost approximation.It can be viewed as either approximation in value space with multistep lookahead, or as a form of rollout algorithm.It is often used in model predictive control (MPC).(d) Parametric cost approximation, where J is obtained from a given parametric class of functions J(x, r), where r is a parameter vector, selected by a suitable algorithm.The parametric class typically involves prominent characteristics of x called features, which can be obtained either through insight into the problem at hand, or by using training data and some form of neural network (see Chapter 3).Such methods include approximate forms of PI, as discussed in Section 1.1 in connection with chess and backgammon.The policy evaluation portion of the PI algorithm can be done by approximating the cost function of the current policy using an approximation architecture such as a neural network (see Chapter 3).It can also be done with stochastic iterative algorithms such as TD(λ), LSPE(λ), and LSTD(λ), which are described in the DP book [Ber12] and the RL book [Ber19a].These methods are somewhat peripheral to our course, and will not be discussed at any length.We note, however, that approximate PI methods do not just yield a parametric approximate cost function J(x, r), but also a suboptimal policy, which can be improved on-line by using (possibly truncated) rollout.Aside from approximate PI, parametric approximate cost functions J(x, r) may be obtained off-line with methods such as Q-learning, linear programming, and aggregation methods, which are also discussed in the books [Ber12] and [Ber19a].Let us also mention that for problems with special structure, J may be chosen so that the one-step lookahead minimization (1.36) is facilitated.In fact, under favorable circumstances, the lookahead minimization may be carried out in closed form.An example is when the system is nonlinear, but the control enters linearly in the system equation and quadratically in the cost function, while the terminal cost approximation is quadratic.Then the one-step lookahead minimization can be carried out analytically, because it involves a function that is quadratic in u.Generally off-line training will produce either just a cost approximation (as in the case of TD-Gammon), or just a policy (as for example by some approximation in policy space/policy gradient approach), or both (as in the case of AlphaZero).We have already discussed in this section one-step lookahead and multistep lookahead schemes to implement on-line approximation in value space using J; cf.Fig. 1.4.3.Let us now consider some additional possibilities, which involve the use of a policy µ that has been obtained off-line (possibly in addition to a terminal cost approximation).Here are some of the main possibilities:(a) Given a policy µ that has been obtained off-line, we may use as terminal cost approximation J the cost function J µ of the policy.For the case of one-step lookahead, this requires a policy evaluation operation, and can be done on-line, by computing (possibly by simulation) just the values offor all states x k+ that are reachable in steps starting from x k are needed.This is the simplest form of rollout, and only requires the off-line construction of the policy µ.(b) Given a terminal cost approximation J that has been obtained offline, we may use it on-line to compute fast when needed the controls of a corresponding one-step or multistep lookahead policy μ.The policy μ can in turn be used for rollout as in (a) above.In a truncated variation of this scheme, we may also use J to approximate the tail end of the rollout process (an example of this is the rollout-based TD-Gammon algorithm).(c) Given a policy µ and a terminal cost approximation J, we may use them together in a truncated rollout scheme, whereby the tail end of the rollout with µ is approximated using the cost approximation J.This is similar to the truncated rollout scheme noted in (b) above, except that the policy µ is computed off-line rather than on-line using J and one-step or multistep lookahead.The preceding three possibilities are the principal ones for using the results of off-line training within on-line play schemes.Naturally, there are variations where additional information is computed off-line to facilitate and/or expedite the on-line play algorithm.As an example, in MPC, in addition to a terminal cost approximation, a target tube may need to be computed off-line in order to guarantee that some state constraints can 54Chap. 1be satisfied on-line; see the discussion of MPC in Section 1.6.7.Other examples of this type will be noted in the context of specific applications.Finally, let us note that while we have emphasized approximation in value space with cost function approximation, our discussion applies to Q-factor approximation, involving functionsThe corresponding one-step lookahead scheme has the form μ(x) ∈ arg mincf. Eq. (1.36).The second term on the right in the above equation represents the cost function approximationThe use of Q-factors is common in the "model-free" case where a computer simulator is used to generate samples of w, and corresponding values of g and f .Then, having obtained Q through off-line training, the one-step lookahead minimization in Eq. (1.37) must be performed on-line with the use of the simulator.We will now discuss some of our objectives as we try to get insight into the process of approximation in value space.Clearly, it makes sense to approximate J * with a function J that is as close as possible to J * .However, we should also try to understand quantitatively the relation between J and J μ, the cost function of the resulting one-step lookahead (or multistep lookahead) policy μ.Interesting questions in this regard are the following:(a) How is the quality of the lookahead policy μ affected by the quality of the off-line training?A related question is how much should we care about improving J through a longer and more sophisticated training process, for a given approximation architecture?A fundamental fact that provides a lot of insight in this respect is that J μ is the result of a step of Newton's method that starts at J and is applied to the Bellman Eq. (1.33).This will be the focus of our discussion in the next section, and has been a major point in the narrative of the author's books, [Ber20a] and [Ber22a].A related fact is that in approximation in value space with multistep lookahead, J μ is the result of a step of Newton's method that starts at the function obtained by applying multiple value iterations to J.(b) How do simplifications in the multistep lookahead implementation affect J μ? The Newton step interpretation of approximation in value space leads to an important insight into the special character of the initial step of the multistep lookahead.In particular, it is only the first step that acts as the Newton step, and needs to be implemented with precision.The subsequent steps are value iterations, which only serve to enhance the quality of the starting point of the Newton step, and hence their precise implementation is not critical .This idea suggests that simplifications of the lookahead steps after the first can be implemented with relatively small (if any) performance loss for the multistep lookahead policy.Important examples of such simplifications are the use of certainty equivalence (Sections 1.6.7,2.7.2, 2.8.3), and forms of pruning of the lookahead tree (Section 2.4).In practical terms, simplifications after the first step of the multistep lookahead can save a lot of on-line computation, which can be fruitfully invested in extending the length of the lookahead.(c) When is μ stable?The question of stability is very important in many control applications where the objective is to keep the state near some reference point or trajectory.Indeed, in such applications, stability is the dominant concern, and optimality is secondary by comparison.Among others, here we are interested to characterize the set of terminal cost approximations J that lead to a stable μ.(d) How does the length of lookahead minimization or the length of the truncated rollout affect the stability and quality of the multistep lookahead policy μ?While it is generally true that the length of lookahead has a beneficial effect on quality, it turns out that it also has a beneficial effect on the stability properties of the multistep lookahead policy, and we are interested in the mechanism by which this occurs.In what follows we will be keeping in mind these questions.In particular, in the next section, we will discuss them in the context of the simple and convenient linear quadratic problem.Our conclusions, however, hold within a far more general context with the aid of the abstract DP formalism; see the author's books [Ber20a] and [Ber22a] for a broader presentation and analysis, which address these questions in greater detail and generality.We will now aim to understand the character of the Bellman equation, approximation in value space, and the VI and PI algorithms within the context of an important deterministic problem.This is the classical continuous-spaces problem where the system is linear, with no control constraints, and the cost function is nonnegative quadratic.While this prob-lem can be solved analytically, it provides a uniquely insightful context for understanding visually the Bellman equation and its algorithmic solution, both exactly and approximately.In its general form, the problem deals with the system In what follows, we will focus for simplicity only on the one-dimensional version of the problem, where the system has the formHere the state x k and the control u k are scalars, and the coefficients a and b are also scalars, with b = 0.The cost function is undiscounted and has the formwhere q and r are positive scalars.The one-dimensional case allows a convenient and insightful analysis of the algorithmic issues that are central for our purposes.This analysis generalizes to multidimensional linear quadratic problems and beyond, but requires a more demanding mathematical treatment.The analytical results for our problem may be obtained by taking the limit in the results derived in the finite horizon Example 1.3.1, as the horizon length tends to infinity.In particular, we can show that the optimal cost function is expected to be quadratic of the formwhere the scalar K * solves the equationSec. 1.5Newton's Method -Linear Quadratic Problems 57with F defined byThis is the limiting form of Eq. (1.19).Moreover, the optimal policy is linear of the formwhere L * is the scalar given by(1.44)To justify Eqs.(1.41)-(1.44),we show that J * as given by Eq. (1.40), satisfies the Bellman equationand that µ * (x), as given by Eqs.(1.43)-(1.44),attains the minimum above for every x when J = J * .Indeed for any quadratic cost function J(x) = Kx 2 with K ≥ 0, the minimization in Bellman's equation (1.45) is written as minThus it involves minimization of a positive definite quadratic in u and can be done analytically.By setting to 0 the derivative with respect to u of the expression in braces in Eq. (1.46), we obtain 0 = 2ru + 2bK(ax + bu), so the minimizing control and corresponding policy are given bywhereBy substituting this control, the minimized expression (1.46) takes the formAfter straightforward algebra, using Eq.(1.48) for L K , it can be verified that this expression is written as F (K)x 2 , with F given by Eq. (1.42).Thus when J(x) = Kx 2 , the Bellman equation (1.45) takes the formExact and Approximate Dynamic Programming Chap. 1In conclusion, when restricted to quadratic functions J(x) = Kx 2 with K ≥ 0, the Bellman equation (1.45) is equivalent to the equationWe can also characterize the cost function of a policy µ that is linear of the form µ(x) = Lx, and is also stable, in the sense that the scalar L satisfies |a + bL| < 1, so that the corresponding closed-loop systemIn particular, we can show that its cost function has the form† This is an algebraic form of the Riccati differential equation, which was invented in its one-dimensional form by count Jacopo Riccati in the 1700s, and has played an important role in control theory.It has been studied extensively in its differential and difference matrix versions; see the book by Lancaster and Rodman [LR95], and the paper collection by Bittanti, Laub, and Willems [BLW91], which also includes a historical account by Bittanti [Bit91] of Riccati's remarkable life and accomplishments.‡ The Riccati operator is a special case of the Bellman operator , denoted by T , which transforms a function J into the right side of Bellman's equation:Thus the Bellman operator T transforms a function J of x into another function T J also of x.Bellman operators allow a succinct abstract description of the problem's data, and are fundamental in the theory of abstract DP (see the author's monographs [Ber22a] and [Ber22b]).We may view the Riccati operator as the restriction of the Bellman operator to the subspace of quadratic functions of x.Sec. 1.5Newton's Method -Linear Quadratic Problems 59 0Note that F is concave and monotonically increasing in the interval (−r/b 2 , ∞) and "flattens out" as K → ∞, as shown in the figure.The quadratic Riccati equation K = F (K) also has another solution, denoted by K, which is negative and is thus of no interest.where K L solves the equationwith F L defined byThis equation is called the Riccati equation for the stable policy µ(x) = Lx.It is illustrated in Fig. 1.5.2, and it is linear, with linear coefficient (a+bL) 2 that is strictly less than 1.Hence the line that represents the graph of F L intersects the 45-degree line at a unique point, which defines the quadratic cost coefficient K L .The Riccati equation (1.50)-(1.51)for µ(x) = Lx may be justified by verifying that it is in fact the Bellman equation for µ,State F L (K) for an Unstable and a Stable Lis the Riccati equation operator corresponding to µ(x) = Lx.If µ is not stable, i.e., |a + bL| ≥ 1, we have Jµ(x) = ∞ for all x = 0, but the equation has K = F L (K) still has a solution that is of no interest within our context.[cf.Eq. (1.34)], restricted to quadratic functions of the form J(x) = Kx 2 .We note, however, that J µ (x) = K L x 2 is the solution of the Riccati equation (1.50)-(1.51)only when µ(x) = Lx is stable.If µ is not stable, i.e., |a + bL| ≥ 1, then (since q > 0 and r > 0) we have J µ (x) = ∞ for all x = 0.Then, the Riccati equation (1.50)-(1.51) is still defined, but its solution is negative and is of no interest within our context.The VI algorithm for our linear quadratic problem is given byNewton's Method -Linear Quadratic Problems 61 0The algorithm converges to K * starting from any K 0 ≥ 0.with F being the Riccati operator of Eq. (1.49).The algorithm is illustrated in Fig. 1.5.3.As can be seen from the figure, when starting from any K 0 ≥ 0, the algorithm generates a sequence {K k } of nonnegative scalars that converges to K * .The use of Riccati equations allows insightful visualization of approximation in value space.This visualization, although specialized to linear quadratic problems, is consistent with related visualizations for more general infinite horizon problems; this is a recurring theme in what follows.In particular, in the books [Ber20a] and [Ber22a], Bellman operators, which define the Bellman equations, are used in place of Riccati operators, which define the Riccati equations.In summary, we will aim to show that:(a) Approximation in value space with one-step lookahead can be viewed as a Newton step for solving the Bellman equation, and maps the terminal cost function approximation J to the cost function J μ of the one-step lookahead policy; see Fig. NEWTON STEP NEWTON STEP for solving the Bellman Eq. for solving the Bellman Eq.Kx 2 = F (K)x 2 or or K = F (K) Our derivation will be given for the one-dimensional linear quadratic problem, but applies far more generally.The reason is that the Bellman equation is valid universally in DP, and the corresponding Bellman operator has a concavity property that is well-suited for the application of Newton's method; see the books [Ber20a] and [Ber22a], where the connection of approximation in value space with Newton's method was first developed in detail.Let us consider one-step lookahead minimization with any terminal cost function approximation of the form J(x) = Kx 2 , where K ≥ 0. We have derived the one-step lookahead policy µ K (x) in Eqs.(1.47)-(1.48),by minimizing the right side of Bellman's equation whenWe can break this minimization into a sequence of two minimizations as follows:  From this equation, it follows thatwhere the function F L (K) is defined byLet us now fix the terminal cost function approximation to some Kx 2 , where K ≥ 0, and consider the corresponding one-step lookahead policy, which we will denote by μ.Chap. 1 0Tangent Riccati Operator at Tangent Riccati Operator at Kas L ranges over the real numbers.We havecf. Eq. (1.52).Moreover, for any fixed K, the scalar L that attains the minimum is given by, and is such that the line corresponding to the graph of F L is tangent to the graph of F at K, as shown in the figure.Thus the function F L can be viewed as a linearization of F at the point K, and defines a linearized problem: to find a solution of the equationThe important point now is that the solution of this equation, denoted K L, is the same as the one obtained from a single iteration of Newton's method for solving the Riccati equation, starting from the point K.This is illustrated in Fig. 1.5.7, and is also justified analytically in Exercise 1.7.To explain this connection, we note that the classical form of Newton's method for solving a fixed point problem of the form y = T (y), where y is an n-dimensional vector, operates as follows: At the current iterate y k , we linearize T and find the solution y k+1 of the corresponding linear fixed Sec. 1.5 Newton's Method -Linear Quadratic Problems 65 0 point problem.Assuming T is differentiable, the linearization is obtained by using a first order Taylor expansion:where ∂T (y k )/∂y is the n × n Jacobian matrix of T evaluated at the vector y k , as indicated in Fig. 1.5.7.The most commonly given convergence rate property of Newton's method is quadratic convergence.It states that near the solution y * , we have Also Region of Convergence of Newton's Method Also Region of Convergence of Newton's Method also K S Figure 1.5.8Illustration of the region of stability, i.e., the set of K ≥ 0 such that the one-step lookahead policy µ K is stable.This is also the set of initial conditions for which Newton's method converges to K * asymptotically.Note also that if the one-step lookahead policy is stable, i.e., |a+b L| < 1, then K L is the quadratic cost coefficient of its cost function, i.e.,The reason is that J μ solves the Bellman equation for policy μ.On the other hand, if μ is not stable, then in view of the positive definite quadratic cost per stage, we have J μ(x) = ∞ for all x = 0.In the case of -step lookahead minimization, a similar Newton step interpretation is possible.Instead of linearizing F at K, we linearize ati.e., the result of − 1 successive applications of F starting with K.Each application of F corresponds to a value iteration.Thus the effective starting point for the Newton step is F −1 ( K). Figure 1.5.9 depicts the case = 2.5.9 Illustration of approximation in value space with two-step lookahead for the linear quadratic problem.Starting with a terminal cost approximation J = Kx 2 , we obtain K 1 using a single value iteration.We then compute the corresponding linear policy μ(x) = Lx, whereand the corresponding cost function K Lx 2 , using the Newton step shown.The figure shows that for any K ≥ 0, the corresponding -step lookahead policy will be stable for all larger than some threshold.It is also useful to define the region of stability as the set of K ≥ 0 such thatwhere L K is the linear coefficient of the one-step lookahead policy corresponding to K; cf.Eq. (1.48).The region of stability may also be viewed as the region of convergence of Newton's method .It is the set of starting points K for which Newton's method, applied to the Riccati equation F = F (K), converges to K * asymptotically, and with a quadratic convergence rate (asymptotically as K → K * ).Note that for our one-dimensional problem, the region of stability is the interval (K S , ∞) that is characterized by the single point K S where F has derivative equal to 1; see Fig.Riccati equation for a linear policy µ(xCost coefficient K L of a stable linear policy µ(x) = LxLinear coefficient L K of the one-step lookahead linear policy µ K for K in the region of stability [cf.Eq. (1.48)]Quadratic cost coefficient K of a one-step lookahead linear policy µ K for K in the region of stability Obtained as the solution of the linearized Riccati equationor equivalently by a Newton iteration starting from K.For multidimensional problems, the region of stability may not be characterized as easily.Still, however, it is generally true that the region of stability is enlarged as the length of the lookahead increases.Indeed, with increased lookahead, the effective starting pointis pushed more and more within the region of stability.In particular, for any given K ≥ 0, the corresponding -step lookahead policy will be stable for all larger than some threshold ; see Fig. 1.5.9.The book [Ber22a],Section 3.3, contains a broader discussion of the region of stability and the role of multistep lookahead in enhancing it; see also Exercise 1.8.The interpretation of approximation in value space as a Newton step, and related notions of stability that we have discussed in this section admit a broad generalization to the infinite horizon problems that we consider in this book and beyond.The key fact in this respect is that our DP problem formulation allows arbitrary state and control spaces, both discrete and continuous, and can be extended even further to general abstract models with a DP structure; see the abstract DP book [Ber22b].Within this context, the Riccati operator is replaced by an abstract Bellman operator, and valuable insight can be obtained from graphical interpretations of the Bellman equation, the VI and PI algorithms, onestep and multistep approximation in value space, the region of stability, and exceptional behavior; see the book [Ber22a] for an extensive discussion.Naturally, the graphical interpretations and visualizations are limited to one dimension.However, the visualizations provide insight and motivate conjectures and mathematical analysis, much of which is given in the book [Ber20a].The Newton step interpretation of approximation in value space leads to an important insight into the special character of the initial step in -step lookahead implementations.In particular, it is only the first step that acts as the Newton step, and needs to be implemented with precision; cf.Fig. 1.5.5.The subsequent − 1 steps are a sequence of value iterations starting with J, and only serve to enhance the quality of the starting point of the Newton step.As a result, their precise implementation is not critical , a major point in the narrative of the author's book [Ber22a].This idea suggests that we can simplify (within reason) the lookahead steps after the first with small (if any) performance loss for the multistep lookahead policy.An important example of such a simplification is the use of certainty equivalence, which will be discussed later in various contexts (Sections 1.6.7,2.7.2, 2.8.3).Other possibilities include the "pruning" of the lookahead tree after the first step; see Section 2.4.In practical terms, simplifications after the first step of the multistep lookahead can save a lot of on-line computation, which can be fruitfully invested in extending the length of the lookahead.This insight is supported by substantial computational experimentation, starting with the paper by Bertsekas and Castañon [BeC98], which verified the beneficial effect of using certainty equivalence after the first step.Chap. 1We will now consider the rollout algorithm for the linear quadratic problem, starting from a linear stable base policy µ.It generates the rollout policy μ by using a policy improvement operation, which by definition, yields the one-step lookahead policy that corresponds to terminal cost approximation J = J µ .Figure 1.5.10 illustrates the rollout algorithm.It can be seen from the figure that the rollout policy is in fact an improved policy, in the sense that J μ(x) ≤ J µ (x) for all x.Among others, this implies that the rollout policy is stable, since µ is assumed stable so that J µ (x) < ∞ for all x.Since the rollout policy is a one-step lookahead policy, it can also be described using the formulas that we developed earlier in this section.In particular, let the base policy have the formwhere L 0 is a scalar.We require that the base policy must be stable, i.e., |a + bL 0 | < 1.From our earlier calculations, we have that the cost function of µ 0 iswhereMoreover, the rollout policy µ 1 has the form µ 1 (x) = L 1 x, whereThe PI algorithm is simply the repeated application of nontruncated rollout, and generates a sequence of stable linear policies {µ k }.By replicating our earlier calculations, we see that the policies have the formwhere L k is generated by the iterationwith K k given byAn m-step truncated rollout scheme with a stable linear base policy µ(x) = Lx, one-step lookahead minimization, and terminal cost approximation J(x) = Kx 2 is geometrically interpreted as in Fig. 1.5.11.The truncated rollout policy μ is obtained by starting at K, executing m VI steps using µ, followed by a Newton step for solving the Riccati equation.We mentioned some interesting performance issues in our discussion of truncated rollout in Section 1.1.In particular we noted that: Chap. 1of Truncated Rollout Policy Cost of Truncated Rollout Policy μ (a) Lookahead by rollout may be an economic substitute for lookahead by minimization, in the sense that it may achieve a similar performance for the truncated rollout policy at significantly reduced computational cost.(b) Lookahead by rollout with a stable policy has a beneficial effect on the stability properties of the lookahead policy.These statements are difficult to establish analytically in some generality.However, they can be intuitively understood in the context with our onedimensional linear quadratic problem, using geometrical constructions like the one of Fig. 1.5.11.They are also consistent with the results of computational experimentation.We refer to the monograph [Ber22a] for further discussion.In approximation in value space, an important analytical issue is to quantify the level of suboptimality of the one-step or multistep lookahead policy obtained.It is thus important to understand the character of the critical mapping between the approximation error J − J * and the performance error J μ − J * , where as earlier, J μ is the cost function of the lookahead policy μ and J * is the optimal cost function.(for -step lookahead)Step "Future" CRITICAL MAPPING12 Illustration of the linear error bound (1.58) for -step lookahead approximation in value space.For = 1, we obtain the one-step bound (1.57).There is a classical one-step lookahead error bound for the case of an α-discounted problem with finite state space X, which has the formwhere • denotes the maximum norm,see e.g., [Ber19a], Prop.5.1.1.The bound (1.57) predicts a linear relation between the size of the approximation error J − J * and the performance error J μ −J * .For a generalization, we may view -step lookahead as onestep lookahead with a terminal cost function T −1 J, i.e., J transformed by − 1 value iterations.We then obtain the -step boundThe linear bounds (1.57)-(1.58)are illustrated in Fig. 1.5.12, and apply beyond the α-discounted case, to problems where the Bellman equation involves a contraction mapping over a subset of functions; see the RL book [Ber19a], Section 5.9.1, or the abstract DP book [Ber22b], Section 2.2.Unfortunately, the linear error bounds are very conservative, and do not reflect practical reality, even qualitatively so.The main reason is that they are global error bounds, i.e., they hold for all J, even the worst possible.In practice, J is often chosen sufficiently close to J * , so that the error J μ−J * behaves consistently with the superlinear convergence rate of the Newton Chap. 1For 1-step lookahead for the case of -step lookahead approximation in value space scheme.The performance error rises rapidly outside the region of convergence of Newton's method [the illustration in the figure is not realistic; in fact the region of convergence is not bounded as it contains lines of the form γe, where γ is a scalar and e is the unit vector (all components equal to 1)].Note that this region expands as the size of lookahead increases.Furthermore, with long enough lookahead , thestep lookahead policy μ can be shown to be exactly optimal for many problems of interest; this is a theoretical result, which holds for α-discounted finite-state problems, among others, and has been known since the 60s-70s (Prop.2.3.1 of [Ber22a] proves a general form of this result that applies beyond discounted problems).step that starts at J. In other words, for J relatively close to J * , we have the local estimate A salient characteristic of this superlinear relation is that the performance error rises rapidly outside the region of superlinear convergence of Newton's method.Note that small improvements in the quality of J (e.g., better sampling methods, improved confidence intervals, and the like, without changing the approximation architecture) have little effect, both inside and outside the region of convergence.In practical terms, there is often a huge difference, both quantitative and qualitative, between the linear error bounds (1.57)-(1.58)and the superlinear error bound (1.59).Moreover, the linear bounds, despite their popularity in academia, often misdirect academic research and confuse practitioners.† Note that as we have mentioned earlier, the qualitative † A study by Laidlaw, Russell, and Dragan [LRD23] has assessed the practical performance of popular methods on a set of 155 problems, and found wide disparities relative to theoretical predictions.Quoting from this paper: "we find that prior bounds do not correlate well with when deep RL succeeds vs. fails."performace behavior predicted in Fig. 1.5.13holds very broadly in approximation in value space, because it relies on notions of abstract DP that apply very generally, for arbitrary state spaces, control spaces, and other problem characteristics; see the abstract DP book [Ber22a].We illustrate the failure of the linear error bound (1.57) to predict the performance of the one-step lookahead policy with an example given in Fig. 1.5.14.Example 1.5.1 (Global and Actual Error Bounds for a Linear Quadratic Problem)Consider the one-dimensional linear quadratic problem, involving the system x k+1 = ax k + bu k , and the cost per stage qx 2 k + ru 2 k .We will consider one-step lookahead, and a quadratic cost function approximationwith K within the region of stability, which is some interval of the form (S, ∞).The Riccati operator isand the one-step lookahead policy μ has cost functionwhere Kμ is obtained by applying one step of Newton's method for solving the Riccati equation K = F (K), starting at K = K.Let S be the boundary of the region of stability, i.e., the value of K at which the derivative of F with respect to K is equal to 1:Then the Riccati operator F is a contraction within any interval [S, ∞) with S > S, with a contraction modulus α that depends on S. In particular, α is given byThe study goes on to assert the importance of long multistep lookahead (the size of ) in stabilizing the performance of approximation in value space schemes.It also confirms computationally a known theoretical result, namely that with long enough lookahead , the -step lookahead policy μ is exactly optimal (but the required length of depends on the approximation error J − J * ).This fact has been known since the 60s-70s for α-discounted finite-state problems.A generalization of this result is given as Prop.2.3.1 of the abstract DP book [Ber22b]; see also Section A.4 of the book [Ber22a], which discusses the convergence of Newton's method for systems of equations that involve nondifferentiable mappings (such as the Bellman operator).Chap. 1  Depending on the chosen value of S, α can be arbitrarily close to 1, but decreases as S increases.Note that the error K μ − K * is much smaller when K is larger than K * than when it is lower, because the slope of F diminishes as K increases.This is not reflected by the global error bound.and satisfies 0 < α < 1 because S > S, and the derivative of F is positive and monotonically decreasing to 0 as K increases to ∞.The error bound (1.57) can be rederived for the case of quadratic functions and can be rewritten in terms of quadratic cost coefficients aswhere Kμ is the quadratic cost coefficient of the lookahead policy μ [and also the result of a Newton step for solving the fixed point Riccati equationA plot of (Kμ−K * ) as a function of K, compared with the bound on the right side of this equation is shown in Fig. 1.5.14.It can be seen that (Kμ − K * ) exhibits the qualitative behavior of Newton's method, which is very different than the bound (1.60).An interesting fact is that the bound (1.60) depends on α, which in turn depends on how close K is to the boundary S of the region of stability, while the local behavior of Newton's method is independent of S.Examples, Reformulations, and Simplifications 77Let us finally discuss the most common way that approximation in value space can fail.Consider the case where the terminal cost approximation J is obtained through training with data of an approximation architecture such as a neural network (e.g., as in AlphaZero and TD-Gammon).Then there are three components that determine the approximation error J − J * :(a) The power of the architecture, which roughly speaking is a measure of the error that would be obtained if infinite data were available and were used optimally to obtain J.(b) The error degradation due the limited availability of training data.(c) The additional error degradation due to imperfections in the training methodology.Thus if the architecture is not powerful enough to bring J − J * within the region of convergence of Newton's method, approximation in value space with one-step lookahead will likely fail, no matter how much data is collected and how effective the associated training method is.In this case, there are two potential practical remedies:(1) Use a more powerful architecture/neural network for representing J.(2) Extend the combined length of the lookahead minimization and truncated rollout in order to bring the effective value of J within the region of convergence of Newton's method.The first remedy typically requires a deep neural network or transformer, which uses more weights and requires more expensive training (see Chapter 3).† The second remedy requires longer on-line computation and/or simulation, which may run up against some practical real-time implementation constraint.Parallel computation and sophisticated multistep lookahead implementation methods may help to mitigate these requirements (see Chapter 2).In ).An important fact to keep in mind is that there are many ways to model a given practical problem in terms of DP, and that there is no unique choice for state and control variables.This will be brought out by the examples in this section, and is facilitated by the generality of DP: its basic algorithmic principles apply for arbitrary state, control, and disturbance spaces, and system and cost functions.In practice, optimization problems seldom come neatly packaged as mathematical problems that can be solved by DP/RL or some other methodology.Generally, a practical problem is a prime candidate for a DP formulation if it involves multiple sequential decisions, which are separated by feedback, i.e., by observations that can be used to enhance the effectiveness of future decisions.However, there are other types of problems that can be fruitfully formulated by DP.These include the entire class of deterministic problems, where there is no information to be collected: all the information needed in a deterministic problem is either known or can be predicted from the problem data that is available at time 0 (see, e.g., the traveling salesman Example 1.2.3).Moreover, for deterministic problems there is a plethora of non-DP methods, such as linear, nonlinear, and integer programming, random and nonrandom search, discrete optimization heuristics, etc.Still, however, the use of RL methods for deterministic optimization is a major subject in this book, which will be discussed in Chapter 2. We will argue there that rollout and its variations, when suitably applied, can improve substantially on the performance of other heuristic or suboptimal methods, however derived.Moreover, we will see that often for discrete optimization problems the DP sequential structure is introduced artificially, with the aim to facilitate the use of approximate DP/RL methods.There are also problems that fit quite well into the sequential structure of DP, but can be fruitfully addressed by RL methods that do not have a fundamental connection with DP.An important case in point is policy gradient and policy search methods, which will not be considered in this book.Here the policy of the problem is parametrized by a set of parameters, so that the cost of the policy becomes a function of these parameters, and can be optimized by non-DP methods such as gradient or random search-based suboptimal approaches.This generally relates to the approximation in policy space approach, which we have discussed in Section 1.3.3 and we will discuss further in Section 3.4; see also Section 5.7 of the RL book [Ber19a].As a guide for formulating optimal control problems in a manner that is suitable for a DP solution the following two-stage process is suggested:(a) Identify the controls/decisions u k and the times k at which these controls are applied.Usually this step is fairly straightforward.However, in some cases there may be some choices to make.For example in deterministic problems, where the objective is to select an optimal sequence of controls {u 0 , . . ., u N −1 }, one may lump multiple controls to be chosen together, e.g., view the pair (u 0 , u 1 ) as a single choice.This is usually not possible in stochastic problems, where distinct decisions are differentiated by the information/feedback available when making them.(b) Select the states x k .The basic guideline here is that x k should encompass all the information that is relevant for future optimization, i.e., the information that is known to the controller at time k and can be used with advantage in choosing u k .In effect, at time k the state x k should separate the past from the future, in the sense that anything that has happened in the past (states, controls, and disturbances from stages prior to stage k) is irrelevant to the choices of future controls as long we know x k .Sometimes this is described by saying that the state should have a "Markov property" to express an analogy with states of Markov chains, where (by definition) the conditional probability distribution of future states depends on the past history of the chain only through the present state.The control and state selection may also have to be refined or specialized in order to enhance the application of known results and algorithms.This includes the choice of a finite or an infinite horizon, and the availability of good base policies or heuristics in the context of rollout.Note that there may be multiple possibilities for selecting the states, because information may be packaged in several different ways that are equally useful from the point of view of control.It may thus be worth considering alternative ways to choose the states; for example try to use states that minimize the dimensionality of the state space.For a trivial example that illustrates the point, if a quantity x k qualifies as state, then (x k−1 , x k ) also qualifies as state, since (x k−1 , x k ) contains all the information contained within x k that can be useful to the controller when selecting u k .However, using (x k−1 , x k ) in place of x k , gains nothing in terms of optimal cost while complicating the DP algorithm that would have to be executed over a larger space.The concept of a sufficient statistic, which refers to a quantity that summarizes all the essential content of the information available to the controller, may be useful in providing alternative descriptions of the state space.An important paradigm is problems involving partial or imperfect state information, where x k evolves over time but is not fully accessible for measurement (for example, x k may be the position/velocity vector of a moving vehicle, but we may obtain measurements of just the position).If I k is the collection of all measurements and controls up to time k (the information vector ), it is correct to use I k as state in a reformulated DP problem that involves perfect state observation.However, a better alternative may be to use as state the conditional probability distributioncalled belief state, which (as it turns out) subsumes all the information that is useful for the purposes of choosing a control.On the other hand, the belief state P k (x k | I k ) is an infinite-dimensional object, whereas I k may be finite dimensional, so the best choice may be problem-dependent.Still, in either case, the stochastic DP algorithm applies, with the sufficient statistic [whether I k or P k (x k | I k )] playing the role of the state.An attractive aspect of the current RL methodology, inherited by the generality of our DP formulation, is that it can address a very broad range of challenging problems, deterministic as well as stochastic, discrete as well as continuous, etc.However, in the practical application of RL methods one has to contend with limited theoretical guarantees.In particular, several of the RL methods that have been successful in practice have less than solid performance properties, and may not work on a given problem, even one of the type for which they are designed.This is a reflection of the state of the art in the field: there are no methods that are guaranteed to work for all or even most DP problems.However, there are enough methods to try on a given problem with a reasonable chance of success in the end (after some heuristic and problem specific tuning).For this reason, it is important to develop insight into the inner workings of various methods, as a means of selecting the proper type of methodology to try on a given problem.† A related consideration is the context within which a method is applied.In particular, is it a single problem that is being addressed, such as chess that has fixed rules and a fixed initial condition, or is it a family of related problems that must be periodically be solved with small variations in its data or its initial conditions?Also, are the problem data fixed or may they change over time as the system is being controlled?Generally, convenient but relatively unreliable methods, which can be tuned to the problem at hand, may be tried with a reasonable chance of success if a single problem is addressed.Similarly, RL methods that require † Aside from insight and intuition, it is also important to have a foundational understanding of the analytical principles of the field and of the mechanisms underlying the central computational methods.The role of the theory in this respect is to structure mathematically the methodology, guide the art, and delineate the sound from the flawed ideas.Sec. 1.6 extensive tuning of parameters, including ones that involve approximation in policy space and the use of neural networks, may be well suited for a stable problem environment and a single problem solution.However, they not well suited for problems with a variable environment and/or real-time changes of model parameters.For such problems, RL methods based on approximation in value space and on-line play, possibly involving on-line replanning, are much better suited.Note also that even when on-line replanning is not needed, on-line play may improve substantially the performance of off-line trained policies, so we may wish to use it in conjunction with off-line training.This is due to the Newton step that is implicit in one-step or multistep lookahead minimization, cf.our discussion of the AlphaZero and TD-Gammon architectures in Section 1.1.Of course the computational requirements of an on-line play method may be substantial and have to be taken into account when assessing its suitability for a particular application.In this connection, deterministic problems are better suited than stochastic problems for on-line play.Moreover, methods that are well-suited for parallel computation, and/or involve the use of certainty equivalence approximations are generally better suited for a stochastic control environment.Many DP problems of interest involve a termination state, i.e., a state t that is cost-free and absorbing in the sense that for all k,for all w k and u k ∈ U k (t).Thus the control process essentially terminates upon reaching t, even if this happens before the end of the horizon.One may reach t by choice if a special stopping decision is available, or by means of a random transition from another state.Problems involving games, such as chess, Go, backgammon, and others involve a termination state (the end of the game) and have played an important role in the development of the RL methodology.† Generally, when it is known that an optimal policy will reach the termination state with certainty within at most some given number of stages N , the DP problem can be formulated as an N -stage horizon problem, with a very large termination cost for the nontermination states.‡ The reason † Games often involve two players/decision makers, in which case they can be addressed by suitably modified exact or approximate DP algorithms.The DP algorithm that we have discussed in this chapter involves a single decision maker, but can be used to find an optimal policy for one player against a fixed and known policy of the other player.‡ When an upper bound on the number of stages to termination is not known, the problem may be formulated as an infinite horizon problem of the stochastic shortest path problem.Chap. 1 is that even if the termination state t is reached at a time k < N, we can extend our stay at t for an additional N − k stages at no additional cost, so the optimal policy will still be optimal, since it will not incur the large termination cost at the end of the horizon.Example 1.6.1 (Parking)A driver is looking for inexpensive parking on the way to his destination.The parking area contains N spaces, numbered 0, . . ., N − 1, and a garage following space N − 1.The driver starts at space 0 and traverses the parking spaces sequentially, i.e., from space k he goes next to space k + 1, etc.Each parking space k costs c(k) and is free with probability p(k) independently of whether other parking spaces are free or not.If the driver reaches the last parking space N − 1 and does not park there, he must park at the garage, which costs C. The driver can observe whether a parking space is free only when he reaches it, and then, if it is free, he makes a decision to park in that space or not to park and check the next space.The problem is to find the minimum expected cost parking policy.We formulate the problem as a DP problem with N stages, corresponding to the parking spaces, and an artificial termination state t that corresponds to having parked; see Fig. 1.6.1.At each stage k = 1, . . ., N − 1, we have three states: the artificial termination state t, and the two states F and F , corresponding to space k being free or taken, respectively.At stage 0, we have only two states, F and F , and at the final stage there is only one state, the termination state t.The decision/control is to park or continue at state F [there is no choice at states F and state t].From location k, the termination state t is reached at cost c(k) when a parking decision is made (assuming location k is free).Otherwise, the driver continues to the next state at no cost.At stage N , the driver must park at cost C.Let us now derive the form of the DP algorithm, denoting:The optimal cost-to-go upon arrival at a space k that is free.The optimal cost-to-go upon arrival at a space k that is taken.J * k (t): The cost-to-go of the "parked"/termination state t.Examples, Reformulations, and Simplifications 83The DP algorithm for k = 0, . . ., N − 1 takes the formfor the states other than the termination state t, while for t we haveThe minimization above corresponds to the two choices (park or not park) at the states F that correspond to a free parking space.While this algorithm is easily executed, it can be written in a simpler and equivalent form.This can be done by introducing the scalarswhich can be viewed as the optimal expected cost-to-go upon arriving at space k but before verifying its free or taken status.Indeed, from the preceding DP algorithm, we haveFrom this algorithm we can also obtain the optimal parking policy:Park at space k = 0, . . ., N − 1 if it is free and we have c(k) ≤ Ĵk+1 .This is an example of DP simplification that occurs when the state involves components that are not affected by the choice of control, and will be addressed in the next section.Let us now extend the ideas of the examples just noted to the general discrete optimization problem:where U is a finite set of feasible solutions and G(u) is a cost function.We assume that each solution u has N components; i.e., it has the form u = (u 0 , . . ., u N −1 ),where N is a positive integer.We can then view the problem as a sequential decision problem, where the components u 0 , . . ., u N −1 are selected one-ata-time.A k-tuple (u 0 , . . ., u k−1 ) consisting of the first k components of a solution is called a k-solution.We associate k-solutions with the kth stage of the finite horizon discrete optimal control problem shown in Fig. 1.6.2.In particular, for k = 1, . . ., N, we view as the states of the kth stage all the k-tuples (u 0 , . . ., u k−1 ).For stage k = 0, . . ., N − 1, we view u k as the control.The initial state is an artificial state denoted s.From this state, by applying u 0 , we may move to any "state" (u 0 ), with u 0 belonging to the set U 0 = ũ0 | there exists a solution of the form (ũ 0 , ũ1 , . . ., ũN−1 ) ∈ U .(1.62) Thus U 0 is the set of choices of u 0 that are consistent with feasibility.More generally, from a state (u 0 , . . ., u k−1 ), we may move to any state of the form (u 0 , . . ., u k−1 , u k ), upon choosing a control u k that belongs to Sec. 1.6 the set U k (u 0 , . . ., u k−1 ) = u k | for some u k+1 , . . ., u N −1 we have (u 0 , . . ., u k−1 , u k , u k+1 , . . ., u N −1 ) ∈ U .(1.63)These are the choices of u k that are consistent with the preceding choices u 0 , . . ., u k−1 , and are also consistent with feasibility [we do not exclude the possibility that the set (1.63) is empty].The last stage corresponds to the N -solutions u = (u 0 , . . ., u N −1 ), and the terminal cost is G(u); see Fig. 1.6.2.All other transitions in this DP problem formulation have cost 0.Let J * k (u 0 , . . ., u k−1 ) denote the optimal cost starting from the ksolution (u 0 , . . ., u k−1 ), i.e., the optimal cost of the problem over solutions whose first k components are constrained to be equal to u 0 , . . ., u k−1 .The DP algorithm is described by the equationwith the terminal conditionThis algorithm executes backwards in time: starting with the known function J * N = G, we compute J * N −1 , then J * N −2 , and so on up to computing J * 0 .An optimal solution (u * 0 , . . ., u * N −1 ) is then constructed by going forward through the algorithm(1.64) where U 0 is given by Eq. (1.62), and U k is given by Eq. (1.63): first compute u * 0 , then u * 1 , and so on up to u * N −1 ; cf.Eq. (1.8).Of course here the number of states typically grows exponentially with N , but we can use the DP minimization (1.64) as a starting point for approximation methods.For example we may try to use approximation in value space, whereby we replace J * k+1 with some suboptimal Jk+1 in Eq. (1.64).One possibility is to use asthe cost generated by a heuristic method that solves the problem suboptimally with the values of the first k + 1 decision components fixed at u * 0 , . . ., u * k−1 , u k .This is the rollout algorithm, which turns out to be a very simple and effective approach for approximate combinatorial optimization.Let us finally note that while we have used a general cost function G and constraint set U in our discrete optimization model of this section, in Chap. 1 Let us consider an N -gram model, whereby a text string consisting of N words is transformed into another string of N words by adding a word at the front of the string and deleting the word at the back of the string.We view the text strings as states of a dynamic system affected by the added word choice, which we view as the control.We denote by x k the string obtained at time k, and by u k the word added at time k.We assume that u k is chosen from a given set U (x k ).Thus we have a controlled dynamic system, which is deterministic and is described by an equation of the formwhere f specifies the operation of adding u k at the front of x k and removing the word at the back of x k .The initial string x0 is assumed given.If we have a cost function G by which to evaluate a text string, we can pose a DP problem with either a finite or an infinite horizon.For example if the string evolution terminates after exactly N steps, we obtain the finite horizon problem of minimizing the function G(xN ) of the final text string xN .In this case, xN is obtained after we have a chance to change successively all the words of the initial string x0, subject to the constraints u k ∈ U (x k ).Another possibility is to introduce a termination action, whereby addition/deletion of words is optionally stopped at some time and the final text string x is obtained with cost G(x).In such a problem formulation, we may also include an additive stage cost that depends on u.This is an infinite horizon formulation that involves an additional termination state t in the manner of Section 1.6.2.Note that in both the finite and the infinite horizon formulation of the problem, the initial string x0 may include a "prompt," which may be subject to optimization through some kind of "prompt engineering."Depending on the context, this may include the use of another optimization or heuristic algorithm, perhaps unrelated to DP, which searches for a favorable prompt from within a given set of choices.Interesting policies for the preceding problem formulation may be provided by a neural network, such as a Generative Pretrained Transformer (GPT).In our terms, the GPT can be viewed simply as a policy that generates next words.This policy may be either deterministic, i.e., u k = µ(x k ) for some function µ, or it may be a "randomized" policy, which generates u k according to a probability distribution that depends on x k .Our DP formulation can also form the basis for policy improvement algorithms such as rollout, which aim to improve the quality of the output generated by the GPT.Another, more ambitious, possibility is to consider an approximate, neural network-based, policy iteration/self-training scheme, such as the ones discussed earlier, based on the AlphaZero/TD-Gammon architecture.Such a scheme generates a sequence of GPTs, with each GPT trained with data provided by the preceding GPT, a form of self-learning in the spirit of the AlphaZero and TD-Gammon policy iteration algorithms, cf.Section 1.1.It is also possible to provide an infinite horizon formulation of the general disrete optimization problem minimize G(u)where U is a finite set of feasible solutions, G(u) is a cost function, and u consists of N components, u = (u 0 , . . ., u N −1 ); cf.Eq. (1.61).To this end, we introduce a termination state t that the system enters after N steps.where the constraint set U k (u 0 , . . ., u k−1 is given by Eq. (1.63).This is a special case of a general finite to infinite horizon stochastic DP problem reformulation, which we describe in the next section.There is a conceptually important reformulation that transforms a finite horizon problem, possibly involving a nonstationary system and cost per stage, to an equivalent infinite horizon problem.It is based on introducing an expanded state space, which is the union of the state spaces of the finite horizon problem plus an artificial cost-free termination state that the system moves into at the end of the horizon.This reformulation is of great conceptual value, as it provides a mechanism to bring to bear ideas that can be most conveniently understood within an infinite horizon context.For example, it helps to understand the synergy of off-line training and on-line play based on Newton's method, and the related insights that explain the good performance of rollout algorithms in practice.To define the reformulation, let us consider the N -stage horizon stochastic problem of Section 1.3.1,whose system has the formOptimal cost and policy Optimal cost and policy Optimal cost and policy J * 0 (x0)) gN (xN ) 0Transitions from states x k ∈ X k lead to states in x k+1 ∈ X k+1 according to the system equation x k+1 = f k (x k , u k , w k ), and they are stochastic when they involve the random disturbance w k .The transition from states x N ∈ X N lead deterministically to the termination state at cost g N (x N ).The termination state t is cost-free and absorbing.The infinite horizon optimal cost J * (x k ) and optimal policy µ * (x k ) at state x k ∈ X k of the infinite horizon problem are equal to optimal cost-to-go J * k (x k ) and optimal policy µ * k (x k ) of the finite horizon problem.an artificial termination state t, and we consider an infinite horizon problem with state and control spaces X and U given byThe system equation and the control constraints of this problem are also reformulated so that states in X k , k = 0, . . ., N − 1, are mapped to states in X k+1 , according to Eq. (1.66), while states x N ∈ X N are mapped to the termination state t at cost g N (x N ).Upon reaching t, the state stays at t at no cost.Thus the policies of the infinite horizon problem map statesare policies of the finite horizon problem.Moreover, the Bellman equation for the infinite horizon problem is identical to the DP algorithm for the finite horizon problem.It can be seen that the optimal cost and optimal control, J * (x k ) and µ * (x k ), at a state x k ∈ X k in the infinite horizon problem are equal to the optimal cost-to-go J * k (x k ) and optimal control µ * k (x k ) of the original finite horizon problem, respectively; cf.Fig. 1.6.4.Moreover approximation in value space and rollout in the finite horizon problem translate to infinite horizon counterparts, and can be understood as Newton steps for solving the Bellman equation of the infinite horizon problem (or equivalently the DP algorithm of the finite horizon problem).In summary, finite horizon problems can be viewed as infinite horizon problems with a special structure that involves a termination state t, and the state and control spaces of Eq. (1.67), as illustrated in Fig. 1.6.4.The Bellman equation of the infinite horizon problem coincides with the DP algorithm of the finite horizon problem.The PI algorithm for the infinite horizon problem can be translated directly to a PI algorithm for the finite horizon problem, involving repeated policy evaluations and policy improvements.Finally, the Newton step interpretations for approximation in value space and rollout schemes for the infinite horizon problem have straightforward analogs for finite horizon problems, and explain the powerful cost improvement mechanism that underlies the rollout algorithm and its variations.In practice, we are often faced with situations where some of the assumptions of our stochastic optimal control problem formulation are violated.For example, the disturbances may involve a complex probabilistic description that may create correlations that extend across stages, or the system equation may include dependences on controls applied in earlier stages, which affect the state with some delay.Generally, in such cases the problem can be reformulated into our DP problem format through a technique, which is called state augmentation because it typically involves the enlargement of the state space.The general intuitive guideline in state augmentation is to include in the enlarged state at time k all the information that is known to the controller at time k and can be used with advantage in selecting u k .State augmentation allows the treatment of time delays in the effects of control on future states, correlated disturbances, forecasts of probability distributions of future disturbances, and many other complications.We note, however, that state augmentation often comes at a price: the reformulated problem may have a very complex state space.We provide some examples.Chap. 1In some applications the system state x k+1 depends not only on the preceding state x k and control u k , but also on earlier states and controls.Such situations can be handled by expanding the state to include an appropriate number of earlier states and controls.As an example, assume that there is at most a single stage delay in the state and control; i.e., the system equation has the formx 1 = f 0 (x 0 , u 0 , w 0 ).If we introduce additional state variables y k and s k , and we make the identificationsBy defining xk = (x k , y k , z k ) as the new state, we havewhere the system function fk is defined from Eq. (1.69).By using the preceding equation as the system equation and by expressing the cost function in terms of the new state, the problem is reduced to a problem without time delays.Naturally, the control u k should now depend on the new state xk , or equivalently a policy should consist of functions µ k of the current state x k , as well as the preceding state x k−1 and the preceding control u k−1 .When the DP algorithm for the reformulated problem is translated in terms of the variables of the original problem, it takes the formExamples, Reformulations, and Simplifications 91E w 0 g 0 (x 0 , u 0 , w 0 ) + J * 1 f 0 (x 0 , u 0 , w 0 ), x 0 , u 0 .Similar reformulations are possible when time delays appear in the cost or the control constraints; for example, in the case where the cost isThe extreme case of time delays in the cost arises in the nonadditive formThen, the problem can be reduced to the standard problem format, by using as augmented state xk = x k , x k−1 , . . ., x 0 , u k−1 , . . ., u 0 , w k−1 , . . ., w 0 and E g N (x N ) as reformulated cost.Policies consist of functions µ k of the present and past states x k , . . ., x 0 , the past controls u k−1 , . . ., u 0 , and the past disturbances w k−1 , . . ., w 0 .Naturally, we must assume that the past disturbances are known to the controller.Otherwise, we are faced with a problem where the state is imprecisely known to the controller, which will be discussed in the next section.Consider a situation where at time k the controller has access to a forecast y k that results in a reassessment of the probability distribution of the subsequent disturbance w k and, possibly, future disturbances.For example, y k may be an exact prediction of w k or an exact prediction that the probability distribution of w k is a specific one out of a finite collection of distributions.Forecasts of interest in practice are, for example, probabilistic predictions on the state of the weather, the interest rate for money, and the demand for inventory.Generally, forecasts can be handled by introducing additional state variables corresponding to the information that the forecasts provide.We will illustrate the process with a simple example.Assume that at the beginning of each stage k, the controller receives an accurate prediction that the next disturbance w k will be selected according to a particular probability distribution out of a given collection of distributions {P 1 , . . ., P m }; i.e., if the forecast is i, then w k is selected according to P i .The a priori probability that the forecast will be i is denoted by p i and is given.The forecasting process can be represented by means of the equationwhere y k+1 can take the values 1, . . ., m, corresponding to the m possible forecasts, and ξ k is a random variable taking the value i with probability p i .The interpretation here is that when ξ k takes the value i, then w k+1 will occur according to the distribution P i .By combining the system equation with the forecast equation y k+1 = ξ k , we obtain an augmented system given byThe new state and disturbance areThe probability distribution of wk is determined by the distributions P i and the probabilities p i , and depends explicitly on xk (via y k ) but not on the prior disturbances.Thus, by suitable reformulation of the cost, the problem can be cast as a stochastic DP problem.Note that the control applied depends on both the current state and the current forecast.The DP algorithm takes the form(1.70) where y k may take the values 1, . . ., m, and the expectation over w k is taken with respect to the distribution P y k .Note that the preceding formulation admits several extensions.One example is the case where forecasts can be influenced by the control action (e.g., pay extra for a more accurate forecast), and may involve several future disturbances.However, the price for these extensions is increased complexity of the corresponding DP algorithm.In many problems of interest the natural state of the problem consists of several components, some of which cannot be affected by the choice of control.In such cases the DP algorithm can be simplified considerably, and be executed over the controllable components of the state.As an example, let the state of the system be a composite (x k , y k ) of two components x k and y k .The evolution of the main component, x k , is affected by the control u k according to the equationExamples, Reformulations, and Simplifications 93where the distribution P k (w k | x k , y k , u k ) is given.The evolution of the other component, y k , is governed by a given conditional distribution P k (y k | x k ) and cannot be affected by the control, except indirectly through x k .One is tempted to view y k as a disturbance, but there is a difference: y k is observed by the controller before applying u k , while w k occurs after u k is applied, and indeed w k may probabilistically depend on u k .It turns out that we can formulate a DP algorithm that is executed over the controllable component of the state, with the dependence on the uncontrollable component being "averaged out" (see also the parking Example 1.6.1).In particular, let J * k (x k , y k ) denote the optimal cost-to-go at stage k and state (x k , y k ), and defineNote that the preceding expression can be interpreted as an "average costto-go" at x k (averaged over the values of the uncontrollable component y k ).Then, similar to the parking Example 1.6.1, a DP algorithm that generates Ĵk (x k ) can be obtained, and has the following form:(1.71)This is a consequence of the calculationNote that the minimization in the right-hand side of the preceding equation must still be performed for all values of the full state (x k , y k ) in order to yield an optimal control law as a function of (x k , y k ).Nonetheless, the equivalent DP algorithm (1.71) has the advantage that it is executed over a significantly reduced state space.Later, when we consider approximation in value space, we will find that it is often more convenient to approximate Ĵk (x k ) than to approximate J * k (x k , y k ); see the following discussions of forecasts and of the game of tetris.As an example, consider the augmented state resulting from the incorporation of forecasts, as described earlier.Then, the forecast y k represents an uncontrolled state component, so that the DP algorithm can be simplified as in Eq. (1.71).In particular, assume that the forecast y k can take values i = 1, . . ., m with probability p i .Then, by definingand ĴN (x N ) = g N (x N ), we have, using Eq.(1.70),which is executed over the space of x k rather than x k and y k .Note that this is a simpler algorithm to approximate than the one of Eq. (1.70).Uncontrollable state components often occur in arrival systems, such as queueing, where action must be taken in response to a random event (such as a customer arrival) that cannot be influenced by the choice of control.Then the state of the arrival system must be augmented to include the random event, but the DP algorithm can be executed over a smaller space, as per Eq.(1.71).Here is an example of this type.Tetris is a popular video game played on a two-dimensional grid.Each square in the grid can be full or empty, making up a "wall of bricks" with "holes" and a "jagged top" (see Fig. 1.6.5).The squares fill up as blocks of different shapes fall from the top of the grid and are added to the top of the wall.As a given block falls, the player can move horizontally and rotate the block in all possible ways, subject to the constraints imposed by the sides of the grid and the top of the wall.The falling blocks are generated independently according to some probability distribution, defined over a finite set of standard shapes.The game starts with an empty grid and ends when a square in the top row becomes full and the top of the wall reaches the top of the grid.When a row of full squares is created, this row is removed, the bricks lying above this row move one row downward, and the player scores a point.The player's objective is to maximize the score attained (total number of rows removed) up to termination of the game, whichever occurs first.We can model the problem of finding an optimal tetris playing strategy as a finite horizon stochastic DP problem, with very long horizon.The state consists of two components:(1) The board position, i.e., a binary description of the full/empty status of each square, denoted by x.Sec. 1.6 (2) The shape of the current falling block, denoted by y.The control, denoted by u, is the horizontal positioning and rotation applied to the falling block.There is also an additional termination state which is cost-free.Once the state reaches the termination state, it stays there with no change in score.Moreover there is a very large amount added to the score when the end of the horizon is reached without the game having terminated.The shape y is generated according to a probability distribution p(y), independently of the control, so it can be viewed as an uncontrollable state component when the state is (x, y) and control u is applied, respectively.The DP algorithm (1.72) assumes a finite horizon formulation of the problem.Alternatively, we may consider an undiscounted infinite horizon formulation, involving a termination state (i.e., a stochastic shortest path problem).The "reduced" form of Bellman's equation, which corresponds to the DP algorithm (1.72), has the formfor all x.The value Ĵ(x) can be interpreted as an "average score" at x (averaged over the values of the uncontrollable block shapes y).Chap. 1Finally, let us note that despite the simplification achieved by eliminating the uncontrollable portion of the state, the number of states x is still enormous, and the problem can only be addressed by suboptimal methods.†We have assumed so far that the controller has access to the exact value of the current state x k , so a policy consists of a sequence of functions of x k .However, in many practical settings, this assumption is unrealistic because some components of the state may be inaccessible for observation, the sensors used for measuring them may be inaccurate, or the cost of measuring them more accurately may be prohibitive.Often in such situations, the controller has access to only some of the components of the current state, and the corresponding observations may also be corrupted by stochastic uncertainty.For example in threedimensional motion problems, the state may consist of the six-tuple of position and velocity components, but the observations may consist of noisecorrupted radar measurements of the three position components.This gives rise to problems of partial or imperfect state information, which have received a lot of attention in the optimization and artificial intelligence literature (see e.g., [Ber17a], [RuN16]; these problems are also popularly referred to with the acronym POMDP for partially observed Markovian Decision problem).Generally, solving a POMDP exactly is typically intractable, even though there are DP algorithms for doing so.Thus in practice, POMDP are solved approximately, except under very special circumstances.Despite their inherent computational difficulty, it turns out that conceptually, partial state information problems are no different than the perfect state information problems we have been addressing so far.In fact by various reformulations, we can reduce a partial state information problem to one with perfect state information, which involves a different and more complicated state, called a sufficient statistic.Once this is done, we can state an exact DP algorithm that is defined over the space of the sufficient statistic.Roughly speaking, a sufficient statistic is a quantity that summarizes the content of the information available up to k for the purposes of optimal control.This statement can be made more precise, but we will not elaborate further in this book; see e.g., the DP textbook [Ber17a].† Tetris is generally considered to be an interesting and challenging stochastic testbed for RL algorithms, and has received a lot of attention over a period spanning 20 years (1995-2015), starting with the papers [TsV96], [BeI96], and the neuro-dynamic programming book [BeT96], and ending with the papers [GGS13], [SGG15], which contain many references to related works in the intervening years.All of these works are based on approximation in value space and various forms of approximate policy iteration.A common sufficient statistic is the belief state, which we will denote by b k .It is the probability distribution of x k given all the observations that have been obtained by the controller and all the controls applied by the controller up to time k, and it can serve as "state" in an appropriate DP algorithm.The belief state can in principle be computed and updated by a variety of methods that are based on Bayes' rule, such as Kalman filtering (see e. Let us consider a more complex version of the parking problem of Example 1.6.1.As in that example, a driver is looking for inexpensive parking on the way to his destination, along a line of N parking spaces with a garage at the end.The difference is that the driver can move in either direction, rather than just forward towards the garage.In particular, at space i, the driver can park at cost c(i) if i is free, can move to i − 1 at a cost t − i or can move to i + 1 at a cost t + i .Moreover, the driver records and remembers the free/taken status of the spaces previously visited and may return to any of these spaces; see Fig. 1.6.6.We assume that the probability p(i) of a space i being free changes over time, i.e., a space found free (or taken) at a given visit may get taken (or become free, respectively) by the time of the next visit.The initial probabilities p(i), before visiting any spaces, are known, and the mechanism by which these probabilities change over time is also known to the driver.As an example, we may assume that at each time stage, p(i) increases by a certain known factor with some probability ξ and decreases by another known factor with the complementary probability 1 − ξ.Here the belief state is the vector of current probabilities p(0), . . ., p(N − 1) , and it can be updated with a simple algorithm at each time based on the new observation: the free/taken status of the space visited at that time.We can use the belief state as the basis of an exact DP algorithm for computing an optimal policy.This algorithm is typically intractable computationally, but it is conceptually useful, and it can form the starting point for approximations.It has the form F k (b k , u k , z k+1 ) denotes the belief state at the next stage, given that the current belief state is b k , control u k is applied, and observation z k+1 is received following the application of u k :(1.74)This is the system equation for a perfect state information problem with state b k , control u k , "disturbance" z k+1 , and cost per stage ĝk (b k , u k ).The function F k is viewed as a sequential belief estimator , which updates the current belief state b k based on the new observation z k+1 .It is given by either an explicit formula or an algorithm (such as Kalman filtering or particle filtering) that is based on the probability distribution of z k and the use of Bayes' rule.The expected value E z k+1 {• b k , u k } is taken with respect to the distribution of z k+1 , given b k and u k .Note that z k+1 is random, and its distribution depends on x k and u k , so the expected valueBelief Estimator We refer to the textbook [Ber17a], Chapter 4, for a detailed derivation of the DP algorithm (1.73), and to the monograph [BeS78] for a mathematical treatment that applies to infinite-dimensional state and disturbance spaces as well.The DP algorithm (1.73) is not the only one that can be used for POMDP.There is also an exact DP algorithm that operates in the space of information vectors I k , defined bywhere z k is the observation received at time k.This is another sufficient statistic, and hence an alternative to the belief state b k .In particular, we can view I k as a state of the POMDP, which evolves over time according to the equationDenoting by J * k (I k ) the optimal cost starting at information vector I k at time k, the DP algorithm takes the form A drawback of the preceding approach is that the information vector I k is growing in size over time, thereby leading to a nonstationary system even in the case of an infinite horizon problem with a stationary system and cost function.This difficulty can be remedied in an approximation scheme that uses a finite history of the system (a fixed number of most recent observations) as state, thereby working effectively with a stationary finitestate system; see the paper by White and Scherer [WhS94].In particular, this approach is used in large language models such as ChatGPT.Finite-memory approximations for POMDP can be viewed within the context of feature-based approximation architectures, as we will discuss in Chapter 3 (see Example 3.1.6).Moreover, the finite-history scheme can be generalized through the concept of a finite-state controller ; see the paper by Yu and Bertsekas [YuB08], which also addresses the issue of convergence of the approximation error to zero as the size of the finite-history or finitestate controller is increased.In this book, we will view a multiagent system as a collection of decision making entities, called agents, which aim to optimally achieve a common goal.† The agents accomplish this by collecting and exchanging information, and otherwise interacting with each other.The agents can be software programs or physical entities such as robots, and they may have different capabilities.Among the generic challenges of efficient implementation of multiagent systems, one may note issues of limited communication and lack of fully shared information, due to factors such as limited bandwidth, noisy channels, and lack of synchronization.Another important generic issue is that as the number of agents increases, the size of the set of possible joint decisions of the agents increases exponentially, thereby complicating control selection by lookahead minimization.In this section, we will focus on ways to resolve this latter difficulty for problems where the agents fully share information, and in Section 2.9 we will address some of the challenges of problems where the agents may have some autonomy, and act without fully coordinating with each other.For a mathematical formulation, let us consider the discounted infinite horizon problem and a special structure of the control space, whereby the control u consists of m components, u = (u 1 , . . ., u m ), with a separable control constraint structure u ∈ U (x), = 1, . . ., m.Thus the control constraint set is the Cartesian product(1.76) † In a more general version of a multiagent system, which is outside our scope, the agents may have different goals, and act in their own self-interest.where the sets U (x) are given.This structure arises in applications involving distributed decision making by multiple agents; see Fig. 1.6.8.In particular, we will view each component u , = 1, . . ., m, as being chosen from within U (x) by a separate "agent" (a decision making entity).For the sake of the following discussion, we assume that each set U (x) is finite.Then the one-step lookahead minimization of the standard rollout scheme with base policy µ is given by ũ ∈ arg min u∈U(x) E w g(x, u, w) + αJ µ f (x, u, w) ,(1.77) and involves as many as n m Q-factors, where n is the maximum number of elements of the sets U (x) [so that n m is an upper bound to the number of controls in U (x), in view of its Cartesian product structure (1.76)].Thus the standard rollout algorithm requires an exponential [order O(n m )] number of Q-factor computations per stage, which can be overwhelming even for moderate values of m.This potentially large computational overhead motivates a far more computationally efficient rollout algorithm, whereby the one-step lookahead minimization (1.77) is replaced by a sequence of m successive minimizations, one-agent-at-a-time, with the results incorporated into the subsequent minimizations.In particular, given a base policy µ = (µ 1 , . . ., µ m ), we perform at state x the sequence of minimizations μ1 (x) ∈ arg minE w g x, μ1 (x), μ2 (x), . . ., μm−1 (x), u m , w + αJ µ f (x, μ1 (x), μ2 (x), . . ., μm−1 (x), u m , w) .Thus each agent component u is obtained by a minimization with the preceding agent components u 1 , . . ., u −1 fixed at the previously computed values of the rollout policy, and the following agent components u +1 , . . ., u m fixed at the values given by the base policy.This algorithm requires order O(nm) Q-factor computations per stage, a potentially huge computational saving over the order O(n m ) computations required by standard rollout.A key idea here is that the computational requirements of the rollout one-step minimization (1.77) are proportional to the number of controls in the set U (x k ) and are independent of the size of the state space.This motivates a reformulation of the problem, first suggested in the book [BeT96], Section 6.1.4,whereby control space complexity is traded off with state space complexity, by "unfolding" the control u k into its m components, which are applied one agent-at-a-time rather than all-agents-at-once.In particular, we can reformulate the problem by breaking down the collective decision u k into m individual component decisions, thereby reducing the complexity of the control space while increasing the complexity of the state space.The potential advantage is that the extra state space complexity does not affect the computational requirements of some RL algorithms, including rollout .To this end, we introduce a modified but equivalent problem, involving one-at-a-time agent control selection.At the generic state x, we break down the control u into the sequence of the m controls u 1 , u 2 , . . ., u m , and between x and the next state x = f (x, u, w), we introduce artificial intermediate "states" (x, u 1 ), (x, u 1 , u 2 ), . . ., (x, u 1 , . . ., u m−1 ), and corresponding transitions.The choice of the last control component u m at "state" (x, u 1 , . . ., u m−1 ) marks the transition to the next state x = f (x, u, w) according to the system equation, while incurring cost g(x, u, w); see Fig. 1.6.9.It is evident that this reformulated problem is equivalent to the original, since any control choice that is possible in one problem is also possible in the other problem, while the cost structure of the two problems is the same.In particular, every policy µ = (µ 1 , . . ., µ m ) of the original problem, including a base policy in the context of rollout, is admissible for the reformulated problem, and has the same cost function for the original as well as the reformulated problem....) Random Cost x x xx x = f (x, u, w)) g(x, u, w)Figure 1.6.9Equivalent formulation of the stochastic optimal control problem for the case where the control u consists of m components u 1 , u 2 , . . ., u m :The figure depicts the kth stage transitions.Starting from state x, we generate the intermediate states (x, u 1 ), (x, u 1 , u 2 ), . . ., (x, u 1 , . . ., u m−1 ), using the respective controls u 1 , . . ., u m−1 .The final control u m leads from (x, u 1 , . . ., u m−1 ) to x = f (x, u, w), and the random cost g(x, u, w) is incurred.The motivation for the reformulated problem is that the control space is simplified at the expense of introducing m − 1 additional layers of states, and the corresponding m − 1 cost-to-go functionsThe increase in size of the state space does not adversely affect the operation of rollout, since the Q-factor minimization (1.77) is performed for just one state at each stage.The major fact that can be proved about multiagent rollout (see Section 2.9 and the end-of-chapter references) is that it achieves cost improvement :J μ(x) ≤ J µ (x), for all x, where J µ (x) is the cost function of the base policy µ = (µ 1 , . . ., µ m ), and J μ(x) is the cost function of the rollout policy μ = (μ 1 , . . ., μm ), starting from state x.Furthermore, this cost improvement property can be extended to multiagent PI schemes that involve one-agent-at-a-time policy improvement operations, and have sound convergence properties.Moreover, multiagent rollout becomes the starting point for related PI schemes that are well suited for distributed operation in contexts involving multiple autonomous decision makers; see Section 2.9, the book [Ber20a], the papers [Ber20b] and [BKB20], and the tutorial survey [Ber21a].Example 1.6.5 (Spiders and Flies)This example is representative of a broad range of practical problems such as multirobot service systems involving delivery, maintenance and repair, search Chap. 1 During a stage, each fly moves to a some other position according to a given state-dependent probability distribution.Each spider learns the current state (the vector of spiders and fly locations) at the beginning of each stage, and either moves to a neighboring location or stays where it is.Thus each spider has as many as 5 choices at each stage.The control is u = (u 1 , . . ., u m ), where u is the choice of the th spider, so there are about 5 m possible values of u.To apply multiagent rollout, we need a base policy.A simple possibility is to use the policy that directs each spider to move on the path of minimum distance to the closest fly position.According to the multiagent rollout formalism, the spiders choose their moves one-at-time in the order from 1 to m, taking into account the current positions of the flies and the earlier moves of other spiders, and assuming that future moves will be chosen according to the base policy, which is a tractable computation.In particular, at the beginning at the typical stage, spider 1 selects its best move (out of the no more than 5 possible moves), assuming the other spiders 2, . . ., m will move towards their closest surviving fly during the current stage, and all spiders will move towards their closest surviving fly during the following stages, up to the time where no surviving flies remain.Spider 1 then broadcasts its selected move to all other spiders.Then spider 2 selects its move taking into account the move already chosen by spider 1, and assuming that spiders 3, . . ., m will move towards their closest surviving fly during the current stage, and all spiders will move towards their closest surviving fly during the following stages, up to the time where no surviving flies remain.Spider 2 then broadcasts its choice to all other spiders.This process of one-spider-at-a-time move selection is repeated for the remaining spiders 3, . . ., m, marking the end of the stage.Note that while standard rollout computes and compares 5 m Q-factors (actually a little less to take into account edge effects), multiagent rollout computes and compares ≤ 5 moves per spider, for a total of less than 5m.Despite this tremendous computational economy, experiments with this type of spiders and flies problems have shown that multiagent rollout achieves a comparable performance to the one of standard rollout.Our discussion so far dealt with problems with a known mathematical model, i.e., one where the system equation, cost function, control constraints, and probability distributions of disturbances are perfectly known.The mathematical model may be available through explicit mathematical formulas and assumptions, or through a computer program that can emulate all of the mathematical operations involved in the model, including Monte Carlo simulation for the calculation of expected values.It is important to note here that from our point of view, it makes no difference whether the mathematical model is available through closed form mathematical expressions or through a computer simulator : the methods that we discuss are valid either way, only their suitability for a given problem may be affected by the availability of mathematical formulas.In practice, however, it is common that the system parameters are either not known exactly or can change over time, and this introduces potentially enormous complications.† As an example consider our oversimplified cruise control system that we noted in Example 1.3.1 or its infinite horizon version.The state evolves according towhere x k is the deviation v k − v of the vehicle's velocity v k from the nominal v, u k is the force that propels the car forward, and w k is the disturbance † The difficulties of decision and control within a changing environment are often underestimated.Among others, they complicate the balance between offline training and on-line play, which we discussed in Section 1.1 in connection the AlphaZero.It is worth keeping in mind that as much as learning to play high quality chess is a great challenge, the rules of play are stable and do not change unpredictably in the middle of a game!Problems with changing system parameters can be far more challenging!Exact and Approximate Dynamic Programming Chap. 1that has nonzero mean.However, the coefficient b and the distribution of w k change frequently, and cannot be modeled with any precision because they depend on unpredictable time-varying conditions, such as the slope and condition of the road, and the weight of the car (which is affected by the number of passengers).Moreover, the nominal velocity v is set by the driver, and when it changes it may affect the parameter b in the system equation, and other parameters.† In this section, we will briefly review some of the most commonly used approaches for dealing with unknown parameters in optimal control theory and practice.We should note also that unknown problem environments are an integral part of the artificial intelligence view of RL.In particular, to quote from the popular book by Sutton and Barto [SuB18], RL is viewed as "a computational approach to learning from interaction," and "learning from interaction with the environment is a foundational idea underlying nearly all theories of learning and intelligence."The idea of learning from interaction with the environment is often connected with the idea of exploring the environment to identify its characteristics.In control theory this is often viewed as part of the system identification methodology, which aims to construct mathematical models of dynamic systems.The system identification process is often combined with the control process to deal with unknown or changing problem parameters, in a framework that is sometimes called dual control .This is one of the most challenging areas of stochastic optimal and suboptimal control, and has been studied intensively since the early 1960s.Given a controller design that has been obtained assuming a nominal DP problem model, one possibility is to simply ignore changes in problem parameters.We may then try to investigate the performance of the current design for a suitable range of problem parameter values, and ensure that it is adequate for the entire range.This is sometimes called a robust controller design.For example, consider the oversimplified cruise control system of Eq. (1.78) with a linear controller of the form µ(x) = Lx for some scalar L. Then we check the range of parameters b for which the current controller is stable (this is the interval of values b for which |1 + bL| < 1), and ensure that b remains within that range during the system's operation.The more general class of methods where the controller is modified in response to problem parameter changes is part of a broad field known as adaptive control , i.e., control that adapts to changing parameters.This is a rich methodology with many and diverse applications.Our discussion of † Adaptive cruise control, which can also adapt the car's velocity based on its proximity to other cars, has been studied extensively and has been incorporated in several commercially sold car models.Examples, Reformulations, and Simplifications 107 adaptive control in this book will be limited.Let us just mention for the moment a simple time-honored adaptive control approach for continuousstate problems called PID (Proportional-Integral-Derivative) control , for which we refer to the control literature, including the books by Aström and Hagglund [AsH95], [AsH06], and the end-of-chapter references on adaptive control (also the discussion in Section 5.7 of the RL textbook [Ber19a]).In particular, PID control aims to maintain the output of a singleinput single-output dynamic system around a set point or to follow a given trajectory, as the system parameters change within a relatively broad range.In its simplest form, the PID controller is parametrized by three scalar parameters, which may be determined by a variety of methods, some of them manual/heuristic.PID control is used widely and with success, although its range of application is mainly restricted to single-input, single-output continuous-state control systems.In PID control, no attempt is made to maintain a mathematical model and to track unknown model parameters as they change.An alternative and apparently reasonable form of suboptimal control is to separate the control process into two phases, a system identification phase and a control phase.In the first phase the unknown parameters are estimated, while the control takes no account of the interim results of estimation.The final parameter estimates from the first phase are then used to implement an optimal or suboptimal policy in the second phase.This alternation of estimation and control phases may be repeated several times during any system run in order to take into account subsequent changes of the parameters.Moreover, it is not necessary to introduce a hard separation between the identification and the control phases.They may be going on simultaneously, with new parameter estimates being introduced into the control process, whenever this is thought to be desirable; see Fig. 1.6.11.One drawback of this approach is that it is not always easy to determine when to terminate one phase and start the other.A second difficulty, of a more fundamental nature, is that the control process may make some of the unknown parameters invisible to the estimation process.This is known as the problem of parameter identifiability, which is discussed in the context of optimal control in several sources, including [BoV79] and [Kum83]; see also [Ber17a], Section 6.7.For a simple example, consider the scalar system  Assuming perfect state information, if the parameters a and b are known, it can be seen that the optimal control law iswhich sets all future states to 0. Assume now that the parameters a and b are unknown, and consider the two-phase method.During the first phase the control law μk (is used (γ is some scalar; for example, γ = − a b , where a and b are some a priori estimates of a and b, respectively).At the end of the first phase, the control law is changed towhere â and b are the estimates obtained from the estimation process.However, with the control law (1.79), the closed-loop system is x k+1 = (a + bγ)x k , so the estimation process can at best yield the value of (a + bγ) but not the values of both a and b.In other words, the estimation process cannot discriminate between pairs of values (a1, b1) and (a2, b2) such thatTherefore, a and b are not identifiable when feedback control of the form (1.79) is applied.Examples, Reformulations, and Simplifications 109On-line parameter estimation algorithms, which address among others the issue of identifiability, have been discussed extensively in the control theory literature, but the corresponding methodology is complex and beyond our scope in this book.However, assuming that we can make the estimation phase work somehow, we are free to revise the controller using the newly estimated parameters in a variety of ways, in an on-line replanning process.Unfortunately, there is still another difficulty with this type of online replanning: it may be hard to recompute an optimal or near-optimal policy on-line, using a newly identified system model.In particular, it may be impossible to use time-consuming methods that involve for example the training of a neural network or discrete/integer control constraints.A simpler possibility is to use rollout, which we discuss next.†We will now consider an approach for dealing with unknown or changing parameters, which is based on on-line replanning.We have discussed this approach in the context of rollout and multiagent rollout, where we stressed the importance of fast on-line policy improvement.Let us assume that some problem parameters change and the current controller becomes aware of the change "instantly" (i.e., very quickly before the next stage begins).The method by which the problem parameters are recalculated or become known is immaterial for the purposes of the following discussion.It may involve a limited form of parameter estimation, whereby the unknown parameters are "tracked" by data collection over a few time stages, with due attention paid to issues of parameter identifiability; or it may involve new features of the control environment, such as a changing number of servers and/or tasks in a service system (think of new spiders and/or flies appearing or disappearing unexpectedly in the spiders-and-flies Example 1.6.5).We thus assume away/ignore issues of parameter estimation, and focus on revising the controller by on-line replanning based on the newly obtained parameters.This revision may be based on any suboptimal method, but rollout with the current policy used as the base policy is particularly † Another possibility is to deal with this difficulty by precomputation.In particular, assume that the set of problem parameters may take a known finite set of values (for example each set of parameter values may correspond to a distinct maneuver of a vehicle, motion of a robotic arm, flying regime of an aircraft, etc).Then we may precompute a separate controller for each of these values.Once the control scheme detects a change in problem parameters, it switches to the corresponding predesigned current controller.This is sometimes called a multiple model control design or gain scheduling, and has been applied with success in various settings over the years.12 Schematic illustration of adaptive control by rollout.One-step lookahead is followed by simulation with the base policy, which stays fixed.The system, cost, and constraint parameters are changing over time, and the most recent values are incorporated into the lookahead minimization and rollout operations.For the discussion in this section, we may assume that all the changing parameter information is provided by some computation and sensor "cloud" that is beyond our control.The base policy may also be revised based on various criteria.attractive.Here the advantage of rollout is that it is simple and reliable.In particular, it does not require a complicated training procedure to revise the current policy, based for example on the use of neural networks or other approximation architectures, so no new policy is explicitly computed in response to the parameter changes.Instead the current policy is used as the base policy for rollout, and the available controls at the current state are compared by a one-step or mutistep minimization, with cost function approximation provided by the base policy (cf.Fig. 1.6.12).Note that over time the base policy may also be revised (on the basis of an unspecified rationale), in which case the rollout policy will be revised both in response to the changed current policy and in response to the changing parameters.This is necessary in particular when the constraints of the problem change.The principal requirement for using rollout in an adaptive control context is that the rollout control computation should be fast enough to be performed between stages.Note, however, that accelerated/truncated versions of rollout, as well as parallel computation, can be used to meet this time constraint.The following example considers on-line replanning with the use of rollout in the context of the simple one-dimensional linear quadratic problem that we discussed earlier in this chapter.The purpose of the example is to illustrate analytically how rollout with a policy that is optimal for a nominal set of problem parameters works well when the parameters change from their nominal values.This property is not practically useful in linear quadratic problems because when the parameter change, it is possible to calculate the new optimal policy in closed form, but it is indicative of the performance robustness of rollout in other contexts.Generally, adaptive control by rollout and on-line replanning makes sense in situations where the calculation of the rollout controls for a given set of problem parameters is faster and/or more convenient than the calculation of the optimal controls for the same set of parameter values.These problems include cases involving nonlinear systems and/or difficult (e.g., integer) constraints.Example 1.6.7 (On-Line Replanning for Linear Quadratic Problems Based on Rollout)Consider the deterministic undiscounted infinite horizon linear quadratic problem.It involves the linear systemand the quadratic cost function limThe optimal cost function is given bywhere K * is the unique positive solution of the Riccati equationThe optimal policy has the formwhere(1.82) Chap. 1As an example, consider the optimal policy that corresponds to the nominal problem parameters b = 2 and r = 0.5: this is the policy (1.81)-(1.82),with K obtained as the positive solution of the quadratic Riccati Eq. (1.80) for b = 2 and r = 0.5.In particular, we can verify that.From Eq. (1.82) we then obtain(1.83)We will now consider changes of the values of b and r while keeping L constant, and we will compare the quadratic cost coefficient of the following three cost functions as b and r vary:(a) The optimal cost function K * x 2 , where K * is given by the positive solution of the Riccati Eq. (1.80).(b) The cost function KLx 2 that corresponds to the base policywhere L is given by Eq. (1.83).From our earlier discussion, we have2 that corresponds to the rollout policy μL(x) = Lx, obtained by using the policy µL as base policy.Using the formulas given earlier, we have   The rollout policy performance is very close to the one of the exactly reoptimized policy, while the base policy yields much worse performance.This is a consequence of the quadratic convergence rate of Newton's method that underlies rollout: limwhere for a given initial state, J is the rollout performance, J * is the optimal performance, and J is the base policy performance.Chap. 1 to use the policy µL, which is optimal for the nominal values b = 2 and r = 0.5, but suboptimal for other values of b and r.The difference KL −K * is indicative of the performance loss due to using on-line replanning by rollout rather than using optimal replanning.Finally, the difference KL − KL is indicative of the performance improvement due to on-line replanning using rollout rather than keeping the policy µL unchanged.Note that Fig. 1.6.13illustrates the behavior of the error ratiowhere for a given initial state, J is the rollout performance, J * is the optimal performance, and J is the base policy performance.This ratio approaches 0 as J − J * becomes smaller because of the quadratic convergence rate of Newton's method that underlies the rollout algorithm.The preceding adaptive control formulation strictly separates the dual objective of estimation and control: first parameter identification and then controller reoptimization (either exact or rollout-based).In an alternative adaptive control formulation, the parameter estimation and the application of control are done simultaneously, and indeed part of the control effort may be directed towards improving the quality of future estimation.This alternative (and more principled) approach is based on a view of adaptive control as a partially observed Markovian decision problem (POMDP) with a special structure.We will see in Section 2.11 that this approach is well-suited for approximation in value space schemes, including forms of rollout.To describe briefly the adaptive control reformulation as POMDP, we introduce a system whose state consists of two components: (a) A perfectly observed component x k that evolves over time according to a discrete-time equation.(b) A component θ which is unobserved but stays constant, and is estimated through the perfect observations of the component x k .We view θ as a parameter in the system equation that governs the evolution of x k .Thus we havewhere u k is the control at time k, selected from a set U k (x k ), and w k is a random disturbance with given probability distribution that depends on (x k , θ, u k ).For convenience, we will assume that θ can take one of m known values θ 1 , . . ., θ m .The a priori probability distribution of θ is given and is updated based on the observed values of the state components x k and the applied controls u k .In particular, the information vectoris available at time k, and is used to compute the conditional probabilities As discussed in Section 1.6.4,an exact DP algorithm can be written for the equivalent POMDP, and this algorithm is suitable for the use of approximation in value space and rollout.We will describe this approach in some detail in Section 2.11.Related ideas will also be discussed in the context of Bayesian estimation and sequential estimation in Section 2.10.Note that the case of a deterministic systemis particularly interesting, because we can then typically expect that the true parameter θ * will be identified in a finite number of stages.The reason is that at each stage k, we are receiving a noiseless observation relating to θ, namely the state x k .Once the true parameter θ * is identified, the problem becomes one of perfect state information.In this section, we will provide a brief summary of the model predictive control (MPC) methodology for control system design, with a view towards its connection with approximation in value space and rollout schemes.† We will focus on classical control problems, where the objective is to keep the state of a deterministic system close to the origin of the state space (see Fig. 1.6.15).Another type of classical control problem is to keep the system close to a given trajectory (see Fig. 1.6.16).It can also be treated by forms of MPC, but will not be discussed in this book.We discussed earlier the linear quadratic approach, whereby the system is represented by a linear model, the cost is quadratic in the state and the control, and there are no state and control constraints.The linear quadratic and other approaches based on state variable system representations and optimal control became popular, starting in the late 50s and early 60s.Unfortunately, however, the analytically convenient linear quadratic problem formulations are often not satisfactory.There are two main reasons for this:(a) The system may be nonlinear, and it may be inappropriate to use for control purposes a model that is linearized around the desired point or †  trajectory.Moreover, some of the control variables may be naturally discrete, and this is incompatible with the linear system viewpoint.(b) There may be control and/or state constraints, which are not handled adequately through quadratic penalty terms in the cost function.For example, the motion of a car may be constrained by the presence of obstacles and hardware limitations (see Fig. 1.6.16).The solution obtained from a linear quadratic model may not be suitable for such a problem, because quadratic penalties treat constraints "softly" and may produce trajectories that violate the constraints.These inadequacies of the linear quadratic formulation have motivated MPC, which combines elements of several ideas that we have discussed so far, such as multistep lookahead, rollout with a base policy, and certainty equivalence.Aside from dealing adequately with state and control constraints, MPC is well-suited for on-line replanning, like all rollout methods.Note that the ideas of MPC were developed independently of the approximate DP/RL methodology.However, the two fields are closely related, and there is much to be gained from understanding one field within the context of the other, as the subsequent development will aim to show.A major difference between MPC and finite-state stochastic control problems that are popular in the RL/artificial intelligence literature is that in MPC the state and control spaces are continuous/infinite, such as for example in self-driving cars, the control of aircraft and drones, or the operation of chemical processes.In this section, we will primarily focus on the undiscounted infinite horizon deterministic problem, which involves the systemwhose state x k and control u k are finite-dimensional vectors.The cost per stage is assumed nonnegativefor all (x k , u k ), (e.g., a positive definite quadratic cost).There are control constraints u k ∈ U (x k ), and to simplify the following discussion, we will initially consider no state constraints.We assume that the system can be kept at the origin at zero cost, i.e.,For a given initial state x 0 , we want to obtain a sequence {u 0 , u 1 , . ..} that satisfies the control constraints, while minimizing the total cost.This is a classical problem in control system design, known as the regulation problem, where the aim is to keep the state of the system near the origin (or more generally some desired set point), in the face of disturbances and/or parameter changes.In an important variant of the problem, there are additional state constraints of the form x k ∈ X, and there arises the issue of maintaining the state within X, not just at the present time but also in future times.We will address this issue later in this section.We will first focus on a classical form of the MPC algorithm, proposed in the form given here by Keerthi and Gilbert [KeG88].In this algorithm, at each encountered state x k , we apply a control ũk that is computed as follows; see Fig. 1.6.17:(a) We solve an -stage optimal control problem involving the same cost function and the requirement that the state after steps is driven to 0, i.e., x k+ = 0.This is the problem min .We minimize the cost function over the next stages while imposing the requirement that x k+ = 0. We then apply the first control of the optimizing sequence.In the context of rollout, the minimization over u k is the one-step lookahead, while the minimization over u k+1 , . . ., u k+ −1 that drives x k+ to 0 is the base heuristic.subject to the system equation constraintsthe control constraintsand the terminal state constraintHere is an integer with > 1, which is chosen in some largely empirical way.(b) If {ũ k , . . ., ũk+ −1 } is the optimal control sequence of this problem, we apply ũk and we discard the other controls ũk+1 , . . ., ũk+ −1 .(c) At the next stage, we repeat this process, once the next state x k+1 is revealed.To make the connection of the preceding MPC algorithm with rollout, we note that the one-step lookahead function J implicitly used by MPC [cf.Eq. (1.85)] is the cost function of a certain stable base policy.This is the policy that drives to 0 the state after − 1 stages (not stages) and keeps the state at 0 thereafter, while observing the state and control constraints, and minimizing the associated ( −1)-stages cost.This rollout view of MPC was first discussed in the author's paper [Ber05].It is useful for making a connection with the approximate DP/RL, rollout, and its interpretation in terms of Newton's method.In particular, an important consequence is that the MPC policy is stable, since rollout with a stable base policy can be shown to yield a stable policy under very general conditions, as we have noted earlier for the special case of linear quadratic problems in Section 1.5; cf.Fig. 1.5.10.We may also equivalently view the preceding MPC algorithm as rollout with ¯ -step lookahead, where 1 < ¯ < , with the base policy that drives to 0 the state after − ¯ stages and keeps the state at 0 thereafter.This suggests variations of MPC that involve truncated rollout with terminal cost function approximation, which we will discuss shortly.In a common variant of MPC, the requirement of driving the system state to 0 in steps in the -stage MPC problem (1.85), is replaced by a terminal cost G(x k+ ), which is positive everywhere except at 0. Thus at state x k , we solve the problem mininstead of problem (1.85) where we require that x k+ = 0.This variant can be viewed as rollout with one-step lookahead, and a base policy, which at state x k+1 applies the first control ũk+1 of the sequence {ũ k+1 , . . ., ũk+ −1 } that minimizesIt can also be viewed outside the context of rollout, as approximation in value space with -step lookahead minimization and terminal cost approximation given by G. Thus the cost function of the preceding MPC controller may be much closer to J * than G is.An important question is to choose the terminal cost approximation so that the resulting MPC controller is stable.Our discussion of Section 1.5 on the region of stability of approximation in value space schemes applies here.In particular, under the nonnegative cost assumption of this section, the MPC controller can be proved to be stable if a single value iteration (VI) starting from G produces a function that takes uniformly smaller values than G:for all x. Figure 1.6.18provides a graphical illustration.It shows that this condition guarantees that successive iterates of value iteration, as implemented through multistep lookahead, lie within the region of stability, so that the policy produced by MPC is stable.We also expect that as the length of the lookahead minimization is increased, the stability properties of the MPC controller are improved.In particular, given G ≥ 0, the resulting MPC controller is likely to be stable for sufficiently large, since the VI algorithm ordinarily converges to J * , which lies within the region of stability.Results of this type are known within the MPC framework under various conditions (see the papers byand the author's book [Ber20a], Section 3.1.2).Our discussion of stability in Section 1.5 is also relevant within this context; cf.Fig. 1.5.9.In another variant of MPC, in addition to the terminal cost function approximation G, we use truncated rollout, which involves running some Thus, truncated rollout applies m value iterations with base policy µ, starting with the function G and yielding the function T m µ G. Then − 1 value iterations are applied to T m µ G through the ( − 1)-step minimization.Finally, the Newton step is applied toto yield the cost function of the MPC policy μ.As m increases, the starting point for the Newton step moves closer to Jµ, which lies within the region of stability.stable base policy µ for a number of steps m; see Fig. 1.6.19.This is quite similar to standard truncated rollout, except that the computational solution of the lookahead minimization problem (1.89) may become complicated when the control space is infinite.As discussed earlier in Section 1.5, increasing the length of the truncated rollout enlarges the region of stability of the MPC controller .The reason is that by increasing the length of the truncated rollout, we push the start of the Newton step towards of the cost function J µ of the stable policy, which lies within the region of stability.The base policy may also be used Finally, let is note that when faced with changing problem parameters, it is natural to consider on-line replanning as per our earlier adaptive control discussion.In this context, once new estimates of system and/or cost function parameters become available, MPC can adapt accordingly by introducing the new parameter estimates into the -stage optimization problem in (a) above.Our discussion so far has skirted a major issue in MPC, which is that there may be additional state constraints of the form x k ∈ X, for all k, where X is some subset of the true state space.Indeed much of the original work on MPC was motivated by control problems with state constraints, imposed by the physics of the problem, which could not be handled effectively with the nice unconstrained framework of the linear quadratic problem that we have discussed in Section 1.5.To deal with additional state constraints of the form x k ∈ X, where X is some subset of the state space, the MPC problem to be solved at the kth stage [cf.Eq. (1.89)] must be modified.Assuming that the current state x k belongs to the constraint set X, the MPC problem should take the form min u t , t=k,...,k+ −1(1.91) subject to the control constraintsand the state constraintsThe control ũk thus obtained will generate a statethat will belong to X, and similarly the entire state trajectory thus generated will satisfy the state constraint x t ∈ X for all t, assuming that the initial state does.However, there is an important difficulty with the preceding MPC scheme, namely there is no guarantee that the problem (1.91)-(1.93)has a feasible solution for all initial states x k ∈ X.Here is a simple example.Chap. 1and state constraints of the form x k ∈ X, for all k, whereThen if β > 1, the state constraint cannot be satisfied for all initial states x0 ∈ X.In particular, if we take x0 = β, then 2x0 > 2 and x1 = 2x0 + u0 will satisfy x1 > x0 = β for any value of u0 with |u0| ≤ 1. Similarly the entire sequence of states {x k } generated by any set of feasible controls will satisfyThe state constraint can be satisfied only for initial states x0 in the set X given by Xsee Fig. 1.6.20,which also illustrates the trajectories generated by the MPC scheme of Eq. (1.89), which does not involve state constraints.Examples, Reformulations, and Simplifications 125The preceding example illustrates a fundamental point in state-constrained MPC: the state constraint set X must be invariant in the sense that starting from any one of its points x k there must exist a control u k ∈ U (x k ) for which the next state x k+1 = f (x k , u k ) must belong to X. Mathematically, X is invariant if for every x ∈ X, there exists u ∈ U (x) such that f (x, u) ∈ X.In particular, it can be seen that the set X of Eq. (1.94) is invariant if and only if β ≤ 1.Given an MPC calculation of the form (1.91)-(1.93),we must make sure that the set X is invariant, or else it should be replaced by an invariant subset X ⊂ X.Then the MPC calculation (1.91)-(1.93)will be feasible provided the initial state x 0 belongs to X.This brings up the question of how we compute an invariant subset of a given constraint set, which is typically an off-line calculation that cannot be performed during on-line play.It turns out that given X there exists a largest possible invariant subset of X, which can be computed in the limit with an algorithm that resembles value iteration.In particular, starting with X 0 = X, we obtain a nested sequence of subsets through the recursion(1.95) Clearly, we have X k+1 ⊂ X k for all k, and under mild conditions it can be shown that the intersection set X = ∩ ∞ k=0 X k , is the largest invariant subset of X; see the author's PhD thesis [Ber71] and subsequent paper [Ber72a], which introduced the concept of invariance and its use in satisfying state constraints in control over a finite and an infinite horizon.†As an example, it can be verified that the sequence of value iterates (1.95) starting with a set X 0 = {x | |x| ≤ β} with β > 1 is given byIt can thus be seen that we have β k+1 < β k for all k and β k ↓ 1, so that the intersection X = ∩ ∞ k=0 X k yields the largest invariant setThere are several ways to compute invariant subsets of constraint sets X, for which we refer to the aforementioned author's work and the MPC literature; see e.g., the book by Rawlings, Mayne, and Diehl [RMD17], and † The term used in [Ber71] and [Ber72a] is reachability of a target tube {X, X, . ..}, which is synonymous to invariance of X.the survey by Mayne [May14], which give additional references.An important point here is that the computation of an invariant subset of the given constraint set X must be done off-line with one of several available algorithmic approaches, so it becomes part of the off-line training (in addition to the terminal cost function G).A relatively simple possibility is to compute an invariant subset X that corresponds to some nominal policy μ [i.e., starting from any point x ∈ X, the state f x, μ(x) belongs to X].Such an invariant subset may be obtained by some form of simulation using the policy μ.Moreover, μ can also be used for truncated rollout and also provide a terminal cost function approximation.Given an off-line training process, which provides an invariant set X, a terminal cost function G, and possibly a base policy for truncated rollout, MPC becomes an on-line play algorithm for which our earlier discussion applies.Note, however, that in an adaptive control context, where a model is estimated on-line as it is changing, it may be difficult to recompute online an invariant set that can be used to enforce the state constraints of the problem.This is particularly so if the state constraints change themselves as part of the changing problem data.Let us finally mention that while in this section we have focused on deterministic problems, there are variants of MPC, which include the treatment of uncertainty.The books and papers cited earlier contain several ideas along these lines; see e.In this connection, it is also worth mentioning the certainty equivalence approach that we discussed briefly earlier.In particular, upon reaching state x k we may perform the MPC calculations after replacing the uncertain quantities w k+1 , w k+2 , . . .with deterministic quantities w k+1 , w k+2 , . .., while allowing for the stochastic character of the disturbance w k of just the current stage k.Note that only the first step of this MPC calculation is stochastic.Thus the calculation needed per stage is not much more difficult than the one for deterministic problems, while still implementing a Newton step for solving the associated Bellman equation; see our earlier discussion, and also Section 2.5.3 of the RL book [Ber19a] and Section 3.2 of the book [Ber22a].The current state of RL has greatly benefited from the cross-fertilization of ideas from decision and control, and from artificial intelligence; see Fig. 1.7.1.The strong connections between these two fields are now widely recognized.Still, however, there are cultural differences, including the  traditional reliance on mathematical analysis for the decision and control field, and the emphasis on challenging problem implementations in the artificial intelligence field.Moreover, substantial differences in language and emphasis remain between RL-based discussions (where artificial intelligence-related terminology is used) and DP-based discussions (where optimal control-related terminology is used).The terminology used in this book is standard in DP and optimal control, and in an effort to forestall confusion of readers that are accustomed to either the AI or the optimal control terminology, we provide a list of terms commonly used in RL, and their optimal control counterparts.(h) Action (or state-action) value = Q-factor (or Q-value) of a statecontrol pair.(Q-value is also used often in RL.) ever, they involve different cultures and application contexts, so it is worth reflecting on their similarities and differences.Machine learning can be broadly categorized into three main types of methods, all of which involve the collection and use of data in some form:(a) Supervised learning: Here a dataset of many input-output pairs is collected.An optimization algorithm is used to create a parametrized function that fits well the data, as well as make accurate predictions on new, unseen data.Supervised learning problems are typically formulated as optimization problems, examples of which we will see in Chapter 3. A common algorithmic approach is to use a gradient-type algorithm to minimize a loss function that measures the difference between the actual outputs of the dataset and the predicted outputs of the parametrized model.(b) Unsupervised learning: Here the dataset is "unlabeled" in the sense that the data are not separated into input and matching output pairs.Unsupervised learning algorithms aim to identify patterns and structures in the data, in applications such as clustering, dimensionality reduction, and density estimation.The main objective is to extract meaningful insights and features from the data.Some unsupervised learning techniques can be approached by DP, but the connection is not strong.Generally speaking, unsupervised learning does not seem to connect well with the types of sequential decision making applications of this book.(c) Reinforcement learning: RL differs in an important way from supervised and unsupervised learning.It does not use a dataset as a starting point .Instead, it generates data on-line or off-line as dictated by the needs of the optimization algorithm it uses, be it multistep lookahead minimization, approximate policy iteration and rollout, or approximation in policy space.† Optimization problems and algorithms on the other hand may or may not involve the collection and use of data.They involve data only in the context of special applications, most of which are related to machine learning.In theoretical terms, optimization problems are categorized in terms of their mathematical structure, which is the primary determinant of the suitability of particular types of methods for their solution.In particular, it is common to distinguish between static optimization problems and dynamic optimization problems.The latter problems involve sequential decision making, with feedback between decisions, while the former problems involve a single decision.Stochastic problems with perfect or imperfect state observations are dynamic (unless they involve open-loop † A variant of RL called offline RL or batch RL, starts from a historical dataset, and does not explore the environment to collect new data.Sec. 1.7 decision making without the use of any feedback), and they require the use of DP for their optimal solution.Deterministic problems can be formulated as static problems, but they can also be formulated as dynamic problems for reasons of algorithmic expediency.In this case, the decision making process is (sometimes artificially) broken down into stages, as is often done in this book in the context of discrete optimization and other contexts.Another important categorization of optimization problems is based on whether their search space is discrete or is continuous.Discrete problems include deterministic problems such as integer and combinatorial optimization problems, and can be addressed by formal methods of integer programming as well as by DP.Also, because they tend to be difficult, they are often addressed (suboptimally) with the use of heuristics.Continuous problems are usually addressed with very different methods, which are based on calculus and convexity, such as Lagrange multiplier theory and duality, and the computational machinery of linear, nonlinear, and convex programming.Special cases of discrete problems that involve the use of graphs, such as matching, transportation, and transhipment, may also be addressed with network optimization methods, which involve the use of continuous optimization approaches that are based on linear programming and duality.Hybrid problems, which involve both continuous and discrete variables, usually require the use of discrete optimization methods, although they can often be addressed with the use of convex duality methods, which fundamentally have a continuous character.The DP methodology, generally speaking, applies to just about any kind of optimization problem, deterministic or stochastic, static or dynamic, discrete or continuous, as long as it is formulated as a sequential decision problem, in the manner described in Sections 1.2-1.4.In terms of its algorithmic structure, DP is very different from other optimization methodologies, particularly the ones that are based on calculus and convexity.Among others, this is evident from the fact that DP can deal with discrete problems as well as continuous, and does not address issues of local minima (it aims exclusively at global minima).Notice a qualitative difference between optimization and machine learning: the former is mostly organized around mathematical structures and the analysis of the foundational issues of the corresponding algorithms, while the latter is mostly organized around how data is collected, used, and analyzed, often with a strong emphasis on statistical issues.This is an important distinction, which affects profoundly the perspectives of researchers in the two fields.In comparing the RL and DP methodologies, we should note that they are fundamentally connected through their corresponding problem formu-132Chap. 1 lations: they both involve sequential decision making.Thus any problem addressed by DP can in principle be addressed by RL, and reversely.However, the RL algorithmic methodology is broader than DP, and includes the use of optimization algorithms of the gradient descent and random search type, simulation-based methodologies, statistical methods of sampling and performance evaluation, and neural network design and training ideas.Moreover, in the artificial intelligence view of RL, a machine learns through trial and error by interacting with an environment.† In practical terms, this is more or less the same as what DP aims to do, but in RL there is often an emphasis on the presence of uncertainty and exploration of the environment.This is different from DP, which in addition to stochastic problems, it is often applied to deterministic problems that do not involve uncertainty or exploration (adaptive control is the only decision and control problem type, where uncertainty and exploration arise in a significant way).We may also add that RL has brought into the field of sequential decision making a fresh and ambitious spirit that has made possible the solution of problems thought to be well outside the capabilities of DP.On the other hand, a substantial portion of the decision, control, and optimization community views the RL methodology essentially as an approximate form of DP, which can be applied to difficult problems that are beyond the reach of exact optimization.In the context of this view, there is a lot of interest in using RL methods to address intractable problems, including deterministic discrete/integer optimization, which need not involve data collection, interaction with the environment, uncertainty, and learning.In terms of applications, DP was originally developed in the 1950s and 1960s as part of the then emerging methodologies of operations research and optimal control.These methodologies are now mature and provide important tools and perspectives, as well as a rich variety of applications, such as robotics, autonomous transportation, and aerospace, which can benefit from the use of RL.Moreover, DP has been used in a broad range of applications in industrial engineering, economics, and finance, so these applications can also benefit from the use of RL methods and perspectives.At the same time, RL and machine learning have ushered opportunities for the application of DP techniques in new domains, such as machine translation, image recognition, knowledge representation, database organization, large language models, and automated planning, where they can have a significant practical impact.† A common description it is that "the machine learns sequentially how to make decisions that maximize a reward signal, based on the feedback received from the environment."The Use of Mathematics in Optimization and Machine LearningLet us now discuss some differences between the research cultures of optimization and machine learning, as they pertain to the use of mathematics.In optimization, the emphasis is often on general purpose methods that offer broad and mathematically rigorous performance guarantees, for a wide variety of problems.In particular, it is widely believed that a solid mathematical foundation for a given optimization methodology enhances its reliability and clarifies the boundaries of its applicability.Furthermore, it is recognized that formulating practical problems and matching them to the right algorithms is greatly enhanced by one's understanding of the mathematical structure of the underlying optimization methodology.Machine learning research includes important lines of analysis that have a strongly mathematical character, particularly relating to theoretical computer science, complexity theory, and statistical analysis.At the same time, in machine learning there are eminently useful algorithmic structures, such as neural networks, large language models, and image generative models, which are not well-understood mathematically and defy to a large extent mathematical analysis.† This can add to a perception that focusing on rigorous mathematics, as opposed to practical implementation, may be a low payoff investment in many practical machine learning contexts.Moreover, as we have mentioned earlier, the starting point in machine learning is often a type of dataset or a specialized type of training problem (e.g., language translation or image recognition), so what is needed is a method that works well on that dataset or type of problem, and not necessarily on other datasets or problems.Thus specialized approximation architectures, implementation techniques, and heuristics, which perform well for the given problem and dataset type, may be perfectly acceptable in a machine learning context, even if they do not provide rigorous and generally applicable performance guarantees.In conclusion, both optimization and machine learning use mathematical models and rigorous analysis in important ways, and often overlap in the techniques and tools that they use, as well as in the practical applications that they address.However, depending on the type of problem considered, there may be differences in the emphasis and priority placed on mathematical analysis, insight, and generality versus practical effectiveness and problem-specific efficiency.This is particularly true in certain special- † As an illustration, the paper by He et al., "Deep Residual Learning for Image Recognition," published in Proc. of the IEEE Conf. on Computer Vision and Pattern Recognition, 2016, has been cited over 162,000 times as of May 2023, and contains only two equations.The famous neural network architecture paper by Vaswani et al., "Attention is all you Need," published in NIPS, 2017, which laid the foundation for GPT, has been cited over 73,000 times as of May 2023, and contains only six equations.Chap. 1ized contexts, and can lead to some tension, as different fields may not fully appreciate each other's perspective.We will now summarize this chapter and describe how it can be used flexibly as a foundation for a few different courses.We will also provide a selective overview of the DP and RL literature, and also give a few exercises that have been used in ASU classes.In this chapter, we have aimed to provide an overview of the approximate DP/RL landscape, which can serve as the foundation for a deeper in-class development of other RL topics.In particular, we have described in varying levels of depth the following:(a) The algorithmic foundation of exact DP in all its major forms: deterministic and stochastic, discrete and continuous, finite and infinite horizon.(b) Approximation in value space with one-step and multistep lookahead, the workhorse of RL, which underlies its major success stories, including AlphaZero.We contrasted approximation in value space with approximation in policy space, and discussed how the two may be combined.(c) The important division between off-line training and on-line play in the context of approximation in value space.We highlighted how their synergy can be intuitively explained in terms of Newton's method.(d) The fundamental methods of policy iteration and rollout, the former being primarily an off-line method, and the latter being primarily a less ambitious on-line method.Both methods and their variants bear close relation to Newton's method and draw their effectiveness from this relation.(e) Some major models with a broad range of applications, such as discrete optimization, POMDP, multiagent problems, adaptive control, and model predictive control.We delineated their principal characteristics and the major RL implementation issues within their contexts.(f) The use of function approximation, which has been a recurring theme in our presentation.We have touched upon some of the principal schemes for approximation, e.g., neural networks and feature-based architectures.One of the principal aims of this chapter was to provide a foundational platform for multiple RL courses that explore at a deeper level various algorithmic methodologies, such as:Notes, Sources, and Exercises 135(1) Rollout and policy iteration.(2) Neural networks and other approximation architectures for off-line training.(3) Aggregation, which can be used for cost function approximation in the context of approximation in value space.(4) A broader discussion of sequential decision making in contexts involving changing system parameters, sequential estimation, and simultaneous system identification and control.(5) Stochastic algorithms, such as temporal difference methods and Qlearning, which can be used for off-line policy evaluation in the context of approximate policy iteration.(6) Sampling methods to collect data for off-line training in the context of cost and policy approximations.(7) Statistical estimates and efficiency enhancements of various sampling methods used in simulation-based schemes.This includes confidence intervals and computational complexity estimates.(8) On-line methods for specially structured contexts, including problems of the multi-armed bandit type.(9) Simulation-based algorithms for approximation in policy space, including policy gradient and random search methods.(10) A deeper exploration of control system design methodologies such as model predictive control and adaptive control, and their applications in robotics and automated transportation.In our course we have focused selectively on the methodologies (1)-( 4), with a limited coverage of (9) in Section 3.5.In a different course, other choices from the above list may be made, by building on the content of the present chapter.In the literature review that follows, we will focus primarily on textbooks, research monographs, and broad surveys, which supplement our discussions, express related viewpoints, and collectively provide a guide to the literature.Inevitably our referencing reflects a cultural bias, and an overemphasis on sources that are familiar to the author and are written in a similar style to the present book (including the author's own works).Thus we wish to apologize in advance for the many omissions of important research references that are somewhat outside our own understanding and view of the field.Sections 1.1-1.4:Our discussion of exact DP in this chapter has been brief since our focus in this book will be on approximate DP and RL.TheChap. 1author's DP textbook [Ber17a] provides an extensive discussion of finite horizon exact DP, and its applications to discrete and continuous spaces problems, using a notation and style that is consistent with the one used here.The books by Puterman [Put94] and the author [Ber12] provide detailed (but substantially different) treatments of infinite horizon finite-state stochastic DP problems.The book [Ber12] also covers continuous/infinite state and control spaces problems, including the linear quadratic problems that we have discussed for one-dimensional problems in this chapter.Continuous spaces problems present special analytical and computational challenges, which are at the forefront of research of the RL methodology.Some of the more complex mathematical aspects of exact DP are discussed in the monograph by Bertsekas and Shreve [BeS78], particularly the probabilistic/measure-theoretic issues associated with stochastic optimal control, including partial state information problems.This monograph provides an extensive treatment of these issues.The followup work by Huizhen Yu and the author [YuB15] resolves the special measurability issues that relate to policy iteration, and provides additional analysis relating to value iteration.The second volume of the author's DP book [Ber12], Appendix A, provides an accessible summary introduction of the measure-theoretic framework of the book [BeS78].† In the RL literature, the mathematical difficulties around measurability are usually neglected (as they are in this book), and this is fine because they do not play an important role in applications.Moreover, measurability issues do not arise for problems involving finite or countably infinite state and control spaces.We note, however, that there are quite a few published works in RL as † The rigorous mathematical theory of stochastic optimal control, including the development of an appropriate measure-theoretic framework, dates to the 60s and 70s, with the work of Blackwell and other great mathematicians.It culminated in the monograph [BeS78], which provides the now "standard" framework, based on the formalism of Borel spaces, lower semianalytic functions, and universally measurable policies.This development involves daunting mathematical complications, which stem, among others, from Blackwell's observation that when a Borel measurable function F (x, u), of the two variables x and u, is minimized with respect to u, the resulting function G(x) = minu F (x, u) need not be Borel measurable (it belongs to the broader class of lower semianalytic functions; see [BeS78]).Moreover, even if the minimum is attained by several functions/policies µ, i.e., G(x) = F x, µ(x) for all x, it is possible that none of these µ is Borel measurable (however, there does exist a minimizing policy that belongs to the broader class of universally measurable policies).Thus, starting with a Borel measurability framework for cost functions and policies, we quickly get outside that framework when executing DP algorithms, such as value and policy iteration.The broader framework of universal measurability is required to correct this deficiency, in the absence of additional (fairly strong) assumptions.Notes, Sources, and Exercises 137 well as exact DP, which purport to address measurability issues with a mathematical narrative that is either confusing or plain incorrect.The third edition of the author's abstract DP monograph [Ber22b], expands on the original 2013 first edition, and aims at a unified development of the core theory and algorithms of total cost sequential decision problems.It addresses simultaneously stochastic, minimax, game, risksensitive, and other DP problems, through the use of abstract DP operators (or Bellman operators as we call them here).The idea is to gain insight through abstraction.In particular, the structure of a DP model is encoded in its abstract Bellman operator, which serves as the "mathematical signature" of the model.Thus, characteristics of this operator (such as monotonicity and contraction) largely determine the analytical results and computational algorithms that can be applied to that model.Abstract DP ideas are also useful for visualizations and interpretations of RL methods using the Newton method formalism that we have discussed somewhat briefly in this book in the context of linear quadratic problems.Approximation in value space, rollout, and policy iteration are the principal subjects of this book.† These are very powerful and general techniques: they can be applied to deterministic and stochastic problems, finite and infinite horizon problems, discrete and continuous spaces problems, and mixtures thereof.Moreover, rollout is reliable, easy to implement, and can be used in conjunction with on-line replanning.It is also compatible with new and exciting technologies such as transformer networks and large language models.As we have noted, rollout with a given base policy is simply the first iteration of the policy iteration algorithm starting from the base policy.Truncated rollout can be interpreted as an "optimistic" form of a single policy iteration, whereby a policy is evaluated inexactly, by using a limited number of value iterations; see the books [Ber20a], [Ber22a].‡ † The name "rollout" (also called "policy rollout") was introduced by Tesauro and Galperin [TeG96] in the context of rolling the dice in the game of backgammon.In Tesauro's proposal, a given backgammon position is evaluated by "rolling out" many games starting from that position to the end of the game.To quote from the paper [TeG96]: "In backgammon parlance, the expected value of a position is known as the "equity" of the position, and estimating the equity by Monte-Carlo sampling is known as performing a "rollout."This involves playing the position out to completion many times with different random dice sequences, using a fixed policy to make move decisions for both sides."‡ Truncated rollout was also proposed in the context of backgammon in the paper [TeG96].To quote from this paper: "Using large multi-layer networks to do full rollouts is not feasible for real-time move decisions, since the large networks are at least a factor of 100 slower than the linear evaluators described previously.We have therefore investigated an alternative Monte-Carlo algorithm, using so-called "truncated rollouts."In this technique trials are not played out Policy iteration, which will be viewed here as the repeated use of rollout, is more ambitious and challenging than rollout.It requires off-line training, possibly in conjunction with the use of neural networks.Together with its neural network and distributed implementations, it will be discussed in more detail later.Note that rollout does not require any off-line training, once the base policy is available; this is its principal advantage over policy iteration.There is a vast literature on linear quadratic problems.The connection of policy iteration with Newton's method within this context and its quadratic convergence rate was first derived by Kleinman [Kle68] for continuous-time linear quadratic problems (the corresponding discretetime result was given by Hewer [Hew71]).For followup work, which relates to policy iteration with approximations, see Feitzinger, Hylla, and Sachs [FHS09], and Hylla [Hyl11].The general relation of approximation in value space with Newton's method, beyond policy iteration, and its connections with MPC and adaptive control was first presented in the author's book [Ber20a], the papers [Ber21b], [Ber22c], and in the book [Ber22a], which contains an extensive discussion.This relation provides the starting point for an in-depth understanding of the synergy between the off-line training and the on-line play components of the approximation in value space architecture, including the role of multistep lookahead in enhancing the starting point of the Newton step.The monograph [Ber22a] also provides convergence analysis of variants of Newton's method applied to the solution of nondifferentiable fixed point problems.Note that in approximation in value space, we are applying Newton's method to the solution of a system of equations (the Bellman equation).This context has no connection with the "gradient descent" methods that are popular for the solution of special types of optimization problems in RL, arising for example in neural network training problems (see Chapter 3).In particular, there are no gradient descent methods that can be used for the solution of systems of equations such as the Bellman equation.There are, however, "first order" deterministic algorithms such as the Gauss-Seidel and Jacobi methods (and stochastic asynchronous extensions) that can to completion, but instead only a few steps in the simulation are taken, and the neural net's equity estimate of the final position reached is used instead of the actual outcome.The truncated rollout algorithm requires much less CPU time, due to two factors: First, there are potentially many fewer steps per trial.Second, there is much less variance per trial, since only a few random steps are taken and a real-valued estimate is recorded, rather than many random steps and an integer final outcome.These two factors combine to give at least an order of magnitude speed-up compared to full rollouts, while still giving a large error reduction relative to the base player."Analysis and computational experience with truncated rollout since 1996 are consistent with the preceding assessment.We note that the term "multiagent" has been used with several different meanings in the literature.For example, some authors place emphasis on the case where the agents do not have common information when selecting their decisions.This gives rise to sequential decision problems with "nonclassical information patterns," which can be very complex, partly because they cannot be addressed by exact DP.Other authors adopt as their starting point a problem where the agents are "weakly" coupled through the system equation, the cost function, or the constraints, and consider methods whereby the weak coupling is exploited to address the problem through (suboptimal) decoupled computations.Agent-by-agent minimization in multiagent approximation in value space and rollout was proposed in the author's paper [Ber19c], which also discusses extensions to infinite horizon policy iteration algorithms, and explores connections with the concept of person-by-person optimality from team theory; see also the lems, where several of the multiagent algorithmic ideas were adapted, tested, and validated.These papers consider large-scale multi-robot and vehicle routing problems, involving partial state information, and explore some of the attendant implementation issues, including autonomous multiagent rollout, through the use of policy neural networks and other precomputed signaling policies.The papers also provide a comparison with alternative approaches to multiagent problems, some of which are based on policy gradient methods.A different type of distributed computation and multiagent optimization, whereby each agent has a partial/local model of the system within part of the state space and relies on aggregate information from other agents to execute a DP computation is proposed in the author's DP book [Ber12], Section 6.5.4; see also Section 3.5.8 of the present book.The DP framework for adaptive control was introduced in a series of papers by Feldbaum, starting in 1960 with [Fel60], under the name dual control theory.These papers emphasized the division of effort between system estimation and control, now more commonly referred to as the exploration-exploitation tradeoff .In the last paper of the series [Fel63], Feldbaum prophetically concluded as follows: "At the present time, the most important problem for the immediate future is the development of approximate solution methods for dual control theory problems, the formulation of sub-optimal strategies, the determination of the numerical value of risk in quasi-optimal systems and its comparison with the value of risk in existing systems."The research on problems involving unknown models and using data for model identification simultaneously with control was rekindled with the advent of the artificial intelligence side of RL and its focus on the active exploration of the environment.Here there is emphasis on "learning from interaction with the environment" [SuB18] through the use of (possibly hidden) Markov decision models, machine learning, and neural networks, in a wide array of methods that are under active development at present.This is more or less the same as the classical problems of dual and adaptive control that have been discussed since the 60s from a control theory perspective.The formulation of adaptive and dual control problems as POMDP (cf.Section 2.11) is classical.The use of rollout within this context was first suggested in the author's book [Ber22a], Section 6.7.The first DP/RL books were written in the 1990s, setting the tone for subsequent developments in the field.One in 1996 by Bertsekas and Tsitsiklis [BeT96], which reflects a decision, control, and optimization viewpoint, and another in 1998 by Sutton and Barto, which is culturally di↵erent and reflects an artificial intelligence viewpoint (a 2nd edition, [SuB18], was published in 2018).We refer to the former book and also to the author's DP textbooks [Ber12], [Ber17a] for a broader discussion of some of the topics of this book, including algorithmic convergence issues and additional DP models, such as those based on average cost and semi-Markov problem optimization.Note that both of these books deal with finite-state Markovian decision models and use a transition probability notation, as they do not address continuous spaces problems, which are one of the major focal points of this book.More recent books are by Gosavi [Gos15] (a much expanded 2nd edition of his 2003 monograph), which emphasizes simulation-based optimization and RL algorithms, Cao [Cao07], which focuses on a sensitivity approach to simulation-based methods, Chang, Fu, Hu, and Marcus [CFH13] (a 2nd edition of their 2007 monograph), which emphasizes finite-horizon/multistep lookahead schemes and adaptive sampling, Busoniu, Babuska, De Schutter, and Ernst [BBD10a], which focuses on function , which deals with forms of adaptive dynamic programming, and topics in both RL and optimal control, and Zoppoli, Sanguineti, Gnecco, and Parisini [ZSG20], which addresses neural network approximations in optimal control as well as multiagent/team problems with nonclassical information patterns.The book by Meyn [Mey22] focuses on the connections of RL and optimal control, similar to the present book, but is more mathematically oriented, and treats stochastic problems and algorithms in far more detail.There are also several books that, while not exclusively focused on DP and/or RL, touch upon several of the topics of the present book.The book by Borkar [Bor08] is an advanced monograph that addresses rigorously many of the convergence issues of iterative stochastic algorithms in approximate DP, mainly using the so-called ODE approach.The book by Meyn [Mey07] is broader in its coverage, but discusses some of the popular approximate DP/RL algorithms.The book by Haykin [Hay08] discusses approximate DP in the broader context of neural network-related subjects.The book by Krishnamurthy [Kri16] focuses on partial state information problems, with a discussion of both exact DP, and approximate DP/RL methods.The textbooks by Kouvaritakis and Cannon [KoC16], Borrelli, Bemporad, and Morari [BBM17], and Rawlings, Mayne, and Diehl [RMD17] collectively provide a comprehensive view of the MPC methodology.The book by Lattimore and Szepesvari [LaS20] is focused on multiarmed bandit methods.The book by Brandimarte [Bra21] is a tutorial introduction to DP/RL that emphasizes operations research applications and includes MATLAB codes.The book by Hardt and Recht [HaR21] focuses on broader subjects of machine learning but covers selectively approximate DP and RL topics as well.The present book is similar in style, terminology, and notation to the author's recent RL textbooks [Ber19a], [Ber20a], [Ber22a], and the 3rd edition of the abstract DP monograph [Ber22b], which collectively provide a fairly comprehensive and more mathematical account of the subject.In particular, the 2019 RL textbook includes a broader coverage of approximation in value space methods, including certainty equivalent control and    (a) Use exact DP with starting city A to verify that the optimal tour is AB-DECA with cost 20.(b) Verify that the nearest neighbor heuristic starting with city A generates the tour ACDBEA with cost 48.(c) Apply rollout with one-step lookahead minimization, using as base heuristic the nearest neighbor heuristic.Show that it generates the tour AECDBA with cost 37.At AE, the rollout algorithm considers the three options of moving to cities B, C, D, or equivalently to states AEB, AEC, AED, and it computes the nearest neighbor-generated tours corresponding to each of these states.These tours are AEBCDA with cost 42, AECDBA with cost 37, AEDCBA with cost 63.The tour AECDBA has the least cost, so the rollout algorithm moves to city C or equivalently to state AEC.At AEC, the rollout algorithm considers the two options of moving to cities B, D, and compares the nearest neighbor-generated tours corresponding to each of these.These tours are AECBDA with cost 52 and AECDBA with cost 37.The tour AECDBA has the least cost, so the rollout algorithm moves to city D or equivalently to state AECD.Then the rollout algorithm has only one option and generates the tour AECDBA with cost 37.(d) Apply rollout with two-step lookahead minimization, using as base heuristic the nearest neighbor heuristic.This rollout algorithm operates as follows.For k = 1, 2, 3, it starts with a k-city partial tour, it generates every possible two-city addition to this tour, uses the nearest neighbor heuristic to complete the tour, and selects as next city to add to the k-city partial tour the city that corresponds to the best tour thus obtained (only one city is added to the current tour at each step of the algorithm, not two).Show that this algorithm generates the optimal tour.The rollout algorithm at stage k runs the nearest neighbor heuristic N − k times, so it must run the heuristic O(N 2 ) times for a total computation of O(N 4 ).Thus the rollout algorithm's complexity involves a low order polynomial increase over the complexity of the base heuristic, something that is generally true for practical discrete optimization problems.Note that even though this may represent a substantial increase in computation over the base heuristic, it is a potentially enormous improvement over the complexity of the exact DP algorithm.In this problem we focus on the one-dimensional linear quadratic problem of Section 1. (f) Plot graphical interpretations of rollout and truncated rollout in the manner of Figs.1.5.10 and 1.5.11using a stable starting linear policy of your choice.Consider the spiders and flies problem of Example 1.6.5 with two differences: the five flies stay still (rather than moving randomly), and there are only two spiders, both of which start at the fourth square from the right at the top row of the grid of Fig. 1.6.10.The base policy is to move each spider one square towards its nearest fly, with distance measured by the Manhattan metric, and with preference given to a horizontal direction over a vertical direction in case of a tie.Apply the multiagent rollout algorithm of Section 1.6.5, and compare its performance with the one of the ordinary rollout algorithm, and with the one of the base policy.This problem is also discussed in Section 2.9.This exercise deals with a computational comparison of the optimal policy, a heuristic policy, and on-line approximation in value space using the heuristic policy, in the context of a problem that involves the timing of the sale of a stock.An investor has the option to sell a given amount of stock at any one of N time periods.The initial price of the stock is an integer x0.The price x k , if it is positive and it is less than a given positive integer value x, it evolves according towith probability 1 − p + − p − , x k − 1 with probability p − , where p + and p − have known values with 0 < p − ≤ p + , p + + p − < 1.If x k = 0, then x k+1 moves to 1 with probability p + , and stays unchanged at 0 with probability 1 − p + .If x k = x, then x k+1 moves to x − 1 with probability p − , and stays unchanged at x with probability 1 − p − .Notes, Sources, and Exercises 147At each period k = 0, . . ., N − 1 for which the stock has not yet been sold, the investor (with knowledge of the current price x k ), can either sell the stock at the current price x k or postpone the sale for a future period.If the stock has not been sold at any of the periods k = 0, . . ., N − 1, it must be sold at period N at price xN .The investor wants to maximize the expected value of the sale.(a) Formulate the problem as a finite horizon DP problem by identifying the state, control, and disturbance spaces, the system equation, the cost function, and the probability distribution of the disturbance.Write the corresponding exact DP algorithm, and use it to compute the optimal policy and the optimal cost as a function of x0.The optimal reward-to-go is generated by the following DP algorithm:and for k = 0, . . ., N − 1, if x k = 0, then(since the price cannot go higher than x, once at x, but can go lower), and if 0 < x k < x, then.(1.99) The optimal policy is to sell at x k = 1, . . ., x−1, if x k attains the maximum in the above equation, and not to sell otherwise.When x k = 0, it is optimal not to sell, while when x k = x, it is optimal to sell.The values of J * k (x k ) and the optimal policy are tabulated as shown in Fig. 1 Then the bounds 0 ≤ x k and x k ≤ x never become "active," and it can be verified that the optimal expected reward is J * (x0) = x0, while all policies are optimal and attain this optimal expected reward.(b) Suppose the investor adopts a heuristic, referred to as base heuristic, whereby he/she sells the stock if its price is greater or equal to βx0, where β is some number with β > 1. Write an exact DP algorithm to compute the expected value of the sale under this heuristic.The reward-to-go for the base heuristic starting from state x k , denoted Jx k k (x k ), can be generated by the following (exact) DP algorithm.(Note here the use of superscript x k in the quantities J x k n (xn) computed by the algorithm.The reason is that the computed values J x k n (xn) depend on x k , which incidentally implies that base heuristic is not sequentially consistent, as defined later in Section 2.3.2 of this book.)The algorithm is given by Jand for n = k, . . ., N − 1, if 0 < xn < βx k , then While the reward-to-go for the base heuristic starting from state x k is very simple to compute for our problem, in order to apply the rollout algorithm only the values Jx k +1 k+1 (x k + 1), Jx k k+1 (x k ), and Jx k −1 k+1 (x k − 1) need to be calculated for each state x k encountered during on-line operation.Moreover, the base heuristic's reward-to-go J x k k (x k ) can also be computed on-line by Monte Carlo simulation for the relevant states x k .This would be the principal option in a more complicated problem where the exact DP algorithm is too time-consuming.(c) Apply approximation in value space with one-step lookahead minimization and with function approximation that is based on the heuristic of part (b).In particular, use J N (xN ) = xN , and for k = 1, . . ., N − 1, use J k (x k ) that is equal to the expected value of the sale when starting at x k and using the heuristic that sells the stock when its price exceeds βx k .Use exact DP as well as Monte Carlo simulation to compute/approximate on-line the needed values J k (x k ).Compare the expected values of sale price computed with the optimal, heuristic, and approximation in value space methods.Solution: The rollout policy π = {μ0, . . ., μN−1} is determined by the base heuristic, where for every possible state x k , and stage k = 0, . . ., N − 1, the rollout decision μk (Exact and Approximate Dynamic Programming Chap. 1 and μk (x k ) = don't sell at x k , otherwise.The sell or don't sell decision of the rollout algorithm is made on-line according to the preceding criterion, at each state x k encountered during on-line operation.Figure 1.8.4 shows the rollout policy, which is computed by the preceding equations using the rewards-to-go of the base heuristic J x k k (x k ), as given in Fig. 1.8.3.Once the rollout policy is computed, the corresponding reward function Jk (x k ) can be calculated similar to the case of the base heuristic.Of course, during on-line operation, the rollout decision need only be computed for the states x k encountered on-line.The important observation when comparing Figs.1.8.3 and 1.8.4 is that the rewards-to-go of the rollout policy are greater or equal to the ones for the base heuristic.In particular, starting from x0, the rollout policy attains reward 2.269, and the base heuristic attains reward 2.268.The optimal policy attains reward 2.4.The rollout policy reward is slightly closer to the optimal than the base heuristic reward.The rollout reward-to-go values shown in Fig. 1.8.4 are "exact," and correspond to the favorable case where the heuristic rewards needed at x k , J When finite-sample Monte Carlo simulation is used to approximate the needed base heuristic rewards at state x k , i.e., Jx k −1 k+1 (x k −1), the performance of the rollout algorithm will be degraded.In particular, by using a computer program to implement rollout with Monte Carlo simulation, it can be shown that when Jx k +1 k+1 (x k + 1), Jx k k+1 (x k ), and Jx k −1 k+1 (x k − 1) are approximated using a 20-sample Monte-Carlo simulation per reward value, the rollout algorithm achieves reward 2.264 starting from x0.This reward is evaluated by (almost exact) 400-sample Monte Carlo simulation of the rollout algorithm.When Jx k +1 k+1 (x k + 1), Jx k k+1 (x k ), and Jx k −1 k+1 (x k − 1) were approximated using a 200-sample Monte-Carlo simulation per reward value, the rollout algorithm achieves reward 2.273 [as evaluated by (almost exact) 400-sample Monte Carlo simulation of the rollout algorithm].Thus with 20-sample simulation, the rollout algorithm performs worse than the base heuristic starting from x0.With the more accurate 200-sample simulation, the rollout algorithm performs better than the base heuristic starting from x0, and performs nearly as well as the optimal policy (but still somewhat worse than in the case where exact values of the needed base heuristic rewards are used (based on an "infinite" number Monte Carlo samples).It is worth noting here that the heuristic is not a legitimate policy because at any state xn is makes a decision that depends on the state x k where it started.Thus the heuristic's decision at xn depends not just on xn, but also on the starting state x k .However, the rollout algorithm is always an approximation in value space scheme with approximation reward J k (x k ) defined by the heuristic, and it provides a legitimate policy.Answer : The implementation is very similar to the one-step lookahead case.The main difference is that at state x k , the rollout algorithm needs to calculate the base heuristic reward values Jx k +2 k+2 (x k + 2), Jx k +1 k+2 (x k + 1), Jx k k+2 (x k ), Jx k −1 k+2 (x k − 1), and Jx k −2 k+2 (x k − 2).Thus the on-line Monte Carlo simulation work is accordingly increased.Generally the simulation work per stage of the rollout algorithm is proportional to 2 + 1, when -stage lookahead minimization is used, since the number of leafs at the end of the lookahead tree is 2 + 1.In a more realistic version of the cruise control system of Example 1.3.1, the system has the formwhere the coefficient a satisfies 0 < a ≤ 1, and the disturbance w k has zero mean and variance σ 2 .The cost function has the formwhere x0, . . ., xN are given nonpositive target values (a velocity profile) that serve to adjust the vehicle's velocity, in order to maintain a safe distance from Chap. 1 the vehicle ahead, etc.In a practical setting, the velocity profile is recalculated by using on-line radar measurements.Design an experiment to compare the performance of a fixed linear policy π, derived for a fixed nominal velocity profile, and the performance of the algorithm that uses on-line replanning, whereby the optimal policy π * is recalculated each time the velocity profile changes.Compare with the performance of the rollout policy π that uses π as the base policy and on-line replanning.In reference to Example 1.6.4,a driver aims to park at an inexpensive space on the way to his destination.There are L parking spaces available and a garage at the end.The driver can move in either direction.For example if he is in space i he can either move to i − 1 with a cost t − i , or to i + 1 with a cost t + i, or he can park at a cost c(i) (if the parking space i is free).The only exception is when he arrives at the garage (indicated by index N ) and he has to park there at a cost C.Moreover, after the driver visits a parking space he remembers its free/taken status and has an option to return to any parking space he has already visited.However, the driver must park within a given number of stages N , so that the problem has a finite horizon.The initial probability of space i being free is given, and the driver can only observe the free/taken status of a parking only after he/she visits the space.Moreover, the free/taken status of a parking visited so far does not change over time.Write a program to calculate the optimal solution using exact dynamic programming over a state space that is as small as possible.Try to experiment with different problem data, and try to visualize the optimal cost/policy with suitable graphical plots.Comment on run-time as you increase the number of parking spots L.The classical form of Newton's method applied to a scalar equation of the form H(K) = 0 takes the formwhereis the derivative of H, evaluated at the current iterate K k .This exercise shows algebraically (rather than graphically), within the context of linear quadratic problems, that in approximation in value space with quadratic cost approximation, the cost function of the corresponding one-step lookahead policy is the result of a Newton step for solving the Riccati equation.To this end, we will apply Newton's method to the solution of the Riccati Eq. (1.42), which we write in the form H(K) = 0, where(1.105)Sec. 1.8Notes, Sources, and Exercises 153 (a) Show that the operation that generates KL starting from K is a Newton iteration of the form (1.104).In other words, show that for all K that lead to a stable one-step lookahead policy, we havewhere we denote by KL = q + rL 2 1 − (a + bL) 2 (1.107) the quadratic cost coefficient of the one-step lookahead linear policy µ(x) = Lx corresponding to the cost function approximation J(x) = Kx 2 :Proof: Our approach for showing the Newton step formula (1.106) is to express each term in this formula in terms of L, and then show that the formula holds as an identity for all L. To this end, we first note from Eq.(1.108) that K can be expressed in terms of L as.(1.109) Furthermore, by using Eqs.(1.108) and (1.109), H(K) as given in Eq.(1.105) can be expressed in terms of L as follows:(1.110)Moreover, by differentiating the function H of Eq. (1.105), we obtain after a straightforward calculationwhere the second equation follows from Eq. (1.108).Having expressed all the terms in the Newton step formula (1.106) in terms of L through Eqs.(1.107), (1.109), (1.110), and (1.111), we can write this formula in terms of L only asor equivalently asExact and Approximate Dynamic Programming Chap. 1A straightforward calculation now shows that this equation holds as an identity for all L.(b) What happens when K lies outside the region of stability?(c) Show that in the case of `-step lookahead, the analog of the quadratic convergence rate estimate has the formwhere F ` 1 ( K) is the result of the (` 1)-fold application of the mapping F to K. Thus a stronger bound for |KL K ⇤ | is obtained.In Section 1.5, we discussed the concept of the region of stability in the context of linear quadratic problems.The concept extends to far more general infinite horizon problems (see e.g., the book [Ber22a], Section 3.3).The idea is to call a stationary policy µ unstable if Jµ(x) = 1 for some states x, and call it stable otherwise.For ` 1, the `-step region of stability is the set of J for which the corresponding `-step lookahead policy is stable.Generally, the `-step region of stability expands as `increases.Note also that in finite-state discounted problems all policies are stable, so all J belong to the region of stability.However, for SSP this is not so: there are policies, called improper , that do not terminate with positive probability for some initial states (see the books [Ber12] and [Ber22b] for extensive discussions).Such policies can be unstable.In the following example the region of instability includes functions that are very close to J ⇤ , even with large `.This example involves small stage costs, a class of problems that pose challenges for approximation in value space; see Section 2.6.Consider a shortest path problem with a single state 1, plus the termination state t.At state 1 we can either stay at that state at cost ✏ > 0 or move to the state t at cost 1.Thus the optimal policy at state 1 is to move to t, the optimal cost J ⇤ (1) = 1, and is the unique solution of Bellman's equation J ⇤ (1) = min 1, ✏ + J ⇤ (1) .(In SSP the optimal cost at t is 0 by assumption, and Bellman's equation involves only the costs of the states other than t.) (a) Show that the one-step region of stability is the set of all J(1) > 1 ✏.What happens in the case where J(1) = 1 ✏? Show also that the `-step region of stability is the set of all J(1) > 1 `✏.Note: The `-step region of stability becomes arbitrarily large for su ciently large `.However, the boundary of the `-step region of stability is arbitrarily close to J ⇤ (1) for su ciently small ✏.(b) What happens in the case where there are additional states i = 2, . . ., n, and for each of these states i there is the option of staying at i at cost ✏ or moving to i 1 at cost 0? Partial answer : The one-step region of stability consists of all J = J(n), . . ., J(1) such that ✏ + J(i) > J(i 1) for all i 2 and ✏ + J(1) > 1.In this chapter, we discuss various aspects of approximation in value space and rollout algorithms, focusing primarily on the case where the state and control spaces are finite.In Sections 2.1-2.6,we consider finite horizon deterministic problems, which in addition to arising often in practice, offer some important advantages in the context of RL.In particular, a finite horizon is well suited for the use of rollout, while the deterministic character of the problem eliminates the need for costly on-line Monte Carlo simulation.An interesting aspect of our methodology for discrete deterministic problems is that it admits extensions that we have not discussed so far.The extensions include multistep lookahead variants, as well as variants that apply to constrained forms of DP, which involve constraints on the entire system trajectory, and also allow the use of heuristic algorithms that are more general than policies within the context of rollout.These variants rely on the problem's deterministic structure, and do not extend to stochastic problems.Another interesting aspect of finite state deterministic problems is that they can serve as a framework for an important class of commonly encountered discrete optimization problems, including integer programming and combinatorial optimization problems such as scheduling, assignment, routing, etc.This brings to bear the methodology of approximation in value space, rollout, adaptive control, and MPC, and provides effective suboptimal solution methods for these problems.In Sections 2.7-2.11,we consider various problems that involve stochastic uncertainty.In Section 2.12, we consider minimax problems that involve set membership uncertainty.The present chapter draws heavily on Chapters 2 and 3 of the book [Ber20a], and Chapter 6 of the book [Ber22a].These books may be consulted for more details and additional examples.While our focus in this chapter will be on finite horizon problems, our discussion applies to infinite horizon problems as well, because approximation in value space and rollout are essentially finite-stages algorithms, while the nature of the original problem horizon (be it finite or infinite) affects only the terminal cost function approximation.Thus in implementing onestep or multistep approximation in value space, it makes little difference whether the original problem has finite or infinite horizon.At the same time, for conceptual purposes, we can argue that finite horizon problems, even when they involve a nonstationary system and cost per stage, can be transformed to infinite horizon problems, by introducing an artificial costfree termination state that the system moves into at the end of the horizon; see Sections 1.6.3 and 1.6.4.Through this transformation, the synergy of off-line training and on-line play based on Newton's method is brought to bear, and the insights that we discussed in Chapter 1 in the context of an infinite horizon apply and explain the good performance of our methods in practice.Starting from state x k , the next state under control u k is generated nonrandomly, according toand a stage cost g k (x k , u k ) is incurred.We recall from Chapter 1, Section 1.2, that in deterministic finite horizon DP problems, the state is generated nonrandomly over N stages, through a system equation of the formwhere k is the time index, andx k is the state of the system, an element of some state space X k , u k is the control or decision variable, to be selected at time k from some given set U k (x k ), a subset of a control space U k , that depends on x k , f k is a function of (x k , u k ) that describes the mechanism by which the state is updated from time k to time k + 1.The state space X k and control space U k are arbitrary sets and may depend on k.Similarly, the system function f k can be arbitrary and may depend on k.The cost incurred at time k is denoted by g k (x k , u k ), and the function g k may depend on k.For a given initial state x 0 , the total cost of a control sequence {u 0 , . . ., u N −1 } iswhere g N (x N ) is a terminal cost incurred at the end of the process.This is a well-defined number, since the control sequence {u 0 , . . ., u N −1 } together with x 0 determines exactly the state sequence {x 1 , . . ., x N } via the system equation (2.1); see Figure 2.1.1.We want to minimize the cost (2.2) over all sequences {u 0 , . . ., u N −1 } that satisfy the control constraints, thereby obtaining the optimal value as a function of x 0 J * (x 0 ) = min u k ∈U k (x k ) k=0,...,N−1 J(x 0 ; u 0 , . . ., u N −1 ).Notice an important difference from the stochastic case: we optimize over sequences of controls {u 0 , . . ., u N −1 }, rather than over policies that consist of a sequence of functions π = {µ 0 , . . ., µ N −1 }, where µ k maps states x k into controls u k = µ k (x k ), and satisfies the control constraints µ k (x k ) ∈ U k (x k ) for all x k .It is well-known that in the presence of stochastic uncertainty, policies are more effective than control sequences, and can result in improved cost.On the other hand for deterministic problems, minimizing over control sequences yields the same optimal cost as over policies, since the cost of any policy starting from a given state determines with certainty the controls applied at that state and the future states, and hence can also be achieved by the corresponding control sequence.This point of view allows more general forms of rollout, which we will discuss in this chapter: instead of using a policy for rollout, we will allow the use of more general heuristics for choosing future controls.We recall from Chapter 1, Section 1.2, the DP algorithm for finite horizon deterministic problems.It constructs functionssequentially, starting from J * N , and proceeding backwards to J * N −1 , J * N −2 , etc.The value J * k (x k ) will be viewed as the optimal cost of the tail subproblem that starts at state x k at time k and ends at some state x N .Start withand for k = 0, . . ., N − 1, let(2.4)Note that at stage k, the calculation in Eq. (2.4) must be done for all states x k before proceeding to stage k − 1.The key fact about the DP algorithm is that for every initial state x 0 , the number J * 0 (x 0 ) obtained at the last step, is equal to the optimal cost J * (x 0 ).Indeed, a more general fact was shown in Section 1.2, namely that for all k = 0, 1, . . ., N − 1, and all states x k at time k, we have where J(x k ; u k , . . ., u N −1 ) is the cost generated by starting at x k and using subsequent controls u k , . . ., u N −1 :Thus, J * k (x k ) is the optimal cost for an (N − k)-stage tail subproblem that starts at state x k and time k, and ends at time N .Based on this interpretation of J * k (x k ), we call it the optimal cost-to-go from state x k at stage k, and refer to J * k as the optimal cost-to-go function or optimal cost function at time k.We have also discussed in Section 1.2 the construction of an optimal control sequence.Once the functions J * 0 , . . ., J * N have been obtained, we can use a forward algorithm to construct an optimal control sequence {u * 0 , . . ., u  and x * 1 = f 0 (x 0 , u * 0 ).Sequentially, going forward, for k = 1, 2, . . ., N − 1, setandNote an interesting conceptual division of the optimal control sequence construction: there is off-line training to obtain J * k by precomputation [cf. the DP Eqs.(2.3)-(2.4)],which is followed by on-line play to obtain u * k [cf.Eq. (2.6)].This is analogous to the two algorithmic processes described in Section 1.1 in connection with computer chess and backgammon.For the first five sections of this chapter, we will consider the case where the state and control spaces are discrete and consist of a finite number of Shortest path problems arise in a great variety of application domains.While there are quite a few efficient polynomial algorithms for solving them, some practical shortest path problems are extraordinarily difficult because they involve an astronomically large number of nodes.For example deterministic scheduling problems of the type discussed in Example 1.2.1 can be formulated as shortest path problems, but with a number of nodes that grows exponentially with the number of tasks.For such problems neither exact DP nor any other shortest path algorithm can compute an exact optimal solution in practice.In what follows, we will aim to show that suboptimal solution methods, and rollout algorithms in particular, offer a viable alternative.Many types of search problems involving games and puzzles also ad-  mit in principle exact solution by DP, but have to be solved by suboptimal methods in practice.The following is a characteristic example.Example 2.1.1(The Four Queens Problem)Four queens must be placed on a 4 × 4 portion of a chessboard so that noqueen can attack another.In other words, the placement must be such that every row, column, or diagonal of the 4 × 4 board contains at most one queen.Equivalently, we can view the problem as a sequence of problems; first, placing a queen in one of the first two squares in the top row, then placing another queen in the second row so that it is not attacked by the first, and similarly placing the third and fourth queens.(It is sufficient to consider only the first two squares of the top row, since the other two squares lead to symmetric positions; this is an example of a situation where we have a choice between several possible state spaces, but we select the one that is smallest.)We can associate positions with nodes of an acyclic graph where the root node s corresponds to the position with no queens and the terminal nodes correspond to the positions where no additional queens can be placed without some queen attacking another.Let us connect each terminal position with an artificial terminal node t by means of an arc.Let us also assign to all arcs cost zero except for the artificial arcs connecting terminal positions with less than four queens with the artificial node t.These latter arcs are assigned a cost of 1 (see Fig. 2.1.3)to express the fact that they correspond to dead-end positions that cannot lead to a solution.Then, the four queens problem reduces to finding a minimal cost path from node s to node t, with an optimal sequence of queen placements corresponding to cost 0.Note that once the states/nodes of the graph are enumerated, the problem is essentially solved.In this 4 × 4 problem the states are few and can be easily enumerated.However, we can think of similar problems with much larger state spaces.For example consider the problem of placing N queens on an N × N board without any queen attacking another.Even for moderate values of N , the state space for this problem can be extremely large (for N = 8 the number of possible placements with exactly one queen in each row is 8 8 = 16, 777, 216).It can be shown that there exist solutions to the N queens problem for all N ≥ 4 (for N = 2 and N = 3, clearly there is no solution).Moreover effective (non-DP) search algorithms have been devised for its solution up to very large values of N .The preceding example illustrates some of the difficulties of applying exact DP to discrete/combinatorial problems with the type of formulation that we have described.The state space typically becomes very large, particularly as k increases.In the preceding example, to start a backward DP algorithm, we need to consider all the possible terminal positions, which are too many when N is large.There is an alternative exact DP algorithm for deterministic problems, which proceeds forwards from the initial state.It is simply the backward DP algorithm applied to an equivalent shortest path problem, derived form one of Fig. 2.1.2by reversing the directions of all the arcs, and exchanging the roles of the origin and the destination.It will be discussed in Section 2.4; see also [Ber17a], Chapter 2. Still, however, this forward DP algorithm cannot overcome the difficulty with a very large state space.A major class of deterministic problems that can be formulated as DP problems involves the minimization of a cost function G(u) over all u within a constraint set U .For the purposes of this chapter, we assume that U is finite, although a similar DP formulation is also possible for the more general case of an infinite set U .In Section 1.6.3,we discussed the case where u consists of N components, u = (u 0 , . . ., u N −1 ), the system equation takes the simple formand there is just a terminal cost G(x N ) = G(u); see Fig. 2.1.4.The following is a simple but challenging example, which we will further discuss in Section 2.6.An interesting special case of the general optimization problem minu∈U G(u), where u = (u0, . . ., uN−1), is a feasibility problem, where G(u) ≡ 0, and the problem reduces to finding a value of u that satisfies the constraint.Generally, the structure of the constraint set U is encoded in a graph representing the problem such as the one of Fig. 2 Constraint programming problems can also be transformed into equivalent unconstrained (or less constrained) problems by using problem-dependent penalty functions that eliminate constraints while quantifying the level of constraint violation.As an illustration, consider the case where the problem is to find a feasible solution of a system of constraints of the formThis problem can be transformed into the equivalent DP problem of minimizingsubject to the system equation x k+1 = u k , and the control constraints u k ∈ U k , k = 0, . . ., N − 1.Other penalty functions can also be used, such as a quadratic; see the author's nonlinear programming text [Ber16].This approach is convenient, but it offers no guarantee that it can find a complete feasible solution (u0, . . ., uN−1), even if one exists.It simply aims to minimize (suboptimally) a measure of the total constraint violation.However, in the process it may be able to find a complete feasible solution, or an infeasible solution that is adequate for practical purposes.The forward optimal control sequence construction of Eq. (2.6) is possible only after we have computed J * k (x k ) by DP for all x k and k.Unfortunately, in practice this is often prohibitively time-consuming.However, a similar forward algorithmic process can be used if the optimal cost-to-go functions J * k are replaced by some approximations Jk .This is the idea of approximation in value space that we discussed in Section 1.2.3.It constructs a suboptimal solution {ũ 0 , . . ., ũN−1 } in place of the optimal {u * 0 , . . ., u * N −1 }, by using Jk in place of J * k in the DP procedure (2.6).and set x1 = f 0 (x 0 , ũ0 ).Sequentially, going forward, for k = 1, 2, . . ., N − 1, set ũk ∈ arg min)which is minimized in approximation in value space [cf.Eq. (2.7)] is known as the (approximate) Q-factor of (x k , u k ).Note that the computation of the suboptimal control (2.7) can be done through the Q-factor minimization ũk ∈ arg minThis suggests the possibility of using approximate off-line trained Q-factors in place of cost functions in approximation in value space schemes.However, contrary to the cost approximation scheme (2.7) and its multistep counterparts, the performance may be degraded through the errors in the off-line training of the Q-factors (depending on how the training is done).An important practical idea is to choose the cost function approximation Jk+1 in Eq. (2.7) in a way that exploits the problem's structure to expedite the computation of the one-step lookahead minimizing control ũk  ).There are N + 1 node layers each consisting of m nodes (here m = 4).Each grouping consists of N + 1 nodes, one from each layer, and N corresponding arcs.An (N + 1)-dimensional assignment consists of m node-disjoint groupings, where each node belongs to one and only one grouping (illustrated in the figure with thick lines).For each grouping, there is an associated cost that depends on the N -tuple of arcs comprising the grouping.The cost of an (N + 1)-dimensional assignment is the sum of the costs of its m groupings.The difficulty here is that the cost of a grouping does not decompose into a sum of its N arc costs, so the problem cannot be solved by solving N decoupled 2-dimensional assignment problems (for a suboptimal approach based on enforced decoupling, see [Ber20a], Section 3.4.2).They arise in various settings including scheduling, resource allocation, and data association.The general idea is to group together tasks, or resources, or data points, so as to optimize some objective.An example is when we are given a set of m jobs, m persons, and m machines, and we want to select m nonoverlapping (job, person, machine) triplets that correspond to minimum cost.Another example is data association problems, whereby we have collected data relating to the movement of some entities (e,g., persons, vehicles) sequentially over a number of time periods, and we want to group together data points that correspond to distinct entities for better inference purposes.Mathematically, multidimensional assignment problems involve graphs consisting of N +1 subsets of nodes N0, N1, . . ., NN , and referred to as layers.The arcs of the graphs are directed and are of the form (i, j), where i is a node in a layer N k , k = 0, 1, . . ., N − 1, and j is a node in the corresponding next layer N k+1 .Thus we have a directed graph with nodes arranged in N + 1 layers, and arcs connecting the nodes of each layer to the nodes in their adjacent layers; see Fig. 2.2.1.We assume that N ≥ 2, so there are at least three layers.For simplicity, we also assume that each of the layers N k contains the same number of nodes, say m, and that there is a unique arc connecting each node in a given layer with each of the nodes of the adjacent layers.We will present an approximation in value space approach that can be implemented by solving 2-dimensional assignment problems for which very fast algorithms exist, such as the Hungarian method and the auction algorithm (see e.g., [Ber98]).Chap. 2 may also consider -step lookahead , which involves the solution of an -step deterministic DP problem, where is an integer, 1 < < N − k, with a terminal cost function approximation Jk+ .As we have noted in Chapter 1, multistep lookahead typically provides better performance over one-step lookahead in approximation in value space schemes.For example in AlphaZero chess, long multistep lookahead is critical for good on-line performance.On the negative side, the solution of the multistep lookahead minimization problem is more time consuming than its one-step lookahead counterpart.However, the deterministic character of the lookahead minimization problem and the fact that it is solved for the single initial state x k at each time k helps to limit the growth of the lookahead tree and to keep the computation manageable.Moreover, one may try to approximate the solution of the multistep lookahead minimization problem (see Section 2.4).The construction of suitable approximate cost-to-go functions Jk+1 for approximation in value space can be done in many different ways, including some of the principal RL methods.A method of particular interest for our course is rollout , whereby the approximate values Jk+1 (x k+1 ) in Eq. (2.7) are obtained when needed by running for each u k ∈ U k (x k ) a heuristic control scheme, called base heuristic, for a suitably large number of steps, starting fromThe base heuristic can be any method, which starting from a state x k+1 generates a sequence of controls u k+1 , . . ., u N −1 , the corresponding sequence of states x k+2 , . . ., x N , and the cost of the heuristic starting from x k+1 , which we will generically denote by H k+1 (x k+1 ) in this chapter:This value of H k+1 (x k+1 ) is the one used as the approximate cost Jk+1 (x k+1 ) in the corresponding approximation in value space scheme (2.7).In this section, we will develop in more detail the theory of rollout with one-step lookahead minimization for deterministic problems, including the important issue of cost improvement.We will also illustrate several variants of the method, and we will consider questions of efficient implementation.We will then discuss examples of discrete optimization applications.Let us consider a deterministic DP problem with a finite number of controls and a given initial state (so the number of states that can be reached from the initial state is also finite).We first focus on the pure form of rollout that uses one-step lookahead without truncation, and hence no terminal cost approximation.Given a state x k at time k, this algorithm considers the tail subproblems that start at every possible next state x k+1 , and solves them suboptimally with the base heuristic.x 0 0 x 1 ) . . .and the rollout algorithm selects the control μk (x k ) with minimal Q-factor.Thus when at x k , rollout generates on-line the next states x k+1 that correspond to all u k ∈ U k (x k ), and uses the base heuristic to compute the sequence of states {x k+1 , . . ., x N } and controls {u k+1 , . . ., u N −1 } such thatand the corresponding costThe rollout algorithm then applies the control that minimizes over u k ∈ U k (x k ) the tail cost expression for stages k to N :Equivalently, and more succinctly, the rollout algorithm applies at state x k the control μk (x k ) given by the minimization μk (x k ) ∈ arg minwhere Qk (x k , u k ) is the approximate Q-factor defined by   incurred costs up to the time when both tasks are completed.For example starting from the vehicle positions/next state (3,5), the heuristic will produce the following sequence of moves:• Vehicles 1 and 2 move from (3,5) to (6,2).• Vehicles 1 and 2 move from (6,2) to (9,4), and the task at 9 is performed.• Vehicles 1 and 2 move from (9,4) to (12,7), and the task at 7 is performed.The two tasks are thus performed in a total of 6 vehicles moves once the move to (3,5) has been made.The process of running the heuristic is repeated from the other three vehicle position pairs/next states (4,5), (3,4) (4,4), and the heuristic cost (number of moves) is recorded.We then choose the next state that corresponds to minimum cost.In our case the joint move to state x k+1 that involves the pair (3,4) produces the sequence • Vehicles 1 and 2 move from (3,4) to (6,7), and the task at 7 is performed.• Vehicles 1 and 2 move from (6,7) to (9,4), and the task at 9 is performed.and performs the two tasks in a total of 6 vehicle moves.It can be verified that it yields minimum first stage cost plus heuristic cost from the next state, as per Eq.(2.7).Thus, the rollout algorithm will choose to move the vehicles to state (3,4) from state (1,2).At that state the rollout process will be repeated, i.e., consider the possible next joint moves to the node pairs (6,7), (6,2), (6,1), One may use DP to discover a free path (if one exists) by starting from the last stage and by proceeding backwards to the root node.The kth step of the algorithm determines for each node of stage N − k whether there is a free path from that node to some leaf node, by using the results of the preceding step.The amount of calculation at the kth step is O(2 N−k ).Adding the computations for the N stages, we see that the total amount of calculation is O(N 2 N ), so it increases exponentially with the number of stages.For this reason it is interesting to consider heuristics requiring computation that is linear or polynomial in N , but may sometimes fail to determine a free path, even when a free path exists.Thus, one may suboptimally use a greedy algorithm, which starts at the root node, selects a free outgoing arc (if one is available), and tries to construct a free path by adding successively nodes to the path.At the current node, if one of the outgoing arcs is free and the other is blocked, the greedy algorithm selects the free arc.Otherwise, it selects one of the two outgoing arcs according to some fixed rule that depends only on the current node (and not on the status of other arcs).Clearly, the greedy algorithm may fail to find a free path even if such a path exists, as can be seen from Fig. 2.3.3.On the other hand the amount of computation associated with the greedy algorithm is O(N ), which is much faster than the O(N 2 N ) computation of the DP algorithm.Thus we may view the greedy algorithm as a fast heuristic, which is suboptimal in the sense that there are problem instances where it fails while the DP algorithm succeeds.One may also consider a rollout algorithm that uses the greedy algorithm as the base heuristic.There is an analysis that compares the probability of finding a breakthrough solution with the greedy and with the rollout algorithm for random instances of binary trees (each arc is independently free or blocked with given probability p).This analysis is given in Section 6.4 of the book [Ber17a], and shows that asymptotically, the rollout algorithm requires O(N ) times more computation, but has an O(N ) times larger probability of finding a free path than the greedy algorithm.This tradeoff is qualitatively typical: the rollout algorithm achieves a substantial performance improvement over the base heuristic at the expense of extra computation that is equal to the computation time of the base heuristic times a factor that is a low order polynomial of the problem size.The definition of the rollout algorithm leaves open the choice of the base heuristic.There are several types of suboptimal solution methods that can be used as base heuristics, such as greedy algorithms, local search, genetic algorithms, and others.Intuitively, we expect that the rollout policy's performance is no worse than the one of the base heuristic: since rollout optimizes over the first control before applying the heuristic, it makes sense to conjecture that it performs better than applying the heuristic without the first control optimization.However, some special conditions must hold in order to guaranteeWe will now show that the rollout algorithm obtained with a sequentially consistent base heuristic has a fundamental cost improvement property: it yields no worse cost than the base heuristic.The amount of cost improvement cannot be easily quantified, but is determined by the performance of the Newton step associated with the rollout policy, so it can be very substantial; cf. the discussion of Chapter 1.Proposition 2.3.1:(Cost Improvement Under Sequential Consistency) Consider the rollout policy π = {μ 0 , . . ., μN−1 } obtained with a sequentially consistent base heuristic, and let J k,π (x k ) denote the cost obtained with π starting from x k at time k.Then we havewhere H k (x k ) denotes the cost of the base heuristic starting from x k .Proof: We prove the inequality (2.11) by induction.Clearly it holds for k = N , sinceAssume that it holds for index k + 1.For any state x k , let u k be the control applied by the base heuristic at x k .Then we havewhere:• The first equality is the DP equation for the rollout policy π.• The first inequality holds by the induction hypothesis.• The second equality holds by the definition of the rollout algorithm.• The third equality is the DP equation for the policy that corresponds to the base heuristic (this is the step where we need sequential consistency).This completes the proof of the cost improvement property (2.11).Q.E.D.Proof: Follows from the calculation of Eq. (2.12), by replacing the last two steps (which rely on sequential consistency) with Eq. (2.13).Q.E.D.Thus the rollout algorithm obtained with a sequentially improving base heuristic, will improve or at least will perform no worse than the base heuristic, from every starting state x k .In fact the algorithm has a monotonic improvement property, whereby it discovers a sequence of improved trajectories.In particular, let us denote the trajectory generated by the base heuristic starting from x 0 byand the final trajectory generated by the rollout algorithm starting from x 0 byConsider also the intermediate trajectories generated by the rollout algorithm given byis the trajectory generated by the base heuristic starting from xk .Then, by using the sequential improvement condition, it can be proved (see Fig.Cost ofEmpirically, it has been observed that the cost improvement obtained by rollout with a sequentially improving heuristic is typically considerable and often dramatic.In particular, many case studies, dating to the middle 1990s, indicate consistently good performance of rollout; see the last section of this chapter for a bibliography.The DP textbook [Ber17a] provides some detailed worked-out examples (Chapter 6, Examples 6.4.2, 6.4.5, 6.4.6, and Exercises 6.11, 6.14, 6.15, 6.16).The price for the performance improvement is extra computation that is typically equal to the computation time of the base heuristic times a factor that is a low order polynomial of N .It is generally hard to quantify the amount of performance improvement, but the computational results obtained from the case studies are consistent with the Newton step interpretations that we discussed in Chapter 1.The books [Ber19a] (Section 2.5.1) and [Ber20a] (Section 3.1) show that the sequential improvement condition is satisfied in the context of MPC, and is the underlying reason for the stability properties of the MPC scheme.On the other hand the base heuristic underlying the classicalIn this section we describe a rollout variant that implicitly enforces the sequential improvement property.This variant, called the fortified rollout algorithm, starts at x 0 , and generates step-by-step a sequence of states {x 0 , x 1 , . . ., x N } and corresponding sequence of controls.Upon reaching state x k we have the trajectorythat has been constructed by rollout, called permanent trajectory, and we also store a tentative best trajectoryThe tentative best trajectory T k is the end-to-end trajectory that has minimum cost out of all end-to-end trajectories computed up to stage k of the algorithm.Initially, T 0 is the trajectory generated by the base heuristic starting at the initial state x 0 .The idea now is to discard the suggestion of the rollout algorithm at every state x k where it produces a trajectory that is inferior to T k , and use T k instead (see Fig. 2In particular, upon reaching state x k , we run the rollout algorithm as earlier, i.e., for every u k ∈ U k (x k ) and next state x k+1 = f k (x k , u k ), we run the base heuristic from x k+1 , and find the control ũk that gives the best trajectory, denotedWhereas the ordinary rollout algorithm would choose control ũk and move to xk+1 , the fortified algorithm compares C(T k ) and C( Tk ), and depending on which of the two is smaller, chooses u k or ũk and moves to x k+1 or to xk+1 , respectively.In particular, ifthe tentative best trajectory, starting with the one produced by the base heuristic at the initial condition, we can ensure that the fortified rollout algorithm, even with approximate minimization, will not produce an inferior solution to the one of the base heuristic.In many problems, several promising heuristics may be available.It is then possible to use all of these heuristics in the rollout framework.The idea is to construct a superheuristic, which selects the best out of the trajectories produced by the entire collection of heuristics.The superheuristic can then be used as the base heuristic for a rollout algorithm.†In particular, let us assume that we have m heuristics, and that the th of these, given a state x k+1 , produces a trajectoryand corresponding cost C( T k+1 ).The superheuristic then produces at x k+1 the trajectory T k+1 for which C( T k+1 ) is minimum.The rollout algorithm selects at state x k the control u k that minimizes the minimal Q-factor: ũk ∈ arg minis the cost of the trajectory (x k , u k , T k+1 ).Note that the Q-factors of the different heuristics can be computed independently and in parallel.In view of this fact, the rollout scheme just described is sometimes referred to as parallel rollout.An interesting property, which can be readily verified by using the definitions, is that if all the heuristics are sequentially improving, the same is true for the superheuristic, something that is also suggested by Fig. 2.3.4.Indeed, let us write the sequential improvement condition (2.13) for each of the base heuristics min, . . ., m, † A related practically interesting possibility is to introduce a partition of the state space into subsets, and a collection of multiple heuristics that are specially tailored to the subsets.We may then select the appropriate heuristic to use on each subset of the partition.In fact one may use a collection of multiple heuristics tailored to each subset of the state space partition, and at each state, select out of all the heuristics that apply, the one that yields minimum cost.It turns out that the proof of the cost improvement property of Prop.2.3.2,J k,π (x k ) ≤ H k (x k ), for all x k and k, goes through if the following modified sequential improvement property holds: minIn particular, for the property (2.18) to hold, it is sufficient that U k (x k ) contains the base heuristic choice u k .The idea of replacing the minimization (2.16) by the simpler minimization (2.17) can be extended.In particular, by working through the preceding argument, it can be seen that any policyfor all x k and k, guarantees the modified sequential improvement property (2.18), and hence also the cost improvement property.A prominent example of such an algorithm arises in the multiagent case where u has m components, u = (u 1 , . . ., u m ), and the minimization over) is replaced by a sequence of single component minimizations, one-componentat-a-time; cf.Section 1.6.5.Of course in the multiagent case, the onecomponent-at-a-time implementation has an additional favorable property: it can be viewed as rollout (without simplification) for a modified but equivalent DP problem (see Section 1.6.5).An important variation of rollout algorithms is truncated rollout with terminal cost approximation.Here the rollout trajectories are obtained by running the base policy from the leaf nodes of the lookahead tree, but they are truncated after a given number of steps, while a terminal cost approximation is added to the heuristic cost to compensate for the resulting error.This is important for problems with a large number of stages, and it is also essential for infinite horizon problems where the rollout trajectories have infinite length.One possibility that works well for many problems is to simply set the terminal cost approximation to zero.Alternatively, the terminal cost function approximation may be obtained by using some sophisticated offline training process that may involve an approximation architecture such as a neural network, or by using some heuristic calculation based on a simplified version of the problem.This form of truncated rollout may also be viewed as an intermediate approach between standard rollout where there is no truncation (and hence no cost function approximation), and approximation in value space without any rollout.We will now consider a rollout algorithm for discrete deterministic optimization for the case where we do not know the cost function and the constraints of the problem.Instead we have access to a base heuristic, and also a human or software "expert" who can rank any two feasible solutions without assigning numerical values to them.We consider the general discrete optimization problem of selecting a control sequence u = (u 0 , . . ., u N −1 ) to minimize a function G(u).For simplicity we assume that each component u k is constrained to lie in a given constraint set U k , but extensions to more general constraint sets are possible.We assume the following: (a) A base heuristic with the following property is available: Given any k < N − 1, and a partial solution (u 0 , . . ., u k ), it generates, for every ũk+1 ∈ U k+1 , a complete feasible solution by concatenating the given partial solution (u 0 , . . ., u k ) with a sequence (ũ k+1 , . . ., ũN−1 ).This complete feasible solution is denoted S k (u 0 , . . ., u k , ũk+1 ) = (u 0 , . . ., u k , ũk+1 , . . ., ũN−1 ).The base heuristic is also used to start the algorithm from an artificial empty solution, by generating all components ũ0 ∈ U 0 and a complete feasible solution (ũ 0 , . . ., ũN−1 ), starting from each ũ0 ∈ U 0 .(b) An "expert" is available that can compare any two feasible solutions u and u, in the sense that he/she can determine whetherIt can be seen that deterministic rollout can be applied to this problem, even though the cost function G is unknown.The reason is that the rollout algorithm uses the cost function only as a means of ranking complete solutions in terms of their cost.Hence, if the ranking of any two to generate all possible one-step-extended solutions (u 0 , . . ., u k , ũk+1 ), ũk+1 ∈ U k+1 , and the set of complete solutions S k (u 0 , . . ., u k , ũk+1 ), ũk+1 ∈ U k+1 .We then use the expert to rank this set of complete solutions.Finally, we select the component u k+1 that is ranked best by the expert, extend the partial solution (u 0 , . . ., u k ) by adding u k+1 , and repeat with the new partial solution (u 1 , . . ., u k , u k+1 ).Except for the (mathematically inconsequential) use of an expert rather than a cost function, the preceding rollout algorithm can be viewed as a special case of the one given earlier.As a result several of the rollout variants that we have discussed so far (rollout with multiple heuristics, simplified rollout, and fortified rollout) can also be easily adapted.The problem of minimizing G(u) over a constraint set can be viewed as an N -gram optimization problem, discussed in Example 1.6.2,where the text window consists of a partial solution (u0, . . ., u k , u k+1 ) (preceded by N −k −2 "default" words to bring the total to N ).We noted in that example that a GPT can be used as a policy that generates next words within the context of N -gram optimization.Thus the problem can be addressed within the model-free rollout framework of this section, whereby a GPT is used as a base heuristic for completion of partial solutions.The main issues with this approach are how to train a GPT for the problem at hand, and also in the absence of an explicit cost function G(u), how to properly design the expert software for comparing complete solutions.Both of these issues are actively researched at present.In a classical problem from computational biology, we are given a sequence of nucleotides, represented by circles in Fig. 2.3.8, and we want to "fold" the sequence in an "interesting" way (introduce pairings of nucleotides that result in an "interesting" structure).There are some constraints on which pairings are possible, but we will not go into the details of this (some types of constraints may require the use of the constrained rollout framework of Section 2.5).A common constraint is that the pairings should not "cross," i.e., given a pairing (i1, i2) there should be no pairing (i3, i4) where either i3 < i1 and i1 < i4 < i2, or i1 < i3 < i2 and i2 < i4.This type of problem has a long history of solution by DP, starting with the paper by Zuker and Stiegler [ZuS81].There are several formulations, where the aim is to optimize some criterion, e.g., the number of pairings, or the "energy" of the folding.However, biologists do not agree on a suitable criterion, and have developed  software to generate "reasonable" foldings, based on semi-heuristic reasoning.We will develop a rollout approach that makes use of such software without discussing their underlying principles.We formulate the folding problem as a discrete optimization problem involving a pairing decision at each nucleotide in the sequence with at most three choices (open a pairing, close a pairing, do nothing); see Fig. 2.3.8.To apply rollout, we need a base heuristic, which given a partial folding, generates a complete folding (this is the partial folding software shown in Fig. 2.3.8).Two complete foldings can be compared by some other software, called the expert software.An interesting aspect of this problem is that there is no explicit cost function here (it is internal to the expert software).Thus by trying different partial folding and expert software, we may obtain multiple solutions, which may be used for further screening and/or experimental evaluation.For a recent implementation and variations, see Liu et al. [LPS21].One more aspect of the problem that is worth noting is that there are at most three choices for control at each state, while the problem is deterministic.As a result, the problem is a good candidate for the use of multistep lookahead.In particular, with -step lookahead, the number of Q-factors to be computed at each state increases from 3 (or less) to 3 (or less).To implement model-free rollout, we need both a base heuristic and an expert.None of these may be readily available, particularly the expert, which involves a hidden cost function that is implicitly used to rank complete solutions.Within this context, it is worth considering the case where an expert is not available but can be emulated by training with the use of data.In particular, suppose that we are given a set of control sequence likely selection policy.This is a consequence of classical policy iteration convergence results, which establish the finite convergence of the policy iteration algorithm for finite-state Markovian decision problems, see e.g., [Ber12], [Ber17a], [Ber19a].Performance improvement can also be established for the -step lookahead version of the rollout policy, using an induction proof that is similar to the one given above for the one-step lookahead case.Moreover, the books [Ber20a], [Ber22a] describe conditions under which simplified rollout maintains the performance improvement property.However, it is not necessarily true that the performance of the -step lookahead rollout policy improves as increases; see an example in the book [Ber19a], Section 2.1.1.Similarly, it is not necessarily true that the m-step truncated rollout policy performs better than the greedy policy.† On the other hand, known performance deterioration examples of this type are artificial and are apparently rare in practice.We will now present some illustrative computational comparisons, using Markov chains that are small enough for the optimal/most likely selection policy to be computed exactly via the DP-like algorithm (2.21)-(2.22).Thus, the performance differences between the rollout, greedy, and optimal policies can be accurately assessed.The experiments are presented in more detail in the paper [LiB24], which also contains qualitatively similar computational results with much larger Markov chains involving n-grams, and a trained transformer neural network.We used Markov chains where there is a fixed number q of distinct states y such that p(y | x) > 0, with q being the same for all states x.These states were selected according to a uniform distribution, whereby all y with p(y | x) > 0 are equally likely.The probabilities p(y | x) were also generated according to a uniform distribution.Let us denote the state space by X and the number of states by |X|.We refer to the ratio q/|X| (in percent) as the branching factor of the chain.We represent the probability of an entire sequence generated by a policy as the average of its constituent transition probabilities (i.e., a geometric mean over N as will be described below).In particular, given a sample set C of Markov chains, we compute the optimal occurrence probability of generated sequences, averaged over all chains, states, and transitions, and † It performs better than an m-step version of the greedy policy, which generates a sequence of m + 1 states, starting from the current state and using the greedy policy.In the experiments presented here we have used small Markov chains with |X| = 100 states, branching factor q = 5%, and sequence length N = 100.In summary, the percentage recovery has ranged roughly from 60% to 90% for one-step to five-step lookahead, untruncated and truncated rollout with m = 10 steps up to truncation; see Fig. Figure 2.3.11provides corresponding results using double rollout, which show a significant improvement over the case of single rollout.Moreover, truncating the rollout horizon by 90% has remarkably small effect on the percentage recovery, similar to the case of a single rollout.The preceding results are consistent with those of other computational studies using rollout and its variants.The sequences produced by rollout We run the base heuristic from each leaf x k+ at the end of the lookahead graph.We then construct an optimal solution for the lookahead minimization problem, where the heuristic cost is used as terminal cost approximation.We thus obtain an optimal -step control sequence through the lookahead graph, use the first control in the sequence as the rollout control, discard the remaining controls, move to the next state, and repeat.Note that the multistep lookahead minimization may involve approximations aimed at simplifying the associated computations.improve substantially over those generated by the base policy.Moreover, there is typically a relatively small degradation of performance when applying the truncated rollout compared with untruncated rollout.This is significant as truncated rollout greatly reduces the computation if m is substantially smaller than N , and also makes possible the use of double rollout.We will now consider approximation in value space with multistep lookahead minimization, possibly also involving some form of rollout.Figure 2.4.1 describes the case of pure (nontruncated) form of rollout with twostep lookahead for deterministic problems.In particular, suppose that after k steps we have reached state x k .We then consider the set of all possible two-step-ahead states x k+2 , we run the base heuristic starting from each of them, and compute the two-stage cost to get from x k to x k+2 , plus the cost of the base heuristic from x k+2 .We select the state, say xk+2 , that is associated with minimum cost, compute the controls ũk and ũk+1 that   The extension of the algorithm to lookahead of more than two steps is straightforward: instead of the two-step-ahead states x k+2 , we run the base heuristic starting from all the possible -step ahead states x k+ , etc.For cases where the -step lookahead minimization is very time consuming, we may consider variants involving approximations aimed at simplifying the associated computations.An important variation is truncated rollout with terminal cost approximation.Here the rollout trajectories are obtained by running the base heuristic from the leaf nodes of the lookahead graph, and they are truncated after a given number of steps, while a terminal cost approximation is added to the heuristic cost to compensate for the resulting error; see Fig. 2.4.2.One possibility that works well for many problems, particularly when the combined lookahead for minimization and base heuristic simulation is long, is to simply set the terminal cost approximation to zero.Alternatively, the terminal cost function approximation can be obtained by problem approximation or by using some sophisticated off-line training process that may involve an approximation architecture such as a neural network.Generally, the terminal cost approximation is especially important if a large portion of the total cost is incurred upon termination (this is true for example in games).Note that the preceding algorithmic scheme can be viewed as multistep approximation in value space, and it can be interpreted as a Newton step, with suitable starting point that is determined by the truncated rollout with the base heuristic, and the terminal cost approximation.This interpretation is possible once the discrete optimal control problem is reformulated to an equivalent infinite horizon SSP problem; cf. the discussion of Sections 1.6.It involves an acyclic graph of layers, with layer n, n = 1, . . ., , consisting of all the states xn that can be reached from x 0 with a sequence of n feasible controls.In -step approximation in value space, we obtain a trajectory {x 0 , x * 1 , . . ., x * } that minimizes the shortest distance from x 0 to x plus J(x ).We then use the control that corresponds to the first move x 0 → x * 1 .The architecture of Fig. 2.4.2 contains as a special case the general multistep approximation in value space scheme, where there is no rollout at all; i.e., the leaves of the multistep lookahead tree are evaluated with the function J . Figure 2.4.3 illustrates this special case, where for notational simplicity we have denoted the current state by x 0 .The illustration involves an acyclic graph with a single root (the current state) and layers, with the nth layer consisting of the states x n that are reachable from x 0 with a feasible sequence of n controls.In particular, there is an arc for every state x 1 of the 1st layer that can be reached from x 0 with a feasible control, and similarly an arc for every pair of states (x n , x n+1 ), of layers n and n + 1, respectively, for which x n+1 can be reached from x n with a feasible control.The cost of each of these arcs is the stage cost of the corresponding statecontrol pair, minimized over all possible controls that correspond to the same pair (x n , x n+1 ).Mathematically, the cost of the arc (x n , x n+1 ) is ĝn (x n , x n+1 ) = min {un∈Un(xn) | x n+1 =fn(xn,un)}g n (x n , u n ).(2.32)For the states x of the last layer there is also the terminal cost approximation J(x ), which may be obtained through off-line training or some other means.It can be thought of as the cost of an artificial arc connecting x to an artificial termination state.Once we have computed all the shortest distances D(x ) from x 0 to all states x of the last layer , we obtain the -step lookahead control to be applied at the current state x 0 , by minimizing over x the sum D(x ) + J(x ).If x * is the state that attains the minimum, we generate the corresponding trajectory (x 0 , x * 1 , . . ., x * ), and then use the control that corresponds to the first move x 0 → x * 1 ; see Fig. 2.4.3.Note that the shortest path problems from x 0 to all states x n of all the layers n = 1, . . ., can be solved simultaneously by backward DP (start from layer and go back towards x 0 ).The architecture of Figs.2.4.2 and 2.4.3 is similar to the one we discussed in Section 1.1 for AlphaZero and related programs.However, because it is adapted to deterministic problems, it is much simpler to implement and to use.In particular, the truncated rollout portion does not involve expensive Monte Carlo simulation, while the multistep lookahead minimization portion involves a deterministic shortest path problem, which is much easier to solve than its stochastic counterpart.These favorable characteristics can be exploited to facilitate implementations that involve very long lookahead.Generally speaking, longer lookahead is desirable because it typically results in improved performance.We will adopt this as a working hypothesis.It is typically true in practice, although it cannot be established analytically in the absence of additional assumptions.† On the other hand, † Indeed, there are examples where as the size of the lookahead becomes longer, the performance of the multistep lookahead policy deteriorates (see [Ber17a], Section 6.1.2,or [Ber19a], Section 2.2.1).However, these examples are isolated and artificial.They are not representative of practical experience.controls using the base policy (2.33) for stages (k + 1) to (k + ).Each of these − 1 controls requires m calculations of the value of J. Thus, for the − 1 stages of truncated rollout, there are m • ( − 1) calculations of the value of J per state x k+1 , for a total of as many as m 2 • ( − 1) calculations.Adding the m calculations at state x k , we conclude that scheme (b) requires a total of as many as m 2 • calculations of the value of J.In conclusion, both schemes (a) and (b) above look forward for stages, but their associated total computation grows exponentially and linearly with , respectively.Thus, for a given computational budget, short lookahead minimization with long truncated rollout, can increase the total amount of lookahead and improve the performance of approximation in value space schemes.This is particularly so since based on the Newton step interpretations of approximation in value space of Section 1.5, truncated rollout with a reasonably good (e.g., stable) base policy often works about as well as long lookahead minimization.Extensive computational practice, starting with the rollout/TD-Gammon scheme of [TeG96], is consistent with this assessment.In the following two sections, we will explore two alternative ways to speed up the lookahead minimization calculation, thereby allowing a larger number of computational stages for a given on-line computational budget.These are based on iterative deepening of the shortest path computation, and pruning of the lookahead minimization graph.As noted earlier, the shortest path problems from x 0 to x in Fig. 2.4.3 can be solved simultaneously by the familiar backward DP that starts from layer and goes towards x 0 .An important alternative for solving these problems is the forward DP algorithm.This is the same as the backwards DP algorithm with the direction of the arcs reversed (start from x 0 and go towards layer ).In particular, the shortest distances D n+1 (x n+1 ) to layer n + 1 states are obtained from the shortest distances D n (x n ) to layer n states through the equationwhich is also illustrated in Fig. 2.4.4.Here ĝn (x n , x n+1 ) is the cost (or length) of the arc (x n , x n+1 ); cf.Eq. (2.32).In particular, the solution of the -step lookahead problem is obtained from the shortest path to the state x * of layer that minimizes D (x ) + J(x ).The idea of iterative deepening is to progressively solve the n-step lookahead problem first for n = 1, then for n = 2, and so on, until our on-line computational budget is exhausted .In addition to fitting perfectly the mechanism of the forward DP algorithm, this scheme has the character of an "anytime" algorithm; i.e., it returns the shortest distances to some  depth n ≤ , which can in turn yield a solution of an n-step lookahead minimization after adding a suitable terminal cost function.In practice, this is an important advantage, well known from chess programming, which allows us to keep on aiming for longer lookahead minimization, within the limit imposed by our computational budget constraint.A principal difficulty in approximation in value space with -step lookahead stems from the rapid expansion of the lookahead graph as increases.One way to mitigate this difficulty is to "prune" the lookahead minimization graph, i.e., to delete some of its arcs in order to expedite the shortest path computations from the current state to the states of subsequent layers; see Fig. 2.4.5.One possibility is to combine pruning with iterative deepening by eliminating from the computation states xn of layer n such that the n-step lookahead cost D n (x n ) + J(x n ) is "far from the minimum" over x n .This in turn prunes automatically some of the states of the next layer n+1.The rationale is that such states are "unlikely" to be part of the shortest path that we aim to compute.Note that this type of pruning is progressive, i.e., we prune states in layer n before pruning states in layer n + 1.We will now consider a more flexible form of the rollout scheme, which we call incremental multistep rollout (IMR).It applies a base heuristic and a forward DP computation to a sequence of subgraphs of a multistep lookahead graph, with the size of the subgraphs expanding iteratively.In particular, in incremental rollout a connected subgraph of multiple paths is iteratively extended starting from the current state going towards the end of the lookahead horizon, instead of extending a single path as in rollout.This is similar to what is done in Monte Carlo Tree Search (MCTS, to be discussed later), which is also designed to solve approximately general multistep lookahead minimization problems (including stochastic ones), and involves iterative expansion of an acyclic lookahead graph to new nodes, as well as backtracking to previously encountered nodes.However, incremental rollout seems to be more appropriate than MCTS for deterministic problems, where there are no random variables in the problem's model and therefore Monte Carlo simulation does not make sense.The IMR algorithm starts with and maintains a connected acyclic subgraph S of the given multistep lookahead graph G, which contains x 0 .At each iteration it expands S by selecting a leaf node of S and by adding its neighbor nodes to S (if not already in S); see Fig. 2.4.6.The leaf node, lie in more distant layers from the root x 0 .It encourages the algorithm to "backtrack" and select nodes x * that lie in layers closer to x 0 .(Other ways to define P may also be considered.)The algorithm starts with a connected acyclic subgraph S of the given multistep lookahead graph G, which contains x 0 , and with a path P that starts at x 0 , goes through S and ends at one of the terminal nodes of S (one possibility is take S and P equal to just the root node x 0 ).It terminates when the end node of P belongs to layer (or earlier if a computational budget constraint is reached).The role of the parameter δ is noteworthy and affects significantly the nature of the algorithm.When δ = 0, the initial graph S consists of the single state x 0 , and the base heuristic is sequentially improving, it can be seen that IMR performs exactly like the rollout algorithm for solving the -step lookahead minimization problem.On the other hand when δ is large enough, the algorithm operates like the forward DP algorithm.The reason is that a very large value of δ forces the algorithm to expand all nodes of a given layer before proceeding to the next layer.Generally, as δ increases, the algorithm tends to backtrack more often, and to generate more paths through the graph, thereby visiting more nodes and increasing the number of applications of the base heuristic.Thus δ may be viewed as an exploration parameter ; when δ is large the algorithm tends to explore more paths thereby improving the quality of the multistep lookahead minimization, at the expense of greater computational effort.In the absence of additional problem-specific information, favorable values of δ should be obtained through experimentation.One may also consider alternative and more adaptive schemes; for example with a δ that depends on x 0 , and is adjusted in the course of the computation.Finally, we note that if the base heuristic used in the calculation of H(x) is sequentially consistent, a local optimality property of the incremental rollout trajectory can be shown (cf.Section 2.3.1).Let us now consider variants of the IMR scheme, which are aimed towards expediting its calculations, possibly at the expense of some degradation in its performance guarantees.To this end, it is useful to view the algorithm as maintaining a list L of the current leaf nodes of S. At each step a node is removed from L, and its neighbor nodes are added to S and to L, if not already there.This process continues until a path through S that starts at the root node x 0 and ends at some node of the last layer is constructed.In particular, each step of the IMR algorithm consists of: The values D(x) and H(x) are also computed at the time when x enters the list L.(c) The removal of x * from L.Since evidently a node can enter the list L at most once, the algorithm will terminate with a path that starts at x 0 and ends at some node of layer .Moreover, this will happen regardless of which leaf node of the current subgraph S is chosen at each step for expansion.It follows from the preceding argument that selecting the leaf node x * that minimizes D(x) + H(x) at each step is not essential to the algorithm's termination.This allows some flexibility in designing variations of the IMR algorithm, which aim at expediting the computation.In particular, we may consider selecting a leaf node x that has a "low" value of D(x) + H(x) instead of selecting the one that has minimum value, while maintaining an appropriate cost improvement property.Two possibilities of this type are as follows:(1) Replace D(x), the shortest distance from x 0 to x through the subgraph S, with an upper bound D(x), which is the shortest distance from x 0 to x through the subgraph S at the time that x becomes a leaf node of S and enters the list L of leaf nodes.Note that as the set S grows, D(x) may become smaller than D(x), as more nodes are added to S and additional paths from x 0 to x through S are created.The important point here is that in general D(x) is in many cases likely to be either equal or not too different than D(x).Moreover, D(x) is computed only once, at the time when x becomes a leaf node of S, thus saving in computational overhead.(2) Organize L as a priority queue and instead of selecting a node x * that minimizes D(x) + H(x) or D(x) + H(x) over all nodes x of L, simply let x * be the top node of the queue.This will save the overhead for minimizing over the nodes of L, which can be very large in number, depending on the problem at hand.We note here that it is possible to reduce this overhead by organizing L with a heap or bucket data structure, whereby the minimizing node is efficiently selected; such schemes are well known from implementations of label setting shortest path computations (i.e., Dijkstra's algorithm, see e.g., [Ber98], Ch. 2).However, simpler schemes to organize L are possible, which place nodes with small value of D(x)+H(x) near the top of the queue.Such schemes have been proposed by the author for label correcting methods for shortest paths, in the context of approximations to Dijkstra's algorithm.We refer to the SLF (Small Label First), LLL (Last Label Last), and threshold shortest path algorithms, which are described in the network optimization book [Ber98] (Chapter 2) and the references given in that book.In practice, these algorithms work very well, and typically better than the heap or bucket schemes.An important point is that when δ is small or is 0, the preceding variants of the IMR algorithm can be easily modified to coincide with the rollout algorithm, and inherit the corresponding cost improvement property.Moreover the cost improvement property can be restored by using the fortified rollout ideas of Section 2.3.In addition, the flexibility afforded by the modifications (1) and ( 2) above allow variations of the IMR scheme that are tailored to the problem at hand.In this section we will discuss constrained deterministic DP problems, including challenging combinatorial optimization and integer programming problems.We introduce a rollout algorithm, which relies on a base heuristic and applies to problems with general trajectory constraints.Under suitable assumptions, we will show that if the base heuristic produces a feasible solution, the rollout algorithm has a cost improvement property: it produces a feasible solution, whose cost is no worse than the base heuristic's cost.Before going into formal descriptions of the constrained DP problem formulation and the corresponding algorithms, it is worth to revisit the broad outline of the rollout algorithm for deterministic DP:(a) It constructs a sequence {T 0 , T 1 , . . ., T N } of complete system trajectories with monotonically nonincreasing cost (assuming a sequential improvement condition).(b) The initial trajectory T 0 is the one generated by the base heuristic starting from x 0 , and the final trajectory T N is the one generated by the rollout algorithm.(c) For each k, the trajectories T k , T k+1 , . . ., T N share the same initial portion (x 0 , ũ0 , . . ., ũk−1 , xk ).(d) For each k, the base heuristic is used to generate a number of candidate trajectories, all of which share the initial portion with T k , up to state xk .These candidate trajectories correspond to the controls u k ∈ U k (x k ).(In the case of fortified rollout, these trajectories include the current "tentative best" trajectory.)(e) For each k, the next trajectory T k+1 is the candidate trajectory that is best in terms of total cost.In our constrained DP formulation, to be described shortly, we introduce a trajectory constraint T ∈ C, where C is some subset of admissible trajectories.A consequence of this is that some of the candidate trajectories in (d) above, may be infeasible.Our modification to deal with this situation is simple: we discard all the candidate trajectories that violate the constraint, and we choose T k+1 to be the best of the remaining candidate trajectories, the ones that are feasible.Of course, for this modification to be viable, we have to guarantee that at least one of the candidate trajectories will satisfy the constraint for every k.For this we will rely on a sequential improvement condition that we will introduce shortly.For the case where this condition does not hold, we will introduce a fortified version of the algorithm, which requires only that the base heuristic generates a feasible trajectory T 0 starting from the initial condition x 0 .Thus to apply reliably the constrained rollout algorithm, we only need to know a single feasible solution, i.e., a trajectory T 0 that starts at x 0 and satisfies the constraint T 0 ∈ C.We assume that the state x k takes values in some (possibly infinite) set and the control u k takes values in some finite set.The finiteness of the control space is only needed for implementation purposes of the rollout algorithms to be described shortly.The algorithm can be defined without the finiteness condition, and makes sense, provided the implementation issues associated with infinite control spaces can be dealt with.A sequence of the form T = (x 0 , u 0 , x 1 , u 1 , . . ., u N −1 , x N ), (whereis referred to as a complete trajectory.Our problem is stated succinctly as min T ∈C G(T ), (2.36) where G is some cost function and C is the constraint set.Note that G need not have the additive formwhich we have assumed so far.Thus, except for the finiteness of the control space, which is needed for implementation of rollout, this is a very general optimization problem.In fact, later we will simplify the problem further by eliminating the state transition structure of Eq. (2.35).† † Actually, similar to our discussion on model-free rollout in Section 2.3.6, it is not essential that we know the explicit form of the cost function G and the constraint set C. For our constrained rollout algorithms, it is sufficient to have access to a human or software expert that can determine whether a given trajectory T is feasible, i.e., satisfies the constraint T ∈ C, and also to be able to compare any two feasible trajectories T1 and T2, based on some internal process that is unknown to us, without assigning numerical values to them.Trajectory constraints can arise in a number of ways.A relatively simple example is the standard problem formulation for deterministic DP: an additive cost of the form (2.37), where the controls satisfy the timeuncoupled constraints u k ∈ U k (x k ) [so here C is the set of trajectories that are generated by the system equation with controls satisfying u k ∈ U k (x k )].In a more complicated constrained DP problem, there may be constraints that couple the controls of different stages such aswhere g m k and b m are given functions and scalars, respectively.Examples of this type include multiobjective or Pareto optimization problems, where there are multiple cost functions of interest, and all but one of the cost functions are treated through constraints (see e.g., [Ber17a], Ch. 2).Examples where difficult trajectory constraints arise also include situations where the control contains some discrete components, which once chosen must remain fixed for multiple time periods.Here is a discrete optimization example involving the traveling salesman problem.A related classical example is the knapsack problem, described in most books on discrete optimization algorithms.Let us consider a constrained version of the traveling salesman problem of Example 1.2.2.We want to find a minimum travel cost tour that additionally satisfies a safety constraint that the "safety cost" of the tour should be less than a certain threshold; see Fig. 2.5.1.This constraint need not have the additive structure of Eq. (2.38).We are simply given a safety cost for each tour (see the table at the bottom right), which is calculated in a way that is of no further concern to us.In this example, for a tour to be admissible, its safety cost must be less than or equal to 10.Note that the (unconstrained) minimum cost tour, ABDCA, does not satisfy the safety constraint.We will now describe formally the constrained rollout algorithm.We assume the availability of a base heuristic, which for any given partial trajectory y k = (x 0 , u 0 , x 1 , . . ., u k−1 , x k ), can produce a (complementary) partial trajectory R(y k ) = (x k , u k , x k+1 , u k+1 , . . ., u N −1 , x N ),Chap. 1 (i) Planning = Solving a DP problem with a known mathematical model.(j) Learning = Solving a DP problem without using an explicit mathematical model.(This is the principal meaning of the term "learning" in RL.Other meanings are also common.)(k) Self-learning (or self-play in the context of games) = Solving a DP problem using some form of policy iteration.(l) Deep reinforcement learning = Approximate DP using value and/or policy approximation with deep neural networks.(m) Prediction = Policy evaluation.(n) Generalized policy iteration = Optimistic policy iteration.(o) State abstraction = State aggregation.(p) Temporal abstraction = Time aggregation.(q) Learning a model = System identification.(r) Episodic task or episode = Finite-step system trajectory.(s) Continuing task = Infinite-step system trajectory.(t) Experience replay = Reuse of samples in a simulation process.(u) Bellman operator = DP mapping or operator.(v) Backup = Applying the DP operator at some state.(w) Sweep = Applying the DP operator at all states.(x) Greedy policy with respect to a cost function J = Minimizing policy in the DP expression defined by J.(y) Afterstate = Post-decision state.(z) Ground truth = Empirical evidence or information provided by direct observation.Some of the preceding terms will be introduced in future chapters; see also the RL textbook [Ber19a].The reader may then wish to return to this section as an aid in connecting with the relevant RL literature.Unfortunately, the confusion arising from different terminology has been exacerbated by the use of different notations.This book roughly follows the "standard" notation of the Bellman/Pontryagin optimal control era; see e.g., the books by Athans and Falb [AtF66], Bellman [Bel67], and Bryson and Ho [BrH75].This notation is consistent with the author's other DP books and is the most appropriate for a unified treatment of the subject, which simultaneously addresses discrete and continuous spaces problems.Sec. 1.7A summary of our most prominently used symbols is as follows:(a) x: state.(b) u: control.(c) J: cost function.(d) g: cost per stage.(e) f : system function.(f) w: stochastic disturbance.(g) i: discrete state.(h) p xy (u): transition probability from state x to state y under control u.(i) α: discount factor in discounted problems.The x-u-J notation is standard in optimal control textbooks (e.g., the classical books [AtF66] and [BrH75], noted earlier, as well as the more recent books by Stengel [Ste94], Kirk [Kir04], and Liberzon [Lib11]).The notations f and g are also used most commonly in the literature of the early optimal control period as well as later (unfortunately the more natural symbol "c" has not been used much in place of "g" for the cost per stage).The discrete system notations i and p ij (u) are common in the discrete-state Markov decision problem and operations research literature, where discretestate problems have been treated extensively [sometimes the alternative notation p(j | i, u) is used for the transition probabilities].The artificial intelligence literature addresses for the most part finitestate Markov decision problems, most frequently the discounted and stochastic shortest path infinite horizon problems.The most commonly used notation is s for state, a for action, r(s, a, s ) for reward per stage, p(s | s, a) or p(s, a, s ) for transition probability from s to s under action a, and γ for discount factor.However, this type of notation is not well suited for continuous spaces models, which are of major interest in this book.The reason is that it requires the use of transition probability distributions defined over continuous spaces, and it leads to more complex and less intuitive mathematics.Moreover the transition probability notation is cumbersome for deterministic problems, which involve no probabilistic structure at all.Machine learning and optimization are closely intertwined fields, as they focus on related mathematical models and computational algorithms.† How- † Both machine learning and optimization are also closely connected with the field of statistical analysis.However, in this section, we will not focus on this connection, as it is less relevant to the content of this book.Chap. 2We consider subsets of N + 1 nodes, referred to as groupings, which consist of a single node from every layer.For each grouping, there is an associated cost, which depends on the N -tuple of arcs that comprise the grouping.A partition of the set of nodes into m disjoint groupings (so that each node belongs to one and only one grouping) is called an (N + 1)-dimensional assignment.The cost of an (N + 1)-dimensional assignment is the sum of the costs of its m groupings.The problem is to find an (N + 1)-dimensional assignment of minimum cost.The exact solution of the problem is very difficult when the cost of a grouping does not decompose into the sum of costs of the N arcs of the grouping (in which case the problem decouples into N easily solvable 2-dimensional assignment problems).We formulate the problem as an N -stage sequential decision problem where at the kth stage we select the assignment arcswhich connect the nodes in of layer N k to nodes jn of layer N k+1 on a one-toone basis.The control costraint set U k is the set of legitimate assignments, namely those involving exactly one incident arc per node in ∈ N k , and exactly one incident arc per node jn of layer N k+1 .We assume a cost function that has the general form G(u0, . . ., uN−1).We use as state x k the partial solution up to time k,x k = (u0, . . ., u k−1 ), so the system equation takes the simple formcf. Fig. 2.1.4and Section 1.6.3.The exact DP algorithm takes the formand,3) and (2.4).Since this DP algorithm is intractable, we resort to approximation of J * k+1 by a function J k+1 .The one-step lookahead minimization becomes minbut is still formidable because its search space U k is very large.However, it can be greatly simplified by using a cost function approximation Jk+1 with a  Once the arc costs c ij k+1 (x k ) have been calculated, the assignment u k and corresponding multiassignment trajectory x k+1 are obtained by solving a 2-dimensional assignment problem, using for example the Hungarian method or the auction algorithm.special structure that is suitable for the use of fast 2-dimensional assignment algorithms.This cost function approximation has the formwhere {(i, j) ∈ u k } denotes the set of m arcs (i, j) that correspond to the 2-dimensional assignment u k , cf.Eq. (2.8) [thus the dependence of the righthand side on u k comes through the choice of arcs (i, j) specified by u k ].The arc costs c ij k+1 (x k ) in this equation must be calculated for every possible arc (i, j) that connects a node i ∈ N k with a node j ∈ N k+1 ; see Fig. 2.2.2.Note that the arc costs c ij k+1 (x k ) may depend in a complicated way on the entire trajectory of previous 2-dimensional assignment choices x k = (u0, . . ., u k−1 ) and the problem data.For problems that involve tracking the movement of people or vehicles over time, the computation of c ij k+1 (x k ) relies on sensor data and problem-dependent circumstances.For other problems, enforced decomposition methods may be useful; see [Ber20a], Section 3.4.2.We refer to the data association literature for further details; see e.g., the survey by Emami et al. [EPE20].The approximation in value space algorithm (2.7) involves a one-step lookahead minimization, since it solves a one-stage DP problem for each k.WeChap. 2Note that the rollout algorithm requires running the base heuristic for a number of times that is bounded by N n, where n is an upper bound on the number of control choices available at each state.Thus if n is small relative to N , the algorithm requires computation equal to a small multiple of N times the computation time for a single application of the base heuristic.Similarly, if n is bounded by a polynomial in N , the ratio of the rollout algorithm computation time to the base heuristic computation time is a polynomial in N .In Section 1.2 we considered an example of rollout involving the traveling salesman problem and the nearest neighbor heuristic (cf.Examples 1.2.2 and 1.2.3).Let us consider another example, which involves a classical discrete optimization problem.Consider m vehicles that move along the arcs of a given graph.Some of the nodes of the graph include a task to be performed by the vehicles.Each task will be performed only once, immediately after some vehicle reaches the corresponding node for the first time.We assume a horizon that is large enough to allow every task to be performed.The problem is to find a route for each vehicle so that the tasks are collectively performed by the vehicles in a minimum number of moves.To express this objective, we assume that for each move by a vehicle there is a cost of one unit.These costs are summed up to the point where all the tasks have been performed.For a large number m of vehicles and a complicated graph, this is a nontrivial combinatorial problem.It can be approached by DP, like any discrete deterministic optimization problem, as we have discussed.In particular, we can view as state at a given stage the m-tuple of current positions of the vehicles together with the list of pending tasks.Unfortunately, however, the number of these states can be enormous (it increases exponentially with the number of tasks and the number of vehicles), so an exact DP solution is intractable.This motivates an optimization in value space approach based on rollout.For this we need an easily implementable base heuristic that will solve suboptimally the problem starting from any state x k+1 , and will provide the cost approximation Jk+1 (x k+1 ) in Eq. (2.7).One possibility is based on the vehicles choosing their actions selfishly and without coordination, along shortest paths to their nearest pending task.To illustrate, consider the two-vehicle problem of Fig. 2.3.2.The base heuristic is to move each vehicle one step at a time towards its nearest pending task, until all tasks have been performed.The rollout algorithm will work as follows.At a given state x k [involving for example vehicle positions at the node pair (1, 2) and tasks at nodes 7 and 9, as in Fig. 2.3.2],we consider all possible joint vehicle moves (the controls u k at the state) resulting in the node pairs (3,5), (4,5), (3,4), (4,4), corresponding to the next states x k+1 [thus, as an example (3,5)  (, perform a heuristic calculation from each, compare, etc.It can be verified that the rollout algorithm starting from the state (1,2) shown in Fig. 2.3.2 will attain the optimal cost (a total of 6 vehicle moves).It will perform much better than the heuristic, which starting from state (1,2), will move the two vehicles together to state (4,4), then to (7,7), then to (10,10), then to (12,12), and finally to (9,9), (a total of 10 vehicle moves).This is an instance of the cost improvement property of the rollout algorithm: it performs better than its base heuristic (under appropriate conditions).Let us finally note that the computation required by in rollout algorithm increases exponentially with the number m of vehicles, since the number of mtuples of moves at each stage increases exponentially with m.This is the type of problem where multiagent rollout can attain great computational savings; cf.Section 1.6.5, and the subsequent Section 2.9.Here is an example of a search problem, whose exact solution complexity grows exponentially with the problem size, but can be addressed with a greedy heuristic as well as with the corresponding rollout algorithm.Consider a binary tree with N stages as shown in Fig. 2.3.3.Stage k of the tree has 2 k nodes, with the node of stage 0 called root and the nodes of stage N called leaves.There are two types of tree arcs: free and blocked .A free (or blocked) arc can (cannot, respectively) be traversed in the direction from the root to the leaves.The objective is to break through the graph with a sequence of free arcs (a free path) starting from the root, and ending at one of the leaves.(A variant of this problem is to introduce a positive cost c > 0 for traversing a blocked arc, and 0 cost for traversing a free arc.)In other words, the base heuristic is sequentially consistent if it "stays the course": when the starting state x k is moved forward to the next state x k+1 of its state trajectory, the heuristic will not deviate from the remainder of the trajectory.As an example, the reader may verify that the nearest neighbor heuristic described in the traveling salesman Example 1.2.3 and the heuristics used in the multivehicle routing Example 2.3.1 are sequentially consistent.Similar examples include the use of various types of greedy/myopic heuristics (Section 6.4 of the book [Ber17a] provides additional examples).† Generally most heuristics used in practice satisfy the sequential consistency condition at "most" states x k .However, some heuristics of interest may violate this condition at some states.A sequentially consistent base heuristic can be recognized by the fact that it will apply the same control u k at a state x k , no matter what position x k occupies in a trajectory generated by the base heuristic.Thus a base heuristic is sequentially consistent if and only if it defines a legitimate DP policy.This is the policy that moves from x k to the state x k+1 that lies on the state trajectory {x k , x k+1 , . . ., x N } that the base heuristic generates.Similarly the policy moves from x n to the state x n+1 for n = k+1, . . ., N −1.† A subtle but important point relates to how one breaks ties while implementing greedy base heuristics.For sequential consistency, one must break ties in a consistent way at various states, i.e., using a fixed rule at each state encountered by the base heuristic.In particular, randomization among multiple controls, which are ranked as equal by the greedy optimization of the heuristic, violates sequential consistency, and can lead to serious degradation of the corresponding rollout algorithm's performance.Chap. 2We will next show that the rollout policy has no worse performance than its base heuristic under a condition that is weaker than sequential consistency.Let us recall that the rollout algorithm π = {μ 0 , . . ., μN−1 } is defined by the minimization μk (x k ) ∈ arg minwhere Qk (x k , u k ) is the approximate Q-factor defined by[cf.Eq. (2.10)], and H k+1 f k (x k , u k ) denotes the cost of the trajectory of the base heuristic starting from state f k (x k , u k ).Definition 2.3.2:We say that the base heuristic is sequentially improving if for all x k and k, we have minIn words, the sequential improvement property (2.13) states that Minimal heuristic Q-factor at x k ≤ Heuristic cost at x k .Note that when the heuristic is sequentially consistent it is also sequentially improving.This follows from the preceding relation, since for a sequentially consistent heuristic, the heuristic cost at x k is equal to the Q-factor of the control u k that the heuristic applies at x k ,which is greater or equal to the minimal Q-factor at x k .This implies Eq. (2.13).A sequentially improving heuristic yields policy improvement as the next proposition shows.Proposition 2.3.2:(Cost Improvement Under Sequential Improvement) Consider the rollout policy π = {μ 0 , . . ., μN−1 } obtained with a sequentially improving base heuristic, and let J k,π (x k ) denote the cost obtained with π starting from x k at time k.Thenwhere H k (x k ) denotes the cost of the base heuristic starting from x k .Approximation in Value Space -Rollout Algorithms Chap. 2) . . .) . . .) . . .Trajectory T kTrajectory T k+1 Optimal Base Rollout Terminal Score Approximation CurrentIf strict inequality holds, the rollout algorithm will switch from T k and follow T k+1 ; cf. the traveling salesman Example 1.2.3.form of the MPC scheme is not sequentially consistent (see the preceding references).Generally, the sequential improvement condition may not hold for a given base heuristic.This is not surprising since any heuristic (no matter how inconsistent or silly) is in principle admissible to use as base heuristic.Here is an example:  The rollout choice at the initial state x 0 is strictly suboptimal, while the base heuristic choice is optimal.The reason is that the base heuristic is not sequentially improving and makes the suboptimal choice u 1 at x * 1 , but makes the different (optimal) choice u * 1 when run from x 0 .is arbitrary), and moreover the cost g1(x * 1 , ū1) + g2(x2), which is equal to H1(x * 1 ) is high enough.Let us also verify that if the inequality (2.15) holds then the heuristic is not sequentially improving at x0, i.e., thatIndeed, this is true because H0(x0) is the optimal costand must be smaller than bothwhich is the cost of the trajectory (x0, u * 0 , x * 1 , u1, x2), andwhich is the cost of the trajectory (x0, ũ0, x1, ũ1, x2).The preceding example and the monotonicity property (2.14) suggest a simple enhancement to the rollout algorithm, which detects when the sequential improvement condition is violated and takes corrective measures.In this algorithmic variant, called fortified rollout , we maintain the best trajectory obtained so far, and keep following that trajectory up to the point where we discover another trajectory that has improved cost (see the next section).x 0) . . .and the tentative best trajectorythe best end-to-end trajectory computed so far.We now run the rollout algorithm at x k , i.e., we find the control ũk that minimizes over u k the sum of g k (x k , u k ) plus the heuristic cost from the state x k+1 = f k (x k , u k ), and the corresponding trajectoryIf the cost of the end-to-end trajectory Tk is lower than the cost of T k , we add (ũ k , xk+1 ) to the permanent trajectory and set the tentative best trajectory to T k+1 = Tk .Otherwise we add (u k , x k+1 ) to the permanent trajectory and keep the tentative best trajectory unchanged:the algorithm sets the next state and corresponding tentative best trajectory toit sets the next state and corresponding tentative best trajectory toIn other words the fortified rollout at x k follows the current tentative best trajectory T k unless a lower cost trajectory Tk is discovered by running the base heuristic from all possible next states x k+1 .† It follows that at every state the tentative best trajectory has no larger cost than the initial tentative best trajectory, which is the one produced by the base heuristic starting from x 0 .Moreover, it can be seen that if the base heuristic is sequentially improving, the rollout algorithm and its fortified version coincide.Experimental evidence suggests that it is often important to use the fortified version if the base heuristic is not known to be sequentially improving.Fortunately, the fortified version involves hardly any additional computational cost.As expected, when the base heuristic generates an optimal trajectory, the fortified rollout algorithm will also generate the same trajectory.This is illustrated by the following example.Let us consider the application of the fortified rollout algorithm to the problem of Example 2.3.3 and see how it addresses the issue of cost improvement.The fortified rollout algorithm stores as initial tentative best trajectory the optimal trajectory (x0, u * 0 , x * 1 , u * 1 , x * 2 ) generated by the base heuristic at x0.Then, starting at x0, it runs the heuristic from x * 1 and x1, and (despite the fact that the ordinary rollout algorithm prefers going to x1 rather than x * 1 ) it discards the control ũ0 in favor of u * 0 , which is dictated by the tentative best trajectory.It then sets the tentative best trajectory to (x0,We finally note that the fortified rollout algorithm can be used in a different setting to restore and maintain the cost improvement property.Suppose in particular that the rollout minimization at each step is performed with approximations.For example the control u k may have multiple independently constrained components, i.e.,Then, to take advantage of distributed computation, it may be attractive to decompose the optimization over u k in the rollout algorithm, μk (x k ) ∈ arg mininto an (approximate) parallel optimization over the components u i k (or subgroups of these components).However, as a result of approximate optimization over u k , the cost improvement property may be degraded, even if the sequential improvement assumption holds.In this case by maintaining † The base heuristic may also be run from a subset of the possible next states x k+1 , as in the case where a simplified version of rollout is used; cf.Section 2.3.4.Then fortified rollout will still guarantee a cost improvement property.Chap. 2where Q k (x k , u k ) and H k (x k ) are Q-factors and heuristic costs that correspond to the th heuristic.Then by taking minimum over , we have min =1,...,m minfor all x k and k.By interchanging the order of the minimizations of the left side, we then obtain min, which is precisely the sequential improvement condition (2.13) for the superheuristic.We will now consider a rollout variant, called simplified rollout , which is motivated by problems where the control constraint set U k (x k ) is either infinite or finite but very large.Then the minimization μk (x k ) ∈ arg min[cf.Eqs.(2.9) and (2.10)], may be unwieldy, since the number of Q-factorsis accordingly infinite or large.To remedy this situation, we may replace U k (x k ) with a smaller finite subset U k (x k ):The rollout control μk (x k ) in this variant is one that attains the minimum of Qk (x k , u k ) over(2.17An example is when U k (x k ) results from discretization of an infinite set U k (x k ).Another possibility is when by using some preliminary approximate optimization, we can identify a subset U k (x k ) of promising controls by using some heuristic method, and to save computation, we restrict attention to this subset.A related possibility is to generate U k (x k ) by some iterative or random search method that explores intelligently the set U k (x k ) with the aim to minimize Qk (x k , u k ) [cf.Eq. (2.16)].We assume that we do not know G and/or U 0 , . . ., U N−1 .Instead we have a base heuristic, which given a partial solution (u 0 , . . ., u k ), outputs all next controls ũk+1 ∈ U k+1 , and generates from each a complete solution S k (u 0 , . . ., u k , ũk+1 ) = (u 0 , . . ., u k , ũk+1 , . . ., ũN−1 ).Also, we have a human or software "expert" that can rank any two complete solutions without assigning numerical values to them.The control that is selected from U k+1 by the rollout algorithm is the one whose corresponding complete solution is ranked best by the expert.solutions can be revealed by the expert, this is all that is needed.† In fact, the constraint sets U 0 , . . ., U N −1 need not be known either, as long as they can be generated by the base heuristic.Thus, the rollout algorithm can be described as follows (see Fig. 2We start with an artificial empty solution, and at the typical step, given the partial solution (u 0 , . . ., u k ), k < N − 1, we use the base heuristic † Note that for this to be true, it is important that the problem is deterministic, and that the expert ranks solutions using some underlying (though unknown) cost function.In particular, the expert's rankings should have a transitivity property: if u is ranked better than u and u is ranked better than u , then u is ranked better than u .Chap. 2 pairs (u s , u s ), s = 1, . . ., q, with G(u s ) > G(u s ), s= 1, . . ., q, (2.19) which we can use for training.Such a set may be obtained in a variety of ways, including querying the expert.We may then train a parametric approximation architecture such as a neural network to produce a function G(u, r), where r is a parameter vector, and use this function in place of the unknown G(u) to implement the preceding rollout algorithm.A method, known as comparison training, has been suggested for this purpose, and has been used in a variety of game contexts, including backgammon and chess by Tesauro [Tes89b], [Tes01].Briefly, given the training set of pairs (u s , u s ), s = 1, . . ., q, which satisfy Eq. (2.19), we generate for each (u s , u s ), two solution-cost pairs (u s , 1), (u s , −1), s= 1, . . ., q.A parametric architecture G(•, r), involving a parameter vector r, such as a neural network, is then trained by some form of regression with these data to produce an approximation G(•, r) to be used in place of G(•) in a rollout scheme.We refer to Chapter 3 and to the aforementioned papers by Tesauro for implementation details of the regression procedure.See also Section 3.4 on parametric approximation in policy space through the use of classification methods.In another type of imitation approach, we view the base policy decisions as being selected by a process the mechanics of which are not observed except through its generated cost samples at the various stages.In particular, the stage costs starting from any given partial solution (u 0 , . . ., u k ) are added to form samples of the base policy's Q-factors Q k (u 0 , . . ., u k ).In this way we can obtain Q-factor samples starting from many partial solutions (u 0 , . . ., u k ).Moreover, a single complete solution (u 0 , . . ., u N −1 ) generated by the base policy provides multiple Q-factor samples, one for each of the partial solutions (u 0 , . . ., u k ).We can then use the sample (partial solution, cost) pairs in conjunction with a training method (see Chapter 3) in order to construct parametric approximations Qk (u 0 , . . ., u k , r k ), k = 1, . . ., N, to the true Q-factors Q k (u 0 , . . ., u k ), where r k is the parameter vector.Once the training has been completed and the Q-factors Qk (u 0 , . . ., u k , r k ) have been obtained for all k, we can construct complete solutions step-by-step, by selecting the next component ũk+1 , given the partial solution (u 0 , . . ., u k ), through the minimization ũk+1 ∈ arg minQk+1 (ũ 0 , . . ., ũk , u k+1 , r k+1 ).Note that even though we are "learning" the base policy, our aim is not to imitate it, but rather to generate a rollout policy.The latter policywill make better decisions than the base policy, thanks to the cost improvement property of rollout.This points to an important issue of exploration: we must ensure that the training set of sample (partial solution, cost) pairs is broadly representative, in the sense that it is not unduly biased towards sample pairs that are generated by the base policy.In this section we consider a type of deterministic sequential decision problem involving n-grams and transformers, which provide next word probabilities that can be used to generate word sequences (cf.Section 1.6, Example 1.6.2).We consider methods for computing N -step word sequences that are highly likely, based on these probabilities.† Computing the optimal (i.e., most likely) word sequence starting with a given initial state is an intractable problem, so we consider rollout algorithms that compute highly likely N -word sequences in time that is a low order polynomial in N and in the vocabulary size of the n-gram.Our n-gram model generates a sequence {x 1 , . . ., x N } of text strings, starting from some initial string x 0 (here n and N are fixed positive integers).Each string x k consists of a sequence of n words, chosen from a given list (the vocabulary of the n-gram).The kth string x k is transformed into the next string x k+1 by adding a word at the front end of x k and deleting the word at the back end of x k ; see Example 1.6.2.Given a text string x k , the n-gram provides probabilities p(x k+1 | x k ) for the next text string x k+1 .These probabilities also define the probabilities of the possible next words, since x k+1 is determined by the next word that is added to the front of x k .We assume that the probabilities p(x k+1 | x k ) depend only on x k .Thus they can be viewed as the transition probabilities of a stationary Markov chain, whose state space is the set of all n-word sequences x k .‡ Bearing this context in mind, we also refer to x k as the state (of the underlying Markov chain).The transition probabilities p(x k+1 | x k ) can provide guidance for generating state sequences with some specific purpose in mind.To this end, a transformer may use a (next word) selection policy, i.e., a (possibly time-dependent) function µ k , which selects the text string that follows x k asx k+1 = µ k (x k ).† This section is based on joint work with Yuchao Li; see the paper by Li and Bertsekas [LiB24], which also contains extensive computational experimentation results.‡ The stationarity assumption simplifies our notation, but is not essential to our methodology, as we will discuss later.Approximation in Value Space -Rollout Algorithms Chap. 2We are generally interested in selection policies that give preference to high-probability future words.Our methods also apply to the problem of finding the most likely sequence generated by a general finite-state Markov chain.This problem arises in many important contexts.A major example is inference of the sequence of states of a Hidden Markov Model (HMM), given an associated sequence of observed data.This is the problem where Viterbi Compared to these fields, the transformer/n-gram context tends to involve Markov chains with an intractably larger state space.A DP-oriented discussion of the Viterbi algorithm and its applications to HMM inference is given in Section 2.2.2 of the textbook [Ber17].In this section, we will not consider the problem of most likely sequence selection in the context of inference of state sequences in HMMs, but our methods fully apply to that context.We will next consider a finite-state stationary Markov chain and various policies for generating highly likely sequences according to the transition probabilities of the Markov chain.We will generally use the symbols x and y for states, and we will denote the chain's transition probabilities by p(y | x).We assume that given a state x, the probabilities p(y | x) are either known or can be generated on-line by means of software such as a transformer.We assume stationarity of the Markov chain in part to alleviate an overburdened notation, and also because n-gram and transformer models are typically assumed to be stationary.However, the rollout methodology and the manner in which we use it do not depend at all on stationarity of the transition probabilities, or infinite horizon properties of Markov chains, such as ergodic classes, transient states, etc.In fact, they also do not depend on the stationarity of the state space either.Only the Markov property is used in our discussion, i.e., the probability of the next state depends on the immediately preceding state, and not on earlier states.A selection policy π is a sequence of functions {µ 0 , . . ., µ N −1 }, which given the current state x k , determines the next state x k+1 asNote that for a given π, the state evolution is deterministic; so for a given π and x 0 , the generated state sequence {x 1 , . . ., x N } is fully determined.x yx y k+1,k (x, π)) y N,k (x, π) Moreover the choice of the policy π is arbitrary, although we are primarily interested in π that give preference to high probability next states.Given a policy π = {µ 0 , . . ., µ N −1 } and a starting state x at time k, the state at future times m > k is denoted by y m,k (x, π): y m,k (x, π) = state at time m > k starting at state x and using π.The state trajectory generated by a policy π, starting at state x at time k, is the sequence y k+1,k (x, π), . . ., y N,k (x, π), (cf.Fig. 2.3.9), and the probability of its occurrence in the given Markov chain isaccording to the multiplication rule for conditional probabilities.The most likely selection policy, denoted by π * = {µ * 0 , . . ., µ * N −1 }, maximizes over all policies π the probabilities P k (x, π) for every initial state x and time k.The corresponding probabilities of π * , starting at state x at time k, are denoted by P * k (x):This is similar to our finite-horizon DP context, except that we consider a multiplicative reward function, instead of an additive cost function [P * k (x) can be viewed as an optimal reward-to-go from state x at time k, in place of the optimal cost-to-to J * k (x) that we have considered so far in the additive DP case].†The policy π * and its probabilities P * k (x) can be generated by the following DP-like algorithm, which operates in two stages:(a) It first computes the probabilities P * k (x) backwards, for all x, according tostarting withstarting with x * 0 = x 0 .This algorithm is equivalent to the usual DP algorithm for multistage additive costs, after we take logarithms of the multiplicative expressions defining the probabilities P k (x, π).At any given state x k , the greedy policy produces the next state by maximization of the corresponding transition probability over all y:We assume that ties in the above maximization are broken according to some prespecified deterministic rule.For example if the states are labeled by distinct integers, one possibility is to specify the greedy selection at x k as the state y with minimal label, among those that attain the maximum above.Note that the greedy policy is not only deterministic, but it is also stationary (its selections depend only on the current state and not on the time k).We will consequently use the notation π = {µ, . . ., µ} for the greedy policy, where(2.23) † Generally multiplicative reward problems can be converted to additive cost DP problems involving negative logarithms of the multiplicative reward factors (assuming they are positive).Rollout Algorithms for Discrete Optimization 197 and µ(x k ) is uniquely defined according to our deterministic convention for breaking ties in the maximization above.The corresponding probabilities P k (x k , π) are given by the DP-like algorithmstarting with P N (x, π) ≡ 1.Equivalently, we can compute P k (x, π) by using forward multiplication of the transition probabilities along the trajectory generated by the greedy policy, starting from x; cf.Eq. (2.20).The limitation of the greedy policy is that it chooses the locally optimal next state without considering the impact of this choice on future state selections.The rollout approach, to be discussed next, mitigates this limitation with a mechanism for looking into the future, and balancing the desire for a high-probability next transition with the potential undesirability of low-probability future transitions.At any given state x k , the rollout policy with one-step lookahead produces the next state, denoted μk (x k ), by maximizing p(y | x k )P k+1 (y, π) over all y:Thus it optimizes the selection of the first state y, assuming that the subsequent states will be chosen using the greedy policy.By comparing the maximization (2.25) with the one for the most likely selection policy [cf.Eq. (2.22)], we see that it chooses the next state similarly, except that P * k+1 (y) (which is hard to compute) is replaced by the (much more easily computable) probability P k+1 (y, π).In particular, the latter probability is computed for every y by running the greedy policy forward starting from y and multiplying the corresponding transition probabilities along the generated state trajectory.This is a polynomial computation, which is roughly larger by a factor N over the greedy selection method.However, there are ways to reduce this computation, including the use of parallel processing and other possibilities, which we will discuss later.The expression p(y | x k )P k+1 (y, π) that is maximized over y in Eq. (2.25) can be viewed as the Q-factor of the pair (x k , y) corresponding to the base policy π, and is denoted by(2.26)This is similar to the approximation in value space context, except that we consider a multiplicative reward function, whereby at state x k we choose the action y that yields the maximal Q-factor.Approximation in Value Space -Rollout Algorithms Chap. 2Another rollout possibility includes rollout with -step lookahead ( > 1), whereby given x k we maximize over all sequences {y 1 , y 2 , . . ., y } up to steps ahead, the -step Q-factor) and if {ỹ 1 , ỹ2 , . . ., ỹ } is the maximizing sequence, we select ỹ1 at x k , and discard the remaining states ỹ2 , . . ., ỹ .† In practice the performance of -step lookahead rollout policies almost always improves with increasing .However, artificial examples have been constructed where this is not so; see the book [Ber19a], Section 2.1.1.Moreover, the computational overhead of -step lookahead increases with .As we have already noted, one of the difficulties that arises in the application of rollout is the potentially very large number of the Q-factors that need to be calculated at each time step at the current state x [it is equal to the number of states y for which p(y | x) > 0].In practice the computation of Q-factors can be restricted to a subset of most probable next states, as per the transition probabilities p(y | x) (this is similar to simplified rollout that we discussed in Section 2.3.4).For example, often many of the transition probabilities p(y | x) are very close to 0, and can be safely ignored.Another possibility to reduce computation is to truncate the trajectories generated from the next states y by the greedy policy, up to m steps (assuming that k + m < N, i.e., if we are more than m steps away from the end of the horizon).This is essentially the truncated rollout algorithm, discussed in Section 2.3.5, whereby we maximize over y the m-step Q-factor of the greedy policy π:Still another possibility is to apply the rollout approach successively, in multiple policy iterations, by using the rollout policy obtained at each iteration as base policy for the next iteration.This corresponds to the fundamental DP algorithm of policy iteration.Performing on-line just two policy iterations amounts to using the rollout algorithm as a base policy for another rollout algorithm.This has been called double rollout , and it has been discussed in Section 2.3.5 of the book [Ber20] and Section 6.5 of the book [Ber22].Generally, one-step lookahead rollout requires O(q • N ) applications of the base policy where q is the number of Q-factors calculated at each time step.† Thus with each new policy iteration, there is an amplification factor O(q • N ) of the computational requirements.Still, however, the multiple iteration approach may be viable, even on-line, when combined with some of the other time-saving computational devices described above (e.g., truncation and simplification to reduce q), in view of the relative simplicity of the calculations involved and their suitability for parallel computation.This is particularly so for double rollout.Policy iteration/double rollout is discussed by Yan et al.[YDR04] in the context of the game of solitaire, and by Silver and Barreto [SiB22] in the context of a broader class of search methods.We next show that the rollout selection policy with one-step lookahead has a performance improvement property: it generates more likely state sequences than the greedy policy, starting from any state, and the improvement is often very substantial.† For a more accurate estimate of the complexity of the greedy, rollout, and double rollout algorithms, note that the basic operation of the greedy operation is the maximization over the q numbers p(y | x k ).Thus m steps of the greedy algorithm, as in an m-step Q-factor calculation, costs q•m comparisons.In m-step truncated rollout, we compare q greedy Q-factors so the number of comparisons per rollout time step is q 2 m + q.Over N time steps the total is (q 2 m + q) • N comparisons, while for the greedy algorithm starting from the initial state x0, the corresponding number is q • N .Thus there is an amplification factor of qm + 1 for the computation of simplified m-step truncated rollout over the greedy policy.Similarly it can be estimated that there is an amplification factor of no more than qm + 1 for using double rollout with (single) rollout as a base policy.Chap. 2We will show by induction a performance improvement property of the rollout algorithm with one-step lookahead, namely that for all states x ∈ X and k, we havei.e., the probability of the sequence generated by the rollout policy is greater or equal to the probability of the sequence generated by the greedy policy; this is true for any starting state x at any time k.This is similar to our earlier rollout cost improvement results in Section 2.3.1.Indeed, for k = N this relation holds, since we haveAssuming that P k+1 (x, π) ≤ P k+1 (x, π), for all x, we will show thatIndeed, we use the preceding relations to writewhere• The first equality holds from the definition of the probabilities corresponding to the rollout policy π.Approximation in Value Space -Rollout Algorithms Chap. 2 It can be seen that on average, rollout and its variants provide a substantial improvement over the greedy policy, the improvement increases with the size of the lookahead, and truncated rollout methods perform comparable to their exact counterparts.denoted by (P * 0 ) 1/N , according to the average geometric mean formulawhere P * 0,c (x) is the optimal occurrence probability with x 0 = x and Markov chain c in the sample set.Similarly, we compute the occurrence probabilities of sequences generated by the greedy policy averaged over all chains, states, and transitions, and denoted by (P 0 ) 1/N , according towhere P 0,c (x, π) is the transition probability of the sequence generated by the greedy policy with x 0 = x and Markov chain indexed by c.For the rollout algorithm (or variants thereof), we compute its averaged occurrence probability ( P0 ) 1/N similar to Eq. (2.30) with π in place of π.Then the performance of the rollout algorithm can be measured by its percentage recovery of the optimality loss of the greedy policy, given byThis performance measure describes accurately how the rollout performance compares with the greedy policy and how close it comes to optimality.Approximation in Value Space -Rollout Algorithms Chap. 2 the on-line computational cost of multistep lookahead increases, often exponentially, with the length of lookahead.We conclude that we should aim to use a lookahead that is as long as is allowed by the on-line computational budget (the amount of time that is available for calculating a control to apply at the current state).Long Lookahead by Rollout is Far More Economical than Long Lookahead MinimizationOur preceding discussion leads to the question of how to economize in computation in order to effectively increase the length of the multistep lookahead within a given on-line computational budget.One way to do this, which we have already discussed, is the use of truncated rollout that explores forward through a deterministic base policy at far less computational cost than lookahead minimization of equal length.As an example, let us consider the possibility of starting with a terminal cost function J, possibly generated by off-line training, and use as base policy for rollout the one-step lookahead policy μ, defined by J using the equation †Let us assume that the principal computation in the minimization of Eq. (2.33) is the calculation of J f (x, u) , and compare two possibilities: (b) Using one-step lookahead minimization, with ( − 1)-step truncated rollout and J as the terminal cost approximation.Note that scheme (b) is the one used by the TD-Gammon program of Tesauro and Galperin [TeG96], out of necessity: multistep lookahead minimization is very expensive in backgammon, due to the rapid growth of the lookahead graph as increases (cf. the discussion of Section 1.1).Suppose that the control set U (x) has m elements for every x.Then the -step lookahead minimization scheme (a) requires the calculation of as many as m values of J , because the number of leaves of the m-step lookahead graph are as many as m .The corresponding number of calculations of the value of J for scheme (b) is clearly much smaller.To verify this, let us calculate this number as a function of m and .In particular, the first lookahead stage starting from the current state x k requires m calculations corresponding to the m controls in U (x k ), and yields corresponding states x k+1 , which are as many as m.For each of these states x k+1 , we must calculate a sequence of − 1 † For simplicity, we use stationary system notation, omitting the time subscripts of U , g, and f .denoted x * , is the one that minimizes (over all leaf nodes x of S) the sumwhere D(x) is the shortest distance from x 0 to the leaf node x using only arcs that belong to S. This can be computed by forward DP. [A noteworthy possibility is to replace D(x) with a conveniently computed approximation, which may be problem-specific.We will discuss such schemes later in this section.]H(x) is a "heuristic cost" corresponding to x.This is defined as the sum of three terms:(a) The cost of the base heuristic starting from node x and ending at one of the states x in the last layer .(b) The terminal cost approximation J(x ), where x is the state obtained via the base heuristic as in (a) above.(c) An additional penalty P (x) that depends on the layer to which x belongs.As an example, we will assume here thatwhere δ is a positive parameter.Thus P (x) adds a cost of δ for each extra arc to reach x from x 0 , and penalizes nodes x that   that starts at x k and satisfies the system equation x t+1 = f t (x t , u t ), t= k, . . ., N − 1.Thus, given y k and any control u k , we can use the base heuristic to obtain a complete trajectory as follows:(a) Generate the next state x k+1 = f k (x k , u k ).(b) Extend y k to obtain the partial trajectory(c) Run the base heuristic from y k+1 to obtain the partial trajectory R(y k+1 ).(d) Join the two partial trajectories y k+1 and R(y k+1 ) to obtain the complete trajectory y k , u k , R(y k+1 ) , which is denoted by T k (y k , u k ):T k (y k , u k ) = y k , u k , R(y k+1 ) .(2.39)Unfortunately, the confusion arising from different terminology has been exacerbated by the use of different notations.This book roughly follows the "standard" notation of the Bellman/Pontryagin optimal control era; see e.g., the books by Athans and Falb [AtF66], Bellman [Bel67], and Bryson and Ho [BrH75].This notation is consistent with the author's other DP books and is the most appropriate for a unified treatment of the subject, which simultaneously addresses discrete and continuous spaces problems.Sec. 1.7A summary of our most prominently used symbols is as follows:(a) x: state.(b) u: control.(c) J: cost function.(d) g: cost per stage.(e) f : system function.(f) w: stochastic disturbance.(g) i: discrete state.(h) p xy (u): transition probability from state x to state y under control u.(i) α: discount factor in discounted problems.The x-u-J notation is standard in optimal control textbooks (e.g., the classical books [AtF66] and [BrH75], noted earlier, as well as the more recent books by Stengel [Ste94], Kirk [Kir04], and Liberzon [Lib11]).The notations f and g are also used most commonly in the literature of the early optimal control period as well as later (unfortunately the more natural symbol "c" has not been used much in place of "g" for the cost per stage).The discrete system notations i and p ij (u) are common in the discrete-state Markov decision problem and operations research literature, where discretestate problems have been treated extensively [sometimes the alternative notation p(j | i, u) is used for the transition probabilities].The artificial intelligence literature addresses for the most part finitestate Markov decision problems, most frequently the discounted and stochastic shortest path infinite horizon problems.The most commonly used notation is s for state, a for action, r(s, a, s ) for reward per stage, p(s | s, a) or p(s, a, s ) for transition probability from s to s under action a, and γ for discount factor.However, this type of notation is not well suited for continuous spaces models, which are of major interest in this book.The reason is that it requires the use of transition probability distributions defined over continuous spaces, and it leads to more complex and less intuitive mathematics.Moreover the transition probability notation is cumbersome for deterministic problems, which involve no probabilistic structure at all.Machine learning and optimization are closely intertwined fields, as they focus on related mathematical models and computational algorithms.† How- † Both machine learning and optimization are also closely connected with the field of statistical analysis.However, in this section, we will not focus on this connection, as it is less relevant to the content of this book.Chap. 2We consider subsets of N + 1 nodes, referred to as groupings, which consist of a single node from every layer.For each grouping, there is an associated cost, which depends on the N -tuple of arcs that comprise the grouping.A partition of the set of nodes into m disjoint groupings (so that each node belongs to one and only one grouping) is called an (N + 1)-dimensional assignment.The cost of an (N + 1)-dimensional assignment is the sum of the costs of its m groupings.The problem is to find an (N + 1)-dimensional assignment of minimum cost.The exact solution of the problem is very difficult when the cost of a grouping does not decompose into the sum of costs of the N arcs of the grouping (in which case the problem decouples into N easily solvable 2-dimensional assignment problems).We formulate the problem as an N -stage sequential decision problem where at the kth stage we select the assignment arcswhich connect the nodes in of layer N k to nodes jn of layer N k+1 on a one-toone basis.The control costraint set U k is the set of legitimate assignments, namely those involving exactly one incident arc per node in ∈ N k , and exactly one incident arc per node jn of layer N k+1 .We assume a cost function that has the general form G(u0, . . ., uN−1).We use as state x k the partial solution up to time k,x k = (u0, . . ., u k−1 ), so the system equation takes the simple formcf. Fig. 2.1.4and Section 1.6.3.The exact DP algorithm takes the formand,3) and (2.4).Since this DP algorithm is intractable, we resort to approximation of J * k+1 by a function J k+1 .The one-step lookahead minimization becomes minbut is still formidable because its search space U k is very large.However, it can be greatly simplified by using a cost function approximation Jk+1 with a  Once the arc costs c ij k+1 (x k ) have been calculated, the assignment u k and corresponding multiassignment trajectory x k+1 are obtained by solving a 2-dimensional assignment problem, using for example the Hungarian method or the auction algorithm.special structure that is suitable for the use of fast 2-dimensional assignment algorithms.This cost function approximation has the formwhere {(i, j) ∈ u k } denotes the set of m arcs (i, j) that correspond to the 2-dimensional assignment u k , cf.Eq. (2.8) [thus the dependence of the righthand side on u k comes through the choice of arcs (i, j) specified by u k ].The arc costs c ij k+1 (x k ) in this equation must be calculated for every possible arc (i, j) that connects a node i ∈ N k with a node j ∈ N k+1 ; see Fig. 2.2.2.Note that the arc costs c ij k+1 (x k ) may depend in a complicated way on the entire trajectory of previous 2-dimensional assignment choices x k = (u0, . . ., u k−1 ) and the problem data.For problems that involve tracking the movement of people or vehicles over time, the computation of c ij k+1 (x k ) relies on sensor data and problem-dependent circumstances.For other problems, enforced decomposition methods may be useful; see [Ber20a], Section 3.4.2.We refer to the data association literature for further details; see e.g., the survey by Emami et al. [EPE20].The approximation in value space algorithm (2.7) involves a one-step lookahead minimization, since it solves a one-stage DP problem for each k.WeChap. 2Note that the rollout algorithm requires running the base heuristic for a number of times that is bounded by N n, where n is an upper bound on the number of control choices available at each state.Thus if n is small relative to N , the algorithm requires computation equal to a small multiple of N times the computation time for a single application of the base heuristic.Similarly, if n is bounded by a polynomial in N , the ratio of the rollout algorithm computation time to the base heuristic computation time is a polynomial in N .In Section 1.2 we considered an example of rollout involving the traveling salesman problem and the nearest neighbor heuristic (cf.Examples 1.2.2 and 1.2.3).Let us consider another example, which involves a classical discrete optimization problem.Consider m vehicles that move along the arcs of a given graph.Some of the nodes of the graph include a task to be performed by the vehicles.Each task will be performed only once, immediately after some vehicle reaches the corresponding node for the first time.We assume a horizon that is large enough to allow every task to be performed.The problem is to find a route for each vehicle so that the tasks are collectively performed by the vehicles in a minimum number of moves.To express this objective, we assume that for each move by a vehicle there is a cost of one unit.These costs are summed up to the point where all the tasks have been performed.For a large number m of vehicles and a complicated graph, this is a nontrivial combinatorial problem.It can be approached by DP, like any discrete deterministic optimization problem, as we have discussed.In particular, we can view as state at a given stage the m-tuple of current positions of the vehicles together with the list of pending tasks.Unfortunately, however, the number of these states can be enormous (it increases exponentially with the number of tasks and the number of vehicles), so an exact DP solution is intractable.This motivates an optimization in value space approach based on rollout.For this we need an easily implementable base heuristic that will solve suboptimally the problem starting from any state x k+1 , and will provide the cost approximation Jk+1 (x k+1 ) in Eq. (2.7).One possibility is based on the vehicles choosing their actions selfishly and without coordination, along shortest paths to their nearest pending task.To illustrate, consider the two-vehicle problem of Fig. 2.3.2.The base heuristic is to move each vehicle one step at a time towards its nearest pending task, until all tasks have been performed.The rollout algorithm will work as follows.At a given state x k [involving for example vehicle positions at the node pair (1, 2) and tasks at nodes 7 and 9, as in Fig. 2.3.2],we consider all possible joint vehicle moves (the controls u k at the state) resulting in the node pairs (3,5), (4,5), (3,4), (4,4), corresponding to the next states x k+1 [thus, as an example (3,5)  (, perform a heuristic calculation from each, compare, etc.It can be verified that the rollout algorithm starting from the state (1,2) shown in Fig. 2.3.2 will attain the optimal cost (a total of 6 vehicle moves).It will perform much better than the heuristic, which starting from state (1,2), will move the two vehicles together to state (4,4), then to (7,7), then to (10,10), then to (12,12), and finally to (9,9), (a total of 10 vehicle moves).This is an instance of the cost improvement property of the rollout algorithm: it performs better than its base heuristic (under appropriate conditions).Let us finally note that the computation required by in rollout algorithm increases exponentially with the number m of vehicles, since the number of mtuples of moves at each stage increases exponentially with m.This is the type of problem where multiagent rollout can attain great computational savings; cf.Section 1.6.5, and the subsequent Section 2.9.Here is an example of a search problem, whose exact solution complexity grows exponentially with the problem size, but can be addressed with a greedy heuristic as well as with the corresponding rollout algorithm.Consider a binary tree with N stages as shown in Fig. 2.3.3.Stage k of the tree has 2 k nodes, with the node of stage 0 called root and the nodes of stage N called leaves.There are two types of tree arcs: free and blocked .A free (or blocked) arc can (cannot, respectively) be traversed in the direction from the root to the leaves.The objective is to break through the graph with a sequence of free arcs (a free path) starting from the root, and ending at one of the leaves.(A variant of this problem is to introduce a positive cost c > 0 for traversing a blocked arc, and 0 cost for traversing a free arc.)In other words, the base heuristic is sequentially consistent if it "stays the course": when the starting state x k is moved forward to the next state x k+1 of its state trajectory, the heuristic will not deviate from the remainder of the trajectory.As an example, the reader may verify that the nearest neighbor heuristic described in the traveling salesman Example 1.2.3 and the heuristics used in the multivehicle routing Example 2.3.1 are sequentially consistent.Similar examples include the use of various types of greedy/myopic heuristics (Section 6.4 of the book [Ber17a] provides additional examples).† Generally most heuristics used in practice satisfy the sequential consistency condition at "most" states x k .However, some heuristics of interest may violate this condition at some states.A sequentially consistent base heuristic can be recognized by the fact that it will apply the same control u k at a state x k , no matter what position x k occupies in a trajectory generated by the base heuristic.Thus a base heuristic is sequentially consistent if and only if it defines a legitimate DP policy.This is the policy that moves from x k to the state x k+1 that lies on the state trajectory {x k , x k+1 , . . ., x N } that the base heuristic generates.Similarly the policy moves from x n to the state x n+1 for n = k+1, . . ., N −1.† A subtle but important point relates to how one breaks ties while implementing greedy base heuristics.For sequential consistency, one must break ties in a consistent way at various states, i.e., using a fixed rule at each state encountered by the base heuristic.In particular, randomization among multiple controls, which are ranked as equal by the greedy optimization of the heuristic, violates sequential consistency, and can lead to serious degradation of the corresponding rollout algorithm's performance.Chap. 2We will next show that the rollout policy has no worse performance than its base heuristic under a condition that is weaker than sequential consistency.Let us recall that the rollout algorithm π = {μ 0 , . . ., μN−1 } is defined by the minimization μk (x k ) ∈ arg minwhere Qk (x k , u k ) is the approximate Q-factor defined by[cf.Eq. (2.10)], and H k+1 f k (x k , u k ) denotes the cost of the trajectory of the base heuristic starting from state f k (x k , u k ).Definition 2.3.2:We say that the base heuristic is sequentially improving if for all x k and k, we have minIn words, the sequential improvement property (2.13) states that Minimal heuristic Q-factor at x k ≤ Heuristic cost at x k .Note that when the heuristic is sequentially consistent it is also sequentially improving.This follows from the preceding relation, since for a sequentially consistent heuristic, the heuristic cost at x k is equal to the Q-factor of the control u k that the heuristic applies at x k ,which is greater or equal to the minimal Q-factor at x k .This implies Eq. (2.13).A sequentially improving heuristic yields policy improvement as the next proposition shows.Proposition 2.3.2:(Cost Improvement Under Sequential Improvement) Consider the rollout policy π = {μ 0 , . . ., μN−1 } obtained with a sequentially improving base heuristic, and let J k,π (x k ) denote the cost obtained with π starting from x k at time k.Thenwhere H k (x k ) denotes the cost of the base heuristic starting from x k .Approximation in Value Space -Rollout Algorithms Chap. 2) . . .) . . .) . . .Trajectory T kTrajectory T k+1 Optimal Base Rollout Terminal Score Approximation CurrentIf strict inequality holds, the rollout algorithm will switch from T k and follow T k+1 ; cf. the traveling salesman Example 1.2.3.form of the MPC scheme is not sequentially consistent (see the preceding references).Generally, the sequential improvement condition may not hold for a given base heuristic.This is not surprising since any heuristic (no matter how inconsistent or silly) is in principle admissible to use as base heuristic.Here is an example:  The rollout choice at the initial state x 0 is strictly suboptimal, while the base heuristic choice is optimal.The reason is that the base heuristic is not sequentially improving and makes the suboptimal choice u 1 at x * 1 , but makes the different (optimal) choice u * 1 when run from x 0 .is arbitrary), and moreover the cost g1(x * 1 , ū1) + g2(x2), which is equal to H1(x * 1 ) is high enough.Let us also verify that if the inequality (2.15) holds then the heuristic is not sequentially improving at x0, i.e., thatIndeed, this is true because H0(x0) is the optimal costand must be smaller than bothwhich is the cost of the trajectory (x0, u * 0 , x * 1 , u1, x2), andwhich is the cost of the trajectory (x0, ũ0, x1, ũ1, x2).The preceding example and the monotonicity property (2.14) suggest a simple enhancement to the rollout algorithm, which detects when the sequential improvement condition is violated and takes corrective measures.In this algorithmic variant, called fortified rollout , we maintain the best trajectory obtained so far, and keep following that trajectory up to the point where we discover another trajectory that has improved cost (see the next section).x 0) . . .and the tentative best trajectorythe best end-to-end trajectory computed so far.We now run the rollout algorithm at x k , i.e., we find the control ũk that minimizes over u k the sum of g k (x k , u k ) plus the heuristic cost from the state x k+1 = f k (x k , u k ), and the corresponding trajectoryIf the cost of the end-to-end trajectory Tk is lower than the cost of T k , we add (ũ k , xk+1 ) to the permanent trajectory and set the tentative best trajectory to T k+1 = Tk .Otherwise we add (u k , x k+1 ) to the permanent trajectory and keep the tentative best trajectory unchanged:the algorithm sets the next state and corresponding tentative best trajectory toit sets the next state and corresponding tentative best trajectory toIn other words the fortified rollout at x k follows the current tentative best trajectory T k unless a lower cost trajectory Tk is discovered by running the base heuristic from all possible next states x k+1 .† It follows that at every state the tentative best trajectory has no larger cost than the initial tentative best trajectory, which is the one produced by the base heuristic starting from x 0 .Moreover, it can be seen that if the base heuristic is sequentially improving, the rollout algorithm and its fortified version coincide.Experimental evidence suggests that it is often important to use the fortified version if the base heuristic is not known to be sequentially improving.Fortunately, the fortified version involves hardly any additional computational cost.As expected, when the base heuristic generates an optimal trajectory, the fortified rollout algorithm will also generate the same trajectory.This is illustrated by the following example.Let us consider the application of the fortified rollout algorithm to the problem of Example 2.3.3 and see how it addresses the issue of cost improvement.The fortified rollout algorithm stores as initial tentative best trajectory the optimal trajectory (x0, u * 0 , x * 1 , u * 1 , x * 2 ) generated by the base heuristic at x0.Then, starting at x0, it runs the heuristic from x * 1 and x1, and (despite the fact that the ordinary rollout algorithm prefers going to x1 rather than x * 1 ) it discards the control ũ0 in favor of u * 0 , which is dictated by the tentative best trajectory.It then sets the tentative best trajectory to (x0,We finally note that the fortified rollout algorithm can be used in a different setting to restore and maintain the cost improvement property.Suppose in particular that the rollout minimization at each step is performed with approximations.For example the control u k may have multiple independently constrained components, i.e.,Then, to take advantage of distributed computation, it may be attractive to decompose the optimization over u k in the rollout algorithm, μk (x k ) ∈ arg mininto an (approximate) parallel optimization over the components u i k (or subgroups of these components).However, as a result of approximate optimization over u k , the cost improvement property may be degraded, even if the sequential improvement assumption holds.In this case by maintaining † The base heuristic may also be run from a subset of the possible next states x k+1 , as in the case where a simplified version of rollout is used; cf.Section 2.3.4.Then fortified rollout will still guarantee a cost improvement property.Chap. 2where Q k (x k , u k ) and H k (x k ) are Q-factors and heuristic costs that correspond to the th heuristic.Then by taking minimum over , we have min =1,...,m minfor all x k and k.By interchanging the order of the minimizations of the left side, we then obtain min, which is precisely the sequential improvement condition (2.13) for the superheuristic.We will now consider a rollout variant, called simplified rollout , which is motivated by problems where the control constraint set U k (x k ) is either infinite or finite but very large.Then the minimization μk (x k ) ∈ arg min[cf.Eqs.(2.9) and (2.10)], may be unwieldy, since the number of Q-factorsis accordingly infinite or large.To remedy this situation, we may replace U k (x k ) with a smaller finite subset U k (x k ):The rollout control μk (x k ) in this variant is one that attains the minimum of Qk (x k , u k ) over(2.17An example is when U k (x k ) results from discretization of an infinite set U k (x k ).Another possibility is when by using some preliminary approximate optimization, we can identify a subset U k (x k ) of promising controls by using some heuristic method, and to save computation, we restrict attention to this subset.A related possibility is to generate U k (x k ) by some iterative or random search method that explores intelligently the set U k (x k ) with the aim to minimize Qk (x k , u k ) [cf.Eq. (2.16)].We assume that we do not know G and/or U 0 , . . ., U N−1 .Instead we have a base heuristic, which given a partial solution (u 0 , . . ., u k ), outputs all next controls ũk+1 ∈ U k+1 , and generates from each a complete solution S k (u 0 , . . ., u k , ũk+1 ) = (u 0 , . . ., u k , ũk+1 , . . ., ũN−1 ).Also, we have a human or software "expert" that can rank any two complete solutions without assigning numerical values to them.The control that is selected from U k+1 by the rollout algorithm is the one whose corresponding complete solution is ranked best by the expert.solutions can be revealed by the expert, this is all that is needed.† In fact, the constraint sets U 0 , . . ., U N −1 need not be known either, as long as they can be generated by the base heuristic.Thus, the rollout algorithm can be described as follows (see Fig. 2We start with an artificial empty solution, and at the typical step, given the partial solution (u 0 , . . ., u k ), k < N − 1, we use the base heuristic † Note that for this to be true, it is important that the problem is deterministic, and that the expert ranks solutions using some underlying (though unknown) cost function.In particular, the expert's rankings should have a transitivity property: if u is ranked better than u and u is ranked better than u , then u is ranked better than u .Chap. 2 pairs (u s , u s ), s = 1, . . ., q, with G(u s ) > G(u s ), s= 1, . . ., q, (2.19) which we can use for training.Such a set may be obtained in a variety of ways, including querying the expert.We may then train a parametric approximation architecture such as a neural network to produce a function G(u, r), where r is a parameter vector, and use this function in place of the unknown G(u) to implement the preceding rollout algorithm.A method, known as comparison training, has been suggested for this purpose, and has been used in a variety of game contexts, including backgammon and chess by Tesauro [Tes89b], [Tes01].Briefly, given the training set of pairs (u s , u s ), s = 1, . . ., q, which satisfy Eq. (2.19), we generate for each (u s , u s ), two solution-cost pairs (u s , 1), (u s , −1), s= 1, . . ., q.A parametric architecture G(•, r), involving a parameter vector r, such as a neural network, is then trained by some form of regression with these data to produce an approximation G(•, r) to be used in place of G(•) in a rollout scheme.We refer to Chapter 3 and to the aforementioned papers by Tesauro for implementation details of the regression procedure.See also Section 3.4 on parametric approximation in policy space through the use of classification methods.In another type of imitation approach, we view the base policy decisions as being selected by a process the mechanics of which are not observed except through its generated cost samples at the various stages.In particular, the stage costs starting from any given partial solution (u 0 , . . ., u k ) are added to form samples of the base policy's Q-factors Q k (u 0 , . . ., u k ).In this way we can obtain Q-factor samples starting from many partial solutions (u 0 , . . ., u k ).Moreover, a single complete solution (u 0 , . . ., u N −1 ) generated by the base policy provides multiple Q-factor samples, one for each of the partial solutions (u 0 , . . ., u k ).We can then use the sample (partial solution, cost) pairs in conjunction with a training method (see Chapter 3) in order to construct parametric approximations Qk (u 0 , . . ., u k , r k ), k = 1, . . ., N, to the true Q-factors Q k (u 0 , . . ., u k ), where r k is the parameter vector.Once the training has been completed and the Q-factors Qk (u 0 , . . ., u k , r k ) have been obtained for all k, we can construct complete solutions step-by-step, by selecting the next component ũk+1 , given the partial solution (u 0 , . . ., u k ), through the minimization ũk+1 ∈ arg minQk+1 (ũ 0 , . . ., ũk , u k+1 , r k+1 ).Note that even though we are "learning" the base policy, our aim is not to imitate it, but rather to generate a rollout policy.The latter policywill make better decisions than the base policy, thanks to the cost improvement property of rollout.This points to an important issue of exploration: we must ensure that the training set of sample (partial solution, cost) pairs is broadly representative, in the sense that it is not unduly biased towards sample pairs that are generated by the base policy.In this section we consider a type of deterministic sequential decision problem involving n-grams and transformers, which provide next word probabilities that can be used to generate word sequences (cf.Section 1.6, Example 1.6.2).We consider methods for computing N -step word sequences that are highly likely, based on these probabilities.† Computing the optimal (i.e., most likely) word sequence starting with a given initial state is an intractable problem, so we consider rollout algorithms that compute highly likely N -word sequences in time that is a low order polynomial in N and in the vocabulary size of the n-gram.Our n-gram model generates a sequence {x 1 , . . ., x N } of text strings, starting from some initial string x 0 (here n and N are fixed positive integers).Each string x k consists of a sequence of n words, chosen from a given list (the vocabulary of the n-gram).The kth string x k is transformed into the next string x k+1 by adding a word at the front end of x k and deleting the word at the back end of x k ; see Example 1.6.2.Given a text string x k , the n-gram provides probabilities p(x k+1 | x k ) for the next text string x k+1 .These probabilities also define the probabilities of the possible next words, since x k+1 is determined by the next word that is added to the front of x k .We assume that the probabilities p(x k+1 | x k ) depend only on x k .Thus they can be viewed as the transition probabilities of a stationary Markov chain, whose state space is the set of all n-word sequences x k .‡ Bearing this context in mind, we also refer to x k as the state (of the underlying Markov chain).The transition probabilities p(x k+1 | x k ) can provide guidance for generating state sequences with some specific purpose in mind.To this end, a transformer may use a (next word) selection policy, i.e., a (possibly time-dependent) function µ k , which selects the text string that follows x k asx k+1 = µ k (x k ).† This section is based on joint work with Yuchao Li; see the paper by Li and Bertsekas [LiB24], which also contains extensive computational experimentation results.‡ The stationarity assumption simplifies our notation, but is not essential to our methodology, as we will discuss later.Approximation in Value Space -Rollout Algorithms Chap. 2We are generally interested in selection policies that give preference to high-probability future words.Our methods also apply to the problem of finding the most likely sequence generated by a general finite-state Markov chain.This problem arises in many important contexts.A major example is inference of the sequence of states of a Hidden Markov Model (HMM), given an associated sequence of observed data.This is the problem where Viterbi Compared to these fields, the transformer/n-gram context tends to involve Markov chains with an intractably larger state space.A DP-oriented discussion of the Viterbi algorithm and its applications to HMM inference is given in Section 2.2.2 of the textbook [Ber17].In this section, we will not consider the problem of most likely sequence selection in the context of inference of state sequences in HMMs, but our methods fully apply to that context.We will next consider a finite-state stationary Markov chain and various policies for generating highly likely sequences according to the transition probabilities of the Markov chain.We will generally use the symbols x and y for states, and we will denote the chain's transition probabilities by p(y | x).We assume that given a state x, the probabilities p(y | x) are either known or can be generated on-line by means of software such as a transformer.We assume stationarity of the Markov chain in part to alleviate an overburdened notation, and also because n-gram and transformer models are typically assumed to be stationary.However, the rollout methodology and the manner in which we use it do not depend at all on stationarity of the transition probabilities, or infinite horizon properties of Markov chains, such as ergodic classes, transient states, etc.In fact, they also do not depend on the stationarity of the state space either.Only the Markov property is used in our discussion, i.e., the probability of the next state depends on the immediately preceding state, and not on earlier states.A selection policy π is a sequence of functions {µ 0 , . . ., µ N −1 }, which given the current state x k , determines the next state x k+1 asNote that for a given π, the state evolution is deterministic; so for a given π and x 0 , the generated state sequence {x 1 , . . ., x N } is fully determined.x yx y k+1,k (x, π)) y N,k (x, π) Moreover the choice of the policy π is arbitrary, although we are primarily interested in π that give preference to high probability next states.Given a policy π = {µ 0 , . . ., µ N −1 } and a starting state x at time k, the state at future times m > k is denoted by y m,k (x, π): y m,k (x, π) = state at time m > k starting at state x and using π.The state trajectory generated by a policy π, starting at state x at time k, is the sequence y k+1,k (x, π), . . ., y N,k (x, π), (cf.Fig. 2.3.9), and the probability of its occurrence in the given Markov chain isaccording to the multiplication rule for conditional probabilities.The most likely selection policy, denoted by π * = {µ * 0 , . . ., µ * N −1 }, maximizes over all policies π the probabilities P k (x, π) for every initial state x and time k.The corresponding probabilities of π * , starting at state x at time k, are denoted by P * k (x):This is similar to our finite-horizon DP context, except that we consider a multiplicative reward function, instead of an additive cost function [P * k (x) can be viewed as an optimal reward-to-go from state x at time k, in place of the optimal cost-to-to J * k (x) that we have considered so far in the additive DP case].†The policy π * and its probabilities P * k (x) can be generated by the following DP-like algorithm, which operates in two stages:(a) It first computes the probabilities P * k (x) backwards, for all x, according tostarting withstarting with x * 0 = x 0 .This algorithm is equivalent to the usual DP algorithm for multistage additive costs, after we take logarithms of the multiplicative expressions defining the probabilities P k (x, π).At any given state x k , the greedy policy produces the next state by maximization of the corresponding transition probability over all y:We assume that ties in the above maximization are broken according to some prespecified deterministic rule.For example if the states are labeled by distinct integers, one possibility is to specify the greedy selection at x k as the state y with minimal label, among those that attain the maximum above.Note that the greedy policy is not only deterministic, but it is also stationary (its selections depend only on the current state and not on the time k).We will consequently use the notation π = {µ, . . ., µ} for the greedy policy, where(2.23) † Generally multiplicative reward problems can be converted to additive cost DP problems involving negative logarithms of the multiplicative reward factors (assuming they are positive).Rollout Algorithms for Discrete Optimization 197 and µ(x k ) is uniquely defined according to our deterministic convention for breaking ties in the maximization above.The corresponding probabilities P k (x k , π) are given by the DP-like algorithmstarting with P N (x, π) ≡ 1.Equivalently, we can compute P k (x, π) by using forward multiplication of the transition probabilities along the trajectory generated by the greedy policy, starting from x; cf.Eq. (2.20).The limitation of the greedy policy is that it chooses the locally optimal next state without considering the impact of this choice on future state selections.The rollout approach, to be discussed next, mitigates this limitation with a mechanism for looking into the future, and balancing the desire for a high-probability next transition with the potential undesirability of low-probability future transitions.At any given state x k , the rollout policy with one-step lookahead produces the next state, denoted μk (x k ), by maximizing p(y | x k )P k+1 (y, π) over all y:Thus it optimizes the selection of the first state y, assuming that the subsequent states will be chosen using the greedy policy.By comparing the maximization (2.25) with the one for the most likely selection policy [cf.Eq. (2.22)], we see that it chooses the next state similarly, except that P * k+1 (y) (which is hard to compute) is replaced by the (much more easily computable) probability P k+1 (y, π).In particular, the latter probability is computed for every y by running the greedy policy forward starting from y and multiplying the corresponding transition probabilities along the generated state trajectory.This is a polynomial computation, which is roughly larger by a factor N over the greedy selection method.However, there are ways to reduce this computation, including the use of parallel processing and other possibilities, which we will discuss later.The expression p(y | x k )P k+1 (y, π) that is maximized over y in Eq. (2.25) can be viewed as the Q-factor of the pair (x k , y) corresponding to the base policy π, and is denoted by(2.26)This is similar to the approximation in value space context, except that we consider a multiplicative reward function, whereby at state x k we choose the action y that yields the maximal Q-factor.Approximation in Value Space -Rollout Algorithms Chap. 2Another rollout possibility includes rollout with -step lookahead ( > 1), whereby given x k we maximize over all sequences {y 1 , y 2 , . . ., y } up to steps ahead, the -step Q-factor) and if {ỹ 1 , ỹ2 , . . ., ỹ } is the maximizing sequence, we select ỹ1 at x k , and discard the remaining states ỹ2 , . . ., ỹ .† In practice the performance of -step lookahead rollout policies almost always improves with increasing .However, artificial examples have been constructed where this is not so; see the book [Ber19a], Section 2.1.1.Moreover, the computational overhead of -step lookahead increases with .As we have already noted, one of the difficulties that arises in the application of rollout is the potentially very large number of the Q-factors that need to be calculated at each time step at the current state x [it is equal to the number of states y for which p(y | x) > 0].In practice the computation of Q-factors can be restricted to a subset of most probable next states, as per the transition probabilities p(y | x) (this is similar to simplified rollout that we discussed in Section 2.3.4).For example, often many of the transition probabilities p(y | x) are very close to 0, and can be safely ignored.Another possibility to reduce computation is to truncate the trajectories generated from the next states y by the greedy policy, up to m steps (assuming that k + m < N, i.e., if we are more than m steps away from the end of the horizon).This is essentially the truncated rollout algorithm, discussed in Section 2.3.5, whereby we maximize over y the m-step Q-factor of the greedy policy π:Still another possibility is to apply the rollout approach successively, in multiple policy iterations, by using the rollout policy obtained at each iteration as base policy for the next iteration.This corresponds to the fundamental DP algorithm of policy iteration.Performing on-line just two policy iterations amounts to using the rollout algorithm as a base policy for another rollout algorithm.This has been called double rollout , and it has been discussed in Section 2.3.5 of the book [Ber20] and Section 6.5 of the book [Ber22].Generally, one-step lookahead rollout requires O(q • N ) applications of the base policy where q is the number of Q-factors calculated at each time step.† Thus with each new policy iteration, there is an amplification factor O(q • N ) of the computational requirements.Still, however, the multiple iteration approach may be viable, even on-line, when combined with some of the other time-saving computational devices described above (e.g., truncation and simplification to reduce q), in view of the relative simplicity of the calculations involved and their suitability for parallel computation.This is particularly so for double rollout.Policy iteration/double rollout is discussed by Yan et al.[YDR04] in the context of the game of solitaire, and by Silver and Barreto [SiB22] in the context of a broader class of search methods.We next show that the rollout selection policy with one-step lookahead has a performance improvement property: it generates more likely state sequences than the greedy policy, starting from any state, and the improvement is often very substantial.† For a more accurate estimate of the complexity of the greedy, rollout, and double rollout algorithms, note that the basic operation of the greedy operation is the maximization over the q numbers p(y | x k ).Thus m steps of the greedy algorithm, as in an m-step Q-factor calculation, costs q•m comparisons.In m-step truncated rollout, we compare q greedy Q-factors so the number of comparisons per rollout time step is q 2 m + q.Over N time steps the total is (q 2 m + q) • N comparisons, while for the greedy algorithm starting from the initial state x0, the corresponding number is q • N .Thus there is an amplification factor of qm + 1 for the computation of simplified m-step truncated rollout over the greedy policy.Similarly it can be estimated that there is an amplification factor of no more than qm + 1 for using double rollout with (single) rollout as a base policy.Chap. 2We will show by induction a performance improvement property of the rollout algorithm with one-step lookahead, namely that for all states x ∈ X and k, we havei.e., the probability of the sequence generated by the rollout policy is greater or equal to the probability of the sequence generated by the greedy policy; this is true for any starting state x at any time k.This is similar to our earlier rollout cost improvement results in Section 2.3.1.Indeed, for k = N this relation holds, since we haveAssuming that P k+1 (x, π) ≤ P k+1 (x, π), for all x, we will show thatIndeed, we use the preceding relations to writewhere• The first equality holds from the definition of the probabilities corresponding to the rollout policy π.Approximation in Value Space -Rollout Algorithms Chap. 2 It can be seen that on average, rollout and its variants provide a substantial improvement over the greedy policy, the improvement increases with the size of the lookahead, and truncated rollout methods perform comparable to their exact counterparts.denoted by (P * 0 ) 1/N , according to the average geometric mean formulawhere P * 0,c (x) is the optimal occurrence probability with x 0 = x and Markov chain c in the sample set.Similarly, we compute the occurrence probabilities of sequences generated by the greedy policy averaged over all chains, states, and transitions, and denoted by (P 0 ) 1/N , according towhere P 0,c (x, π) is the transition probability of the sequence generated by the greedy policy with x 0 = x and Markov chain indexed by c.For the rollout algorithm (or variants thereof), we compute its averaged occurrence probability ( P0 ) 1/N similar to Eq. (2.30) with π in place of π.Then the performance of the rollout algorithm can be measured by its percentage recovery of the optimality loss of the greedy policy, given byThis performance measure describes accurately how the rollout performance compares with the greedy policy and how close it comes to optimality.Approximation in Value Space -Rollout Algorithms Chap. 2 the on-line computational cost of multistep lookahead increases, often exponentially, with the length of lookahead.We conclude that we should aim to use a lookahead that is as long as is allowed by the on-line computational budget (the amount of time that is available for calculating a control to apply at the current state).Long Lookahead by Rollout is Far More Economical than Long Lookahead MinimizationOur preceding discussion leads to the question of how to economize in computation in order to effectively increase the length of the multistep lookahead within a given on-line computational budget.One way to do this, which we have already discussed, is the use of truncated rollout that explores forward through a deterministic base policy at far less computational cost than lookahead minimization of equal length.As an example, let us consider the possibility of starting with a terminal cost function J, possibly generated by off-line training, and use as base policy for rollout the one-step lookahead policy μ, defined by J using the equation †Let us assume that the principal computation in the minimization of Eq. (2.33) is the calculation of J f (x, u) , and compare two possibilities: (b) Using one-step lookahead minimization, with ( − 1)-step truncated rollout and J as the terminal cost approximation.Note that scheme (b) is the one used by the TD-Gammon program of Tesauro and Galperin [TeG96], out of necessity: multistep lookahead minimization is very expensive in backgammon, due to the rapid growth of the lookahead graph as increases (cf. the discussion of Section 1.1).Suppose that the control set U (x) has m elements for every x.Then the -step lookahead minimization scheme (a) requires the calculation of as many as m values of J , because the number of leaves of the m-step lookahead graph are as many as m .The corresponding number of calculations of the value of J for scheme (b) is clearly much smaller.To verify this, let us calculate this number as a function of m and .In particular, the first lookahead stage starting from the current state x k requires m calculations corresponding to the m controls in U (x k ), and yields corresponding states x k+1 , which are as many as m.For each of these states x k+1 , we must calculate a sequence of − 1 † For simplicity, we use stationary system notation, omitting the time subscripts of U , g, and f .denoted x * , is the one that minimizes (over all leaf nodes x of S) the sumwhere D(x) is the shortest distance from x 0 to the leaf node x using only arcs that belong to S. This can be computed by forward DP. [A noteworthy possibility is to replace D(x) with a conveniently computed approximation, which may be problem-specific.We will discuss such schemes later in this section.]H(x) is a "heuristic cost" corresponding to x.This is defined as the sum of three terms:(a) The cost of the base heuristic starting from node x and ending at one of the states x in the last layer .(b) The terminal cost approximation J(x ), where x is the state obtained via the base heuristic as in (a) above.(c) An additional penalty P (x) that depends on the layer to which x belongs.As an example, we will assume here thatwhere δ is a positive parameter.Thus P (x) adds a cost of δ for each extra arc to reach x from x 0 , and penalizes nodes x that   that starts at x k and satisfies the system equation x t+1 = f t (x t , u t ), t= k, . . ., N − 1.Thus, given y k and any control u k , we can use the base heuristic to obtain a complete trajectory as follows:(a) Generate the next state x k+1 = f k (x k , u k ).(b) Extend y k to obtain the partial trajectory(c) Run the base heuristic from y k+1 to obtain the partial trajectory R(y k+1 ).(d) Join the two partial trajectories y k+1 and R(y k+1 ) to obtain the complete trajectory y k , u k , R(y k+1 ) , which is denoted by T k (y k , u k ):T k (y k , u k ) = y k , u k , R(y k+1 ) .(2.39)