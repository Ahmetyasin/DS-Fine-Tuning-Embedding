We will not explain the subtleties of random sampling on a computer, and the interested reader is referred to Gentle (2004).In the case of a multivariate Gaussian, this process consists of three stages: first, we need a source of pseudo-random numbers that provide a uniform sample in the interval [0,1]; second, we use a non-linear transformation such as the Box-Müller transform (Devroye, 1986) to obtain a sample from a univariate Gaussian; and third, we collate a vector of these samples to obtain a sample from a multivariate standard normal N 0, I .For a general multivariate Gaussian, that is, where the mean is non zero and the covariance is not the identity matrix, we use the properties of linear transformations of a Gaussian random variable.Assume we are interested in generating samples x i , i = 1, . . ., n, from a multivariate Gaussian distribution with mean µ and covariance matrix Σ.We would To compute the Cholesky factorization of a matrix, it is required that the matrix is symmetric and positive definite (Section 3.2.3).Covariance matrices possess this property.like to construct the sample from a sampler that provides samples from the multivariate standard normal N 0, I .To obtain samples from a multivariate normal N µ, Σ , we can use the properties of a linear transformation of a Gaussian random variable: If x ∼ N 0, I , then y = Ax + µ, where AA ⊤ = Σ is Gaussian distributed with mean µ and covariance matrix Σ.One convenient choice of A is to use the Cholesky decomposition (Section 4.3) of the covariance matrix Σ = AA ⊤ .The Cholesky decomposition has the benefit that A is triangular, leading to efficient computation.Draft (2024-01-15) of "Mathematics for Machine Learning".Feedback: https://mml-book.com.Many of the probability distributions "with names" that we find in statistics textbooks were discovered to model particular types of phenomena.For example, we have seen the Gaussian distribution in Section 6.5.The distributions are also related to each other in complex ways (Leemis and McQueston, 2008).For a beginner in the field, it can be overwhelming to figure out which distribution to use.In addition, many of these distributions were discovered at a time that statistics and computation were done "Computers" used to be a job description.by pencil and paper.It is natural to ask what are meaningful concepts in the computing age (Efron and Hastie, 2016).In the previous section, we saw that many of the operations required for inference can be conveniently calculated when the distribution is Gaussian.It is worth recalling at this point the desiderata for manipulating probability distributions in the machine learning context: 1.There is some "closure property" when applying the rules of probability, e.g., Bayes' theorem.By closure, we mean that applying a particular operation returns an object of the same type.2. As we collect more data, we do not need more parameters to describe the distribution.3. Since we are interested in learning from data, we want parameter estimation to behave nicely.It turns out that the class of distributions called the exponential family exponential family provides the right balance of generality while retaining favorable computation and inference properties.Before we introduce the exponential family, let us see three more members of "named" probability distributions, the Bernoulli (Example 6.8), Binomial (Example 6.9), and Beta (Example 6.10) distributions.The Bernoulli distribution is a distribution for a single binary random Bernoulli distribution variable X with state x ∈ {0, 1}.It is governed by a single continuous parameter µ ∈ [0, 1] that represents the probability of X = 1.The Bernoulli distribution Ber(µ) is defined as 6.94) where E[x] and V[x] are the mean and variance of the binary random variable X.An example where the Bernoulli distribution can be used is when we are interested in modeling the probability of "heads" when flipping a coin.Remark.The rewriting above of the Bernoulli distribution, where we use Boolean variables as numerical 0 or 1 and express them in the exponents, is a trick that is often used in machine learning textbooks.Another occurence of this is when expressing the Multinomial distribution.♢ Example 6.9 (Binomial Distribution)The Binomial distribution is a generalization of the Bernoulli distribution Binomial distribution to a distribution over integers (illustrated in Figure 6.10).In particular, the Binomial can be used to describe the probability of observing m occurrences of X = 1 in a set of N samples from a Bernoulli distribution where p(X = 1) = µ ∈ [0, 1].The Binomial distribution Bin(N, µ) is defined as (6.95)where E[m] and V[m] are the mean and variance of m, respectively.An example where the Binomial could be used is if we want to describe the probability of observing m "heads" in N coin-flip experiments if the probability for observing head in a single experiment is µ.We may wish to model a continuous random variable on a finite interval.The Beta distribution is a distribution over a continuous random variable Beta distribution µ ∈ [0, 1], which is often used to represent the probability for some binary event (e.g., the parameter governing the Bernoulli distribution).The Beta distribution Beta(α, β) (illustrated in Figure 6.11) itself is governed by two parameters α > 0, β > 0 and is defined aswhere Γ(•) is the Gamma function defined asx t−1 exp(−x)dx, t > 0 .(6.100) Γ(t + 1) = tΓ(t) .(6.101)Note that the fraction of Gamma functions in (6.98) normalizes the Beta distribution.Intuitively, α moves probability mass toward 1, whereas β moves probability mass toward 0. There are some special cases (Murphy, 2012):For α = 1 = β, we obtain the uniform distribution U[0, 1].For α, β < 1, we get a bimodal distribution with spikes at 0 and 1.For α, β > 1, the distribution is unimodal.For α, β > 1 and α = β, the distribution is unimodal, symmetric, and centered in the interval [0, 1], i.e., the mode/mean is at 1 2 .Remark.There is a whole zoo of distributions with names, and they are related in different ways to each other (Leemis and McQueston, 2008).It is worth keeping in mind that each named distribution is created for a particular reason, but may have other applications.Knowing the reason behind the creation of a particular distribution often allows insight into how to best use it.We introduced the preceding three distributions to be able to illustrate the concepts of conjugacy (Section 6.6.1) and exponential families (Section 6.6.3).♢According to Bayes' theorem (6.23), the posterior is proportional to the product of the prior and the likelihood.The specification of the prior can be tricky for two reasons: First, the prior should encapsulate our knowledge about the problem before we see any data.This is often difficult to describe.Second, it is often not possible to compute the posterior distribution analytically.However, there are some priors that are computationally convenient: conjugate priors.conjugate prior Definition 6.13 (Conjugate Prior).A prior is conjugate for the likelihood conjugate function if the posterior is of the same form/type as the prior.Conjugacy is particularly convenient because we can algebraically calculate our posterior distribution by updating the parameters of the prior distribution.Remark.When considering the geometry of probability distributions, conjugate priors retain the same distance structure as the likelihood (Agarwal and Daumé III, 2010).♢ To introduce a concrete example of conjugate priors, we describe in Example 6.11 the Binomial distribution (defined on discrete random variables) and the Beta distribution (defined on continuous random variables). is the probability of finding x times the outcome "heads" in N coin flips, where µ is the probability of a "head".We place a Beta prior on the parameter µ, that is, µ ∼ Beta(α, β), whereIf we now observe some outcome x = h, that is, we see h heads in N coin flips, we compute the posterior distribution on µ as ∝ Beta(h + α, N − h + β) , (6.104d)i.e., the posterior distribution is a Beta distribution as the prior, i.e., the Beta prior is conjugate for the parameter µ in the Binomial likelihood function.In the following example, we will derive a result that is similar to the Beta-Binomial conjugacy result.Here we will show that the Beta distribution is a conjugate prior for the Bernoulli distribution.Example 6.12 (Beta-Bernoulli Conjugacy) Let x ∈ {0, 1} be distributed according to the Bernoulli distribution with parameter θ ∈ [0, 1], that is, p(x = 1 | θ) = θ.This can also be expressed as p(x | θ) = θ x (1 − θ) 1−x .Let θ be distributed according to a Beta distribution with parameters α, β, that is,Multiplying the Beta and the Bernoulli distributions, we get(6.105d)The last line is the Beta distribution with parameters (α + x, β + (1 − x)).Multinomial, inverse Gamma, inverse Wishart, and Dirichlet can be found in any statistical text, and are described in Bishop (2006), for example.The Beta distribution is the conjugate prior for the parameter µ in both the Binomial and the Bernoulli likelihood.For a Gaussian likelihood function, we can place a conjugate Gaussian prior on the mean.The reason why the Gaussian likelihood appears twice in the table is that we need to distinguish the univariate from the multivariate case.In the univariate (scalar) case, the inverse Gamma is the conjugate prior for the variance.In the multivariate case, we use a conjugate inverse Wishart distribution as a prior on the covariance matrix.The Dirichlet distribution is the conju-gate prior for the multinomial likelihood function.For further details, we refer to Bishop (2006).Recall that a statistic of a random variable is a deterministic function of that random variable.For example, if x = [x 1 , . . ., x N ] ⊤ is a vector of univariate Gaussian random variables, that is, x n ∼ N µ, σ 2 , then the sample mean μ = 1 N (x 1 + • • • + x N ) is a statistic.Sir Ronald Fisher discovered the notion of sufficient statistics: the idea that there are statistics sufficient statistics that will contain all available information that can be inferred from data corresponding to the distribution under consideration.In other words, sufficient statistics carry all the information needed to make inference about the population, that is, they are the statistics that are sufficient to represent the distribution.For a set of distributions parametrized by θ, let X be a random variable with distribution p(x | θ 0 ) given an unknown θ 0 .A vector ϕ(x) of statistics is called sufficient statistics for θ 0 if they contain all possible information about θ 0 .To be more formal about "contain all possible information", this means that the probability of x given θ can be factored into a part that does not depend on θ, and a part that depends on θ only via ϕ(x).The Fisher-Neyman factorization theorem formalizes this notion, which we state in Theorem 6.14 without proof.Theorem 6.14 (Fisher-Neyman).[Theorem 6.5 in Lehmann and Casella (1998)] Let X have probability density function p(x | θ).Then the statistics Fisher-Neyman theorem ϕ(x) are sufficient for θ if and only if p(x | θ) can be written in the form p(x | θ) = h(x)g θ (ϕ(x)) , (6.106)where h(x) is a distribution independent of θ and g θ captures all the dependence on θ via sufficient statistics ϕ(x).If p(x | θ) does not depend on θ, then ϕ(x) is trivially a sufficient statistic for any function ϕ.The more interesting case is that p(x | θ) is dependent only on ϕ(x) and not x itself.In this case, ϕ(x) is a sufficient statistic for θ.In machine learning, we consider a finite number of samples from a distribution.One could imagine that for simple distributions (such as the Bernoulli in Example 6.8) we only need a small number of samples to estimate the parameters of the distributions.We could also consider the opposite problem: If we have a set of data (a sample from an unknown distribution), which distribution gives the best fit?A natural question to ask is, as we observe more data, do we need more parameters θ to describe the distribution?It turns out that the answer is yes in general, and this is studied in non-parametric statistics (Wasserman, 2007).A converse question is to consider which class of distributions have finite-dimensional Draft (2024-01-15) of "Mathematics for Machine Learning".Feedback: https://mml-book.com.sufficient statistics, that is the number of parameters needed to describe them does not increase arbitrarily.The answer is exponential family distributions, described in the following section.There are three possible levels of abstraction we can have when considering distributions (of discrete or continuous random variables).At level one (the most concrete end of the spectrum), we have a particular named distribution with fixed parameters, for example a univariate Gaussian N 0, 1 with zero mean and unit variance.In machine learning, we often use the second level of abstraction, that is, we fix the parametric form (the univariate Gaussian) and infer the parameters from data.For example, we assume a univariate Gaussian N µ, σ 2 with unknown mean µ and unknown variance σ 2 , and use a maximum likelihood fit to determine the best parameters (µ, σ 2 ).We will see an example of this when considering linear regression in Chapter 9. A third level of abstraction is to consider families of distributions, and in this book, we consider the exponential family.The univariate Gaussian is an example of a member of the exponential family.Many of the widely used statistical models, including all the "named" models in Table 6.2, are members of the exponential family.They can all be unified into one concept (Brown, 1986).Remark.A brief historical anecdote: Like many concepts in mathematics and science, exponential families were independently discovered at the same time by different researchers.In the years 1935-1936 Pitman in Tasmania, Georges Darmois in Paris, and Bernard Koopman in New York independently showed that the exponential families are the only families that enjoy finite-dimensional sufficient statistics under repeated independent sampling (Lehmann and Casella, 1998).♢An exponential family is a family of probability distributions, parameexponential family terized by θ ∈ R D , of the form p(x | θ) = h(x) exp (⟨θ, ϕ(x)⟩ − A(θ)) , (6.107)where ϕ(x) is the vector of sufficient statistics.In general, any inner product (Section 3.2) can be used in (6.107), and for concreteness we will use the standard dot product here (⟨θ, ϕ(x)⟩ = θ ⊤ ϕ(x)).Note that the form of the exponential family is essentially a particular expression of g θ (ϕ(x)) in the Fisher-Neyman theorem (Theorem 6.14).The factor h(x) can be absorbed into the dot product term by adding another entry (log h(x)) to the vector of sufficient statistics ϕ(x), and constraining the corresponding parameter θ 0 = 1.The term A(θ) is the normalization constant that ensures that the distribution sums up or integrates to one and is called the log-partition function.A good intuitive nolog-partition function tion of exponential families can be obtained by ignoring these two terms ©2024 M. P. Deisenroth, A. A. Faisal, C. S. Ong.Published by Cambridge University Press (2020).and considering exponential families as distributions of the form p(x | θ) ∝ exp θ ⊤ ϕ(x) .(6.108)For this form of parametrization, the parameters θ are called the natural natural parameters parameters.At first glance, it seems that exponential families are a mundane transformation by adding the exponential function to the result of a dot product.However, there are many implications that allow for convenient modeling and efficient computation based on the fact that we can capture information about data in ϕ(x).Example 6.13 (Gaussian as Exponential Family)Consider the univariate Gaussian distribution N µ, σ 2 .Let ϕ(x) = x x 2 .Then by using the definition of the exponential family, p(x | θ) ∝ exp(θ 1 x + θ 2 x 2 ) .(6.109)and substituting into (6.109),we obtain (6.111)Therefore, the univariate Gaussian distribution is a member of the exponential family with sufficient statistic ϕ(x) = x x 2 , and natural parameters given by θ in (6.110).Recall the Bernoulli distribution from Example 6.8 (6.112)This can be written in exponential family form(6.113d)The last line (6.113d) can be identified as being in exponential family form (6.107) by observing that h(x) = 1 (6.114)Draft (2024-01-15) of "Mathematics for Machine Learning".Feedback: https://mml-book.com.θ = log µ 1−µ (6.115) ϕ(x) = x (6.116) A(θ) = − log(1 − µ) = log(1 + exp(θ)).(6.117)The relationship between θ and µ is invertible so that µ = 1 1 + exp(−θ).(6.118)The relation (6.118) is used to obtain the right equality of (6.117).Remark.The relationship between the original Bernoulli parameter µ and the natural parameter θ is known as the sigmoid or logistic function.Obsigmoid serve that µ ∈ (0, 1) but θ ∈ R, and therefore the sigmoid function squeezes a real value into the range (0, 1).This property is useful in machine learning, for example it is used in logistic regression (Bishop, 2006, section 4.3.2),as well as as a nonlinear activation functions in neural networks (Goodfellow et al., 2016, chapter 6).♢It is often not obvious how to find the parametric form of the conjugate distribution of a particular distribution (for example, those in Table 6.2).Exponential families provide a convenient way to find conjugate pairs of distributions.Consider the random variable X is a member of the exponential family (6.107): p(x | θ) = h(x) exp (⟨θ, ϕ(x)⟩ − A(θ)) .(6.119)Every member of the exponential family has a conjugate prior (Brown, 1986) (6.120)where γ = γ 1 γ 2 has dimension dim(θ) + 1.The sufficient statistics of the conjugate prior are θ −A(θ). By using the knowledge of the general form of conjugate priors for exponential families, we can derive functional forms of conjugate priors corresponding to particular distributions.Recall the exponential family form of the Bernoulli distribution (6.113d)(6.121)The canonical conjugate prior has the form(6.122) where we defined γ := [α, β + α] ⊤ and h c (µ) := µ/(1 − µ).Equation (6.122) then simplifies to(6.123) Putting this in non-exponential family form yields (6.124) which we identify as the Beta distribution (6.98).In example 6.12, we assumed that the Beta distribution is the conjugate prior of the Bernoulli distribution and showed that it was indeed the conjugate prior.In this example, we derived the form of the Beta distribution by looking at the canonical conjugate prior of the Bernoulli distribution in exponential family form.As mentioned in the previous section, the main motivation for exponential families is that they have finite-dimensional sufficient statistics.Additionally, conjugate distributions are easy to write down, and the conjugate distributions also come from an exponential family.From an inference perspective, maximum likelihood estimation behaves nicely because empirical estimates of sufficient statistics are optimal estimates of the population values of sufficient statistics (recall the mean and covariance of a Gaussian).From an optimization perspective, the log-likelihood function is concave, allowing for efficient optimization approaches to be applied (Chapter 7).It may seem that there are very many known distributions, but in reality the set of distributions for which we have names is quite limited.Therefore, it is often useful to understand how transformed random variables are distributed.For example, assuming that X is a random variable distributed according to the univariate normal distribution N 0, 1 , what is the distribution of X 2 ?Another example, which is quite common in machine learning, is, given that X 1 and X 2 are univariate standard normal, what is the distribution of 1 2 (X 1 + X 2 )?One option to work out the distribution of 1 2 (X 1 + X 2 ) is to calculate the mean and variance of X 1 and X 2 and then combine them.As we saw in Section 6.4.4,we can calculate the mean and variance of resulting random variables when we consider affine transformations of random vari- Draft (2024-01-15) of "Mathematics for Machine Learning".Feedback: https://mml-book.com.ables.However, we may not be able to obtain the functional form of the distribution under transformations.Furthermore, we may be interested in nonlinear transformations of random variables for which closed-form expressions are not readily available.Remark (Notation).In this section, we will be explicit about random variables and the values they take.Hence, recall that we use capital letters X, Y to denote random variables and small letters x, y to denote the values in the target space T that the random variables take.We will explicitly write pmfs of discrete random variables X as P (X = x).For continuous random variables X (Section 6.2.2), the pdf is written as f (x) and the cdf is written as F X (x).♢We will look at two approaches for obtaining distributions of transformations of random variables: a direct approach using the definition of a cumulative distribution function and a change-of-variable approach that uses the chain rule of calculus (Section 5.2.2).The change-of-variable ap-Moment generating functions can also be used to study transformations of random variables (Casella and Berger, 2002, chapter 2).proach is widely used because it provides a "recipe" for attempting to compute the resulting distribution due to a transformation.We will explain the techniques for univariate random variables, and will only briefly provide the results for the general case of multivariate random variables.Transformations of discrete random variables can be understood directly.Suppose that there is a discrete random variable X with pmf P (X = x) (Section 6.2.1), and an invertible function U (x).Consider the transformed random variable Y := U (X), with pmf P (Y = y).Then P (Y = y) = P (U (X) = y) transformation of interest (6.125a) = P (X = U −1 (y)) inverse (6.125b)where we can observe that x = U −1 (y).Therefore, for discrete random variables, transformations directly change the individual events (with the probabilities appropriately transformed).The distribution function technique goes back to first principles, and uses the definition of a cdf F X (x) = P (X ⩽ x) and the fact that its differential is the pdf f (x) (Wasserman, 2004, chapter 2).For a random variable X and a function U , we find the pdf of the random variable Y := U (X) by 1. Finding the cdf:F Y (y) = P (Y ⩽ y) (6.126)2. Differentiating the cdf F Y (y) to get the pdf f (y).f (y) = d dy F Y (y) .(6.127)We also need to keep in mind that the domain of the random variable may have changed due to the transformation by U .Let X be a continuous random variable with probability density function on 0 ⩽ x ⩽ 1 f (x) = 3x 2 .(6.128)We are interested in finding the pdf of Y = X 2 .The function f is an increasing function of x, and therefore the resulting value of y lies in the interval [0,1].We obtaindefinition of cdf (6.129a) = P (X 2 ⩽ y) transformation of interest (6.129b)In Example 6.16, we considered a strictly monotonically increasing function f (x) = 3x 2 .This means that we could compute an inverse function.Functions that have inverses are called bijective functions (Section 2.7).In general, we require that the function of interest y = U (x) has an inverse x = U −1 (y).A useful result can be obtained by considering the cumulative distribution function F X (x) of a random variable X, and using it as the transformation U (x).This leads to the following theorem.Theorem 6.15.[Theorem 2.1.10 in Casella and Berger (2002)] Let X be a continuous random variable with a strictly monotonic cumulative distribution function F X (x).Then the random variable Y defined ashas a uniform distribution.Draft (2024-01-15) of "Mathematics for Machine Learning".Feedback: https://mml-book.com.Theorem 6.15 is known as the probability integral transform, and it is probability integral transform used to derive algorithms for sampling from distributions by transforming the result of sampling from a uniform random variable (Bishop, 2006).The algorithm works by first generating a sample from a uniform distribution, then transforming it by the inverse cdf (assuming this is available) to obtain a sample from the desired distribution.The probability integral transform is also used for hypothesis testing whether a sample comes from a particular distribution (Lehmann and Romano, 2005).The idea that the output of a cdf gives a uniform distribution also forms the basis of copulas (Nelsen, 2006).The distribution function technique in Section 6.7.1 is derived from first principles, based on the definitions of cdfs and using properties of inverses, differentiation, and integration.This argument from first principles relies on two facts:1. We can transform the cdf of Y into an expression that is a cdf of X.2. We can differentiate the cdf to obtain the pdf.Let us break down the reasoning step by step, with the goal of understanding the more general change-of-variables approach in Theorem 6.16.Change of variables in probability relies on the change-of-variables method in calculus (Tandra, 2014).Remark.The name "change of variables" comes from the idea of changing the variable of integration when faced with a difficult integral.For univariate functions, we use the substitution rule of integration, f (g(x))g ′ (x)dx = f (u)du , where u = g(x) .(6.133)The derivation of this rule is based on the chain rule of calculus (5.32) and by applying twice the fundamental theorem of calculus.The fundamental theorem of calculus formalizes the fact that integration and differentiation are somehow "inverses" of each other.An intuitive understanding of the rule can be obtained by thinking (loosely) about small changes (differentials) to the equation u = g(x), that is by considering ∆u = g ′ (x)∆x as a differential of u = g(x).By substituting u = g(x), the argument inside the integral on the right-hand side of (6.133) becomes f (g(x)).By pretending that the term du can be approximated by du ≈ ∆u = g ′ (x)∆x, and that dx ≈ ∆x, we obtain (6.133).♢ Consider a univariate random variable X, and an invertible function U , which gives us another random variable Y = U (X).We assume that random variable X has states x ∈ [a, b].By the definition of the cdf, we have F Y (y) = P (Y ⩽ y) .(6.134)We are interested in a function U of the random variable P (Y ⩽ y) = P (U (X) ⩽ y) , (6.135)where we assume that the function U is invertible.An invertible function on an interval is either strictly increasing or strictly decreasing.In the case that U is strictly increasing, then its inverse U −1 is also strictly increasing.By applying the inverse U −1 to the arguments of P (U (X) ⩽ y), we obtain P (U (X) ⩽ y) = P (U −1 (U (X)) ⩽ U −1 (y)) = P (X ⩽ U −1 (y)) .(6.136)The right-most term in (6.136) is an expression of the cdf of X. Recall the definition of the cdf in terms of the pdf(6.137)Now we have an expression of the cdf of Y in terms of x:To obtain the pdf, we differentiate (6.138) with respect to y:Note that the integral on the right-hand side is with respect to x, but we need an integral with respect to y because we are differentiating with respect to y.In particular, we use (6.133) to get the substitution f (U −1 (y))U −1 ′ (y)dy = f (x)dx where x = U −1 (y) .(6.140)Using (6.140) on the right-hand side of (6.139) gives us f (y) = d dy U −1 (y) a f x (U −1 (y))U −1 ′ (y)dy .(6.141)We then recall that differentiation is a linear operator and we use the subscript x to remind ourselves that f x (U −1 (y)) is a function of x and not y.Invoking the fundamental theorem of calculus again gives usRecall that we assumed that U is a strictly increasing function.For decreasing functions, it turns out that we have a negative sign when we follow the same derivation.We introduce the absolute value of the differential to have the same expression for both increasing and decreasing U : (6.143) Draft (2024-01-15) of "Mathematics for Machine Learning".Feedback: https://mml-book.com.This is called the change-of-variable technique.The term d dy U −1 (y) in change-of-variable technique (6.143) measures how much a unit volume changes when applying U (see also the definition of the Jacobian in Section 5.3).Remark.In comparison to the discrete case in (6.125b), we have an additional factor d dy U −1 (y) .The continuous case requires more care because P (Y = y) = 0 for all y.The probability density function f (y) does not have a description as a probability of an event involving y. ♢ So far in this section, we have been studying univariate change of variables.The case for multivariate random variables is analogous, but complicated by fact that the absolute value cannot be used for multivariate functions.Instead, we use the determinant of the Jacobian matrix.Recall from (5.58) that the Jacobian is a matrix of partial derivatives, and that the existence of a nonzero determinant shows that we can invert the Jacobian.Recall the discussion in Section 4.1 that the determinant arises because our differentials (cubes of volume) are transformed into parallelepipeds by the Jacobian.Let us summarize preceding the discussion in the following theorem, which gives us a recipe for multivariate change of variables.Theorem 6.16.[Theorem 17.2 in Billingsley (1995)] Let f (x) be the value of the probability density of the multivariate continuous random variable X.If the vector-valued function y = U (x) is differentiable and invertible for all values within the domain of x, then for corresponding values of y, the probability density of Y = U (X) is given byThe theorem looks intimidating at first glance, but the key point is that a change of variable of a multivariate random variable follows the procedure of the univariate change of variable.First we need to work out the inverse transform, and substitute that into the density of x.Then we calculate the determinant of the Jacobian and multiply the result.The following example illustrates the case of a bivariate random variable.Consider a bivariate random variable X with states x =x 1 x 2 and probability density function effect of a linear transformation (Section 2.7) of the random variable.Consider a matrix A ∈ R 2×2 defined asWe are interested in finding the probability density function of the transformed bivariate random variable Y with states y = Ax.Recall that for change of variables we require the inverse transformation of x as a function of y.Since we consider linear transformations, the inverse transformation is given by the matrix inverse (see Section 2.2.2).For 2 × 2 matrices, we can explicitly write out the formula, given byObserve that ad − bc is the determinant (Section 4.1) of A. The corresponding probability density function is given byThe partial derivative of a matrix times a vector with respect to the vector is the matrix itself (Section 5.5), and thereforeRecall from Section 4.1 that the determinant of the inverse is the inverse of the determinant so that the determinant of the Jacobian matrix is .150)We are now able to apply the change-of-variable formula from Theorem 6.16 by multiplying (6.148) with (6.150), which yieldsWhile Example 6.17 is based on a bivariate random variable, which allows us to easily compute the matrix inverse, the preceding relation holds for higher dimensions.Remark.We saw in Section 6.5 that the density f (x) in (6.148) is actually the standard Gaussian distribution, and the transformed density f (y) is a bivariate Gaussian with covariance Σ = AA ⊤ .♢We will use the ideas in this chapter to describe probabilistic modeling Draft (2024-01-15) of "Mathematics for Machine Learning".Feedback: https://mml-book.com.in Section 8.4, as well as introduce a graphical language in Section 8.5.We will see direct machine learning applications of these ideas in Chapters 9 and 11.This chapter is rather terse at times.Grinstead and Snell (1997) and Walpole et al. (2011) provide more relaxed presentations that are suitable for self-study.Readers interested in more philosophical aspects of probability should consider Hacking (2001), whereas an approach that is more related to software engineering is presented by Downey (2014).An overview of exponential families can be found in Barndorff-Nielsen (2014).We will see more about how to use probability distributions to model machine learning tasks in Chapter 8. Ironically, the recent surge in interest in neural networks has resulted in a broader appreciation of probabilistic models.For example, the idea of normalizing flows (Jimenez Rezende and Mohamed, 2015) relies on change of variables for transforming random variables.An overview of methods for variational inference as applied to neural networks is described in chapters 16 to 20 of the book by Goodfellow et al. (2016).We side stepped a large part of the difficulty in continuous random variables by avoiding measure theoretic questions (Billingsley, 1995;Pollard, 2002), and by assuming without construction that we have real numbers, and ways of defining sets on real numbers as well as their appropriate frequency of occurrence.These details do matter, for example, in the specification of conditional probability p(y | x) for continuous random variables x, y (Proschan and Presnell, 1998).The lazy notation hides the fact that we want to specify that X = x (which is a set of measure zero).Furthermore, we are interested in the probability density function of y.A more precise notation would have to say E y [f (y) | σ(x)], where we take the expectation over y of a test function f conditioned on the σ-algebra of x.A more technical audience interested in the details of probability theory have many options (Jaynes, 2003;MacKay, 2003;Jacod and Protter, 2004;Grimmett and Welsh, 2014), including some very technical discussions (Shiryayev, 1984;Lehmann and Casella, 1998;Dudley, 2002;Bickel and Doksum, 2006;C ¸inlar, 2011).An alternative way to approach probability is to start with the concept of expectation, and "work backward" to derive the necessary properties of a probability space (Whittle, 2000).As machine learning allows us to model more intricate distributions on ever more complex types of data, a developer of probabilistic machine learning models would have to understand these more technical aspects.Machine learning texts with a probabilistic modeling focus include the books by MacKay (2003); Bishop (2006); Rasmussen and Williams (2006); Barber (2012); Murphy (2012).b.The conditional distributions p(x|Y = y 1 ) and p(y|X = x 3 ).6.2 Consider a mixture of two Gaussian distributions (illustrated in Figure 6.4),a. Compute the marginal distributions for each dimension.b.Compute the mean, mode and median for each marginal distribution.c.Compute the mean and mode for the two-dimensional distribution.6.3 You have written a computer program that sometimes compiles and sometimes not (code does not change).You decide to model the apparent stochasticity (success vs. no success) x of the compiler using a Bernoulli distribution with parameter µ:Choose a conjugate prior for the Bernoulli likelihood and compute the posterior distribution p(µ | x 1 , . . ., x N ).6.4There are two bags.The first bag contains four mangos and two apples; the second bag contains four mangos and four apples.We also have a biased coin, which shows "heads" with probability 0.6 and "tails" with probability 0.4.If the coin shows "heads".we pick a fruit at random from bag 1; otherwise we pick a fruit at random from bag 2. Your friend flips the coin (you cannot see the result), picks a fruit at random from the corresponding bag, and presents you a mango.What is the probability that the mango was picked from bag 2? Hint: Use Bayes' theorem.6.5 Consider the time-series modelwhere w, v are i.i.d.Gaussian noise variables.Further, assume that p(x 0 ) = N µ 0 , Σ 0 .Draft (2024-01-15) of "Mathematics for Machine Learning".Feedback: https://mml-book.com.a. What is the form of p(x 0 , x 1 , . . ., x T )? Justify your answer (you do not have to explicitly compute the joint distribution).b.Assume that p(x t | y 1 , . . ., y t ) = N µ t , Σ t .1. Compute p(x t+1 | y 1 , . . ., y t ). 2. Compute p(x t+1 , y t+1 | y 1 , . . ., y t ). 3. At time t+1, we observe the value y t+1 = ŷ.Compute the conditional distribution p(x t+1 | y 1 , . . ., y t+1 ).6.6 Prove the relationship in (6.44), which relates the standard definition of the variance to the raw-score expression for the variance.6.7 Prove the relationship in (6.45), which relates the pairwise difference between examples in a dataset with the raw-score expression for the variance.6.8 Express the Bernoulli distribution in the natural parameter form of the exponential family, see (6.107).6.9 Express the Binomial distribution as an exponential family distribution.Also express the Beta distribution is an exponential family distribution.Show that the product of the Beta and the Binomial distribution is also a member of the exponential family.6.10 Derive the relationship in Section 6.5.2 in two ways: a.By completing the square b.By expressing the Gaussian in its exponential family formThe product of two GaussiansNote that the normalizing constant c itself can be considered a (normalized) Gaussian distribution either in a or in b with an "inflated" covariance matrixConsider two random variables x, y with joint distribution p(x, y).Show thatHere, E X [x | y] denotes the expected value of x under the conditional distribution p(x | y).6.12 Manipulation of Gaussian Random Variables.Consider a Gaussian random variable x ∼ N x | µ x , Σx , where x ∈ R D .Furthermore, we have c.The random variable y is being transformed according to the measurement mappingWrite down p(z | y).Compute p(z), i.e., the mean µ z and the covariance Σz.Derive your result in detail.d. Now, a value ŷ is measured.Compute the posterior distribution p(x | ŷ).Hint for solution: This posterior is also Gaussian, i.e., we need to determine only its mean and covariance matrix.Start by explicitly computing the joint Gaussian p(x, y).This also requires us to compute the cross-covariances Covx,y[x, y] and Covy,x[y, x].Then apply the rules for Gaussian conditioning.Given a continuous random variable X, with cdf F X (x), show that the random variable Y := F X (X) is uniformly distributed (Theorem 6.15).Since machine learning algorithms are implemented on a computer, the mathematical formulations are expressed as numerical optimization methods.This chapter describes the basic numerical methods for training machine learning models.Training a machine learning model often boils down to finding a good set of parameters.The notion of "good" is determined by the objective function or the probabilistic model, which we will see examples of in the second part of this book.Given an objective function, finding the best value is done using optimization algorithms.This chapter covers two main branches of continuous optimization (Figure 7.1): unconstrained and constrained optimization.We will assume in this chapter that our objective function is differentiable (see Chapter 5), hence we have access to a gradient at each location in the space to help us find the optimum value.By convention, most objective functions in machine learning are intended to be minimized, that is, the best value is the minimum value.Intuitively finding the best value is like finding the valleys of the objective function, and the gradients point us uphill.The idea is to move downhill (opposite to the gradient) and hope to find the deepest point.For unconstrained optimization, this is the only concept we need, but there are several design choices, which we discuss in Section 7.1.For constrained optimization, we need to introduce other concepts to manage the constraints (Section 7.2).We will also introduce a special class of problems (convex optimization problems in Section 7.3) where we can make statements about reaching the global optimum.Consider the function in Figure 7.2.The function has a global minimum global minimum around x = −4.5, with a function value of approximately −47.Since the function is "smooth," the gradients can be used to help find the minimum by indicating whether we should take a step to the right or left.This assumes that we are in the correct bowl, as there exists another local local minimum minimum around x = 0.7.Recall that we can solve for all the stationary points of a function by calculating its derivative and setting it to zero.For Stationary points are the real roots of the derivative, that is, points that have zero gradient.we obtain the corresponding gradient as Since this is a cubic equation, it has in general three solutions when set to zero.In the example, two of them are minimums and one is a maximum (around x = −1.4).To check whether a stationary point is a minimum or maximum, we need to take the derivative a second time and check whether the second derivative is positive or negative at the stationary point.In our case, the second derivative isBy substituting our visually estimated values of x = −4.5, −1.4,0.7, we will observe that as expected the middle point is a maximum d 2 ℓ(x) dx 2 < 0 and the other two stationary points are minimums.Note that we have avoided analytically solving for values of x in the previous discussion, although for low-order polynomials such as the preceding we could do so.In general, we are unable to find analytic solutions, and hence we need to start at some value, say x 0 = −6, and follow the negative gradient.The negative gradient indicates that we should go right, but not how far (this is called the step-size).Furthermore, if we According to the Abel-Ruffini theorem, there is in general no algebraic solution for polynomials of degree 5 or more (Abel, 1826).had started at the right side (e.g., x 0 = 0) the negative gradient would have led us to the wrong minimum.Figure 7.2 illustrates the fact that for x > −1, the negative gradient points toward the minimum on the right of the figure, which has a larger objective value.In Section 7.3, we will learn about a class of functions, called convex functions, that do not exhibit this tricky dependency on the starting point of the optimization algorithm.For convex functions, all local minimums are global minimum.It turns out that many machine learning objective For convex functions all local minima are global minimum.functions are designed such that they are convex, and we will see an example in Chapter 12.The discussion in this chapter so far was about a one-dimensional function, where we are able to visualize the ideas of gradients, descent directions, and optimal values.In the rest of this chapter we develop the same ideas in high dimensions.Unfortunately, we can only visualize the concepts in one dimension, but some concepts do not generalize directly to higher dimensions, therefore some care needs to be taken when reading.We now consider the problem of solving for the minimum of a real-valued function (7.4) where f : R d → R is an objective function that captures the machine learning problem at hand.We assume that our function f is differentiable, and we are unable to analytically find a solution in closed form.Gradient descent is a first-order optimization algorithm.To find a local minimum of a function using gradient descent, one takes steps proportional to the negative of the gradient of the function at the current point.Recall from Section 5.1 that the gradient points in the direction of the We use the convention of row vectors for gradients.steepest ascent.Another useful intuition is to consider the set of lines where the function is at a certain value (f (x) = c for some value c ∈ R), which are known as the contour lines.The gradient points in a direction that is orthogonal to the contour lines of the function we wish to optimize.Let us consider multivariate functions.Imagine a surface (described by the function f (x)) with a ball starting at a particular location x 0 .When the ball is released, it will move downhill in the direction of steepest descent.Gradient descent exploits the fact that f (x 0 ) decreases fastest if one moves from x 0 in the direction of the negative gradient −((∇f )(x 0 )) ⊤ of f at x 0 .We assume in this book that the functions are differentiable, and refer the reader to more general settings in Section 7.4.Then, iffor a small step-size γ ⩾ 0, then f (x 1 ) ⩽ f (x 0 ).Note that we use the transpose for the gradient since otherwise the dimensions will not work out.This observation allows us to define a simple gradient descent algorithm: If we want to find a local optimum f (x * ) of a function f : R n → R, x → f (x), we start with an initial guess x 0 of the parameters we wish to optimize and then iterate according to(7.6)For suitable step-size γ i , the sequence f (x 0 ) ⩾ f (x 1 ) ⩾ . . .converges to a local minimum.Consider a quadratic function in two dimensionsStarting at the initial location x 0 = [−3, −1] ⊤ , we iteratively apply (7.6) to obtain a sequence of estimates that converge to the minimum value Draft (2024-01-15) of "Mathematics for Machine Learning".Feedback: https://mml-book.com.(illustrated in Figure 7.3).We can see (both from the figure and by plugging x 0 into (7.8) with γ = 0.085) that the negative gradient at x 0 points north and east, leading to x 1 = [−1.98,1.21] ⊤ .Repeating that argument gives us x 2 = [−1.32,−0.42] ⊤ , and so on.Remark.Gradient descent can be relatively slow close to the minimum: Its asymptotic rate of convergence is inferior to many other methods.Using the ball rolling down the hill analogy, when the surface is a long, thin valley, the problem is poorly conditioned (Trefethen and Bau III, 1997).For poorly conditioned convex problems, gradient descent increasingly "zigzags" as the gradients point nearly orthogonally to the shortest direction to a minimum point; see Figure 7.3.♢As mentioned earlier, choosing a good step-size is important in gradient descent.If the step-size is too small, gradient descent can be slow.If the The step-size is also called the learning rate.step-size is chosen too large, gradient descent can overshoot, fail to converge, or even diverge.We will discuss the use of momentum in the next section.It is a method that smoothes out erratic behavior of gradient updates and dampens oscillations.Adaptive gradient methods rescale the step-size at each iteration, depending on local properties of the function.There are two simple heuristics (Toussaint, 2012):When the function value increases after a gradient step, the step-size was too large.Undo the step and decrease the step-size.When the function value decreases the step could have been larger.Try to increase the step-size.Although the "undo" step seems to be a waste of resources, using this heuristic guarantees monotonic convergence.When we solve linear equations of the form Ax = b, in practice we solve Ax−b = 0 approximately by finding x * that minimizes the squared errorif we use the Euclidean norm.The gradient of (7.9) with respect to x isWe can use this gradient directly in a gradient descent algorithm.However, for this particular special case, it turns out that there is an analytic solution, which can be found by setting the gradient to zero.We will see more on solving squared error problems in Chapter 9.Remark.When applied to the solution of linear systems of equations Ax = b, gradient descent may converge slowly.The speed of convergence of gradient descent is dependent on the condition number κ = σ(A)max σ(A)min , which condition number is the ratio of the maximum to the minimum singular value (Section 4.5) of A. The condition number essentially measures the ratio of the most curved direction versus the least curved direction, which corresponds to our imagery that poorly conditioned problems are long, thin valleys: They are very curved in one direction, but very flat in the other.Instead of directly solving Ax = b, one could instead solve P −1 (Ax − b) = 0, where P is called the preconditioner.The goal is to design P −1 such that P −1 A preconditioner has a better condition number, but at the same time P −1 is easy to compute.For further information on gradient descent, preconditioning, and convergence we refer to Boyd and Vandenberghe (2004, chapter 9).♢As illustrated in Figure 7.3, the convergence of gradient descent may be very slow if the curvature of the optimization surface is such that there are regions that are poorly scaled.The curvature is such that the gradient descent steps hops between the walls of the valley and approaches the optimum in small steps.The proposed tweak to improve convergence is to give gradient descent some memory.Goh (2017) wrote an intuitive blog post on gradient descent with momentum.Gradient descent with momentum (Rumelhart et al., 1986) is a method that introduces an additional term to remember what happened in the previous iteration.This memory dampens oscillations and smoothes out the gradient updates.Continuing the ball analogy, the momentum term emulates the phenomenon of a heavy ball that is reluctant to change directions.The idea is to have a gradient update with memory to implement Draft (2024-01-15) of "Mathematics for Machine Learning".Feedback: https://mml-book.com.a moving average.The momentum-based method remembers the update ∆x i at each iteration i and determines the next update as a linear combination of the current and previous gradients (7.12)where α ∈ [0, 1].Sometimes we will only know the gradient approximately.In such cases, the momentum term is useful since it averages out different noisy estimates of the gradient.One particularly useful way to obtain an approximate gradient is by using a stochastic approximation, which we discuss next.Computing the gradient can be very time consuming.However, often it is possible to find a "cheap" approximation of the gradient.Approximating the gradient is still useful as long as it points in roughly the same direction as the true gradient.Stochastic gradient descent (often shortened as SGD) is a stochastic approximation of the gradient descent method for minimizing an objective function that is written as a sum of differentiable functions.The word stochastic here refers to the fact that we acknowledge that we do not know the gradient precisely, but instead only know a noisy approximation to it.By constraining the probability distribution of the approximate gradients, we can still theoretically guarantee that SGD will converge.In machine learning, given n = 1, . . ., N data points, we often consider objective functions that are the sum of the losses L n incurred by each example n.In mathematical notation, we have the form (7.13) where θ is the vector of parameters of interest, i.e., we want to find θ that minimizes L.An example from regression (Chapter 9) is the negative loglikelihood, which is expressed as a sum over log-likelihoods of individual examples so that Standard gradient descent, as introduced previously, is a "batch" optimization method, i.e., optimization is performed using the full training set by updating the vector of parameters according tofor a suitable step-size parameter γ i .Evaluating the sum gradient may require expensive evaluations of the gradients from all individual functions L n .When the training set is enormous and/or no simple formulas exist, evaluating the sums of gradients becomes very expensive.Consider the term N n=1 (∇L n (θ i )) in (7.15).We can reduce the amount of computation by taking a sum over a smaller set of L n .In contrast to batch gradient descent, which uses all L n for n = 1, . . ., N , we randomly choose a subset of L n for mini-batch gradient descent.In the extreme case, we randomly select only a single L n to estimate the gradient.The key insight about why taking a subset of data is sensible is to realize that for gradient descent to converge, we only require that the gradient is an unbiased estimate of the true gradient.In fact the term N n=1 (∇L n (θ i )) in (7.15) is an empirical estimate of the expected value (Section 6.4.1) of the gradient.Therefore, any other unbiased empirical estimate of the expected value, for example using any subsample of the data, would suffice for convergence of gradient descent.Remark.When the learning rate decreases at an appropriate rate, and subject to relatively mild assumptions, stochastic gradient descent converges almost surely to local minimum (Bottou, 1998).Why should one consider using an approximate gradient?A major reason is practical implementation constraints, such as the size of central processing unit (CPU)/graphics processing unit (GPU) memory or limits on computational time.We can think of the size of the subset used to estimate the gradient in the same way that we thought of the size of a sample when estimating empirical means (Section 6.4.1).Large mini-batch sizes will provide accurate estimates of the gradient, reducing the variance in the parameter update.Furthermore, large mini-batches take advantage of highly optimized matrix operations in vectorized implementations of the cost and gradient.The reduction in variance leads to more stable convergence, but each gradient calculation will be more expensive.In contrast, small mini-batches are quick to estimate.If we keep the mini-batch size small, the noise in our gradient estimate will allow us to get out of some bad local optima, which we may otherwise get stuck in.In machine learning, optimization methods are used for training by minimizing an objective function on the training data, but the overall goal is to improve generalization performance (Chapter 8).Since the goal in machine learning does not necessarily need a precise estimate of the minimum of the objective function, approximate gradients using mini-batch approaches have been widely used.Stochastic gradient descent is very effective in large-scale machine learning problems (Bottou et al., 2018),such as training deep neural networks on millions of images (Dean et al., 2012), topic models (Hoffman et al., 2013), reinforcement learning (Mnih et al., 2015), or training of large-scale Gaussian process models (Hensman et al., 2013;Gal et al., 2014).In the previous section, we considered the problem of solving for the minimum of a function (7.16)where f : R D → R.In this section, we have additional constraints.That is, for real-valued functions g i : R D → R for i = 1, . . ., m, we consider the constrained optimization problem (see Figure 7.4 for an illustration)It is worth pointing out that the functions f and g i could be non-convex in general, and we will consider the convex case in the next section.One obvious, but not very practical, way of converting the constrained problem (7.17) into an unconstrained one is to use an indicator function (7.18)where 1(z) is an infinite step functionThis gives infinite penalty if the constraint is not satisfied, and hence would provide the same solution.However, this infinite step function is equally difficult to optimize.We can overcome this difficulty by introducing Lagrange multipliers.The idea of Lagrange multipliers is to replace the Lagrange multiplier step function with a linear function.We associate to problem (7.17) the Lagrangian by introducing the La-Lagrangian grange multipliers λ i ⩾ 0 corresponding to each inequality constraint respectively (Boyd and Vandenberghe, 2004, chapter 4) so that (7.20b) where in the last line we have concatenated all constraints g i (x) into a vector g(x), and all the Lagrange multipliers into a vector λ ∈ R m .We now introduce the idea of Lagrangian duality.In general, duality in optimization is the idea of converting an optimization problem in one set of variables x (called the primal variables), into another optimization problem in a different set of variables λ (called the dual variables).We introduce two different approaches to duality: In this section, we discuss Lagrangian duality; in Section 7.3.3,we discuss Legendre-Fenchel duality.Definition 7.1.The problem in (7.17)is known as the primal problem, corresponding to the primal variables x.Remark.In the discussion of Definition 7.1, we use two concepts that are also of independent interest (Boyd and Vandenberghe, 2004).First is the minimax inequality, which says that for any function with minimax inequality two arguments φ(x, y), the maximin is less than the minimax, i.e., Note that taking the maximum over y of the left-hand side of (7.24) maintains the inequality since the inequality is true for all y.Similarly, we can take the minimum over x of the right-hand side of (7.24) to obtain (7.23).The second concept is weak duality, which uses (7.23) to show that weak duality primal values are always greater than or equal to dual values.This is described in more detail in (7.27).♢Recall that the difference between J(x) in (7.18) and the Lagrangian in (7.20b) is that we have relaxed the indicator function to a linear function.Therefore, when λ ⩾ 0, the Lagrangian L(x, λ) is a lower bound of J(x).Hence, the maximum of L(x, λ) with respect to λ isRecall that the original problem was minimizing J(x),By the minimax inequality (7.23), it follows that swapping the order of the minimum and maximum results in a smaller value, i.e.,This is also known as weak duality.Note that the inner part of the rightweak duality hand side is the dual objective function D(λ) and the definition follows.In contrast to the original optimization problem, which has constraints, min x∈R d L(x, λ) is an unconstrained optimization problem for a given value of λ.If solving min x∈R d L(x, λ) is easy, then the overall problem is easy to solve.We can see this by observing from (7.20b) that L(x, λ) is affine with respect to λ. Therefore min x∈R d L(x, λ) is a pointwise minimum of affine functions of λ, and hence D(λ) is concave even though f (•) and g i (•) may be nonconvex.The outer problem, maximization over λ, is the maximum of a concave function and can be efficiently computed.Assuming f (•) and g i (•) are differentiable, we find the Lagrange dual problem by differentiating the Lagrangian with respect to x, setting the differential to zero, and solving for the optimal value.We will discuss two concrete examples in Sections 7.3.1 and 7.3.2,where f (•) and g i (•) are convex.Remark (Equality Constraints).Consider (7.17) with additional equality constraints minsubject to g i (x) ⩽ 0 for all i = 1, . . ., m h j (x) = 0 for all j = 1, . . ., n .(7.28)We can model equality constraints by replacing them with two inequality constraints.That is for each equality constraint h j (x) = 0 we equivalently replace it by two constraints h j (x) ⩽ 0 and h j (x) ⩾ 0. It turns out that the resulting Lagrange multipliers are then unconstrained.Therefore, we constrain the Lagrange multipliers corresponding to the inequality constraints in (7.28) to be non-negative, and leave the Lagrange multipliers corresponding to the equality constraints unconstrained.♢We focus our attention of a particularly useful class of optimization problems, where we can guarantee global optimality.Convex sets are sets such that a straight line connecting any two elements of the set lie inside the set.Figures 7.5 and 7.6 illustrate convex and nonconvex sets, respectively.(7.30)The constraints involving g(•) and h(•) in (7.28) truncate functions at a scalar value, resulting in sets.Another relation between convex functions and convex sets is to consider the set obtained by "filling in" a convex function.A convex function is a bowl-like object, and we imagine pouring water into it to fill it up.This resulting filled-in set, called the epigraph of epigraph the convex function, is a convex set.If a function f : R n → R is differentiable, we can specify convexity in Draft (2024-01-15) of "Mathematics for Machine Learning".Feedback: https://mml-book.com.terms of its gradient ∇ x f (x) (Section 5.2).A function f (x) is convex if and only if for any two points x, y it holds that(7.31)If we further know that a function f (x) is twice differentiable, that is, the Hessian (5.147) exists for all values in the domain of x, then the function f (x) is convex if and only if ∇ 2 x f (x) is positive semidefinite (Boyd and Vandenberghe, 2004).The negative entropy f (x) = x log 2 x is convex for x > 0. A visualization of the function is shown in Figure 7.8, and we can see that the function is convex.To illustrate the previous definitions of convexity, let us check the calculations for two points x = 2 and x = 4.Note that to prove convexity of f (x) we would need to check for all points x ∈ R.Recall Definition 7.3.Consider a point midway between the two points (that is θ = 0.5); then the left-hand side is f (0.5 • 2 + 0.5 • 4) = 3 log 2 3 ≈ 4.75.The right-hand side is 0.5(2 log 2 2) + 0.5(4 log 2 4) = 1 + 4 = 5.And therefore the definition is satisfied.Since f (x) is differentiable, we can alternatively use (7.31).Calculating the derivative of f (x), we obtainUsing the same two test points x = 2 and x = 4, the left-hand side of (7.31) is given by f (4) = 8.The right-hand side is) • 2 ≈ 6.9 .(7.33b)We can check that a function or set is convex from first principles by recalling the definitions.In practice, we often rely on operations that preserve convexity to check that a particular function or set is convex.Although the details are vastly different, this is again the idea of closure that we introduced in Chapter 2 for vector spaces.A nonnegative weighted sum of convex functions is convex.Observe that if f is a convex function, and α ⩾ 0 is a nonnegative scalar, then the function αf is convex.We can see this by multiplying α to both sides of the equation in Definition 7.3, and recalling that multiplying a nonnegative number does not change the inequality.If f 1 and f 2 are convex functions, then we have by the definitionSumming up both sides gives us (7.36)where the right-hand side can be rearranged to (7.37) completing the proof that the sum of convex functions is convex.Combining the preceding two facts, we see that αf 1 (x) + βf 2 (x) is convex for α, β ⩾ 0. This closure property can be extended using a similar argument for nonnegative weighted sums of more than two convex functions.Remark.The inequality in (7.30) is sometimes called Jensen's inequality.Jensen's inequality In fact, a whole class of inequalities for taking nonnegative weighted sums of convex functions are all called Jensen's inequality.♢In summary, a constrained optimization problem is called a convex opticonvex optimization problem mization problem ifsubject to g i (x) ⩽ 0 for all i = 1, . . ., m h j (x) = 0 for all j = 1, . . ., n ,where all functions f (x) and g i (x) are convex functions, and all h j (x) = 0 are convex sets.In the following, we will describe two classes of convex optimization problems that are widely used and well understood.Consider the special case when all the preceding functions are linear, i.e., min variables and m linear constraints.The Lagrangian is given by (7.40) where λ ∈ R m is the vector of non-negative Lagrange multipliers.Rearranging the terms corresponding to x yieldsTaking the derivative of L(x, λ) with respect to x and setting it to zero gives usRecall we would like to maximize D(λ).In addition to the constraint due to the derivative of L(x, λ) being zero, we also have the fact that λ ⩾ 0, resulting in the following dual optimization problemIt is convention to minimize the primal and maximize the dual.maxwhether m or d is larger.Recall that d is the number of variables and m is the number of constraints in the primal linear program.Consider the linear programwith two variables.This program is also shown in Figure 7.9.The objective function is linear, resulting in linear contour lines.The constraint set in standard form is translated into the legend.The optimal value must lie in the shaded (feasible) region, and is indicated by the star.Draft (2024-01-15) of "Mathematics for Machine Learning".Feedback: https://mml-book.com.Consider the case of a convex quadratic objective function, where the constraints are affine, i.e., minof two variables.The program is also illustrated in Figure 7.4.The objective function is quadratic with a positive semidefinite matrix Q, resulting in elliptical contour lines.The optimal value must lie in the shaded (feasible) region, and is indicated by the star.The Lagrangian is given bywhere again we have rearranged the terms.Taking the derivative of L(x, λ) with respect to x and setting it to zero givesSince Q is positive definite and therefore invertible, we get(7.50) Substituting (7.50) into the primal Lagrangian L(x, λ), we get the dual LagrangianTherefore, the dual optimization problem is given bysubject to λ ⩾ 0 .(7.52)We will see an application of quadratic programming in machine learning in Chapter 12.Let us revisit the idea of duality from Section 7.2, without considering constraints.One useful fact about a convex set is that it can be equivalently described by its supporting hyperplanes.A hyperplane is called a supporting hyperplane of a convex set if it intersects the convex set, and supporting hyperplane the convex set is contained on just one side of it.Recall that we can fill up a convex function to obtain the epigraph, which is a convex set.Therefore, we can also describe convex functions in terms of their supporting hyperplanes.Furthermore, observe that the supporting hyperplane just touches the convex function, and is in fact the tangent to the function at that point.And recall that the tangent of a function f (x) at a given point x 0 is the evaluation of the gradient of that function at that point df (x) dx x=x0. In summary, because convex sets can be equivalently described by their supporting hyperplanes, convex functions can be equivalently described by a function of their gradient.The Legendre transform formalizes this concept.Physics students are often introduced to the Legendre transform as relating the Lagrangian and the Hamiltonian in classical mechanics.We begin with the most general definition, which unfortunately has a counter-intuitive form, and look at special cases to relate the definition to the intuition described in the preceding paragraph.The Legendre-Fenchel Legendre-Fenchel transform transform is a transformation (in the sense of a Fourier transform) from a convex differentiable function f (x) to a function that depends on the tangents s(x) = ∇ x f (x).It is worth stressing that this is a transformation of the function f (•) and not the variable x or the function evaluated at x.The Legendre-Fenchel transform is also known as the convex conjugate (for convex conjugate reasons we will see soon) and is closely related to duality (Hiriart-Urruty and Lemaréchal, 2001, chapter 5).(7.53)Note that the preceding convex conjugate definition does not need the function f to be convex nor differentiable.In Definition 7.4, we have used a general inner product (Section 3.2) but in the rest of this section we Draft (2024-01-15) of "Mathematics for Machine Learning".Feedback: https://mml-book.com.will consider the standard dot product between finite-dimensional vectors (⟨s, x⟩ = s ⊤ x) to avoid too many technical details.To understand Definition 7.4 in a geometric fashion, consider a nice This derivation is easiest to understand by drawing the reasoning as it progresses.simple one-dimensional convex and differentiable function, for example f (x) = x 2 .Note that since we are looking at a one-dimensional problem, hyperplanes reduce to a line.Consider a line y = sx+c.Recall that we are able to describe convex functions by their supporting hyperplanes, so let us try to describe this function f (x) by its supporting lines.Fix the gradient of the line s ∈ R and for each point (x 0 , f (x 0 )) on the graph of f , find the minimum value of c such that the line still intersects (x 0 , f (x 0 )).Note that the minimum value of c is the place where a line with slope s "just touches" the function f (x) = x 2 .The line passing through (x 0 , f (x 0 )) with gradient s is given byThe y-intercept of this line is −sx 0 + f (x 0 ).The minimum of c for which y = sx + c intersects with the graph of f is thereforeThe preceding convex conjugate is by convention defined to be the negative of this.The reasoning in this paragraph did not rely on the fact that we chose a one-dimensional convex and differentiable function, and holds for f : R D → R, which are nonconvex and non-differentiable.The classical Legendre transform is defined on convex differentiable functions in R D .Remark.Convex differentiable functions such as the example f (x) = x 2 is a nice special case, where there is no need for the supremum, and there is a one-to-one correspondence between a function and its Legendre transform.Let us derive this from first principles.For a convex differentiable function, we know that at x 0 the tangent touches f (x 0 ) so thatRecall that we want to describe the convex function f (x) in terms of its gradient ∇ x f (x), and that s = ∇ x f (x 0 ).We rearrange to get an expression for −c to obtain − c = sx 0 − f (x 0 ) .(7.57)Note that −c changes with x 0 and therefore with s, which is why we can think of it as a function of s, which we callComparing (7.58) with Definition 7.4, we see that (7.58) is a special case (without the supremum).♢The conjugate function has nice properties; for example, for convex functions, applying the Legendre transform again gets us back to the original function.In the same way that the slope of f (x) is s, the slope of f * (s) is x.The following two examples show common uses of convex conjugates in machine learning.To illustrate the application of convex conjugates, consider the quadratic functionbased on a positive definite matrix K ∈ R n×n .We denote the primal variable to be y ∈ R n and the dual variable to be α ∈ R n .Applying Definition 7.4, we obtain the functionSince the function is differentiable, we can find the maximum by taking the derivative and with respect to y setting it to zero.and hence when the gradient is zero we have y = 1 λ Kα.Substituting into (7.60)yields(7.62)In machine learning, we often use sums of functions; for example, the objective function of the training set includes a sum of the losses for each example in the training set.In the following, we derive the convex conjugate of a sum of losses ℓ(t), where ℓ : R → R.This also illustrates the application of the convex conjugate to the vector case.LetDraft (2024-01-15) of "Mathematics for Machine Learning".Feedback: https://mml-book.com.Recall that in Section 7.2 we derived a dual optimization problem using Lagrange multipliers.Furthermore, for convex optimization problems we have strong duality, that is the solutions of the primal and dual problem match.The Legendre-Fenchel transform described here also can be used to derive a dual optimization problem.Furthermore, when the function is convex and differentiable, the supremum is unique.To further investigate the relation between these two approaches, let us consider a linear equality constrained convex optimization problem.Example 7.9 Let f (y) and g(x) be convex functions, and A a real matrix of appropriate dimensions such that Ax = y.Then(7.64)By introducing the Lagrange multiplier u for the constraints Ax = y,where the last step of swapping max and min is due to the fact that f (y) and g(x) are convex functions.By splitting up the dot product term and collecting x and y,Recall the convex conjugate (Definition 7.4) and the fact that dot prod-For general inner products, A ⊤ is replaced by the adjoint A * .ucts are symmetric,Therefore, we have shown thatThe Legendre-Fenchel conjugate turns out to be quite useful for machine learning problems that can be expressed as convex optimization problems.In particular, for convex loss functions that apply independently to each example, the conjugate loss is a convenient way to derive a dual problem.Continuous optimization is an active area of research, and we do not try to provide a comprehensive account of recent advances.From a gradient descent perspective, there are two major weaknesses which each have their own set of literature.The first challenge is the fact that gradient descent is a first-order algorithm, and does not use information about the curvature of the surface.When there are long valleys, the gradient points perpendicularly to the direction of interest.The idea of momentum can be generalized to a general class of acceleration methods (Nesterov, 2018).Conjugate gradient methods avoid the issues faced by gradient descent by taking previous directions into account (Shewchuk, 1994).Second-order methods such as Newton methods use the Hessian to provide information about the curvature.Many of the choices for choosing step-sizes and ideas like momentum arise by considering the curvature of the objective function (Goh, 2017;Bottou et al., 2018).Quasi-Newton methods such as L-BFGS try to use cheaper computational methods to approximate the Hessian (Nocedal and Wright, 2006).Recently there has been interest in other metrics for computing descent directions, resulting in approaches such as mirror descent (Beck and Teboulle, 2003) and natural gradient (Toussaint, 2012).The second challenge is to handle non-differentiable functions.Gradient methods are not well defined when there are kinks in the function.In these cases, subgradient methods can be used (Shor, 1985).For further information and algorithms for optimizing non-differentiable functions, we refer to the book by Bertsekas (1999).There is a vast amount of literature on different approaches for numerically solving continuous optimization problems, including algorithms for constrained optimization problems.Good starting points to appreciate this literature are the books by Luenberger (1969) and Bonnans et al. (2006).A recent survey of continuous optimization is provided by Bubeck (2015).Hugo Gonc ¸alves' blog is also a good resource for an easier introduction to Legendre-Fenchel transforms: https://tinyurl.com/ydaal7hjModern applications of machine learning often mean that the size of datasets prohibit the use of batch gradient descent, and hence stochastic gradient descent is the current workhorse of large-scale machine learning methods.Recent surveys of the literature include Hazan (2015) and Bottou et al. (2018).For duality and convex optimization, the book by Boyd and Vandenberghe (2004) includes lectures and slides online.A more mathematical treatment is provided by Bertsekas (2009), and recent book by one of Draft (2024-01-15) of "Mathematics for Machine Learning".Feedback: https://mml-book.com.the key researchers in the area of optimization is Nesterov (2018).Convex optimization is based upon convex analysis, and the reader interested in more foundational results about convex functions is referred to Rockafellar (1970), Hiriart-Urruty andLemaréchal (2001), and Borwein and Lewis (2006).Legendre-Fenchel transforms are also covered in the aforementioned books on convex analysis, but a more beginner-friendly presentation is available at Zia et al. (2009).The role of Legendre-Fenchel transforms in the analysis of convex optimization algorithms is surveyed in Polyak (2016).7.1 Consider the univariate functionFind its stationary points and indicate whether they are maximum, minimum, or saddle points.7.2 Consider the update equation for stochastic gradient descent (Equation (7.15)).Write down the update when we use a mini-batch size of one.7.3 Consider whether the following statements are true or false: a.The intersection of any two convex sets is convex.b.The union of any two convex sets is convex.c.The difference of a convex set A from another convex set B is convex.subject to the constraints that ξ ⩾ 0, x 0 ⩽ 0 and x 1 ⩽ 3. 7.6 Consider the linear program illustrated in Figure 7.9,Derive the dual linear program using Lagrange duality.7.7 Consider the quadratic program illustrated in Figure 7.4,Derive the dual quadratic program using Lagrange duality.7.8 Consider the following convex optimization problemDerive the Lagrangian dual by introducing the Lagrange multiplier λ. 7.9 Consider the negative entropy of x ∈ R D ,Derive the convex conjugate function f * (s), by assuming the standard dot product.Hint: Take the gradient of an appropriate function and set the gradient to zero.7.10 Consider the functionwhere A is strictly positive definite, which means that it is invertible.Derive the convex conjugate of f (x).Hint: Take the gradient of an appropriate function and set the gradient to zero.7.11 The hinge loss (which is the loss used by the support vector machine) is given byIf we are interested in applying gradient methods such as L-BFGS, and do not want to resort to subgradient methods, we need to smooth the kink in the hinge loss.Compute the convex conjugate of the hinge loss L * (β) where β is the dual variable.Add a ℓ 2 proximal term, and compute the conjugate of the resulting functionwhere γ is a given hyperparameter.Draft (2024-01-15) of "Mathematics for Machine Learning".Feedback: https://mml-book.com.In the first part of the book, we introduced the mathematics that form the foundations of many machine learning methods.The hope is that a reader would be able to learn the rudimentary forms of the language of mathematics from the first part, which we will now use to describe and discuss machine learning The main aim of this part of the book is to illustrate how the mathematical concepts introduced in the first part of the book can be used to design machine learning algorithms that can be used to solve tasks within the remit of the four pillars.We do not intend to introduce advanced machine learning concepts, but instead to provide a set of practical methods that allow the reader to apply the knowledge they gained from the first part of the book.It also provides a gateway to the wider machine learning literature for readers already familiar with the mathematics.It is worth at this point, to pause and consider the problem that a machine learning algorithm is designed to solve.As discussed in Chapter 1, there are three major components of a machine learning system: data, models, and learning.The main question of machine learning is "What do we mean by good models?".The word model has many subtleties, and we model will revisit it multiple times in this chapter.It is also not entirely obvious how to objectively define the word "good".One of the guiding principles of machine learning is that good models should perform well on unseen data.This requires us to define some performance metrics, such as accuracy or distance from ground truth, as well as figuring out ways to do well under these performance metrics.This chapter covers a few necessary bits and pieces of mathematical and statistical language that are commonly  As mentioned in Chapter 1, there are two different senses in which we use the phrase "machine learning algorithm": training and prediction.We will describe these ideas in this chapter, as well as the idea of selecting among different models.We will introduce the framework of empirical risk minimization in Section 8.2, the principle of maximum likelihood in Section 8.3, and the idea of probabilistic models in Section 8.4.We briefly outline a graphical language for specifying probabilistic models in Section 8.5 and finally discuss model selection in Section 8.6.The rest of this section expands upon the three main components of machine learning: data, models and learning.We assume that our data can be read by a computer, and represented adequately in a numerical format.Data is assumed to be tabular (Figure 8.1), where we think of each row of the table as representing a particular instance or example, and each column to be a particular feature.In recent Data is assumed to be in a tidy format (Wickham, 2014;Codd, 1990).years, machine learning has been applied to many types of data that do not obviously come in the tabular numerical format, for example genomic sequences, text and image contents of a webpage, and social media graphs.We do not discuss the important and challenging aspects of identifying good features.Many of these aspects depend on domain expertise and require careful engineering, and, in recent years, they have been put under the umbrella of data science (Stray, 2016;Adhikari and DeNero, 2018).Even when we have data in tabular format, there are still choices to be made to obtain a numerical representation.For example, in Table 8.1, the gender column (a categorical variable) may be converted into numbers 0 representing "Male" and 1 representing "Female".Alternatively, the gender could be represented by numbers −1, +1, respectively (as shown in Table 8.2).Furthermore, it is often important to use domain knowledge when constructing the representation, such as knowing that university degrees progress from bachelor's to master's to PhD or realizing that the postcode provided is not just a string of characters but actually encodes an area in London.In Table 8.2, we converted the data from Table 8.1 to a numerical format, and each postcode is represented as two numbers, (1) we do not expect the identifier (the Name) to be informative for a machine learning task; and (2) we may wish to anonymize the data to help protect the privacy of the employees.In this part of the book, we will use N to denote the number of examples in a dataset and index the examples with lowercase n = 1, . . ., N .We assume that we are given a set of numerical data, represented as an array of vectors (Table 8.2).Each row is a particular individual x n , often referred to as an example or data point in machine learning.The subscript example data point n refers to the fact that this is the nth example out of a total of N examples in the dataset.Each column represents a particular feature of interest about the example, and we index the features as d = 1, . . ., D. Recall that data is represented as vectors, which means that each example (each data point) is a D-dimensional vector.The orientation of the table originates from the database community, but for some machine learning algorithms (e.g., in Chapter 10) it is more convenient to represent examples as column vectors.Let us consider the problem of predicting annual salary from age, based on the data in Table 8.2.This is called a supervised learning problem where we have a label y n (the salary) associated with each example x n label (the age).The label y n has various other names, including target, response variable, and annotation.A dataset is written as a set of examplelabel pairs {(x 1 , y 1 ), . . ., (x n , y n ), . . ., (x N , y N )}.The table   the machine learning problems such as that in the previous paragraph.Representing data as vectors x n allows us to use concepts from linear algebra (introduced in Chapter 2).In many machine learning algorithms, we need to additionally be able to compare two vectors.As we will see in Chapters 9 and 12, computing the similarity or distance between two examples allows us to formalize the intuition that examples with similar features should have similar labels.The comparison of two vectors requires that we construct a geometry (explained in Chapter 3) and allows us to optimize the resulting learning problem using techniques from Chapter 7. Since we have vector representations of data, we can manipulate data to find potentially better representations of it.We will discuss finding good representations in two ways: finding lower-dimensional approximations of the original feature vector, and using nonlinear higher-dimensional combinations of the original feature vector.In Chapter 10, we will see an example of finding a low-dimensional approximation of the original data space by finding the principal components.Finding principal components is closely related to concepts of eigenvalue and singular value decomposition as introduced in Chapter 4. For the high-dimensional representation, we will see an explicit feature map ϕ(•) that allows us to represent infeature map puts x n using a higher-dimensional representation ϕ(x n ).The main motivation for higher-dimensional representations is that we can construct new features as non-linear combinations of the original features, which in turn may make the learning problem easier.We will discuss the feature map in Section 9.2 and show how this feature map leads to a kernel in kernel Section 12.4.In recent years, deep learning methods (Goodfellow et al., 2016) have shown promise in using the data itself to learn new good features and have been very successful in areas, such as computer vision, speech recognition, and natural language processing.We will not cover neural networks in this part of the book, but the reader is referred toOnce we have data in an appropriate vector representation, we can get to the business of constructing a predictive function (known as a predictor).predictor In Chapter 1, we did not yet have the language to be precise about models.Using the concepts from the first part of the book, we can now introduce what "model" means.We present two major approaches in this book: a predictor as a function, and a predictor as a probabilistic model.We describe the former here and the latter in the next subsection.A predictor is a function that, when given a particular input example (in our case, a vector of features), produces an output.For now, consider the output to be a single number, i.e., a real-valued scalar output.This can be written aswhere the input vector x is D-dimensional (has D features), and the function f then applied to it (written as f (x)) returns a real number.In this book, we do not consider the general case of all functions, which would involve the need for functional analysis.Instead, we consider the special case of linear functionsfor unknown θ and θ 0 .This restriction means that the contents of Chapters 2 and 3 suffice for precisely stating the notion of a predictor for the non-probabilistic (in contrast to the probabilistic view described next)  view of machine learning.Linear functions strike a good balance between the generality of the problems that can be solved and the amount of background mathematics that is needed.We often consider data to be noisy observations of some true underlying effect, and hope that by applying machine learning we can identify the signal from the noise.This requires us to have a language for quantifying the effect of noise.We often would also like to have predictors that express some sort of uncertainty, e.g., to quantify the confidence we have about the value of the prediction for a particular test data point.As we have seen in Chapter 6, probability theory provides a language for quantifying uncertainty.Figure 8.3 illustrates the predictive uncertainty of the function as a Gaussian distribution.Instead of considering a predictor as a single function, we could consider predictors to be probabilistic models, i.e., models describing the distribution of possible functions.We limit ourselves in this book to the special case of distributions with finite-dimensional parameters, which allows us to describe probabilistic models without needing stochastic processes and random measures.For this special case, we can think about probabilistic models as multivariate probability distributions, which already allow for a rich class of models.We will introduce how to use concepts from probability (Chapter 6) to define machine learning models in Section 8.4, and introduce a graphical language for describing probabilistic models in a compact way in Section 8.5.The goal of learning is to find a model and its corresponding parameters such that the resulting predictor will perform well on unseen data.There are conceptually three distinct algorithmic phases when discussing machine learning algorithms:The prediction phase is when we use a trained predictor on previously unseen test data.In other words, the parameters and model choice is already fixed and the predictor is applied to new vectors representing new input data points.As outlined in Chapter 1 and the previous subsection, we will consider two schools of machine learning in this book, corresponding to whether the predictor is a function or a probabilistic model.When we have a probabilistic model (discussed further in Section 8.4) the prediction phase is called inference.Remark.Unfortunately, there is no agreed upon naming for the different algorithmic phases.The word "inference" is sometimes also used to mean parameter estimation of a probabilistic model, and less often may be also used to mean prediction for non-probabilistic models.♢The training or parameter estimation phase is when we adjust our predictive model based on training data.We would like to find good predictors given training data, and there are two main strategies for doing so: finding the best predictor based on some measure of quality (sometimes called finding a point estimate), or using Bayesian inference.Finding a point estimate can be applied to both types of predictors, but Bayesian inference requires probabilistic models.For the non-probabilistic model, we follow the principle of empirical risk empirical risk minimization minimization, which we describe in Section 8.2.Empirical risk minimization directly provides an optimization problem for finding good parameters.With a statistical model, the principle of maximum likelihood is used maximum likelihood to find a good set of parameters (Section 8.3).We can additionally model the uncertainty of parameters using a probabilistic model, which we will look at in more detail in Section 8.4.We use numerical methods to find good parameters that "fit" the data, and most training methods can be thought of as hill-climbing approaches to find the maximum of an objective, for example the maximum of a likelihood.To apply hill-climbing approaches we use the gradients described in The convention in optimization is to minimize objectives.Hence, there is often an extra minus sign in machine learning objectives.Chapter 5 and implement numerical optimization approaches from Chapter 7.As mentioned in Chapter 1, we are interested in learning a model based on data such that it performs well on future data.It is not enough for the model to only fit the training data well, the predictor needs to perform well on unseen data.We simulate the behavior of our predictor on future unseen data using cross-validation (Section 8.2.4).As we will see cross-validation in this chapter, to achieve the goal of performing well on unseen data, we will need to balance between fitting well on training data and finding "simple" explanations of the phenomenon.This trade-off is achieved using regularization (Section 8.2.3) or by adding a prior (Section 8.3.2).In philosophy, this is considered to be neither induction nor deduction, but is called abduction.According to the Stanford Encyclopedia of Philosophy, abduction abduction is the process of inference to the best explanation (Douven, 2017).A good movie title is "AI abduction".We often need to make high-level modeling decisions about the structure of the predictor, such as the number of components to use or the class of probability distributions to consider.The choice of the number of components is an example of a hyperparameter, and this choice can afhyperparameter fect the performance of the model significantly.The problem of choosing among different models is called model selection, which we describe in model selection Section 8.6.For non-probabilistic models, model selection is often done using nested cross-validation, which is described in Section 8.6.1.We also nested cross-validation use model selection to choose hyperparameters of our model.Another way to consider the distinction is to consider parameters as the explicit parameters of a probabilistic model, and to consider hyperparameters (higher-level parameters) as parameters that control the distribution of these explicit parameters.♢In the following sections, we will look at three flavors of machine learning: empirical risk minimization (Section 8.2), the principle of maximum likelihood (Section 8.3), and probabilistic modeling (Section 8.4).After having all the mathematics under our belt, we are now in a position to introduce what it means to learn.The "learning" part of machine learning boils down to estimating parameters based on training data.In this section, we consider the case of a predictor that is a function, and consider the case of probabilistic models in Section 8.3.We describe the idea of empirical risk minimization, which was originally popularized by the proposal of the support vector machine (described in Chapter 12).However, its general principles are widely applicable and allow us to ask the question of what is learning without explicitly constructing probabilistic models.There are four main design choices, which we will cover in detail in the following subsections: Section 8.2.1 What is the set of functions we allow the predictor to take?Section 8.2.2 How do we measure how well the predictor performs on the training data?Section 8.2.3 How do we construct predictors from only training data that performs well on unseen test data?Section 8.2.4What is the procedure for searching over the space of models?Assume we are given N examples x n ∈ R D and corresponding scalar labels y n ∈ R. We consider the supervised learning setting, where we obtain pairs (x 1 , y 1 ), . . ., (x N , y N ).Given this data, we would like to estimate a predictor f (•, θ) : R D → R, parametrized by θ.We hope to be able to find a good parameter θ * such that we fit the data well, that is,In this section, we use the notation ŷn = f (x n , θ * ) to represent the output of the predictor.Remark.For ease of presentation, we will describe empirical risk minimization in terms of supervised learning (where we have labels).This simplifies the definition of the hypothesis class and the loss function.It is also common in machine learning to choose a parametrized class of functions, for example affine functions.♢We introduce the problem of ordinary least-squares regression to illustrate empirical risk minimization.A more comprehensive account of regression is given in Chapter 9. When the label y n is real-valued, a popular choice of function class for predictors is the set of affine functions.We choose a Affine functions are often referred to as linear functions in machine learning.more compact notation for an affine function by concatenating an additional unit featureallowing us to write the predictor as a linear functionThis linear predictor is equivalent to the affine modelThe predictor takes the vector of features representing a single example x n as input and produces a real-valued output, i.e., f : R D+1 → R. The previous figures in this chapter had a straight line as a predictor, which means that we have assumed an affine function.Instead of a linear function, we may wish to consider non-linear functions as predictors.Recent advances in neural networks allow for efficient computation of more complex non-linear function classes.Given the class of functions, we want to search for a good predictor.We now move on to the second ingredient of empirical risk minimization: how to measure how well the predictor fits the training data.Consider the label y n for a particular example; and the corresponding prediction ŷn that we make based on x n .To define what it means to fit the data well, we need to specify a loss function ℓ(y n , ŷn ) that takes the ground loss function truth label and the prediction as input and produces a non-negative number (referred to as the loss) representing how much error we have made on this particular prediction.Our goal for finding a good parameter vectorThe expression "error" is often used to mean loss.θ * is to minimize the average loss on the set of N training examples.One assumption that is commonly made in machine learning is that the set of examples (x 1 , y 1 ), . . ., (x N , y N ) is independent and identically independent and identically distributed distributed.The word independent (Section 6.4.5) means that two data points (x i , y i ) and (x j , y j ) do not statistically depend on each other, meaning that the empirical mean is a good estimate of the population mean (Section 6.4.1).This implies that we can use the empirical mean of the loss on the training data.For a given training set {(x 1 , y 1 ), . . ., (x N , y N )}, training set we introduce the notation of an example matrix X := [x 1 , . . ., x N ] ⊤ ∈ R N ×D and a label vector y := [y 1 , . . ., y N ] ⊤ ∈ R N .Using this matrix notation the average loss is given bywhere ŷn = f (x n , θ).Equation (8.6) is called the empirical risk and deempirical risk pends on three arguments, the predictor f and the data X, y.This general strategy for learning is called empirical risk minimization.Example 8.2 (Least-Squares Loss)Continuing the example of least-squares regression, we specify that we measure the cost of making an error during training using the squared loss ℓ(y n , ŷn ) = (y n − ŷn ) 2 .We wish to minimize the empirical risk (8.6),Draft (2024-01-15) of "Mathematics for Machine Learning".Feedback: https://mml-book.com.which is the average of the losses over the datawhere we substituted the predictor ŷn = f (x n , θ).By using our choice of a linear predictor f (x n , θ) = θ ⊤ x n , we obtain the optimization problemThis equation can be equivalently expressed in matrix formThis is known as the least-squares problem.There exists a closed-form anleast-squares problem alytic solution for this by solving the normal equations, which we will discuss in Section 9.2.We are not interested in a predictor that only performs well on the training data.Instead, we seek a predictor that performs well (has low risk) on unseen test data.More formally, we are interested in finding a predictor f (with parameters fixed) that minimizes the expected risk(8.10)where y is the label and f (x) is the prediction based on the example x.The notation R true (f ) indicates that this is the true risk if we had access to an infinite amount of data.The expectation is over the (infinite) set of all Another phrase commonly used for expected risk is "population risk".possible data and labels.There are two practical questions that arise from our desire to minimize expected risk, which we address in the following two subsections:How should we change our training procedure to generalize well?How do we estimate expected risk from (finite) data?Remark.Many machine learning tasks are specified with an associated performance measure, e.g., accuracy of prediction or root mean squared error.The performance measure could be more complex, be cost sensitive, and capture details about the particular application.In principle, the design of the loss function for empirical risk minimization should correspond directly to the performance measure specified by the machine learning task.In practice, there is often a mismatch between the design of the loss function and the performance measure.This could be due to issues such as ease of implementation or efficiency of optimization.♢This section describes an addition to empirical risk minimization that allows it to generalize well (approximately minimizing expected risk).Recall that the aim of training a machine learning predictor is so that we can perform well on unseen data, i.e., the predictor generalizes well.We simulate this unseen data by holding out a proportion of the whole dataset.This hold out set is referred to as the test set.Given a sufficiently rich class test set Even knowing only the performance of the predictor on the test set leaks information (Blum and Hardt, 2015).of functions for the predictor f , we can essentially memorize the training data to obtain zero empirical risk.While this is great to minimize the loss (and therefore the risk) on the training data, we would not expect the predictor to generalize well to unseen data.(Mitchell, 1997).This general phenomenon of having very small average loss on the training set but large average loss on the test set tends to occur when we have little data and a complex hypothesis class.For a particular predictor f (with parameters fixed), the phenomenon of overfitting occurs when the risk estimate from the training data R emp (f, X train , y train ) underestimates the expected risk R true (f ).Since we estimate the expected risk R true (f ) by using the empirical risk on the test set R emp (f, X test , y test ) if the test risk is much larger than the training risk, this is an indication of overfitting.We revisit the idea of overfitting in Section 8.3.3.Therefore, we need to somehow bias the search for the minimizer of empirical risk by introducing a penalty term, which makes it harder for the optimizer to return an overly flexible predictor.In machine learning, the penalty term is referred to as regularization.Regularization is a way regularization to compromise between accurate solution of empirical risk minimization and the size or complexity of the solution.Regularization is an approach that discourages complex or extreme solutions to an optimization problem.The simplest regularization strategy is to replace the least-squares problem (8.11) in the previous example with the "regularized" problem by adding a penalty term involving only θ:(8.12)The additional term ∥θ∥ 2 is called the regularizer, and the parameter regularizer λ is the regularization parameter.The regularization parameter trades regularization parameter off minimizing the loss on the training set and the magnitude of the parameters θ.It often happens that the magnitude of the parameter values becomes relatively large if we run into overfitting (Bishop, 2006).The regularization term is sometimes called the penalty term, which bipenalty term ases the vector θ to be closer to the origin.The idea of regularization also appears in probabilistic models as the prior probability of the parameters.Recall from Section 6.6 that for the posterior distribution to be of the same form as the prior distribution, the prior and the likelihood need to be conjugate.We will revisit this idea in Section 8.3.2.We will see in Chapter 12 that the idea of the regularizer is equivalent to the idea of a large margin.We mentioned in the previous section that we measure the generalization error by estimating it by applying the predictor on test data.This data is also sometimes referred to as the validation set.The validation set is a subvalidation set set of the available training data that we keep aside.A practical issue with this approach is that the amount of data is limited, and ideally we would use as much of the data available to train the model.This would require us to keep our validation set V small, which then would lead to a noisy estimate (with high variance) of the predictive performance.One solution to these contradictory objectives (large training set, large validation set) is to use cross-validation.K-fold cross-validation effectively partitions cross-validation the data into K chunks, K − 1 of which form the training set R, and the last chunk serves as the validation set V (similar to the idea outlined previously).Cross-validation iterates through (ideally) all combinations of assignments of chunks to R and V; see Figure 8.4.This procedure is repeated for all K choices for the validation set, and the performance of the model from the K runs is averaged.We partition our dataset into two sets D = R ∪ V, such that they do not overlap (R ∩ V = ∅), where V is the validation set, and train our model on R.After training, we assess the performance of the predictor f on the  (k) produces a predictor f (k) , which is then applied to validation set V (k) to compute the empirical risk R(f (k) , V (k) ).We cycle through all possible partitionings of validation and training sets and compute the average generalization error of the predictor.Cross-validation approximates the expected generalization errorwhere) is the risk (e.g., RMSE) on the validation set V (k) for predictor f (k) .The approximation has two sources: first, due to the finite training set, which results in not the best possible f (k) ; and second, due to the finite validation set, which results in an inaccurate estimation of the risk R(f (k) , V (k) ).A potential disadvantage of K-fold cross-validation is the computational cost of training the model K times, which can be burdensome if the training cost is computationally expensive.In practice, it is often not sufficient to look at the direct parameters alone.For example, we need to explore multiple complexity parameters (e.g., multiple regularization parameters), which may not be direct parameters of the model.Evaluating the quality of the model, depending on these hyperparameters, may result in a number of training runs that is exponential in the number of model parameters.One can use nested cross-validation (Section 8.6.1) to search for good hyperparameters.However, cross-validation is an embarrassingly parallel problem, i.e., litembarrassingly parallel tle effort is needed to separate the problem into a number of parallel tasks.Given sufficient computing resources (e.g., cloud computing, server farms), cross-validation does not require longer than a single performance assessment.In this section, we saw that empirical risk minimization is based on the following concepts: the hypothesis class of functions, the loss function and regularization.In Section 8.3, we will see the effect of using a probability distribution to replace the idea of loss functions and regularization.Due to the fact that the original development of empirical risk minimization (Vapnik, 1998) was couched in heavily theoretical language, many of the subsequent developments have been theoretical.The area of study is called statistical learning theory (Vapnik, 1999;Evgeniou et al., 2000; statistical learning theory Hastie et al., 2001;von Luxburg and Schölkopf, 2011).A recent machine learning textbook that builds on the theoretical foundations and develops efficient learning algorithms is Shalev-Shwartz and Ben-David (2014).The concept of regularization has its roots in the solution of ill-posed inverse problems (Neumaier, 1998).The approach presented here is called Tikhonov regularization, and there is a closely related constrained version Tikhonov regularization called Ivanov regularization.Tikhonov regularization has deep relationships to the bias-variance trade-off and feature selection (Bühlmann and Van De Geer, 2011).An alternative to cross-validation is bootstrap and jackknife (Efron and Tibshirani, 1993;Davidson and Hinkley, 1997;Hall, 1992).Thinking about empirical risk minimization (Section 8.2) as "probability free" is incorrect.There is an underlying unknown probability distribution p(x, y) that governs the data generation.However, the approach of empirical risk minimization is agnostic to that choice of distribution.This is in contrast to standard statistical approaches that explicitly require the knowledge of p(x, y).Furthermore, since the distribution is a joint distribution on both examples x and labels y, the labels can be nondeterministic.In contrast to standard statistics we do not need to specify the noise distribution for the labels y.In Section 8.2, we did not explicitly model our problem using probability distributions.In this section, we will see how to use probability distributions to model our uncertainty due to the observation process and our uncertainty in the parameters of our predictors.In Section 8.3.1, we introduce the likelihood, which is analogous to the concept of loss functions (Section 8.2.2) in empirical risk minimization.The concept of priors (Section 8.3.2) is analogous to the concept of regularization (Section 8.2.3).The idea behind maximum likelihood estimation (MLE) is to define a funcmaximum likelihood estimation tion of the parameters that enables us to find a model that fits the data well.The estimation problem is focused on the likelihood function, or likelihood more precisely its negative logarithm.For data represented by a random variable x and for a family of probability densities p(x | θ) parametrized by θ, the negative log-likelihood is given by  (8.14)The notation L x (θ) emphasizes the fact that the parameter θ is varying and the data x is fixed.We very often drop the reference to x when writing the negative log-likelihood, as it is really a function of θ, and write it as L(θ) when the random variable representing the uncertainty in the data is clear from the context.Let us interpret what the probability density p(x | θ) is modeling for a fixed value of θ.It is a distribution that models the uncertainty of the data for a given parameter setting.For a given dataset x, the likelihood allows us to express preferences about different settings of the parameters θ, and we can choose the setting that more "likely" has generated the data.In a complementary view, if we consider the data to be fixed (because it has been observed), and we vary the parameters θ, what does L(θ) tell us?It tells us how likely a particular setting of θ is for the observations x.Based on this second view, the maximum likelihood estimator gives us the most likely parameter θ for the set of data.We consider the supervised learning setting, where we obtain pairs (x 1 , y 1 ), . . ., (x N , y N ) with x n ∈ R D and labels y n ∈ R. We are interested in constructing a predictor that takes a feature vector x n as input and produces a prediction y n (or something close to it), i.e., given a vector x n we want the probability distribution of the label y n .In other words, we specify the conditional probability distribution of the labels given the examples for the particular parameter setting θ.The first example that is often used is to specify that the conditional probability of the labels given the examples is a Gaussian distribution.In other words, we assume that we can explain our observation uncertainty by independent Gaussian noise (refer to Section 6.5) with zero mean, ε n ∼ N 0, σ 2 .We further assume that the linear model x ⊤ n θ is used for prediction.This means we specify a Gaussian likelihood for each example label pair (x n , y n ),An illustration of a Gaussian likelihood for a given parameter θ is shown in Figure 8.3.We will see in Section 9.2 how to explicitly expand the preceding expression out in terms of the Gaussian distribution.We assume that the set of examples (x 1 , y 1 ), . . ., (x N , y N ) are independent independent and identically distributed and identically distributed (i.i.d.).The word "independent" (Section 6.4.5) implies that the likelihood involving the whole dataset (Y = {y 1 , . . ., y N } and X = {x 1 , . . ., x N }) factorizes into a product of the likelihoods of (8.16) where p(y n | x n , θ) is a particular distribution (which was Gaussian in Example 8.4).The expression "identically distributed" means that each term in the product (8.16) is of the same distribution, and all of them share the same parameters.It is often easier from an optimization viewpoint to compute functions that can be decomposed into sums of simpler functions.Hence, in machine learning we often consider the negative log-likelihood Recall log(abWhile it is temping to interpret the fact that θ is on the right of the conditioning in p(y n |x n , θ) (8.15), and hence should be interpreted as observed and fixed, this interpretation is incorrect.The negative log-likelihood L(θ) is a function of θ.Therefore, to find a good parameter vector θ that explains the data (x 1 , y 1 ), . . ., (x N , y N ) well, minimize the negative loglikelihood L(θ) with respect to θ.Remark.The negative sign in (8.17) is a historical artifact that is due to the convention that we want to maximize likelihood, but numerical optimization literature tends to study minimization of functions.♢Example 8.5 Continuing on our example of Gaussian likelihoods (8.15), the negative log-likelihood can be rewritten asAs σ is given, the second term in (8.18d) is constant, and minimizing L(θ) corresponds to solving the least-squares problem (compare with (8.8)) expressed in the first term.It turns out that for Gaussian likelihoods the resulting optimization  problem corresponding to maximum likelihood estimation has a closedform solution.We will see more details on this in Chapter 9. Figure 8.5 shows a regression dataset and the function that is induced by the maximum-likelihood parameters.Maximum likelihood estimation may suffer from overfitting (Section 8.3.3),analogous to unregularized empirical risk minimization (Section 8.2.3).For other likelihood functions, i.e., if we model our noise with non-Gaussian distributions, maximum likelihood estimation may not have a closed-form analytic solution.In this case, we resort to numerical optimization methods discussed in Chapter 7.If we have prior knowledge about the distribution of the parameters θ, we can multiply an additional term to the likelihood.This additional term is a prior probability distribution on parameters p(θ).For a given prior, after observing some data x, how should we update the distribution of θ?In other words, how should we represent the fact that we have more specific knowledge of θ after observing data x? Bayes' theorem, as discussed in Section 6.3, gives us a principled tool to update our probability distributions of random variables.It allows us to compute a posterior distribution posterior p(θ | x) (the more specific knowledge) on the parameters θ from general prior statements (prior distribution) p(θ) and the function p(x | θ) that prior links the parameters θ and the observed data x (called the likelihood):Recall that we are interested in finding the parameter θ that maximizes the posterior.Since the distribution p(x) does not depend on θ, we can ignore the value of the denominator for the optimization and obtainThe preceding proportion relation hides the density of the data p(x), which may be difficult to estimate.Instead of estimating the minimum of the negative log-likelihood, we now estimate the minimum of the negative log-posterior, which is referred to as maximum a posteriori estimamaximum a posteriori estimation tion (MAP estimation).An illustration of the effect of adding a zero-meanGaussian prior is shown in Figure 8.6.In addition to the assumption of Gaussian likelihood in the previous example, we assume that the parameter vector is distributed as a multivariate Gaussian with zero mean, i.e., p(θ) = N 0, Σ , where Σ is the covariance matrix (Section 6.5).Note that the conjugate prior of a Gaussian is also a Gaussian (Section 6.6.1), and therefore we expect the posterior distribution to also be a Gaussian.We will see the details of maximum a posteriori estimation in Chapter 9.The idea of including prior knowledge about where good parameters lie is widespread in machine learning.An alternative view, which we saw in Section 8.2.3, is the idea of regularization, which introduces an additional term that biases the resulting parameters to be close to the origin.Maximum a posteriori estimation can be considered to bridge the nonprobabilistic and probabilistic worlds as it explicitly acknowledges the need for a prior distribution but it still only produces a point estimate of the parameters.Remark.The maximum likelihood estimate θ ML possesses the following properties (Lehmann and Casella, 1998;Efron and Hastie, 2016limit of infinitely many observations, plus a random error that is approximately normal.The size of the samples necessary to achieve these properties can be quite large.The error's variance decays in 1/N , where N is the number of data points.Especially, in the "small" data regime, maximum likelihood estimation can lead to overfitting.The principle of maximum likelihood estimation (and maximum a posteriori estimation) uses probabilistic modeling to reason about the uncertainty in the data and model parameters.However, we have not yet taken probabilistic modeling to its full extent.In this section, the resulting training procedure still produces a point estimate of the predictor, i.e., training returns one single set of parameter values that represent the best predictor.In Section 8.4, we will take the view that the parameter values should also be treated as random variables, and instead of estimating "best" values of that distribution, we will use the full parameter distribution when making predictions.Consider the setting where we are given a dataset, and we are interested in fitting a parametrized model to the data.When we talk about "fitting", we typically mean optimizing/learning model parameters so that they minimize some loss function, e.g., the negative log-likelihood.With maximum likelihood (Section 8.3.1) and maximum a posteriori estimation (Section 8.3.2),we already discussed two commonly used algorithms for model fitting.The parametrization of the model defines a model class M θ with which we can operate.For example, in a linear regression setting, we may define the relationship between inputs x and (noise-free) observations y to be y = ax + b, where θ := {a, b} are the model parameters.In this case, the model parameters θ describe the family of affine functions, i.e., straight lines with slope a, which are offset from 0 by b.Assume the data comes from a model M * , which is unknown to us.For a given training dataset, we optimize θ so that M θ is as close as possible to M * , where the "closeness" is defined by the objective function we optimize (e.g., squared loss on the training data).Figure 8.7 illustrates a setting where we have a small model class (indicated by the circle M θ ), and the data generation model M * lies outside the set of considered models.We begin our parameter search at M θ0 .After the optimization, i.e., when we obtain the best possible parameters θ * , we distinguish three different cases: (i) overfitting, (ii) underfitting, and (iii) fitting well.We will give a high-level intuition of what these three concepts mean.Roughly speaking, overfitting refers to the situation where the paraoverfitting metrized model class is too rich to model the dataset generated by M * , i.e., M θ could model much more complicated datasets.For instance, if the dataset was generated by a linear function, and we define M θ to be the class of seventh-order polynomials, we could model not only linear functions, but also polynomials of degree two, three, etc. Models that overfit typically have a large number of parameters.An observation we often One way to detect overfitting in practice is to observe that the model has low training risk but high test risk during cross validation (Section 8.2.4).make is that the overly flexible model class M θ uses all its modeling power to reduce the training error.If the training data is noisy, it will therefore find some useful signal in the noise itself.This will cause enormous problems when we predict away from the training data.Figure 8.8(a) gives an example of overfitting in the context of regression where the model parameters are learned by means of maximum likelihood (see Section 8.3.1).We will discuss overfitting in regression more in Section 9.2.2.When we run into underfitting, we encounter the opposite problem underfitting where the model class M θ is not rich enough.For example, if our dataset was generated by a sinusoidal function, but θ only parametrizes straight lines, the best optimization procedure will not get us close to the true model.However, we still optimize the parameters and find the best straight line that models the dataset.Figure 8.8(b) shows an example of a model that underfits because it is insufficiently flexible.Models that underfit typically have few parameters.The third case is when the parametrized model class is about right.Then, our model fits well, i.e., it neither overfits nor underfits.This means our model class is just rich enough to describe the dataset we are given.Figure 8.8(c) shows a model that fits the given dataset fairly well.Ideally, this is the model class we would want to work with since it has good generalization properties.In practice, we often define very rich model classes M θ with many parameters, such as deep neural networks.To mitigate the problem of overfitting, we can use regularization (Section 8.2.3) or priors (Section 8.3.2).We will discuss how to choose the model class in Section 8.6.When considering probabilistic models, the principle of maximum likelihood estimation generalizes the idea of least-squares regression for linear models, which we will discuss in detail in Chapter 9. When restricting the predictor to have linear form with an additional nonlinear function φ applied to the output, i.e.,we can consider other models for other prediction tasks, such as binary classification or modeling count data (McCullagh and Nelder, 1989).An alternative view of this is to consider likelihoods that are from the exponential family (Section 6.6).The class of models, which have linear dependence between parameters and data, and have potentially nonlinear transformation φ (called a link function), is referred to as generalized link function generalized linear model linear models (Agresti, 2002, chapter 4).Maximum likelihood estimation has a rich history, and was originally proposed by Sir Ronald Fisher in the 1930s.We will expand upon the idea of a probabilistic model in Section 8.4.One debate among researchers who use probabilistic models, is the discussion between Bayesian and frequentist statistics.As mentioned in Section 6.1.1,it boils down to the definition of probability.Recall from Section 6.1 that one can consider probability to be a generalization (by allowing uncertainty) of logical reasoning (Cheeseman, 1985;Jaynes, 2003).The method of maximum likelihood estimation is frequentist in nature, and the interested reader is pointed to Efron and Hastie (2016) for a balanced view of both Bayesian and frequentist statistics.There are some probabilistic models where maximum likelihood estimation may not be possible.The reader is referred to more advanced statistical textbooks, e.g., Casella and Berger (2002), for approaches, such as method of moments, M -estimation, and estimating equations.In machine learning, we are frequently concerned with the interpretation and analysis of data, e.g., for prediction of future events and decision making.To make this task more tractable, we often build models that describe the generative process that generates the observed data.generative process Draft (2024-01-15) of "Mathematics for Machine Learning".Feedback: https://mml-book.com.For example, we can describe the outcome of a coin-flip experiment ("heads" or "tails") in two steps.First, we define a parameter µ, which describes the probability of "heads" as the parameter of a Bernoulli distribution (Chapter 6); second, we can sample an outcome x ∈ {head, tail} from the Bernoulli distribution p(x | µ) = Ber(µ).The parameter µ gives rise to a specific dataset X and depends on the coin used.Since µ is unknown in advance and can never be observed directly, we need mechanisms to learn something about µ given observed outcomes of coin-flip experiments.In the following, we will discuss how probabilistic modeling can be used for this purpose.A probabilistic model is specified by the joint distribution of all random variables.Probabilistic models represent the uncertain aspects of an experiment as probability distributions.The benefit of using probabilistic models is that they offer a unified and consistent set of tools from probability theory (Chapter 6) for modeling, inference, prediction, and model selection.In probabilistic modeling, the joint distribution p(x, θ) of the observed variables x and the hidden parameters θ is of central importance: It encapsulates information from the following:The prior and the likelihood (product rule, Section 6.3).The marginal likelihood p(x), which will play an important role in model selection (Section 8.6), can be computed by taking the joint distribution and integrating out the parameters (sum rule, Section 6.3).The posterior, which can be obtained by dividing the joint by the marginal likelihood.Only the joint distribution has this property.Therefore, a probabilistic model is specified by the joint distribution of all its random variables.Parameter estimation can be phrased as an optimization problem.A key task in machine learning is to take a model and the data to uncover the values of the model's hidden variables θ given the observed variables x.In Section 8.3.1, we already discussed two ways for estimating model parameters θ using maximum likelihood or maximum a posteriori estimation.In both cases, we obtain a single-best value for θ so that the key algorithmic problem of parameter estimation is solving an optimization problem.Once these point estimates θ * are known, we use them to make predictions.More specifically, the predictive distribution will be p(x | θ * ), where we use θ * in the likelihood function.As discussed in Section 6.3, focusing solely on some statistic of the posterior distribution (such as the parameter θ * that maximizes the posterior) leads to loss of information, which can be critical in a system that uses the prediction p(x | θ * ) to make decisions.These decision-making systems typically have different objective functions than the likelihood, aBayesian inference is about learning the distribution of random variables.squared-error loss or a mis-classification error.Therefore, having the full posterior distribution around can be extremely useful and leads to more robust decisions.Bayesian inference is about finding this posterior distri-Bayesian inference bution (Gelman et al., 2004).For a dataset X , a parameter prior p(θ), and a likelihood function, the posterioris obtained by applying Bayes' theorem.The key idea is to exploit Bayes'Bayesian inference inverts the relationship between parameters and the data.theorem to invert the relationship between the parameters θ and the data X (given by the likelihood) to obtain the posterior distribution p(θ | X ).The implication of having a posterior distribution on the parameters is that it can be used to propagate uncertainty from the parameters to the data.More specifically, with a distribution p(θ) on the parameters our predictions will beand they no longer depend on the model parameters θ, which have been marginalized/integrated out.Equation (8.23) reveals that the prediction is an average over all plausible parameter values θ, where the plausibility is encapsulated by the parameter distribution p(θ).Having discussed parameter estimation in Section 8.3 and Bayesian inference here, let us compare these two approaches to learning.Parameter estimation via maximum likelihood or MAP estimation yields a consistent point estimate θ * of the parameters, and the key computational problem to be solved is optimization.In contrast, Bayesian inference yields a (posterior) distribution, and the key computational problem to be solved is integration.Predictions with point estimates are straightforward, whereas predictions in the Bayesian framework require solving another integration problem; see (8.23).However, Bayesian inference gives us a principled way to incorporate prior knowledge, account for side information, and incorporate structural knowledge, all of which is not easily done in the context of parameter estimation.Moreover, the propagation of parameter uncertainty to the prediction can be valuable in decision-making systems for risk assessment and exploration in the context of data-efficient learning (Deisenroth et al., 2015;Kamthe and Deisenroth, 2018).While Bayesian inference is a mathematically principled framework for learning about parameters and making predictions, there are some practical challenges that come with it because of the integration problems we need to solve; see (8.22) and (8.23).More specifically, if we do not choose a conjugate prior on the parameters (Section 6.6.1), the integrals in (8.22) and (8.23) are not analytically tractable, and we cannot compute the pos-terior, the predictions, or the marginal likelihood in closed form.In these cases, we need to resort to approximations.Here, we can use stochastic approximations, such as Markov chain Monte Carlo (MCMC) (Gilks et al., 1996), or deterministic approximations, such as the Laplace approximation (Bishop, 2006;Barber, 2012;Murphy, 2012), variational inference (Jordan et al., 1999;Blei et al., 2017), or expectation propagation (Minka, 2001a).Despite these challenges, Bayesian inference has been successfully applied to a variety of problems, including large-scale topic modeling (Hoffman et al., 2013), click-through-rate prediction (Graepel et al., 2010), data-efficient reinforcement learning in control systems (Deisenroth et al., 2015), online ranking systems (Herbrich et al., 2007), and large-scale recommender systems.There are generic tools, such as Bayesian optimization (Brochu et al., 2009;Snoek et al., 2012;Shahriari et al., 2016), that are very useful ingredients for an efficient search of meta parameters of models or algorithms.Remark.In the machine learning literature, there can be a somewhat arbitrary separation between (random) "variables" and "parameters".While parameters are estimated (e.g., via maximum likelihood), variables are usually marginalized out.In this book, we are not so strict with this separation because, in principle, we can place a prior on any parameter and integrate it out, which would then turn the parameter into a random variable according to the aforementioned separation.♢In practice, it is sometimes useful to have additional latent variables z latent variable (besides the model parameters θ) as part of the model (Moustaki et al., 2015).These latent variables are different from the model parameters θ as they do not parametrize the model explicitly.Latent variables may describe the data-generating process, thereby contributing to the interpretability of the model.They also often simplify the structure of the model and allow us to define simpler and richer model structures.Simplification of the model structure often goes hand in hand with a smaller number of model parameters (Paquet, 2008;Murphy, 2012).Learning in latent-variable models (at least via maximum likelihood) can be done in a principled way using the expectation maximization (EM) algorithm (Dempster et al., 1977;Bishop, 2006).Examples, where such latent variables are helpful, are principal component analysis for dimensionality reduction (Chapter 10), Gaussian mixture models for density estimation (Chapter 11), hidden Markov models (Maybeck, 1979) or dynamical systems (Ghahramani and Roweis, 1999;Ljung, 1999) for time-series modeling, and meta learning and task generalization (Hausman et al., 2018;Saemundsson et al., 2018).Although the introduction of these latent variables may make the model structure and the generative process easier, learning in latent-variable models is generally hard, as we will see in Chapter 11.Since latent-variable models also allow us to define the process that generates data from parameters, let us have a look at this generative process.Denoting data by x, the model parameters by θ and the latent variables by z, we obtain the conditional distributionthat allows us to generate data for any model parameters and latent variables.Given that z are latent variables, we place a prior p(z) on them.As the models we discussed previously, models with latent variables can be used for parameter learning and inference within the frameworks we discussed in Sections 8.3 and 8.4.2.To facilitate learning (e.g., by means of maximum likelihood estimation or Bayesian inference), we follow a two-step procedure.First, we compute the likelihood p(x | θ) of the model, which does not depend on the latent variables.Second, we use this likelihood for parameter estimation or Bayesian inference, where we use exactly the same expressions as in Sections 8.3 and 8.4.2, respectively.Since the likelihood function p(x | θ) is the predictive distribution of the data given the model parameters, we need to marginalize out the latent variables so thatwhere p(x | z, θ) is given in (8.24) and p(z) is the prior on the latent variables.Note that the likelihood must not depend on the latent variablesThe likelihood is a function of the data and the model parameters, but is independent of the latent variables.z, but it is only a function of the data x and the model parameters θ.The likelihood in (8.25) directly allows for parameter estimation via maximum likelihood.MAP estimation is also straightforward with an additional prior on the model parameters θ as discussed in Section 8.3.2.Moreover, with the likelihood (8.25) Bayesian inference (Section 8.4.2) in a latent-variable model works in the usual way: We place a prior p(θ) on the model parameters and use Bayes' theorem to obtain a posterior distributionover the model parameters given a dataset X .The posterior in (8.26) can be used for predictions within a Bayesian inference framework; see (8.23).One challenge we have in this latent-variable model is that the likelihood p(X | θ) requires the marginalization of the latent variables according to (8.25).Except when we choose a conjugate prior p(z) for p(x | z, θ), the marginalization in (8.25) is not analytically tractable, and we need to resort to approximations (Bishop, 2006;Paquet, 2008;Murphy, 2012;Moustaki et al., 2015).Similar to the parameter posterior (8.26) we can compute a posterior on the latent variables according towhere p(z) is the prior on the latent variables and p(X | z) requires us to integrate out the model parameters θ.Given the difficulty of solving integrals analytically, it is clear that marginalizing out both the latent variables and the model parameters at the same time is not possible in general (Bishop, 2006;Murphy, 2012).A quantity that is easier to compute is the posterior distribution on the latent variables, but conditioned on the model parameters, i.e.,where p(z) is the prior on the latent variables and p(X | z, θ) is given in (8.24).In Chapters 10 and 11, we derive the likelihood functions for PCA and Gaussian mixture models, respectively.Moreover, we compute the posterior distributions (8.28) on the latent variables for both PCA and Gaussian mixture models.Remark.In the following chapters, we may not be drawing such a clear distinction between latent variables z and uncertain model parameters θ and call the model parameters "latent" or "hidden" as well because they are unobserved.In Chapters 10 and 11, where we use the latent variables z, we will pay attention to the difference as we will have two different types of hidden variables: model parameters θ and latent variables z. ♢We can exploit the fact that all the elements of a probabilistic model are random variables to define a unified language for representing them.In Section 8.5, we will see a concise graphical language for representing the structure of probabilistic models.We will use this graphical language to describe the probabilistic models in the subsequent chapters.Probabilistic models in machine learning (Bishop, 2006;Barber, 2012;Murphy, 2012) provide a way for users to capture uncertainty about data and predictive models in a principled fashion.Ghahramani (2015) presents a short review of probabilistic models in machine learning.Given a probabilistic model, we may be lucky enough to be able to compute parameters of interest analytically.However, in general, analytic solutions are rare, and computational methods such as sampling (Gilks et al., 1996;Brooks et al., 2011) and variational inference (Jordan et al., 1999;Blei et al., 2017) are used.Moustaki et al. (2015) and Paquet (2008) provide a good overview of Bayesian inference in latent-variable models.In recent years, several programming languages have been proposed that aim to treat the variables defined in software as random variables corresponding to probability distributions.The objective is to be able to write complex functions of probability distributions, while under the hood the compiler automatically takes care of the rules of Bayesian inference.This rapidly changing field is called probabilistic programming.probabilistic programmingIn this section, we introduce a graphical language for specifying a probabilistic model, called the directed graphical model.It provides a compact directed graphical model and succinct way to specify probabilistic models, and allows the reader to visually parse dependencies between random variables.A graphical model visually captures the way in which the joint distribution over all random variables can be decomposed into a product of factors depending only on a subset of these variables.In Section 8.4, we identified the joint distribution of a probabilistic model as the key quantity of interest because it comprises information about the prior, the likelihood, and the posterior.However, the joint distribution by itself can be quite complicated, and Directed graphical models are also known as Bayesian networks.it does not tell us anything about structural properties of the probabilistic model.For example, the joint distribution p(a, b, c) does not tell us anything about independence relations.This is the point where graphical models come into play.This section relies on the concepts of independence and conditional independence, as described in Section 6.4.5.In a graphical model, nodes are random variables.In Figure 8.9(a), the graphical model nodes represent the random variables a, b, c.Edges represent probabilistic relations between variables, e.g., conditional probabilities.Remark.Not every distribution can be represented in a particular choice of graphical model.A discussion of this can be found in Bishop (2006).♢ Probabilistic graphical models have some convenient properties:They are a simple way to visualize the structure of a probabilistic model.They can be used to design or motivate new kinds of statistical models.Inspection of the graph alone gives us insight into properties, e.g., conditional independence.Complex computations for inference and learning in statistical models can be expressed in terms of graphical manipulations.Directed graphical models/Bayesian networks are a method for representing  (Pearl, 2009).x 1 x 2x 3 x 4x 5 (b) Not fully connected.Directed graphical models can be derived from joint distributions if we know something about their factorization.For the factorization in (8.29), we obtain the directed graphical model in Figure 8.9(a).In general, we can construct the corresponding directed graphical model from a factorized joint distribution as follows:1. Create a node for all random variables.2. For each conditional distribution, we add a directed link (arrow) to the graph from the nodes corresponding to the variables on which the distribution is conditioned.The graph layout depends on the factorization of the joint distribution.The graph layout depends on the choice of factorization of the joint distribution.We discussed how to get from a known factorization of the joint distribution to the corresponding directed graphical model.Now, we will do exactly the opposite and describe how to extract the joint distribution of a set of random variables from a given graphical model.Looking at the graphical model in Figure 8.9(b), we exploit two properties:The joint distribution p(x 1 , . . ., x 5 ) we seek is the product of a set of conditionals, one for each node in the graph.In this particular example, we will need five conditionals.Each conditional depends only on the parents of the corresponding node in the graph.For example, x 4 will be conditioned on x 2 .These two properties yield the desired factorization of the joint distributionIn general, the joint distribution p(x) = p(x 1 , . . ., x K ) is given aswhere Pa k means "the parent nodes of x k ".Parent nodes of x k are nodes that have arrows pointing to x k .We conclude this subsection with a concrete example of the coin-flip experiment.Consider a Bernoulli experiment (Example 6.8) where the probability that the outcome x of this experiment is "heads" is(8.32)We now repeat this experiment N times and observe outcomes x 1 , . . ., x N so that we obtain the joint distributionThe expression on the right-hand side is a product of Bernoulli distributions on each individual outcome because the experiments are independent.Recall from Section 6.4.5 that statistical independence means that the distribution factorizes.To write the graphical model down for this setting, we make the distinction between unobserved/latent variables and observed variables.Graphically, observed variables are denoted by shaded nodes so that we obtain the graphical model in Figure 8.10(a).We see that the single parameter µ is the same for all x n , n = 1, . . ., N as the outcomes x n are identically distributed.A more compact, but equivalent, graphical model for this setting is given in Figure 8.10(b), where we use the plate notation.The plate (box) repeats everything inside (in this case, plate the observations x n ) N times.Therefore, both graphical models are equivalent, but the plate notation is more compact.Graphical models immediately allow us to place a hyperprior on µ.A hyperprior is a second layer hyperprior of prior distributions on the parameters of the first layer of priors.Figure 8.10(c) places a Beta(α, β) prior on the latent variable µ.If we treat α and β as deterministic parameters, i.e., not random variables, we omit the circle around it.Directed graphical models allow us to find conditional independence (Section 6.4.5) relationship properties of the joint distribution only by looking at the graph.A concept called d-separation (Pearl, 1988) is key to this.Consider a general directed graph in which A, B, C are arbitrary nonintersecting sets of nodes (whose union may be smaller than the complete set of nodes in the graph).We wish to ascertain whether a particular conditional independence statement, "A is conditionally independent of B given C", denoted byis implied by a given directed acyclic graph.To do so, we consider all possible trails (paths that ignore the direction of the arrows) from any node in A to any nodes in B. Any such path is said to be blocked if it includes any node such that either of the following are true:The arrows on the path meet either head to tail or tail to tail at the node, and the node is in the set C.The arrows meet head to head at the node, and neither the node nor any of its descendants is in the set C.If all paths are blocked, then A is said to be d-separated from B by C, and the joint distribution over all of the variables in the graph will satisfy A ⊥ ⊥ B | C. Directed graphical models allow a compact representation of probabilistic models, and we will see examples of directed graphical models in Chapters 9, 10, and 11.The representation, along with the concept of conditional independence, allows us to factorize the respective probabilistic models into expressions that are easier to optimize.The graphical representation of the probabilistic model allows us to visually see the impact of design choices we have made on the structure of the model.We often need to make high-level assumptions about the structure of the model.These modeling assumptions (hyperparameters) affect the prediction performance, but cannot be selected directly using the approaches we have seen so far.We will discuss different ways to choose the structure in Section 8.6.An introduction to probabilistic graphical models can be found in Bishop (2006, chapter 8), and an extensive description of the different applications and corresponding algorithmic implications can be found in the book by Koller and Friedman (2009).There are three main types of probabilistic graphical models: Factor graphs; see Figure 8.12(c) Graphical models allow for graph-based algorithms for inference and learning, e.g., via local message passing.Applications range from ranking in online games (Herbrich et al., 2007) and computer vision (e.g., image segmentation, semantic labeling, image denoising, image restoration (Kittler and Föglein, 1984;Sucar and Gillies, 1994;Shotton et al., 2006;Szeliski et al., 2008)) to coding theory (McEliece et al., 1998), solving linear equation systems (Shental et al., 2008), and iterative Bayesian state estimation in signal processing (Bickson et al., 2007;Deisenroth and Mohamed, 2012).One topic that is particularly important in real applications that we do not discuss in this book is the idea of structured prediction (Bakir et al., 2007;Nowozin et al., 2014), which allows machine learning models to tackle predictions that are structured, for example sequences, trees, and graphs.The popularity of neural network models has allowed more flexible probabilistic models to be used, resulting in many useful applications of structured models (Goodfellow et al., 2016, chapter 16).In recent years, there has been a renewed interest in graphical models due to their applications to causal inference (Pearl, 2009;Imbens and Rubin, 2015;Peters et al., 2017;Rosenbaum, 2017).In machine learning, we often need to make high-level modeling decisions that critically influence the performance of the model.The choices we make (e.g., the functional form of the likelihood) influence the number and type of free parameters in the model and thereby also the flexibility and expressivity of the model.More complex models are more flexible in A polynomial y = a 0 +a 1 x+a 2 x 2 can also describe linear functions by setting a 2 = 0, i.e., it is strictly more expressive than a first-order polynomial.the sense that they can be used to describe more datasets.For instance, a polynomial of degree 1 (a line y = a 0 + a 1 x) can only be used to describe linear relations between inputs x and observations y.A polynomial of degree 2 can additionally describe quadratic relationships between inputs and observations.One would now think that very flexible models are generally preferable to simple models because they are more expressive.A general problemWe have already seen an approach (cross-validation in Section 8.2.4) that can be used for model selection., where K is the number of experiments and σ is the standard deviation of the risk of each experiment.where R(V | M ) is the empirical risk (e.g., root mean square error) on the validation set V for model M .We repeat this procedure for all models and choose the model that performs best.Note that cross-validation not only gives us the expected generalization error, but we can also obtain highorder statistics, e.g., the standard error, an estimate of how uncertain the Evidencemean estimate is.Once the model is chosen, we can evaluate the final performance on the test set.There are many approaches to model selection, some of which are covered in this section.Generally, they all attempt to trade off model complexity and data fit.We assume that simpler models are less prone to overfitting than complex models, and hence the objective of model selection is to find the simplest model that explains the data reasonably well.This concept is also known as Occam's razor.Remark.If we treat model selection as a hypothesis testing problem, we are looking for the simplest hypothesis that is consistent with the data (Murphy, 2012).♢One may consider placing a prior on models that favors simpler models.However, it is not necessary to do this: An "automatic Occam's Razor" is quantitatively embodied in the application of Bayesian probability (Smith and Spiegelhalter, 1980;Jefferys and Berger, 1992;MacKay, 1992).Figure 8.14, adapted from MacKay (2003), gives us the basic intuition why complex and very expressive models may turn out to be a less probable choice for modeling a given dataset D. Let us think of the horizontal axis These predictions are quantified by a normalized probability distribution on D, i.e., it needs to integrate/sum to 1.representing the space of all possible datasets D. If we are interested in the posterior probability p(M i | D) of model M i given the data D, we can employ Bayes' theorem.Assuming a uniform prior p(M ) over all models, Bayes' theorem rewards models in proportion to how much they predicted the data that occurred.This prediction of the data given model M i , p(D | M i ), is called the evidence for M i .A simple model M 1 can only evidence predict a small number of datasets, which is shown by p(D | M 1 ); a more powerful model M 2 that has, e.g., more free parameters than M 1 , is able to predict a greater variety of datasets.This means, however, that M 2 does not predict the datasets in region C as well as M 1 .Suppose that equal prior probabilities have been assigned to the two models.Then, if the dataset falls into region C, the less powerful model M 1 is the more probable model.Earlier in this chapter, we argued that models need to be able to explain the data, i.e., there should be a way to generate data from a given model.Furthermore, if the model has been appropriately learned from the data, then we expect that the generated data should be similar to the empirical data.For this, it is helpful to phrase model selection as a hierarchical inference problem, which allows us to compute the posterior distribution over models.Let us consider a finite number of models M = {M 1 , .With a uniform prior p(M k ) = 1 K , which gives every model equal (prior) probability, determining the MAP estimate over models amounts to picking the model that maximizes the model evidence (8.44).).There are some important differences between a likelihood and a marginal likelihood (evidence): While the likelihood is prone to overfitting, the marginal likelihood is typically not as the model parameters have been marginalized out (i.e., we no longer have to fit the parameters).Furthermore, the marginal likelihood automatically embodies a trade-off between model complexity and data fit (Occam's razor).♢Draft (2024-01-15) of "Mathematics for Machine Learning".Feedback: https://mml-book.com.Consider The ratio of the posteriors is also called the posterior odds.The first fracposterior odds tion on the right-hand side of (8.46), the prior odds, measures how much prior odds our prior (initial) beliefs favor M 1 over M 2 .The ratio of the marginal likelihoods (second fraction on the right-hand-side) is called the Bayes factor Bayes factor and measures how well the data D is predicted by M 1 compared to M 2 .Remark.The Jeffreys-Lindley paradox states that the "Bayes factor always Jeffreys-Lindley paradox favors the simpler model since the probability of the data under a complex model with a diffuse prior will be very small" (Murphy, 2012).Here, a diffuse prior refers to a prior that does not favor specific models, i.e., many models are a priori plausible under this prior.♢If we choose a uniform prior over models, the prior odds term in (8.46) is 1, i.e., the posterior odds is the ratio of the marginal likelihoods (Bayes factor)If the Bayes factor is greater than 1, we choose model M 1 , otherwise model M 2 .In a similar way to frequentist statistics, there are guidelines on the size of the ratio that one should consider before "significance" of the result (Jeffreys, 1961).Remark (Computing the Marginal Likelihood).The marginal likelihood plays an important role in model selection: We need to compute Bayes factors (8.46) and posterior distributions over models (8.43).Unfortunately, computing the marginal likelihood requires us to solve an integral (8.44).This integration is generally analytically intractable, and we will have to resort to approximation techniques, e.g., numerical integration (Stoer and Burlirsch, 2002), stochastic approximations using Monte Carlo (Murphy, 2012), or Bayesian Monte Carlo techniques (O'Hagan, 1991;Rasmussen and Ghahramani, 2003).However, there are special cases in which we can solve it.In Section 6.6.1, we discussed conjugate models.If we choose a conjugate parameter prior p(θ), we can compute the marginal likelihood in closed form.In Chapter 9, we will do exactly this in the context of linear regression.♢We have seen a brief introduction to the basic concepts of machine learning in this chapter.For the rest of this part of the book we will see how the three different flavors of learning in Sections 8.2, 8.3, and 8.4 are applied to the four pillars of machine learning (regression, dimensionality reduction, density estimation, and classification).We mentioned at the start of the section that there are high-level modeling choices that influence the performance of the model.Examples include the following:The degree of a polynomial in a regression setting The number of components in a mixture model The network architecture of a (deep) neural network The type of kernel in a support vector machine The dimensionality of the latent space in PCA The learning rate (schedule) in an optimization algorithm In parametric models, the number of parameters is often related to the complexity of the model class.Rasmussen and Ghahramani (2001) showed that the automatic Occam's razor does not necessarily penalize the number of parameters in a model, but it is active in terms of the complexity of functions.They also showed that the automatic Occam's razor also holds for Bayesian nonparametric models with many parameters, e.g., Gaussian processes.If we focus on the maximum likelihood estimate, there exist a number of heuristics for model selection that discourage overfitting.They are called information criteria, and we choose the model with the largest value.The Akaike information criterion (AIC) (Akaike, 1974) Akaike information criterioncorrects for the bias of the maximum likelihood estimator by addition of a penalty term to compensate for the overfitting of more complex models with lots of parameters.Here, M is the number of model parameters.The AIC estimates the relative information lost by a given model.The Bayesian information criterion (BIC) (Schwarz, 1978) Bayesian information criterioncan be used for exponential family distributions.Here, N is the number of data points and M is the number of parameters.BIC penalizes model complexity more heavily than AIC.In the following, we will apply the mathematical concepts from Chapters 2, 5, 6, and 7 to solve linear regression (curve fitting) problems.In regression, we aim to find a function f that maps inputs x ∈ R D to correregression sponding function values f (x) ∈ R. We assume we are given a set of training inputs x n and corresponding noisy observations y n = f (x n )+ϵ, where ϵ is an i.i.d.random variable that describes measurement/observation noise and potentially unmodeled processes (which we will not consider further in this chapter).Throughout this chapter, we assume zero-mean Gaussian noise.Regression is a fundamental problem in machine learning, and regression problems appear in a diverse range of research areas and applica- tions, including time-series analysis (e.g., system identification), control and robotics (e.g., reinforcement learning, forward/inverse model learning), optimization (e.g., line searches, global optimization), and deeplearning applications (e.g., computer games, speech-to-text translation, image recognition, automatic video annotation).Regression is also a key ingredient of classification algorithms.Finding a regression function requires solving a variety of problems, including the following:Choice of the model (type) and the parametrization of the regression function.Given a dataset, what function classes (e.g., polynomi-Normally, the type of noise could also be a "model choice", but we fix the noise to be Gaussian in this chapter.als) are good candidates for modeling the data, and what particular parametrization (e.g., degree of the polynomial) should we choose?Model selection, as discussed in Section 8.6, allows us to compare various models to find the simplest model that explains the training data reasonably well.Finding good parameters.Having chosen a model of the regression function, how do we find good model parameters?Here, we will need to look at different loss/objective functions (they determine what a "good" fit is) and optimization algorithms that allow us to minimize this loss.Overfitting and model selection.Overfitting is a problem when the regression function fits the training data "too well" but does not generalize to unseen test data.Overfitting typically occurs if the underlying model (or its parametrization) is overly flexible and expressive; see Section 8.6.We will look at the underlying reasons and discuss ways to mitigate the effect of overfitting in the context of linear regression.Relationship between loss functions and parameter priors.Loss functions (optimization objectives) are often motivated and induced by probabilistic models.We will look at the connection between loss functions and the underlying prior assumptions that induce these losses.Uncertainty modeling.In any practical setting, we have access to only a finite, potentially large, amount of (training) data for selecting the model class and the corresponding parameters.Given that this finite amount of training data does not cover all possible scenarios, we may want to describe the remaining parameter uncertainty to obtain a measure of confidence of the model's prediction at test time; the smaller the training set, the more important uncertainty modeling.Consistent modeling of uncertainty equips model predictions with confidence bounds.In the following, we will be using the mathematical tools from Chapters 3, 5, 6 and 7 to solve linear regression problems.We will discuss maximum likelihood and maximum a posteriori (MAP) estimation to find optimal model parameters.Using these parameter estimates, we will have a brief look at generalization errors and overfitting.Toward the end of this chapter, we will discuss Bayesian linear regression, which allows us to reason about model parameters at a higher level, thereby removing some of the problems encountered in maximum likelihood and MAP estimation.Because of the presence of observation noise, we will adopt a probabilistic approach and explicitly model the noise using a likelihood function.More specifically, throughout this chapter, we consider a regression problem with the likelihood function(9.1)Here, x ∈ R D are inputs and y ∈ R are noisy function values (targets).With (9.1), the functional relationship between x and y is given aswhere ϵ ∼ N 0, σ 2 is independent, identically distributed (i.i.d.) Gaussian measurement noise with mean 0 and variance σ 2 .Our objective is to find a function that is close (similar) to the unknown function f that generated the data and that generalizes well.In this chapter, we focus on parametric models, i.e., we choose a parametrized function and find parameters θ that "work well" for modeling the data.For the time being, we assume that the noise variance σ 2 is known and focus on learning the model parameters θ.In linear regression, we consider the special case that the parameters θ appear linearly in our model.An example of linear regression is given bywhere θ ∈ R D are the parameters we seek.The class of functions described by (9.4) are straight lines that pass through the origin.In (9.4), we chose a parametrization f (x) = x ⊤ θ.A Dirac delta (delta function) is zero everywhere except at a single point, and its integral is 1.It can be considered a Gaussian in the limit of σ 2 → 0.The likelihood in (9.3) is the probability density function of y evalulikelihood ated at x ⊤ θ.Note that the only source of uncertainty originates from the observation noise (as x and θ are assumed known in (9.3)).Without observation noise, the relationship between x and y would be deterministic and (9.3) would be a Dirac delta.Linear regression refers to models that are linear in the parameters.The linear regression model in (9.3)-(9.4) is not only linear in the parameters, but also linear in the inputs x. Figure 9.2(a) shows examples of such functions.We will see later that y = ϕ ⊤ (x)θ for nonlinear transformations ϕ is also a linear regression model because "linear regression" refers to models that are "linear in the parameters", i.e., models that describe a function by a linear combination of input features.Here, a "feature" is a representation ϕ(x) of the inputs x.In the following, we will discuss in more detail how to find good parameters θ and how to evaluate whether a parameter set "works well".For the time being, we assume that the noise variance σ 2 is known.Consider the linear regression setting (9.corresponding graphical model is given in Figure 9.3.Note that y i and y j are conditionally independent given their respective inputs x i , x j so that the likelihood factorizes according towhere we defined X := {x 1 , . . ., x N } and Y := {y 1 , . . ., y N } as the sets of training inputs and corresponding targets, respectively.The likelihood and the factors p(y n | x n , θ) are Gaussian due to the noise distribution; see (9.3).In the following, we will discuss how to find optimal parameters θ * ∈ R D for the linear regression model (9.4).Once the parameters θ * are found, we can predict function values by using this parameter estimate in (9.4) so that at an arbitrary test input x * the distribution of the corresponding target y * isIn the following, we will have a look at parameter estimation by maximizing the likelihood, a topic that we already covered to some degree in Section 8.3.Draft (2024-01-15) of "Mathematics for Machine Learning".Feedback: https://mml-book.com.A widely used approach to finding the desired parameters θ ML is maximum maximum likelihood estimation likelihood estimation, where we find parameters θ ML that maximize the likelihood (9.5b).Intuitively, maximizing the likelihood means maximiz-Maximizing the likelihood means maximizing the predictive distribution of the (training) data given the parameters.ing the predictive distribution of the training data given the model parameters.We obtain the maximum likelihood parameters asThe likelihood is not a probability distribution in the parameters.Remark.The likelihood p(y | x, θ) is not a probability distribution in θ: It is simply a function of the parameters θ but does not integrate to 1 (i.e., it is unnormalized), and may not even be integrable with respect to θ.However, the likelihood in (9.7) is a normalized probability distribution in y. ♢To find the desired parameters θ ML that maximize the likelihood, we typically perform gradient ascent (or gradient descent on the negative likelihood).In the case of linear regression we consider here, however, Since the logarithm is a (strictly) monotonically increasing function, the optimum of a function f is identical to the optimum of log f .a closed-form solution exists, which makes iterative gradient descent unnecessary.In practice, instead of maximizing the likelihood directly, we apply the log-transformation to the likelihood function and minimize the negative log-likelihood.Remark (Log-Transformation). Since the likelihood (9.5b) is a product of N Gaussian distributions, the log-transformation is useful since (a) it does not suffer from numerical underflow, and (b) the differentiation rules will turn out simpler.More specifically, numerical underflow will be a problem when we multiply N probabilities, where N is the number of data points, since we cannot represent very small numbers, such as 10 −256 .Furthermore, the log-transform will turn the product into a sum of logprobabilities such that the corresponding gradient is a sum of individual gradients, instead of a repeated application of the product rule (5.46) to compute the gradient of a product of N terms.♢To find the optimal parameters θ ML of our linear regression problem, we minimize the negative log-likelihoodwhere we exploited that the likelihood (9.5b) factorizes over the number of data points due to our independence assumption on the training set.In the linear regression model (9.4), the likelihood is Gaussian (due to the Gaussian additive noise term), such that we arrive atwhere the constant includes all terms independent of θ.Using (9.9) in the negative log-likelihood (9.8), we obtain (ignoring the constant terms)where we define the design matrixThe negative log-likelihood function is also called error function.design matrix collection of training inputs and y := [y 1 , . . ., y N ] ⊤ ∈ R N as a vector that collects all training targets.Note that the nth row in the design matrix X corresponds to the training input x n .In (9.10b), we used the fact that theThe squared error is often used as a measure of distance.sum of squared errors between the observations y n and the corresponding model prediction x ⊤ n θ equals the squared distance between y and Xθ.Recall from Section 3.1 that ∥x∥ 2 = x ⊤ x if we choose the dot product as the inner product.With (9.10b), we have now a concrete form of the negative log-likelihood function we need to optimize.We immediately see that (9.10b) is quadratic in θ.This means that we can find a unique global solution θ ML for minimizing the negative log-likelihood L. We can find the global optimum by computing the gradient of L, setting it to 0 and solving for θ.Using the results from Chapter 5, we compute the gradient of L with respect to the parameters asThe maximum likelihood estimator θ ML solves dL dθ = 0 ⊤ (necessary optimality condition) and we obtain Ignoring the possibility of duplicate data points, rk(X) = D if N ⩾ D, i.e., we do not have more parameters than data points.We could right-multiply the first equation by (X ⊤ X) −1 because X ⊤ X is positive definite if rk(X) = D, where rk(X) denotes the rank of X.Remark.Setting the gradient to 0 ⊤ is a necessary and sufficient condition, and we obtain a global minimum since the HessianRemark.The maximum likelihood solution in (9.12c) requires us to solve a system of linear equations of the form Aθ = b with A = (X ⊤ X) and b = X ⊤ y. ♢Draft (2024-01-15) of "Mathematics for Machine Learning".Feedback: https://mml-book.com.Let us have a look at Figure 9.2, where we aim to fit a straight line f (x) = θx, where θ is an unknown slope, to a dataset using maximum likelihood estimation.Examples of functions in this model class (straight lines) are shown in Figure 9.2(a).For the dataset shown in Figure 9.2(b), we find the maximum likelihood estimate of the slope parameter θ using (9.12c) and obtain the maximum likelihood linear function in Figure 9.2(c).So far, we considered the linear regression setting described in (9.4), which allowed us to fit straight lines to data using maximum likelihood estimation.However, straight lines are not sufficiently expressive when it Linear regression refers to "linear-inthe-parameters" regression models, but the inputs can undergo any nonlinear transformation.comes to fitting more interesting data.Fortunately, linear regression offers us a way to fit nonlinear functions within the linear regression framework: Since "linear regression" only refers to "linear in the parameters", we can perform an arbitrary nonlinear transformation ϕ(x) of the inputs x and then linearly combine the components of this transformation.The corresponding linear regression model iswhere ϕ : R D → R K is a (nonlinear) transformation of the inputs x and ϕ k : R D → R is the kth component of the feature vector ϕ.Note that the feature vector model parameters θ still appear only linearly.We are concerned with a regression problem y = ϕ ⊤ (x)θ+ϵ, where x ∈ R and θ ∈ R K .A transformation that is often used in this context isThis means that we "lift" the original one-dimensional input space into a K-dimensional feature space consisting of all monomials x k for k = 0, . . ., K − 1.With these features, we can model polynomials of degree ⩽ K−1 within the framework of linear regression: A polynomial of degreewhere ϕ is defined in (9.14) and θ = [θ 0 , . . ., θ K−1 ] ⊤ ∈ R K contains the (linear) parameters θ k .Let us now have a look at maximum likelihood estimation of the parameters θ in the linear regression model (9.13).We consider training inputs x n ∈ R D and targets y n ∈ R, n = 1, . . ., N , and define the feature matrix feature matrix (design matrix) as design matrix. . .(9.16) where Φ ij = ϕ j (x i ) and ϕ j : R D → R.Example 9.4 (Feature Matrix for Second-order Polynomials) For a second-order polynomial and N training points x n ∈ R, n = 1, . . ., N , the feature matrix is.(9.17)With the feature matrix Φ defined in (9.16), the negative log-likelihood for the linear regression model (9.13) can be written asComparing (9.18) with the negative log-likelihood in (9.10b) for the "feature-free" model, we immediately see we just need to replace X with Φ.Since both X and Φ are independent of the parameters θ that we wish to optimize, we arrive immediately at the maximum likelihood estimate maximum likelihood estimatefor the linear regression problem with nonlinear features defined in (9.13).Remark.When we were working without features, we required X ⊤ X to be invertible, which is the case when rk(X) = D, i.e., the columns of XWe fit a polynomial of degree 4 using maximum likelihood estimation, i.e., parameters θ ML are given in (9.19).The maximum likelihood estimate yields function values ϕ ⊤ (x * )θ ML at any test location x * .The result is shown in Figure 9.4(b).Thus far, we assumed that the noise variance σ 2 is known.However, we can also use the principle of maximum likelihood estimation to obtain the maximum likelihood estimator σ 2 ML for the noise variance.To do this, we follow the standard procedure: We write down the log-likelihood, compute its derivative with respect to σ 2 > 0, set it to 0, and solve.The log-likelihood is given by logThe partial derivative of the log-likelihood with respect to σ 2 is thenso that we identifyTherefore, the maximum likelihood estimate of the noise variance is the empirical mean of the squared distances between the noise-free function values ϕ ⊤ (x n )θ and the corresponding noisy observations y n at input locations x n .We just discussed how to use maximum likelihood estimation to fit linear models (e.g., polynomials) to data.We can evaluate the quality of the model by computing the error/loss incurred.One way of doing this is to compute the negative log-likelihood (9.10b), which we minimized to determine the maximum likelihood estimator.Alternatively, given that the noise parameter σ 2 is not a free model parameter, we can ignore the scaling by 1/σ 2 , so that we end up with a squared-error-loss function ∥y − Φθ∥ 2 .Instead of using this squared loss, we often use the root mean root mean square error square error (RMSE) tion values y n .For example, if we fit a model that maps post-codes (x is given in latitude, longitude) to house prices (y-values are EUR) then the RMSE is also measured in EUR, whereas the squared error is given in EUR 2 .If we choose to include the factor σ 2 from the original negativeThe negative log-likelihood is unitless.log-likelihood (9.10b), then we end up with a unitless objective, i.e., in the preceding example, our objective would no longer be in EUR or EUR 2 .For model selection (see Section 8.6), we can use the RMSE (or the negative log-likelihood) to determine the best degree of the polynomial by finding the polynomial degree M that minimizes the objective.Given that the polynomial degree is a natural number, we can perform a brute-force search and enumerate all (reasonable) values of M .For a training set of size N it is sufficient to test 0 ⩽ M ⩽ N − 1.For M < N , the maximum likelihood estimator is unique.For M ⩾ N , we have more parameters than data points, and would need to solve an underdetermined system of linear equations (Φ ⊤ Φ in (9.19) would also no longer be invertible) so that there are infinitely many possible maximum likelihood estimators.Figure 9.5 shows a number of polynomial fits determined by maximum likelihood for the dataset from Figure 9.4(a) with N = 10 observations.We notice that polynomials of low degree (e.g., constants (M = 0) or linear (M = 1)) fit the data poorly and, hence, are poor representations of the true underlying function.For degrees M = 3, . . ., 6, the fits look plausible and smoothly interpolate the data.When we go to higher-degree The case of M = N − 1 is extreme in the sense that otherwise the null space of the corresponding system of linear equations would be non-trivial, and we would have infinitely many optimal solutions to the linear regression problem.polynomials, we notice that they fit the data better and better.In the extreme case of M = N − 1 = 9, the function will pass through every single data point.However, these high-degree polynomials oscillate wildly and are a poor representation of the underlying function that generated the data, such that we suffer from overfitting.overfitting Note that the noise variance σ 2 > 0.Remember that the goal is to achieve good generalization by making accurate predictions for new (unseen) data.We obtain some quantitative insight into the dependence of the generalization performance on the polynomial of degree M by considering a separate test set comprising 200 data points generated using exactly the same procedure used to generate the training set.As test inputs, we chose a linear grid of 200 points in the interval of [−5, 5].For each choice of M , we evaluate the RMSE (9.23) for both the training data and the test data.Looking now at the test error, which is a qualitive measure of the generalization properties of the corresponding polynomial, we notice that initially the test error decreases; see Figure 9.6 (orange).For fourth-order polynomials, the test error is relatively low and stays relatively constant up to degree 5.However, from degree 6 onward the test error increases significantly, and high-order polynomials have very bad generalization properties.In this particular example, this also is evident from the correspondingWe just saw that maximum likelihood estimation is prone to overfitting.We often observe that the magnitude of the parameter values becomes relatively large if we run into overfitting (Bishop, 2006).To mitigate the effect of huge parameter values, we can place a prior distribution p(θ) on the parameters.The prior distribution explicitly encodes what parameter values are plausible (before having seen any data).For example, a Gaussian prior p(θ) = N 0, 1 on a single parameter θ encodes that parameter values are expected lie in the interval [−2, 2] (two standard deviations around the mean value).Once a dataset X , Y is available, instead of maximizing the likelihood we seek parameters that maximize the posterior distribution p(θ | X , Y).This procedure is called maximum a posteriori (MAP) estimation.The posterior over the parameters θ, given the training data X , Y, is obtained by applying Bayes' theorem (Section 6.3) asSince the posterior explicitly depends on the parameter prior p(θ), the prior will have an effect on the parameter vector we find as the maximizer of the posterior.We will see this more explicitly in the following.The parameter vector θ MAP that maximizes the posterior (9.24) is the MAP estimate.To find the MAP estimate, we follow steps that are similar in flavor to maximum likelihood estimation.We start with the log-transform and compute the log-posterior as where the constant comprises the terms that are independent of θ.We see that the log-posterior in (9.25) is the sum of the log-likelihood p(Y | X , θ) and the log-prior log p(θ) so that the MAP estimate will be a "compromise" between the prior (our suggestion for plausible parameter values before observing data) and the data-dependent likelihood.To find the MAP estimate θ MAP , we minimize the negative log-posterior distribution with respect to θ, i.e., we solve(9.26)The gradient of the negative log-posterior with respect to θ iswhere we identify the first term on the right-hand side as the gradient of the negative log-likelihood from (9.11c).With a (conjugate) Gaussian prior p(θ) = N 0, b 2 I on the parameters θ, the negative log-posterior for the linear regression setting (9.13), we obtain the negative log posterior28) Here, the first term corresponds to the contribution from the log-likelihood, and the second term originates from the log-prior.The gradient of the logposterior with respect to the parameters θ is thenWe will find the MAP estimate θ MAP by setting this gradient to 0 ⊤ and solving for θ MAP .We obtainso that the MAP estimate is (by transposing both sides of the last equality) Φ ⊤ Φ is symmetric, positive semi definite.The additional term in (9.31) is strictly positive definite so that the inverse exists.Comparing the MAP estimate in (9.31) with the maximum likelihood estimate in (9.19), we see that the only difference between both solutions is the additional term σ 2 b 2 I in the inverse matrix.This term ensures that Φ ⊤ Φ + σ 2 b 2 I is symmetric and strictly positive definite (i.e., its inverse exists and the MAP estimate is the unique solution of a system of linear equations).Moreover, it reflects the impact of the regularizer.In the polynomial regression example from Section 9.2.1, we place a Gaussian prior p(θ) = N 0, I on the parameters θ and determine the MAP estimates according to (9.31).In Figure 9.7, we show both the maximum likelihood and the MAP estimates for polynomials of degree 6 (left) and degree 8 (right).The prior (regularizer) does not play a significant role for the low-degree polynomial, but keeps the function relatively smooth for higher-degree polynomials.Although the MAP estimate can push the boundaries of overfitting, it is not a general solution to this problem, so we need a more principled approach to tackle overfitting.Instead of placing a prior distribution on the parameters θ, it is also possible to mitigate the effect of overfitting by penalizing the amplitude of the parameter by means of regularization.In regularized least squares, we useful for variable selection.For p = 1, the regularizer is called LASSO LASSO (least absolute shrinkage and selection operator) and was proposed by Tibshirani (1996).♢The regularizer λ ∥θ∥ 2 2 in (9.32) can be interpreted as a negative log-Gaussian prior, which we use in MAP estimation; see (9.26).More specifically, with a Gaussian prior p(θ) = N 0, b 2 I , we obtain the negative log-Gaussian priorso that for λ = 1 2b 2 the regularization term and the negative log-Gaussian prior are identical.Given that the regularized least-squares loss function in (9.32) consists of terms that are closely related to the negative log-likelihood plus a negative log-prior, it is not surprising that, when we minimize this loss, we obtain a solution that closely resembles the MAP estimate in (9.31).More specifically, minimizing the regularized least-squares loss function yieldswhich is identical to the MAP estimate in (9.31) for λ = σ 2 b 2 , where σ 2 is the noise variance and b 2 the variance of the (isotropic) Gaussian prior p(θ) = N 0, b 2 I .A point estimate is a single specific parameter value, unlike a distribution over plausible parameter settings.So far, we have covered parameter estimation using maximum likelihood and MAP estimation where we found point estimates θ * that optimize an objective function (likelihood or posterior).We saw that both maximum likelihood and MAP estimation can lead to overfitting.In the next section, we will discuss Bayesian linear regression, where we use Bayesian inference (Section 8.4) to find a posterior distribution over the unknown parameters, which we subsequently use to make predictions.More specifically, for predictions we will average over all plausible sets of parameters instead of focusing on a point estimate.Previously, we looked at linear regression models where we estimated the model parameters θ, e.g., by means of maximum likelihood or MAP estimation.We discovered that MLE can lead to severe overfitting, in particular, in the small-data regime.MAP addresses this issue by placing a prior on the parameters that plays the role of a regularizer.Bayesian linear regression pushes the idea of the parameter prior a step further and does not even attempt to compute a point estimate of the parameters, but instead the full posterior distribution over the parameters is taken into account when making predictions.This means we do not fit any parameters, but we compute a mean over all plausible parameters settings (according to the posterior).In Bayesian linear regression, we consider the model (9.35)where we now explicitly place a Gaussian prior p(θ) = N m 0 , S 0 on θ, (9.36)In practice, we are usually not so much interested in the parameter values θ themselves.Instead, our focus often lies in the predictions we make with those parameter values.In a Bayesian setting, we take the parameter distribution and average over all plausible parameter settings when we make predictions.More specifically, to make predictions at an input x * , we integrate out θ and obtainwhich we can interpret as the average prediction of y * | x * , θ for all plausible parameters θ according to the prior distribution p(θ).Note that predictions using the prior distribution only require us to specify the input x * , but no training data.In our model (9.35), we chose a conjugate (Gaussian) prior on θ so that the predictive distribution is Gaussian as well (and can be computed in closed form): With the prior distribution p(θ) = N m 0 , S 0 , we obtain the predictive distribution aswhere we exploited that (i) the prediction is Gaussian due to conjugacy (see Section 6.6) and the marginalization property of Gaussians (see Section 6.5), (ii) the Gaussian noise is independent so that (9.39) and (iii) y * is a linear transformation of θ so that we can apply the rules for computing the mean and covariance of the prediction analytically by using (6.50) and (6.51), respectively.In (9.38), the term ϕ ⊤ (x * )S 0 ϕ(x * ) in the predictive variance explicitly accounts for the uncertainty associated Draft (2024-01-15) of "Mathematics for Machine Learning".Feedback: https://mml-book.com.with the parameters θ, whereas σ 2 is the uncertainty contribution due to the measurement noise.If we are interested in predicting noise-free function values f (x * ) = ϕ ⊤ (x * )θ instead of the noise-corrupted targets y * we obtain  Let us consider a Bayesian linear regression problem with polynomials of degree 5. We choose a parameter prior p(θ) = N 0, 1 4 I .Figure 9.9 visualizes the induced prior distribution over functions (shaded area: dark gray: 67% confidence bound; light gray: 95% confidence bound) induced by this parameter prior, including some function samples from this prior.A function sample is obtained by first sampling a parameter vector θ i ∼ p(θ) and then computing f i (•) = θ ⊤ i ϕ(•).We used 200 input locations x * ∈ [−5, 5] to which we apply the feature function ϕ(•).The uncertainty (represented by the shaded area) in Figure 9.9 is solely due to the parameter uncertainty because we considered the noise-free predictive distribution (9.40).So far, we looked at computing predictions using the parameter prior p(θ).However, when we have a parameter posterior (given some training data X , Y), the same principles for prediction and inference hold as in (9.37) -we just need to replace the prior p(θ) with the posterior p(θ | X , Y).In the following, we will derive the posterior distribution in detail before using it to make predictions.Given a training set of inputs x n ∈ R D and corresponding observations y n ∈ R, n = 1, . . ., N , we compute the posterior over the parameters using Bayes' theorem aswhere X is the set of training inputs and Y the collection of corresponding training targets.Furthermore, p(Y | X , θ) is the likelihood, p(θ) the parameter prior, andthe marginal likelihood/evidence, which is independent of the parameters marginal likelihood evidence θ and ensures that the posterior is normalized, i.e., it integrates to 1.WeThe marginal likelihood is the expected likelihood under the parameter prior.can think of the marginal likelihood as the likelihood averaged over all possible parameter settings (with respect to the prior distribution p(θ)).Theorem 9.1 (Parameter Posterior).In our model (9.35), the parameter posterior (9.41) can be computed in closed form aswhere the subscript N indicates the size of the training set.Proof Bayes' theorem tells us that the posterior p(θ | X , Y) is proportional to the product of the likelihood p(Y | X , θ) and the prior p(θ):Instead of looking at the product of the prior and the likelihood, we can transform the problem into log-space and solve for the mean and covariance of the posterior by completing the squares.The sum of the log-prior and the log-likelihood isDraft (2024-01-15) of "Mathematics for Machine Learning".Feedback: https://mml-book.com.where the constant contains terms independent of θ.We will ignore the constant in the following.We now factorize (9.45b), which yieldswhere the constant contains the black terms in (9.46a), which are independent of θ.The orange terms are terms that are linear in θ, and the blue terms are the ones that are quadratic in θ.Inspecting (9.46b), we find that this equation is quadratic in θ.The fact that the unnormalized log-posterior distribution is a (negative) quadratic form implies that the posterior is Gaussian, i.e.,where we used (9.46b) in the last expression.The remaining task is it to bring this (unnormalized) Gaussian into the form that is proportional to N θ | m N , S N , i.e., we need to identify the mean m N and the covariance matrix S N .To do this, we use the concept of completing the squares.The desired log-posterior is completing the squaresHere, we factorized the quadratic formterm that is quadratic in θ alone (blue), a term that is linear in θ (orange), and a constant term (black).This allows us now to find S N and m N by matching the colored expressions in (9.46b) and (9.48b), which yieldsRemark (General Approach to Completing the Squares).If we are given an equationwhere A is symmetric and positive definite, which we wish to bring into the formwe can do this by settingWe can see that the terms inside the exponential in (9.47b) are of the form (9.51) withSince A, a can be difficult to identify in equations like (9.46a), it is often helpful to bring these equations into the form (9.51) that decouples quadratic term, linear terms, and constants, which simplifies finding the desired solution.In (9.37), we computed the predictive distribution of y * at a test input x * using the parameter prior p(θ).In principle, predicting with the parameter posterior p(θ | X , Y) is not fundamentally different given that in our conjugate model the prior and posterior are both Gaussian (with different parameters).Therefore, by following the same reasoning as in Section 9.3.2,we obtain the (posterior) predictive distributionThe term ϕ ⊤ (x * )S N ϕ(x * ) reflects the posterior uncertainty associatedwith the parameters θ.Note that S N depends on the training inputs through Φ; see (9.43b).The predictive mean ϕ ⊤ (x * )m N coincides with the predictions made with the MAP estimate θ MAP .Remark (Marginal Likelihood and Posterior Predictive Distribution).By replacing the integral in (9.57a), the predictive distribution can be equivalently written as the expectationwhere the expectation is taken with respect to the parameter posterior p(θ | X , Y).Writing the posterior predictive distribution in this way highlights a close resemblance to the marginal likelihood (9.42).The key difference between the marginal likelihood and the posterior predictive distribution are (i) the marginal likelihood can be thought of predicting the training targets y and not the test targets y * , and (ii) the marginal likelihood averages with respect to the parameter prior and not the parameter posterior.♢Remark (Mean and Variance of Noise-Free Function Values).In many cases, we are not interested in the predictive distribution p(y * | X , Y, x * ) of a (noisy) observation y * .Instead, we would like to obtain the distribution of the (noise-free) function values f (x * ) = ϕ ⊤ (x * )θ.We determine the corresponding moments by exploiting the properties of means and variances, which yields(9.59)We see that the predictive mean is the same as the predictive mean for noisy observations as the noise has mean 0, and the predictive variance only differs by σ 2 , which is the variance of the measurement noise: When we predict noisy function values, we need to include σ 2 as a source of uncertainty, but this term is not needed for noise-free predictions.Here, the only remaining uncertainty stems from the parameter posterior.♢ Integrating out parameters induces a distribution over functions.Remark (Distribution over Functions).The fact that we integrate out the parameters θ induces a distribution over functions: If we sample θ i ∼ p(θ | X , Y) from the parameter posterior, we obtain a single function realization θ ⊤ i ϕ(•).The mean function, i.e., the set of all expected function mean function values. The (marginal) variance, i.e., the variance of the function f (•), is given by ϕ ⊤ (•)S N ϕ(•).♢Let us revisit the Bayesian linear regression problem with polynomials of degree 5. We choose a parameter prior p(θ) = N 0, 1 4 I .Figure 9.9 visualizes the prior over functions induced by the parameter prior and sample functions from this prior.The right panels show samples from the posterior over functions: Here, we sampled parameters θ i from the parameter posterior and computed the function ϕ ⊤ (x * )θ i , which is a single realization of a function under the posterior distribution over functions.For low-order polynomials, the parameter posterior does not allow the parameters to vary much: The sampled functions are nearly identical.When we make the model more flexible by adding more parameters (i.e., we end up with a higher-order polynomial), these parameters are not sufficiently constrained by the posterior, and the sampled functions can be easily visually separated.We also see in the corresponding panels on the left how the uncertainty increases, especially at the boundaries.Although for a seventh-order polynomial the MAP estimate yields a reasonable fit, the Bayesian linear regression model additionally tells us that the posterior uncertainty is huge.This information can be critical when we use these predictions in a decision-making system, where bad decisions can have significant consequences (e.g., in reinforcement learning or robotics).In Section 8.6.2, we highlighted the importance of the marginal likelihood for Bayesian model selection.In the following, we compute the marginal likelihood for Bayesian linear regression with a conjugate Gaussian prior on the parameters, i.e., exactly the setting we have been discussing in this chapter.Just to recap, we consider the following generative process:θ ∼ N m 0 , S 0 (9.60a)The marginal likelihood can be interpreted as the expected likelihood under the prior, i.e.,where we integrate out the model parameters θ.We compute the marginal likelihood in two steps: First, we show that the marginal likelihood is Gaussian (as a distribution in y); second, we compute the mean and covariance of this Gaussian.1.The marginal likelihood is Gaussian: From Section 6.5.2, we know that (i) the product of two Gaussian random variables is an (unnormalized) Gaussian distribution, and (ii) a linear transformation of a Gaussian random variable is Gaussian distributed.In (9.61b), we require a linear transformation to bring N y | Xθ, σ 2 I into the form N θ | µ, Σ for some µ, Σ.Once this is done, the integral can be solved in closed form.The result is the normalizing constant of the product of the two Gaussians.The normalizing constant itself has Gaussian shape; see (6.76).2. Mean and covariance.We compute the mean and covariance matrix of the marginal likelihood by exploiting the standard results for means and covariances of affine transformations of random variables; see Section 6.4.4.The mean of the marginal likelihood is computed asNote that ϵ ∼ N 0, σ 2 I is a vector of i.i.d.random variables.The covariance matrix is given asHence, the marginal likelihood isDraft (2024-01-15) of "Mathematics for Machine Learning".Feedback: https://mml-book.com.= N y | Xm 0 , XS 0 X ⊤ + σ 2 I .(9.64b)Given the close connection with the posterior predictive distribution (see Remark on Marginal Likelihood and Posterior Predictive Distribution earlier in this section), the functional form of the marginal likelihood should not be too surprising.Having crunched through much algebra to derive maximum likelihood and MAP estimates, we will now provide a geometric interpretation of maximum likelihood estimation.Let us consider a simple linear regression setting (9.65) in which we consider linear functions f : R → R that go through the origin (we omit features here for clarity).The parameter θ determines the slope of the line.Figure 9.12(a) shows a one-dimensional dataset.With a training data set {(x 1 , y 1 ), . . ., (x N , y N )} we recall the results from Section 9.2.1 and obtain the maximum likelihood estimator for the slope parameter asThis means for the training inputs X we obtain the optimal (maximum likelihood) reconstruction of the training targets asi.e., we obtain the approximation with the minimum least-squares error between y and Xθ.As we are looking for a solution of y = Xθ, we can think of linear regression as a problem for solving systems of linear equations.There-Linear regression can be thought of as a method for solving systems of linear equations.fore, we can relate to concepts from linear algebra and analytic geometry that we discussed in Chapters 2 and 3.In particular, looking carefully at (9.67) we see that the maximum likelihood estimator θ ML in our example from (9.65) effectively does an orthogonal projection of y onto the one-dimensional subspace spanned by X. Recalling the results on or-Maximum likelihood linear regression performs an orthogonal projection.thogonal projections from Section 3.8, we identify XX ⊤ X ⊤ X as the projection matrix, θ ML as the coordinates of the projection onto the one-dimensional subspace of R N spanned by X and Xθ ML as the orthogonal projection of y onto this subspace.Therefore, the maximum likelihood solution provides also a geometrically optimal solution by finding the vectors in the subspace spanned by X that are "closest" to the corresponding observations y, where "closest" means the smallest (squared) distance of the function values y n to x n θ.This is achieved by orthogonal projections.Figure 9.12(b) shows the projection of the noisy observations onto the subspace that minimizes the squared distance between the original dataset and its projection (note that the x-coordinate is fixed), which corresponds to the maximum likelihood solution.In the general linear regression case where y = ϕ ⊤ (x)θ + ϵ, ϵ ∼ N 0, σ 2 (9.68) with vector-valued features ϕ(x) ∈ R K , we again can interpret the maximum likelihood result y ≈ Φθ ML , (9.69)as a projection onto a K-dimensional subspace of R N , which is spanned by the columns of the feature matrix Φ; see Section 3.8.2.If the feature functions ϕ k that we use to construct the feature matrix Φ are orthonormal (see Section 3.7), we obtain a special case where the columns of Φ form an orthonormal basis (see Section 3.5), such that Φ ⊤ Φ = I.This will then lead to the projectionso that the maximum likelihood projection is simply the sum of projections of y onto the individual basis vectors ϕ k , i.e., the columns of Φ.Furthermore, the coupling between different features has disappeared due to the orthogonality of the basis.Many popular basis functions in signal processing, such as wavelets and Fourier bases, are orthogonal basis functions.When the basis is not orthogonal, one can convert a set of linearly independent basis functions to an orthogonal basis by using the Gram-Schmidt process; see Section 3.8.3 and (Strang, 2003).In this chapter, we discussed linear regression for Gaussian likelihoods and conjugate Gaussian priors on the parameters of the model.This allowed for closed-form Bayesian inference.However, in some applications we may want to choose a different likelihood function.For example, in a binary classification setting, we observe only two possible (categorical) classification outcomes, and a Gaussian likelihood is inappropriate in this setting.Instead, we can choose a Bernoulli likelihood that will return a probability of the predicted label to be 1 (or 0).We refer to the books by Barber (2012), Bishop (2006) generalizes linear regression by allowing the linear model to be related to the observed values via a smooth and invertible function σ(•) that may be nonlinear so that y = σ(f (x)), where f (x) = θ ⊤ ϕ(x) is the linear regression model from (9.13).We can therefore think of a generalized linear model in terms of function composition y = σ • f , where f is a linear regression model and σ the activation function.Note that although we are talking about "generalized linear models", the outputs y are no longer linear in the parameters θ.In logistic regression, we choose the logistic regression logistic sigmoid σ(f ) = also clear that generalized linear models are the building blocks of (deep) feedforward neural networks: If we consider a generalized linear model y = σ(Ax + b), where A is a weight matrix and b a bias vector, we identify this generalized linear model as a single-layer neural network with activation function σ(•).We can now recursively compose these functions via A great post on the relation between GLMs and deep networks is available at https://tinyurl.com/glm-dnn.for k = 0, . . ., K − 1, where x 0 are the input features and x K = y are the observed outputs, such thatTherefore, the building blocks of this deep neural network are the generalized linear models defined in (9.72).Neural networks (Bishop, 1995;Goodfellow et al., 2016) are significantly more expressive and flexible than linear regression models.However, maximum likelihood parameter estimation is a non-convex optimization problem, and marginalization of the parameters in a fully Bayesian setting is analytically intractable.We briefly hinted at the fact that a distribution over parameters induces a distribution over regression functions.Gaussian processes (Ras-Gaussian process mussen and Williams, 2006) are regression models where the concept of a distribution over function is central.Instead of placing a distribution over parameters, a Gaussian process places a distribution directly on the space of functions without the "detour" via the parameters.To do so, the Gaussian process exploits the kernel trick (Schölkopf and Smola, 2002), kernel trick which allows us to compute inner products between two function values f (x i ), f (x j ) only by looking at the corresponding input x i , x j .A Gaussian process is closely related to both Bayesian linear regression and support vector regression but can also be interpreted as a Bayesian neural network with a single hidden layer where the number of units tends to infinity (Neal, 1996;Williams, 1997).Excellent introductions to Gaussian processes can be found in MacKay (1998) and Rasmussen and Williams (2006).We focused on Gaussian parameter priors in the discussions in this chapter, because they allow for closed-form inference in linear regression models.However, even in a regression setting with Gaussian likelihoods, we may choose a non-Gaussian prior.Consider a setting, where the inputs are x ∈ R D and our training set is small and of size N ≪ D. This means that the regression problem is underdetermined.In this case, we can choose a parameter prior that enforces sparsity, i.e., a prior that tries to set as many parameters to 0 as possible (variable selection).This prior provides variable selection a stronger regularizer than the Gaussian prior, which often leads to an increased prediction accuracy and interpretability of the model.The Laplace prior is one example that is frequently used for this purpose.A linear regression model with the Laplace prior on the parameters is equivalent to linear regression with L1 regularization (LASSO) (Tibshirani, 1996).The LASSO Laplace distribution is sharply peaked at zero (its first derivative is discontinuous) and it concentrates its probability mass closer to zero than the Gaussian distribution, which encourages parameters to be 0. Therefore, the nonzero parameters are relevant for the regression problem, which is the reason why we also speak of "variable selection".Working directly with high-dimensional data, such as images, comes with A 640 × 480 pixel color image is a data point in a million-dimensional space, where every pixel responds to three dimensions, one for each color channel (red, green, blue).some difficulties: It is hard to analyze, interpretation is difficult, visualization is nearly impossible, and (from a practical point of view) storage of the data vectors can be expensive.However, high-dimensional data often has properties that we can exploit.For example, high-dimensional data is often overcomplete, i.e., many dimensions are redundant and can be explained by a combination of other dimensions.Furthermore, dimensions in high-dimensional data are often correlated so that the data possesses an intrinsic lower-dimensional structure.Dimensionality reduction exploits structure and correlation and allows us to work with a more compact representation of the data, ideally without losing information.We can think of dimensionality reduction as a compression technique, similar to jpeg or mp3, which are compression algorithms for images and music.In this chapter, we will discuss principal component analysis (PCA), an principal component analysis PCA algorithm for linear dimensionality reduction.PCA, proposed by Pearson dimensionality reduction (1901) and Hotelling (1933), has been around for more than 100 years and is still one of the most commonly used techniques for data compression and data visualization.It is also used for the identification of simple patterns, latent factors, and structures of high-dimensional data.In theThis material is published by Cambridge University Press as Mathematics for Machine Learning by Marc Peter Deisenroth, A. Aldo Faisal, and Cheng Soon Ong (2020).This version is free to view and download for personal use only.Not for re-distribution, re-sale, or use in derivative works.©by M. P.  https://mml-book.com.signal processing community, PCA is also known as the Karhunen-Loève Karhunen-Loève transform transform.In this chapter, we derive PCA from first principles, drawing on our understanding of basis and basis change (Sections 2.6.1 and 2.7.2), projections (Section 3.8), eigenvalues (Section 4.2), Gaussian distributions (Section 6.5), and constrained optimization (Section 7.2).Dimensionality reduction generally exploits a property of high-dimensional data (e.g., images) that it often lies on a low-dimensional subspace.Figure 10.1 gives an illustrative example in two dimensions.Although the data in Figure 10.1(a) does not quite lie on a line, the data does not vary much in the x 2 -direction, so that we can express it as if it were on a line -with nearly no loss; see Figure 10.1(b).To describe the data in Figure 10.1(b),only the x 1 -coordinate is required, and the data lies in a one-dimensional subspace of R 2 .In PCA, we are interested in finding projections xn of data points x n that are as similar to the original data points as possible, but which have a significantly lower intrinsic dimensionality.More concretely, we consider an i.i.d.dataset X = {x 1 , . . ., x N }, x n ∈ R D , with mean 0 that possesses the data covariance matrix (6.42)x n x ⊤ n .(10.1) Furthermore, we assume there exists a low-dimensional compressed representation (code)  In PCA, we find a compressed version z of original data x.The compressed data can be reconstructed into x, which lives in the original data space, but has an intrinsic lower-dimensional representation than x.Chapter 2, we know that x ∈ R 2 can be represented as a linear combination of these basis vectors, e.g.,However, when we consider vectors of the formthey can always be written as 0e 1 + ze 2 .To represent these vectors it is sufficient to remember/store the coordinate/code z of x with respect to the e 2 vector.The dimension of a vector space corresponds to the number of its basis vectors (see Section 2.6.1).More precisely, the set of x vectors (with the standard vector addition and scalar multiplication) forms a vector subspace U (see Section 2.4) with dim(U ) = 1 because U = span[e 2 ].In Section 10.2, we will find low-dimensional representations that retain as much information as possible and minimize the compression loss.An alternative derivation of PCA is given in Section 10.3, where we will be looking at minimizing the squared reconstruction error ∥x n − xn ∥ 2 between the original data x n and its projection xn .Figure 10.2 illustrates the setting we consider in PCA, where z represents the lower-dimensional representation of the compressed data x and plays the role of a bottleneck, which controls how much information can flow between x and x.In PCA, we consider a linear relationship between the original data x and its low-dimensional code z so that z = B ⊤ x and x = Bz for a suitable matrix B. Based on the motivation of thinking of PCA as a data compression technique, we can interpret the arrows in Figure 10.2 as a pair of operations representing encoders and decoders.The linear mapping represented by B can be thought of as a decoder, which maps the low-dimensional code z ∈ R M back into the original data space R D .Similarly, B ⊤ can be thought of an encoder, which encodes the original data x as a low-dimensional (compressed) code z.Throughout this chapter, we will use the MNIST digits dataset as a re- occurring example, which contains 60,000 examples of handwritten digits 0 through 9.Each digit is a grayscale image of size 28 × 28, i.e., it contains 784 pixels so that we can interpret every image in this dataset as a vector x ∈ R 784 .Examples of these digits are shown in Figure 10.3.Figure 10.1 gave an example of how a two-dimensional dataset can be represented using a single coordinate.In Figure 10.1(b), we chose to ignore the x 2 -coordinate of the data because it did not add too much information so that the compressed data is similar to the original data in Figure 10.1(a).We could have chosen to ignore the x 1 -coordinate, but then the compressed data had been very dissimilar from the original data, and much information in the data would have been lost.If we interpret information content in the data as how "space filling" the dataset is, then we can describe the information contained in the data by looking at the spread of the data.From Section 6.4.1, we know that the variance is an indicator of the spread of the data, and we can derive PCA as a dimensionality reduction algorithm that maximizes the variance in the low-dimensional representation of the data to retain as much information as possible.Figure 10.4 illustrates this.Considering the setting discussed in Section 10.1, our aim is to find a matrix B (see (10.3)) that retains as much information as possible when compressing data by projecting it onto the subspace spanned by the columns b 1 , . . ., b M of B. Retaining most information after data compression is equivalent to capturing the largest amount of variance in the low-dimensional code (Hotelling, 1933).Remark.(Centered Data) For the data covariance matrix in (10.1), we assumed centered data.We can make this assumption without loss of generality: Let us assume that µ is the mean of the data.Using the properties of the variance, which we discussed in Section 6.4.4,we obtaini.e., the variance of the low-dimensional code does not depend on the mean of the data.Therefore, we assume without loss of generality that the data has mean 0 for the remainder of this section.With this assumption the mean of the low-dimensional code is alsoWe maximize the variance of the low-dimensional code using a sequential approach.We start by seeking a single vector b 1 ∈ R D that maximizes the The vector b 1 will be the first column of the matrix B and therefore the first of M orthonormal basis vectors that span the lower-dimensional subspace.variance of the projected data, i.e., we aim to maximize the variance of the first coordinate z 1 of z ∈ R M so thatis maximized, where we exploited the i.i.d.assumption of the data and defined z 1n as the first coordinate of the low-dimensional representationNote that first component of z n is given by (10.8) i.e., it is the coordinate of the orthogonal projection of x n onto the onedimensional subspace spanned by b 1 (Section 3.8).We substitute (10.8) into (10.7),which yieldswhere S is the data covariance matrix defined in (10.1).In (10.9a), we have used the fact that the dot product of two vectors is symmetric with respect to its arguments, that is, b ⊤ 1 x n = x ⊤ n b 1 .Notice that arbitrarily increasing the magnitude of the vector b 1 increases V 1 , that is, a vector b 1 that is two times longer can result in V 1 that is potentially four times larger.Therefore, we restrict all solutions to ∥b 1 ∥ 2 = 1∥b 1 ∥ 2 = 1, which results in a constrained optimization problem in which we seek the direction along which the data varies most.With the restriction of the solution space to unit vectors the vector b 1 that points in the direction of maximum variance can be found by the constrained optimization problem(10.10) Following Section 7.2, we obtain the Lagrangianto solve this constrained optimization problem.The partial derivatives of L with respect to b 1 and λ 1 are (10.12)respectively.Setting these partial derivatives to 0 gives us the relationsBy comparing this with the definition of an eigenvalue decomposition (Section 4.4), we see that b 1 is an eigenvector of the data covariance matrix S, and the Lagrange multiplier λ 1 plays the role of the corresponding eigenvalue.This eigenvector property (10.13) allows us to rewrite our variance objective (10.10) as (10.15) i.e., the variance of the data projected onto a one-dimensional subspace equals the eigenvalue that is associated with the basis vector b 1 that spans this subspace.Therefore, to maximize the variance of the low-dimensional code, we choose the basis vector associated with the largest eigenvalue of the data covariance matrix.This eigenvector is called the first principal principal component component.We can determine the effect/contribution of the principal component b 1 in the original data space by mapping the coordinate z 1n back into data space, which gives us the projected data point 10.16) in the original data space.Remark.Although xn is a D-dimensional vector, it only requires a single coordinate z 1n to represent it with respect to the basis vector b 1 ∈ R D .♢Assume we have found the first m − 1 principal components as the m − 1 eigenvectors of S that are associated with the largest m − 1 eigenvalues.Since S is symmetric, the spectral theorem (Theorem 4.15) states that we can use these eigenvectors to construct an orthonormal eigenbasis of an Draft (2024-01-15) of "Mathematics for Machine Learning".Feedback: https://mml-book.com.(m − 1)-dimensional subspace of R D .Generally, the mth principal component can be found by subtracting the effect of the first m − 1 principal components b 1 , . . ., b m−1 from the data, thereby trying to find principal components that compress the remaining information.We then arrive at the new data matrix Remark (Notation).Throughout this chapter, we do not follow the convention of collecting data x 1 , . . ., x N as the rows of the data matrix, but we define them to be the columns of X.This means that our data matrix X is a D × N matrix instead of the conventional N × D matrix.The reason for our choice is that the algebra operations work out smoothly without the need to either transpose the matrix or to redefine vectors as row vectors that are left-multiplied onto matrices.♢To find the mth principal component, we maximize the variance (10.18) subject to ∥b m ∥ 2 = 1, where we followed the same steps as in (10.9b)and defined Ŝ as the data covariance matrix of the transformed dataset X := {x 1 , . . ., xN }.As previously, when we looked at the first principal component alone, we solve a constrained optimization problem and discover that the optimal solution b m is the eigenvector of Ŝ that is associated with the largest eigenvalue of Ŝ.It turns out that b m is also an eigenvector of S.More generally, the sets of eigenvectors of S and Ŝ are identical.Since both S and Ŝ are symmetric, we can find an ONB of eigenvectors (spectral theorem 4.15), i.e., there exist D distinct eigenvectors for both S and Ŝ. Next, we show that every eigenvector of S is an eigenvector of Ŝ. Assume we have already found eigenvectors b 1 , . . ., b m−1 of Ŝ.Consider an eigenvector b i of S, i.e., Sb i = λ i b i .In general, (10.19b)We distinguish between two cases.If i ⩾ m, i.e., b i is an eigenvector that is not among the first m − 1 principal components, then b i is orthogonal to the first m−1 principal components and B m−1 b i = 0.If i < m, i.e., b i is among the first m − 1 principal components, then b i is a basis vector of the principal subspace onto which B m−1 projects.Since b 1 , . . ., b m−1 are an ONB of this principal subspace, we obtain B m−1 b i = b i .The two cases can be summarized as follows: In the case i < m, by using (10.20) in (10.19b), we obtainThis means that b 1 , . . ., b m−1 are also eigenvectors of Ŝ, but they are associated with eigenvalue 0 so that b 1 , . . ., b m−1 span the null space of Ŝ.Overall, every eigenvector of S is also an eigenvector of Ŝ.However, if the eigenvectors of S are part of the (m − 1) dimensional principal subspace, then the associated eigenvalue of Ŝ is 0.This derivation shows that there is an intimate connection between the M -dimensional subspace with maximal variance and the eigenvalue decomposition.We will revisit this connection in Section 10.Taking all digits "8" in the MNIST training data, we compute the eigenvalues of the data covariance matrix.Figure 10.5(a) shows the 200 largest eigenvalues of the data covariance matrix.We see that only a few of them have a value that differs significantly from 0. Therefore, most of the variance, when projecting data onto the subspace spanned by the corresponding eigenvectors, is captured by only a few principal components, as shown in Figure 10.5(b).Overall, to find an M -dimensional subspace of R D that retains as much information as possible, PCA tells us to choose the columns of the matrix B in (10.3) as the M eigenvectors of the data covariance matrix S that are associated with the M largest eigenvalues.The maximum amount of variance PCA can capture with the first M principal components is (10.24)where the λ m are the M largest eigenvalues of the data covariance matrix S. Consequently, the variance lost by data compression via PCA isInstead of these absolute quantities, we can define the relative variance captured as V M V D , and the relative variance lost by compression as 1 − V M V D .In the following, we will derive PCA as an algorithm that directly minimizes the average reconstruction error.This perspective allows us to interpret PCA as implementing an optimal linear auto-encoder.We will draw heavily from Chapters 2 and 3.In the previous section, we derived PCA by maximizing the variance in the projected space to retain as much information as possible.In the following, we will look at the difference vectors between the original data x n and their reconstruction xn and minimize this distance so that x n and xn are as close as possible.Figure 10.6 illustrates this setting.We are interested in finding vectors x ∈ R D , which live in lowerdimensional subspaceis as similar to x as possible.Note that at this point we need to assume that the coordinates z m of x and ζ m of x are not identical.In the following, we use exactly this kind of representation of x to find optimal coordinates z and basis vectors b 1 , . . ., b M such that x is as similar to the original data point x as possible, i.e., we aim to minimize the (Euclidean) distance ∥x − x∥. Figure 10.7 illustrates this setting.Without loss of generality, we assume that the dataset X = {x 1 , . . ., x N }, x n ∈ R D , is centered at 0, i.e., E[X ] = 0. Without the zero-mean assump-Draft (2024-01-15) of "Mathematics for Machine Learning".Feedback: https://mml-book.com.tion, we would arrive at exactly the same solution, but the notation would be substantially more cluttered.We are interested in finding the best linear projection of X onto a lowerdimensional subspace U of R D with dim(U ) = M and orthonormal basis vectors b 1 , . . ., b M .We will call this subspace U the principal subspace.principal subspace The projections of the data points are denoted by (10.28)where z n := [z 1n , . . ., z M n ] ⊤ ∈ R M is the coordinate vector of xn with respect to the basis (b 1 , . . ., b M ).More specifically, we are interested in having the xn as similar to x n as possible.The similarity measure we use in the following is the squared distance (Euclidean norm) ∥x − x∥ 2 between x and x.We therefore define our objective as minimizing the average squared Euclidean distance (reconstruction reconstruction error error) (Pearson, 1901) (10.29)where we make it explicit that the dimension of the subspace onto which we project the data is M .In order to find this optimal linear projection, we need to find the orthonormal basis of the principal subspace and the coordinates z n ∈ R M of the projections with respect to this basis.To find the coordinates z n and the ONB of the principal subspace, we follow a two-step approach.First, we optimize the coordinates z n for a given ONB (b 1 , . . ., b M ); second, we find the optimal ONB.Let us start by finding the optimal coordinates z 1n , . . ., z M n of the projections xn for n = 1, . . ., N . Consider Figure 10.7(b),where the principal subspace is spanned by a single vector b.Geometrically speaking, finding the optimal coordinates z corresponds to finding the representation of the linear projection x with respect to b that minimizes the distance between x − x.From Figure 10.7(b), it is clear that this will be the orthogonal projection, and in the following we will show exactly this.We assume an ONB (b 1 , . . ., b M ) of U ⊆ R D .To find the optimal coordinates z m with respect to this basis, we require the partial derivatives  for i = 1, . . ., M , such that we obtain optimal coordinates 10.32) for i = 1, . . ., M and n = 1, . . ., N .This means that the optimal coordinates z in of the projection xn are the coordinates of the orthogonal projection (see Section 3.8) of the original data point x n onto the onedimensional subspace that is spanned by b i .Consequently:The optimal linear projection xn of x n is an orthogonal projection.is the orthogonal projection of x onto the subspace spanned by the jth basis vector, and z j = b ⊤ j x is the coordinate of this projection with respect to the basis vector b j that spans that subspace since z j b j = x.More generally, if we aim to project onto an M -dimensional subspace of R D , we obtain the orthogonal projection of x onto the M -dimensional subspace with orthonormal basis vectors b 1 , . . ., b M as ♢ So far we have shown that for a given ONB we can find the optimal coordinates of x by an orthogonal projection onto the principal subspace.In the following, we will determine what the best basis is.To determine the basis vectors b 1 , . . ., b M of the principal subspace, we rephrase the loss function (10.29) using the results we have so far.This will make it easier to find the basis vectors.To reformulate the loss function, we exploit our results from before and obtain Since we can generally write the original data point x n as a linear combination of all basis vectors, it holds thatwhere we split the sum with D terms into a sum over M and a sum over D − M terms.With this result, we find that the displacement vector x n − xn , i.e., the difference vector between the original data point and its projection, is (10.38b)This means the difference is exactly the projection of the data point onto the orthogonal complement of the principal subspace: We identify the matrix D j=M +1 b j b ⊤ j in (10.38a) as the projection matrix that performs this projection.Hence the displacement vector x n − xn lies in the subspace that is orthogonal to the principal subspace as illustrated in Figure 10.9.Remark (Low-Rank Approximation).In (10.38a), we saw that the projection matrix, which projects x onto x, is given by symmetric and has rank M .Therefore, the average squared reconstruction error can also be written as ence between the original data x n and their projections xn , is equivalent to finding the best rank-M approximation BB ⊤ of the identity matrix I (see Section 4.6).♢Now we have all the tools to reformulate the loss function (10.29).We now explicitly compute the squared norm and exploit the fact that the b j form an ONB, which yieldswhere we exploited the symmetry of the dot product in the last step to write b ⊤ j x n = x ⊤ n b j .We now swap the sums and obtainwhere we exploited the property that the trace operator tr(•) (see (4.18)) is linear and invariant to cyclic permutations of its arguments.Since we assumed that our dataset is centered, i.e., E[X ] = 0, we identify S as the data covariance matrix.Since the projection matrix in (10.43b) is constructed as a sum of rank-one matrices b j b ⊤ j it itself is of rank D − M .Equation (10.43a) implies that we can formulate the average squared reconstruction error equivalently as the covariance matrix of the data, projected onto the orthogonal complement of the principal subspace.Minimizing the average squared reconstruction error is therefore equivalent to Minimizing the average squared reconstruction error is equivalent to minimizing the projection of the data covariance matrix onto the orthogonal complement of the principal subspace.minimizing the variance of the data when projected onto the subspace we ignore, i.e., the orthogonal complement of the principal subspace.Equivalently, we maximize the variance of the projection that we retain in the principal subspace, which links the projection loss immediately to the maximum-variance formulation of PCA discussed in Section 10.2.But this then also means that we will obtain the same solution that we obtained Minimizing the average squared reconstruction error is equivalent to maximizing the variance of the projected data.for the maximum-variance perspective.Therefore, we omit a derivation that is identical to the one presented in Section 10.2 and summarize the results from earlier in the light of the projection perspective.The average squared reconstruction error, when projecting onto the Mdimensional principal subspace, is (10.44)where λ j are the eigenvalues of the data covariance matrix.Therefore, to minimize (10.44) we need to select the smallest D − M eigenvalues, which then implies that their corresponding eigenvectors are the basis of the orthogonal complement of the principal subspace.Consequently, this means that the basis of the principal subspace comprises the eigenvectors b 1 , . . ., b M that are associated with the largest M eigenvalues of the data covariance matrix.Figure 10.10 Embedding of MNIST digits 0 (blue) and 1 (orange) in a two-dimensional principal subspace using PCA.Four embeddings of the digits "0" and "1" in the principal subspace are highlighted in red with their corresponding original digit.Figure 10.10 visualizes the training data of the MMIST digits "0" and "1" embedded in the vector subspace spanned by the first two principal components.We observe a relatively clear separation between "0"s (blue dots) and "1"s (orange dots), and we see the variation within each individual cluster.Four embeddings of the digits "0" and "1" in the principal subspace are highlighted in red with their corresponding original digit.The figure reveals that the variation within the set of "0" is significantly greater than the variation within the set of "1".In the previous sections, we obtained the basis of the principal subspace as the eigenvectors that are associated with the largest eigenvalues of the data covariance matrixNote that X is a D × N matrix, i.e., it is the transpose of the "typical" data matrix (Bishop, 2006;Murphy, 2012).To get the eigenvalues (and the corresponding eigenvectors) of S, we can follow two approaches:Use eigendecomposition or SVD to compute eigenvectors.We perform an eigendecomposition (see Section 4.2) and compute the eigenvalues and eigenvectors of S directly.We use a singular value decomposition (see Section 4.5).Since S is symmetric and factorizes into XX ⊤ (ignoring the factor 1 N ), the eigenvalues of S are the squared singular values of X.More specifically, the SVD of X is given bywhere U ∈ R D×D and V ⊤ ∈ R N ×N are orthogonal matrices and Σ ∈ R D×N is a matrix whose only nonzero entries are the singular values σ ii ⩾ 0. It then follows thatWith the results from Section 4.5, we get that the columns of U are the The columns of U are the eigenvectors of S.eigenvectors of XX ⊤ (and therefore S).Furthermore, the eigenvalues λ d of S are related to the singular values of X via .49)This relationship between the eigenvalues of S and the singular values of X provides the connection between the maximum variance view (Section 10.2) and the singular value decomposition.To maximize the variance of the projected data (or minimize the average squared reconstruction error), PCA chooses the columns of U in (10.48) to be the eigenvectors that are associated with the M largest eigenvalues of the data covariance matrix S so that we identify U as the projection matrix B in (10.3), which projects the original data onto a lower-dimensional subspace of dimension M .The Eckart-Young theorem (Theorem 4.25 in Eckart-Young theorem Section 4.6) offers a direct way to estimate the low-dimensional representation.Consider the best rank-M approximationof X, where ∥•∥ 2 is the spectral norm defined in (4.93).The Eckart-Young theorem states that XM is given by truncating the SVD at the top-M singular value.In other words, we obtainFinding eigenvalues and eigenvectors is also important in other fundamental machine learning methods that require matrix decompositions.In theory, as we discussed in Section 4.2, we can solve for the eigenvalues as roots of the characteristic polynomial.However, for matrices larger than 4×4 this is not possible because we would need to find the roots of a polynomial of degree 5 or higher.However, the Abel-Ruffini theorem (Ruffini, Abel-Ruffini theorem 1799; Abel, 1826) states that there exists no algebraic solution to this problem for polynomials of degree 5 or more.Therefore, in practice, we np.linalg.eighor np.linalg.svdsolve for eigenvalues or singular values using iterative methods, which are implemented in all modern packages for linear algebra.In many applications (such as PCA presented in this chapter), we only require a few eigenvectors.It would be wasteful to compute the full decomposition, and then discard all eigenvectors with eigenvalues that are beyond the first few.It turns out that if we are interested in only the first few eigenvectors (with the largest eigenvalues), then iterative processes, which directly optimize these eigenvectors, are computationally more efficient than a full eigendecomposition (or SVD).In the extreme case of only needing the first eigenvector, a simple method called the power iteration power iteration is very efficient.Power iteration chooses a random vector x 0 that is not in Draft (2024-01-15) of "Mathematics for Machine Learning".Feedback: https://mml-book.com.the null space of S and follows the iteration .52)This means the vector x k is multiplied by S in every iteration and then If S is invertible, it is sufficient to ensure that x 0 ̸ = 0.normalized, i.e., we always have ∥x k ∥ = 1.This sequence of vectors converges to the eigenvector associated with the largest eigenvalue of S. The original Google PageRank algorithm (Page et al., 1999) uses such an algorithm for ranking web pages based on their hyperlinks.In order to do PCA, we need to compute the data covariance matrix.In D dimensions, the data covariance matrix is a D × D matrix.Computing the eigenvalues and eigenvectors of this matrix is computationally expensive as it scales cubically in D. Therefore, PCA, as we discussed earlier, will be infeasible in very high dimensions.For example, if our x n are images with 10,000 pixels (e.g., 100 × 100 pixel images), we would need to compute the eigendecomposition of a 10,000 × 10,000 covariance matrix.In the following, we provide a solution to this problem for the case that we have substantially fewer data points than dimensions, i.e., N ≪ D.Assume we have a centered dataset x 1 , . . ., x N , x n ∈ R D .Then the data covariance matrix is given aswhere X = [x 1 , . . ., x N ] is a D × N matrix whose columns are the data points.We now assume that N ≪ D, i.e., the number of data points is smaller than the dimensionality of the data.If there are no duplicate data points, the rank of the covariance matrix S is N , so it has D − N + 1 many eigenvalues that are 0. Intuitively, this means that there are some redundancies.In the following, we will exploit this and turn the D×D covariance matrix into an N × N covariance matrix whose eigenvalues are all positive.In PCA, we ended up with the eigenvector equationwhere b m is a basis vector of the principal subspace.Let us rewrite this equation a bit: With S defined in (10.53), we obtainWe now multiply X ⊤ ∈ R N ×D from the left-hand side, which yields and we get a new eigenvector/eigenvalue equation: λ m remains eigenvalue, which confirms our results from Section 4.5.3 that the nonzero eigenvalues of XX ⊤ equal the nonzero eigenvalues of X ⊤ X.We obtain the eigenvector of the matrix 1 N X ⊤ X ∈ R N ×N associated with λ m as c m := X ⊤ b m .Assuming we have no duplicate data points, this matrix has rank N and is invertible.This also implies that 1 N X ⊤ X has the same (nonzero) eigenvalues as the data covariance matrix S.But this is now an N × N matrix, so that we can compute the eigenvalues and eigenvectors much more efficiently than for the original D × D data covariance matrix.Now that we have the eigenvectors of 1 N X ⊤ X, we are going to recover the original eigenvectors, which we still need for PCA.Currently, we know the eigenvectors of 1 N X ⊤ X.If we left-multiply our eigenvalue/ eigenvector equation with X, we getand we recover the data covariance matrix again.This now also means that we recover Xc m as an eigenvector of S.Remark.If we want to apply the PCA algorithm that we discussed in Section 10.6, we need to normalize the eigenvectors Xc m of S so that they have norm 1. ♢In the following, we will go through the individual steps of PCA using a running example, which is summarized in Figure 10.11.We are given a two-dimensional dataset (Figure 10.11(a)), and we want to use PCA to project it onto a one-dimensional subspace.We start by centering the data by computing the mean µ of the dataset and subtracting it from every single data point.This ensures that the dataset has mean 0 (Figure 10.11(b)).Mean subtraction is not strictly necessary but reduces the risk of numerical problems.responding eigenvalue.The longer vector spans the principal subspace, which we denote by U .The data covariance matrix is represented by the ellipse.4. Projection We can project any data point x * ∈ R D onto the principal subspace: To get this right, we need to standardize x * using the mean µ d and standard deviation σ d of the training data in the dth dimension, respectively, so thatwhere x (d) * is the dth component of x * .We obtain the projection aswith coordinateswith respect to the basis of the principal subspace.Here, B is the matrix that contains the eigenvectors that are associated with the largest eigenvalues of the data covariance matrix as columns.PCA returns the coordinates (10.60), not the projections x * .Having standardized our dataset, (10.59)only yields the projections in the context of the standardized dataset.To obtain our projection in the original data space (i.e., before standardization), we need to undo the standardization (10.58) and multiply by the standard deviation before adding the mean so that we obtain(10.61)Figure 10.11(f) illustrates the projection in the original data space.Example 10.4 (MNIST Digits: Reconstruction)In the following, we will apply PCA to the MNIST digits dataset, which contains 60,000 examples of handwritten digits 0 through 9.Each digit is an image of size 28×28, i.e., it contains 784 pixels so that we can interpret every image in this dataset as a vector x ∈ R 784 .Examples of these digits are shown in Figure 10.3.For illustration purposes, we apply PCA to a subset of the MNIST digits, and we focus on the digit "8".We used 5,389 training images of the digit "8" and determined the principal subspace as detailed in this chapter.We then used the learned projection matrix to reconstruct a set of test images, which is illustrated in Figure 10.12.The first row of Figure 10.12 shows a set of four original digits from the test set.The following rows show reconstructions of exactly these digits when using a principal subspace of dimensions 1, 10, 100, and 500, respectively.We see that even with a single-dimensional principal subspace we get a halfway decent reconstruction of the original digits, which, however, is blurry and generic.With an increasing number of principal components (PCs), the reconstructions become sharper and more details are accounted for.With 500 prin-cipal components, we effectively obtain a near-perfect reconstruction.If we were to choose 784 PCs, we would recover the exact digit without any compression loss.Figure 10.13 shows the average squared reconstruction error, which isas a function of the number M of principal components.We can see that the importance of the principal components drops off rapidly, and only marginal gains can be achieved by adding more PCs.This matches exactly our observation in Figure 10.5, where we discovered that the most of the variance of the projected data is captured by only a few principal components.With about 550 PCs, we can essentially fully reconstruct the training data that contains the digit "8" (some pixels around the boundaries show no variation across the dataset as they are always black).In the previous sections, we derived PCA without any notion of a probabilistic model using the maximum-variance and the projection perspectives.On the one hand, this approach may be appealing as it allows us to sidestep all the mathematical difficulties that come with probability theory, but on the other hand, a probabilistic model would offer us more flexibility and useful insights.More specifically, a probabilistic model would Come with a likelihood function, and we can explicitly deal with noisy observations (which we did not even discuss earlier) Allow us to do Bayesian model comparison via the marginal likelihood as discussed in Section 8.6 View PCA as a generative model, which allows us to simulate new data Allow us to make straightforward connections to related algorithms Deal with data dimensions that are missing at random by applying Bayes' theorem Give us a notion of the novelty of a new data point Give us a principled way to extend the model, e.g., to a mixture of PCA models Have the PCA we derived in earlier sections as a special case Allow for a fully Bayesian treatment by marginalizing out the model parameters By introducing a continuous-valued latent variable z ∈ R M it is possible to phrase PCA as a probabilistic latent-variable model.Tipping and Bishop (1999) proposed this latent-variable model as probabilistic PCA (PPCA).probabilistic PCA PPCA PPCA addresses most of the aforementioned issues, and the PCA solution that we obtained by maximizing the variance in the projected space or by minimizing the reconstruction error is obtained as the special case of maximum likelihood estimation in a noise-free setting.In PPCA, we explicitly write down the probabilistic model for linear dimensionality reduction.For this we assume a continuous latent variable z ∈ R M with a standard-normal prior p(z) = N 0, I and a linear relationship between the latent variables and the observed x data where (10.63)where ϵ ∼ N 0, σ 2 I is Gaussian observation noise and B ∈ R D×M and µ ∈ R D describe the linear/affine mapping from latent to observed variables.Therefore, PPCA links latent and observed variables viaOverall, PPCA induces the following generative process:To generate a data point that is typical given the model parameters, we follow an ancestral sampling scheme: We first sample a latent variable z n ancestral sampling from p(z).Then we use z n in (10.64) to sample a data point conditioned on the sampled z n , i.e.,This generative process allows us to write down the probabilistic model (i.e., the joint distribution of all random variables; see Section 8.4) as p(x, z|B, µ, σ 2 ) = p(x|z, B, µ, σ 2 )p(z) , (10.67) which immediately gives rise to the graphical model in Figure 10.14 using the results from Section 8.5.Draft (2024-01-15) of "Mathematics for Machine Learning".Feedback: https://mml-book.com.x nRemark.Note the direction of the arrow that connects the latent variables z and the observed data x: The arrow points from z to x, which means that the PPCA model assumes a lower-dimensional latent cause z for highdimensional observations x.In the end, we are obviously interested in finding something out about z given some observations.To get there we will apply Bayesian inference to "invert" the arrow implicitly and go from observations to latent variables.♢ Figure 10.15 shows the latent coordinates of the MNIST digits "8" found by PCA when using a two-dimensional principal subspace (blue dots).We can query any vector z * in this latent space and generate an image x * = Bz * that resembles the digit "8".We show eight of such generated images with their corresponding latent space representation.Depending on where we query the latent space, the generated images look different (shape, rotation, size, etc.).If we query away from the training data, we see more and more artifacts, e.g., the top-left and top-right digits.Note that the intrinsic dimensionality of these generated images is only two.The likelihood does not depend on the latent variables z.Using the results from Chapter 6, we obtain the likelihood of this probabilistic model by integrating out the latent variable z (see Section 8.4.3) so thatFrom Section 6.5, we know that the solution to this integral is a Gaussian distribution with meanand with covariance matrixThe likelihood in (10.68b) can be used for maximum likelihood or MAP estimation of the model parameters.Remark.We cannot use the conditional distribution in (10.64) for maximum likelihood estimation as it still depends on the latent variables.The likelihood function we require for maximum likelihood (or MAP) estimation should only be a function of the data x and the model parameters, but must not depend on the latent variables.♢ From Section 6.5, we know that a Gaussian random variable z and a linear/affine transformation x = Bz of it are jointly Gaussian distributed.We already know the marginals p(z) = N z | 0, I and p(x) = N x | µ, BB ⊤ + σ 2 I .The missing cross-covariance is given as(10.71)Therefore, the probabilistic model of PPCA, i.e., the joint distribution of latent and observed random variables is explicitly given bywith a mean vector of length D + M and a covariance matrix of sizeThe joint Gaussian distribution p(x, z | B, µ, σ 2 ) in (10.72) allows us to determine the posterior distribution p(z | x) immediately by applying the Draft (2024-01-15) of "Mathematics for Machine Learning".Feedback: https://mml-book.com.rules of Gaussian conditioning from Section 6.5.1.The posterior distribution of the latent variable given an observation x is thenNote that the posterior covariance does not depend on the observed data x.For a new observation x * in data space, we use (10.73) to determine the posterior distribution of the corresponding latent variable z * .The covariance matrix C allows us to assess how confident the embedding is.A covariance matrix C with a small determinant (which measures volumes) tells us that the latent embedding z * is fairly certain.If we obtain a posterior distribution p(z * | x * ) with much variance, we may be faced with an outlier.However, we can explore this posterior distribution to understand what other data points x are plausible under this posterior.To do this, we exploit the generative process underlying PPCA, which allows us to explore the posterior distribution on the latent variables by generating new data that is plausible under this posterior:1. Sample a latent variable z * ∼ p(z | x * ) from the posterior distribution over the latent variables (10.73).2. Sample a reconstructed vector x * ∼ p(x | z * , B, µ, σ 2 ) from (10.64).If we repeat this process many times, we can explore the posterior distribution (10.73) on the latent variables z * and its implications on the observed data.The sampling process effectively hypothesizes data, which is plausible under the posterior distribution.We derived PCA from two perspectives: (a) maximizing the variance in the projected space; (b) minimizing the average reconstruction error.However, PCA can also be interpreted from different perspectives.Let us recap what we have done: We took high-dimensional data x ∈ R D and used a matrix B ⊤ to find a lower-dimensional representation z ∈ R M .The columns of B are the eigenvectors of the data covariance matrix S that are associated with the largest eigenvalues.Once we have a low-dimensional representation z, we can get a high-dimensional version of it (in the original data space) as x ≈ x = Bz = BB ⊤ x ∈ R D , where BB ⊤ is a projection matrix.We can also think of PCA as a linear auto-encoder as illustrated in Figauto-encoder ure 10.16.An auto-encoder encodes the data x n ∈ R D to a code z n ∈ R M code and decodes it to a xn similar to x n .The mapping from the data to the code is called the encoder, and the mapping from the code back to the origencoder inal data space is called the decoder.If we consider linear mappings where decoder Figure 10.16 PCA can be viewed as a linear auto-encoder.It encodes the high-dimensional data x into a lower-dimensional representation (code) z ∈ R M and decodes z using a decoder.The decoded vector x is the orthogonal projection of the original data x onto the M -dimensional principal subspace.the code is given by z n = B ⊤ x n ∈ R M and we are interested in minimizing the average squared error between the data x n and its reconstruction xn = Bz n , n = 1, . . ., N , we obtainThis means we end up with the same objective function as in (10.29) that we discussed in Section 10.3 so that we obtain the PCA solution when we minimize the squared auto-encoding loss.If we replace the linear mapping of PCA with a nonlinear mapping, we get a nonlinear auto-encoder.A prominent example of this is a deep auto-encoder where the linear functions are replaced with deep neural networks.In this context, the encoder is also known as a recognition network or inference network, whereas the recognition network inference network decoder is also called a generator.generator Another interpretation of PCA is related to information theory.We can think of the code as a smaller or compressed version of the original data point.When we reconstruct our original data using the code, we do not get the exact data point back, but a slightly distorted or noisy version of it.This means that our compression is "lossy".Intuitively, we wantThe code is a compressed version of the original data.to maximize the correlation between the original data and the lowerdimensional code.More formally, this is related to the mutual information.We would then get the same solution to PCA we discussed in Section 10.3 by maximizing the mutual information, a core concept in information theory (MacKay, 2003).In our discussion on PPCA, we assumed that the parameters of the model, i.e., B, µ, and the likelihood parameter σ 2 , are known.Tipping and Bishop (1999) describe how to derive maximum likelihood estimates for these parameters in the PPCA setting (note that we use a different notation in this chapter).The maximum likelihood parameters, when pro-jecting D-dimensional data onto an M -dimensional subspace, arex n , (10.77)where T ∈ R D×M contains M eigenvectors of the data covariance matrix, The matrix Λ − σ 2 I in (10.78) is guaranteed to be positive semidefinite as the smallest eigenvalue of the data covariance matrix is bounded from below by the noise variance σ 2 .Λ = diag(λ 1 , . . ., λ M ) ∈ R M ×M is a diagonal matrix with the eigenvalues associated with the principal axes on its diagonal, and R ∈ R M ×M is an arbitrary orthogonal matrix.The maximum likelihood solution B ML is unique up to an arbitrary orthogonal transformation, e.g., we can rightmultiply B ML with any rotation matrix R so that (10.78) essentially is a singular value decomposition (see Section 4.5).An outline of the proof is given by Tipping and Bishop (1999).The maximum likelihood estimate for µ given in (10.77) is the sample mean of the data.The maximum likelihood estimator for the observation noise variance σ 2 given in (10.79) is the average variance in the orthogonal complement of the principal subspace, i.e., the average leftover variance that we cannot capture with the first M principal components is treated as observation noise.In the noise-free limit where σ → 0, PPCA and PCA provide identical solutions: Since the data covariance matrix S is symmetric, it can be diagonalized (see Section 4.4), i.e., there exists a matrix T of eigenvectors of S so that S = T ΛT −1 .(10.80)In the PPCA model, the data covariance matrix is the covariance matrix of the Gaussian likelihood p(x | B, µ, σ 2 ), which is BB ⊤ +σ 2 I, see (10.70b).For σ → 0, we obtain BB ⊤ so that this data covariance must equal the PCA data covariance (and its factorization given in (10.80)) so thati.e., we obtain the maximum likelihood estimate in (10.78) for σ = 0. From (10.78) and (10.80), it becomes clear that (P)PCA performs a decomposition of the data covariance matrix.In a streaming setting, where data arrives sequentially, it is recommended to use the iterative expectation maximization (EM) algorithm for maximum likelihood estimation (Roweis, 1998).To determine the dimensionality of the latent variables (the length of the code, the dimensionality of the lower-dimensional subspace onto which we project the data), Gavish and Donoho (2014) suggest the heuristic that, if we can estimate the noise variance σ 2 of the data, we should discard all singular values smaller than 4σ √ D √ 3 .Alternatively, we can use (nested) cross-validation (Section 8.6.1) or Bayesian model selection criteria (discussed in Section 8.6.2) to determine a good estimate of the intrinsic dimensionality of the data (Minka, 2001b).Similar to our discussion on linear regression in Chapter 9, we can place a prior distribution on the parameters of the model and integrate them out.By doing so, we (a) avoid point estimates of the parameters and the issues that come with these point estimates (see Section 8.6) and (b) allow for an automatic selection of the appropriate dimensionality M of the latent space.In this Bayesian PCA, which was proposed by Bishop (1999), Bayesian PCA a prior p(µ, B, σ 2 ) is placed on the model parameters.The generative process allows us to integrate the model parameters out instead of conditioning on them, which addresses overfitting issues.Since this integration is analytically intractable, Bishop (1999) proposes to use approximate inference methods, such as MCMC or variational inference.We refer to the work by Gilks et al. (1996) and Blei et al. (2017) for more details on these approximate inference techniques.In PPCA, we considered the linear model p( (FA) (Spearman, 1904;Bartholomew et al., 2011).This means that FA gives the likelihood some more flexibility than PPCA, but still forces the data to be explained by the model parameters B, µ.However, FA no An overly flexible likelihood would be able to explain more than just the noise.longer allows for a closed-form maximum likelihood solution so that we need to use an iterative scheme, such as the expectation maximization algorithm, to estimate the model parameters.While in PPCA all stationary points are global optima, this no longer holds for FA.Compared to PPCA, FA does not change if we scale the data, but it does return different solutions if we rotate the data.An algorithm that is also closely related to PCA is independent comindependent component analysis ponent analysis (ICA (Hyvarinen et al., 2001)).Starting again with the ICA latent-variable perspective p( As discussed previously in the context of maximum likelihood estimation for PPCA, the original PCA solution is invariant to any rotation.Therefore, PCA can identify the best lower-dimensional subspace in which the signals live, but not the signals themselves (Murphy, 2012).ICA addresses this issue by modifying the prior distribution p(z) on the latent sources Draft (2024-01-15) of "Mathematics for Machine Learning".Feedback: https://mml-book.com.to require non-Gaussian priors p(z).We refer to the books by Hyvarinen et al. (2001) and Murphy (2012) for more details on ICA.PCA, factor analysis, and ICA are three examples for dimensionality reduction with linear models.Cunningham and Ghahramani (2015) provide a broader survey of linear dimensionality reduction.The (P)PCA model we discussed here allows for several important extensions.In Section 10.5, we explained how to do PCA when the input dimensionality D is significantly greater than the number N of data points.By exploiting the insight that PCA can be performed by computing (many) inner products, this idea can be pushed to the extreme by considering infinite-dimensional features.The kernel trick is the basis of kernel kernel trick kernel PCA PCA and allows us to implicitly compute inner products between infinitedimensional features (Schölkopf et al., 1998;Schölkopf and Smola, 2002).There are nonlinear dimensionality reduction techniques that are derived from PCA (Burges (2010) provides a good overview).The autoencoder perspective of PCA that we discussed previously in this section can be used to render PCA as a special case of a deep auto-encoder.In the deep auto-encoder deep auto-encoder, both the encoder and the decoder are represented by multilayer feedforward neural networks, which themselves are nonlinear mappings.If we set the activation functions in these neural networks to be the identity, the model becomes equivalent to PCA.A different approach to nonlinear dimensionality reduction is the Gaussian process latent-variable Gaussian process latent-variable model model (GP-LVM) proposed by Lawrence (2005).The GP-LVM starts off with GP-LVM the latent-variable perspective that we used to derive PPCA and replaces the linear relationship between the latent variables z and the observations x with a Gaussian process (GP).Instead of estimating the parameters of the mapping (as we do in PPCA), the GP-LVM marginalizes out the model parameters and makes point estimates of the latent variables z.Similar to Bayesian PCA, the Bayesian GP-LVM proposed by Titsias and Lawrence Bayesian GP-LVM (2010) maintains a distribution on the latent variables z and uses approximate inference to integrate them out as well.In earlier chapters, we covered already two fundamental problems in machine learning: regression (Chapter 9) and dimensionality reduction (Chapter 10).In this chapter, we will have a look at a third pillar of machine learning: density estimation.On our journey, we introduce important concepts, such as the expectation maximization (EM) algorithm and a latent variable perspective of density estimation with mixture models.When we apply machine learning to data we often aim to represent data in some way.A straightforward way is to take the data points themselves as the representation of the data; see Figure 11.1 for an example.However, this approach may be unhelpful if the dataset is huge or if we are interested in representing characteristics of the data.In density estimation, we represent the data compactly using a density from a parametric family, e.g., a Gaussian or Beta distribution.For example, we may be looking for the mean and variance of a dataset in order to represent the data compactly using a Gaussian distribution.The mean and variance can be found using tools we discussed in Section 8.3: maximum likelihood or maximum a posteriori estimation.We can then use the mean and variance of this Gaussian to represent the distribution underlying the data, i.e., we think of the dataset to be a typical realization from this distribution if we were to sample from it.This material is published by Cambridge University Press as Mathematics for Machine Learning by Marc Peter Deisenroth, A. Aldo Faisal, and Cheng Soon Ong (2020).This version is free to view and download for personal use only.Not for re-distribution, re-sale, or use in derivative works.©by M. P.  https://mml-book.com.In practice, the Gaussian (or similarly all other distributions we encountered so far) have limited modeling capabilities.For example, a Gaussian approximation of the density that generated the data in Figure 11.1 would be a poor approximation.In the following, we will look at a more expressive family of distributions, which we can use for density estimation: mixture models.Mixture models can be used to describe a distribution p(x) by a convex combination of K simple (base) distributionswhere the components p k are members of a family of basic distributions, e.g., Gaussians, Bernoullis, or Gammas, and the π k are mixture weights.mixture weight Mixture models are more expressive than the corresponding base distributions because they allow for multimodal data representations, i.e., they can describe datasets with multiple "clusters", such as the example in Figure 11.1.We will focus on Gaussian mixture models (GMMs), where the basic distributions are Gaussians.For a given dataset, we aim to maximize the likelihood of the model parameters to train the GMM.For this purpose, we will use results from Chapter 5, Chapter 6, and Section 7.2.However, unlike other applications we discussed earlier (linear regression or PCA), we will not find a closed-form maximum likelihood solution.Instead, we will arrive at a set of dependent simultaneous equations, which we can only solve iteratively.A Gaussian mixture model is a density model where we combine a finite Gaussian mixture (11.4)where we defined θ := {µ k , Σ k , π k : k = 1, . . ., K} as the collection of all parameters of the model.This convex combination of Gaussian distribution gives us significantly more flexibility for modeling complex densities than a simple Gaussian distribution (which we recover from (11.3) for K = 1).An illustration is given in Figure 11.2, displaying the weighted components and the mixture density, which is given asAssume we are given a dataset X = {x 1 , . . ., x N }, where x n , n = 1, . . ., N , are drawn i.i.d.from an unknown distribution p(x).Our objective is to find a good approximation/representation of this unknown distribution p(x) by means of a GMM with K mixture components.The parameters of the GMM are the K means µ k , the covariances Σ k , and mixture weights π k .We summarize all these free parameters in θ := {π k , µ k , Σ k : k = 1, . . ., K}.Throughout this chapter, we will have a simple running example that helps us illustrate and visualize important concepts.We consider a one-dimensional dataset X = {−3, −2.5, −1, 0, 2, 4, 5} consisting of seven data points and wish to find a GMM with K = 3 components that models the density of the data.We initialize the mixture components as In the following, we detail how to obtain a maximum likelihood estimate θ ML of the model parameters θ.We start by writing down the likelihood, i.e., the predictive distribution of the training data given the parameters.We exploit our i.i.d.assumption, which leads to the factorized likelihood (11.9)where every individual likelihood term p(x n | θ) is a Gaussian mixture density.Then we obtain the log-likelihood asWe aim to find parameters θ * ML that maximize the log-likelihood L defined in (11.10).Our "normal" procedure would be to compute the gradient dL/dθ of the log-likelihood with respect to the model parameters θ, set it to 0, and solve for θ.However, unlike our previous examples for maximum likelihood estimation (e.g., when we discussed linear regression in Section 9.2), we cannot obtain a closed-form solution.However, we can exploit an iterative scheme to find good model parameters θ ML , which will turn out to be the EM algorithm for GMMs.The key idea is to update one model parameter at a time while keeping the others fixed.Remark.If we were to consider a single Gaussian as the desired density, the sum over k in (11.10) vanishes, and the log can be applied directly to the Gaussian component, such that we getthe log into the sum over k so that we cannot obtain a simple closed-form maximum likelihood solution.♢Any local optimum of a function exhibits the property that its gradient with respect to the parameters must vanish (necessary condition); see Chapter 7. In our case, we obtain the following necessary conditions when we optimize the log-likelihood in (11.10) with respect to the GMM parameters µ k , Σ k , π k : (11.12) (11.13) (11.14)For all three necessary conditions, by applying the chain rule (see Section 5.2.2), we require partial derivatives of the formwhere θ = {µ k , Σ k , π k , k = 1, . . ., K} are the model parameters and(11.16)In the following, we will compute the partial derivatives (11.12) through (11.14).But before we do this, we introduce a quantity that will play a central role in the remainder of this chapter: responsibilities.We define the quantityas the responsibility of the kth mixture component for the nth data point.responsibilityThe responsibility r nk of the kth mixture component for data point x n is proportional to the likelihoodof the mixture component given the data point.Therefore, mixture comrn follows a Boltzmann/Gibbs distribution.ponents have a high responsibility for a data point when the data point could be a plausible sample from that mixture component.Note that r n := [r n1 , . . ., r nK ] ⊤ ∈ R K is a (normalized) probability vector, i.e., Draft (2024-01-15) of "Mathematics for Machine Learning".Feedback: https://mml-book.com.k r nk = 1 with r nk ⩾ 0. This probability vector distributes probability mass among the K mixture components, and we can think of r n as a "soft assignment" of x n to the K mixture components.Therefore, the re-The responsibility r nk is the probability that the kth mixture component generated the nth data point.sponsibility r nk from (11.17) represents the probability that x n has been generated by the kth mixture component.(11.19)Here the nth row tells us the responsibilities of all mixture components for x n .The sum of all K responsibilities for a data point (sum of every row) is 1.The kth column gives us an overview of the responsibility of the kth mixture component.We can see that the third mixture component (third column) is not responsible for any of the first four data points, but takes much responsibility of the remaining data points.The sum of all entries of a column gives us the values N k , i.e., the total responsibility of the kth mixture component.In our example, we get N 1 = 2.058, N 2 = 2.008, N 3 = 2.934.In the following, we determine the updates of the model parameters µ k , Σ k , π k for given responsibilities.We will see that the update equations all depend on the responsibilities, which makes a closed-form solution to the maximum likelihood estimation problem impossible.However, for given responsibilities we will be updating one model parameter at a time, while keeping the others fixed.After this, we will recompute the responsibilities.Iterating these two steps will eventually converge to a local optimum and is a specific instantiation of the EM algorithm.We will discuss this in some more detail in Section 11.3.Theorem 11.1 (Update of the GMM Means).The update of the mean parameters µ k , k = 1, . . ., K, of the GMM is given by (11.20)where the responsibilities r nk are defined in (11.17 Remark.The update of the means µ k of the individual mixture components in (11.20) depends on all means, covariance matrices Σ k , and mixture weights π k via r nk given in (11.17).Therefore, we cannot obtain a closed-form solution for all µ k at once.♢Proof From (11.15), we see that the gradient of the log-likelihood with respect to the mean parameters µ k , k = 1, . . ., K, requires us to compute the partial derivative (11.21b)where we exploited that only the kth mixture component depends on µ k .We use our result from (11.21b) in (11.15) and put everything together so that the desired partial derivative of L with respect to µ k is given asHere we used the identity from (11.16) and the result of the partial derivative in (11.21b) to get to (11.22b).The values r nk are the responsibilities we defined in (11.17).We now solve (11.22c) for µ new k so that (11.23)where we definedas the total responsibility of the kth mixture component for the entire dataset.This concludes the proof of Theorem 11.1.Intuitively, (11.20) can be interpreted as an importance-weighted Monte Carlo estimate of the mean, where the importance weights of data point x n are the responsibilities r nk of the kth cluster for x n , k = 1, . . ., K.Draft (2024-01-15) of "Mathematics for Machine Learning".Feedback: https://mml-book.com.Therefore, the mean µ k is pulled toward a data point x n with strength Figure 11.4 Update of the mean parameter of mixture component in a GMM.The mean µ is being pulled toward individual data points with the weights given by the corresponding responsibilities.given by r nk .The means are pulled stronger toward data points for which the corresponding mixture component has a high responsibility, i.e., a high likelihood.Figure 11.4 illustrates this.We can also interpret the mean update in (11.20) as the expected value of all data points under the distribution given by (11.25) which is a normalized probability vector, i.e.,   The update of the mean parameters in (11.20) look fairly straightforward.However, note that the responsibilities r nk are a function of π j , µ j , Σ j for all j = 1, . . ., K, such that the updates in (11.20) depend on all parameters of the GMM, and a closed-form solution, which we obtained for linear regression in Section 9.2 or PCA in Chapter 10, cannot be obtained.Theorem 11.2 (Updates of the GMM Covariances).The update of the covariance parameters Σ k , k = 1, . . ., K of the GMM is given by (11.30)where r nk and N k are defined in (11.17) and (11.24), respectively.Proof To prove Theorem 11.2, our approach is to compute the partial derivatives of the log-likelihood L with respect to the covariances Σ k , set them to 0, and solve for Σ k .We start with our general approachWe already know 1/p(x n | θ) from (11.16).To obtain the remaining partial derivative ∂p(x n | θ)/∂Σ k , we write down the definition of the Gaussian distribution p(x n | θ) (see (11.9)) and drop all terms but the kth.We then obtainWe now use the identitiesand obtain (after some rearranging) the desired partial derivative required in (11.31) asPutting everything together, the partial derivative of the log-likelihood Draft (2024-01-15) of "Mathematics for Machine Learning".Feedback: https://mml-book.com.with respect to Σ k is given by(11.36d)We see that the responsibilities r nk also appear in this partial derivative.Setting this partial derivative to 0, we obtain the necessary optimality conditionBy solving for Σ k , we obtain (11.38)where r k is the probability vector defined in (11.25).This gives us a simple update rule for Σ k for k = 1, . . ., K and proves Theorem 11.2.Similar to the update of µ k in (11.20), we can interpret the update of the covariance in (11.30) as an importance-weighted expected value of the square of the centered data Xk := {x 1 − µ k , . . ., x N − µ k }.In our example from Figure 11.3, the variances are updated as follows: Here we see that the variances of the first and third component shrink significantly, whereas the variance of the second component increases slightly.Figure 11.6 illustrates this setting.Similar to the update of the mean parameters, we can interpret (11.30) as a Monte Carlo estimate of the weighted covariance of data points x n associated with the kth mixture component, where the weights are the responsibilities r nk .As with the updates of the mean parameters, this update depends on all π j , µ j , Σ j , j = 1, . . ., K, through the responsibilities r nk , which prohibits a closed-form solution.Theorem 11.3 (Update of the GMM Mixture Weights).The mixture weights of the GMM are updated as (11.42)where N is the number of data points and N k is defined in (11.24).Proof To find the partial derivative of the log-likelihood with respect to the weight parameters π k , k = 1, . . ., K, we account for the constraint k π k = 1 by using Lagrange multipliers (see Section 7.2).The Lagrangian isDraft (2024-01-15) of "Mathematics for Machine Learning".Feedback: https://mml-book.com.(11.43b)where L is the log-likelihood from (11.10) and the second term encodes for the equality constraint that all the mixture weights need to sum up to 1.We obtain the partial derivative with respect to π k as (11.44b) and the partial derivative with respect to the Lagrange multiplier λ as (11.45)Setting both partial derivatives to 0 (necessary condition for optimum) yields the system of equations) (11.47) Using (11.46) in (11.47) and solving for π k , we obtain (11.48)This allows us to substitute −N for λ in (11.46) to obtain (11.49) which gives us the update for the weight parameters π k and proves Theorem 11.3.We can identify the mixture weight in (11.42) as the ratio of the total responsibility of the kth cluster and the number of data points.Since N = k N k , the number of data points can also be interpreted as the total responsibility of all mixture components together, such that π k is the relative importance of the kth mixture component for the dataset.Remark.Since N k = N i=1 r nk , the update equation (11.42) for the mixture weights π k also depends on all π j , µ j , Σ j , j = 1, . . ., K via the responsibilities r nk .♢   π1N (x|µ1, σ 2  1 )  π2N (x|µ2, σ 2  2 )  π3N (x|µ3, σ 2 3 ) GMM density (b) GMM density and individual components after updating the mixture weights.In our running example from Figure 11.3, the mixture weights are updated as follows:Here we see that the third component gets more weight/importance, while the other components become slightly less important.Figure 11.7 illustrates the effect of updating the mixture weights.Overall, having updated the means, the variances, and the weights once, we obtain the GMM shown in Figure 11.7(b).Compared with the initialization shown in Figure 11.3, we can see that the parameter updates caused the GMM density to shift some of its mass toward the data points.After updating the means, variances, and weights once, the GMM fit in Figure 11.7(b) is already remarkably better than its initialization from Figure 11.3.This is also evidenced by the log-likelihood values, which increased from −28.3 (initialization) to −14.4 after a full update cycle.Unfortunately, the updates in (11.20), (11.30), and (11.42) do not constitute a closed-form solution for the updates of the parameters µ k , Σ k , π k of the mixture model because the responsibilities r nk depend on those parameters in a complex way.However, the results suggest a simple iterative scheme for finding a solution to the parameters estimation problem via maximum likelihood.The expectation maximization algorithm (EM algo-Draft (2024-01-15) of "Mathematics for Machine Learning".Feedback: https://mml-book.com.rithm) was proposed by Dempster et al. (1977) and is a general iterative scheme for learning parameters (maximum likelihood or MAP) in mixture models and, more generally, latent-variable models.In our example of the Gaussian mixture model, we choose initial values for µ k , Σ k , π k and alternate until convergence between E-step: Evaluate the responsibilities r nk (posterior probability of data point n belonging to mixture component k).M-step: Use the updated responsibilities to reestimate the parameters µ k , Σ k , π k .Every step in the EM algorithm increases the log-likelihood function (Neal and Hinton, 1999).For convergence, we can check the log-likelihood or the parameters directly.A concrete instantiation of the EM algorithm for estimating the parameters of a GMM is as follows:2. E-step: Evaluate responsibilities r nk for every data point x n using current parameters π k , µ k , Σ k :  (11.54) (11.55)3 ) GMM density (a) Final GMM fit.After five iterations, the EM algorithm converges and returns this GMM.(11.57)We applied the EM algorithm to the two-dimensional dataset shown in Figure 11.1 with K = 3 mixture components.Figure 11.9 illustrates some steps of the EM algorithm and shows the negative log-likelihood as a function of the EM iteration (Figure 11.9(b)).the corresponding final GMM fit. Figure 11.10(b) visualizes the final responsibilities of the mixture components for the data points.The dataset is colored according to the responsibilities of the mixture components when EM converges.While a single mixture component is clearly responsible for the data on the left, the overlap of the two data clusters on the right could have been generated by two mixture components.It becomes clear that there are data points that cannot be uniquely assigned to a single component (either blue or yellow), such that the responsibilities of these two clusters for those points are around 0.5.We can look at the GMM from the perspective of a discrete latent-variable model, i.e., where the latent variable z can attain only a finite set of values.This is in contrast to PCA, where the latent variables were continuousvalued numbers in R M .The advantages of the probabilistic perspective are that (i) it will justify some ad hoc decisions we made in the previous sections, (ii) it allows for a concrete interpretation of the responsibilities as posterior probabilities, and (iii) the iterative algorithm for updating the model parameters can be derived in a principled manner as the EM algorithm for maximum likelihood parameter estimation in latent-variable models.To derive the probabilistic model for GMMs, it is useful to think about the generative process, i.e., the process that allows us to generate data, using a probabilistic model.We assume a mixture model with K components and that a data point x can be generated by exactly one mixture component.We introduce a binary indicator variable z k ∈ {0, 1} with two states (see Section 6.2) that indicates whether the kth mixture component generated that data point so that(11.58)We define z := [z 1 , . . ., z K ] ⊤ ∈ R K as a probability vector consisting of K −1 many 0s and exactly one 1.For example, for K = 3, a valid z would be z = [z 1 , z 2 , z 3 ] ⊤ = [0, 1, 0] ⊤ , which would select the second mixture component since z 2 = 1.Remark.Sometimes this kind of probability distribution is called "multinoulli", a generalization of the Bernoulli distribution to more than two values (Murphy, 2012).♢The properties of z imply that K k=1 z k = 1.Therefore, z is a one-hot one-hot encoding encoding (also: 1-of-K representation).Thus far, we assumed that the indicator variables z k are known.However, in practice, this is not the case, and we place a prior distribution (11.59) on the latent variable z.Then the kth entryof this probability vector describes the probability that the kth mixture component generated data point x.Remark (Sampling from a GMM).The construction of this latent-variable model (see the corresponding graphical model in Figure 11.11) lends itself to a very simple sampling procedure (generative process) to generate data:In the first step, we select a mixture component i (via the one-hot encoding z) at random according to p(z) = π; in the second step we draw a sample from the corresponding mixture component.When we discard the samples of the latent variable so that we are left with the x (i) , we have valid samples from the GMM.This kind of sampling, where samples of random variables depend on samples from the variable's parents in the graphical model, is called ancestral sampling.Generally, a probabilistic model is defined by the joint distribution of the data and the latent variables (see Section 8.4).With the prior p(z) defined in (11.59) and (11.60) and the conditional p(x | z) from (11.58), we obtain all K components of this joint distribution via (11.62) which fully specifies the probabilistic model.To obtain the likelihood p(x | θ) in a latent-variable model, we need to marginalize out the latent variables (see Section 8.4.3).In our case, this can be done by summing out all latent variables from the joint p(x, z) in (11.62) so that(11.63)We now explicitly condition on the parameters θ of the probabilistic model, which we previously omitted.In (11.63), we sum over all K possible onehot encodings of z, which is denoted by z .Since there is only a single nonzero single entry in each z there are only K possible configurations/ settings of z.For example, if K = 3, then z can have the configurationsSumming over all possible configurations of z in (11.63) is equivalent to looking at the nonzero entry of the z-vector and writingso that the desired marginal distribution is given as (11.66b) which we identify as the GMM model from (11.3).Given a dataset X , we immediately obtain the likelihood (11.67) Figure 11.12 Graphical model for a GMM with N data points.which is exactly the GMM likelihood from (11.9).Therefore, the latentvariable model with latent indicators z k is an equivalent way of thinking about a Gaussian mixture model.Let us have a brief look at the posterior distribution on the latent variable z.According to Bayes' theorem, the posterior of the kth component having generated data point xwhere the marginal p(x) is given in (11.66b).This yields the posterior distribution for the kth indicator variable z kwhich we identify as the responsibility of the kth mixture component for data point x.Note that we omitted the explicit conditioning on the GMM parameters π k , µ k , Σ k where k = 1, . . ., K.Thus far, we have only discussed the case where the dataset consists only of a single data point x.However, the concepts of the prior and posterior can be directly extended to the case of N data points X := {x 1 , . . ., x N }.In the probabilistic interpretation of the GMM, every data point x n possesses its own latent variable(11.70)Previously (when we only considered a single data point x), we omitted the index n, but now this becomes important.We share the same prior distribution π across all latent variables z n .The corresponding graphical model is shown in Figure 11.12,where we use the plate notation.The conditional distribution p(x 1 , . . ., x N | z 1 , . . ., z N ) factorizes over the data points and is given as(11.71)To obtain the posterior distribution p(z nk = 1 | x n ), we follow the same reasoning as in Section 11.4.3 and apply Bayes' theorem to obtain (11.72b)This means that p(z k = 1 | x n ) is the (posterior) probability that the kth mixture component generated data point x n and corresponds to the responsibility r nk we introduced in (11.17).Now the responsibilities also have not only an intuitive but also a mathematically justified interpretation as posterior probabilities.The EM algorithm that we introduced as an iterative scheme for maximum likelihood estimation can be derived in a principled way from the latentvariable perspective.Given a current setting θ (t) of model parameters, the E-step calculates the expected log-likelihood (11.73b)where the expectation of log p(x, z | θ) is taken with respect to the posterior p(z | x, θ (t) ) of the latent variables.The M-step selects an updated set of model parameters θ (t+1) by maximizing (11.73b).Although an EM iteration does increase the log-likelihood, there are no guarantees that EM converges to the maximum likelihood solution.It is possible that the EM algorithm converges to a local maximum of the log-likelihood.Different initializations of the parameters θ could be used in multiple EM runs to reduce the risk of ending up in a bad local optimum.We do not go into further details here, but refer to the excellent expositions by Rogers and Girolami (2016) and Bishop (2006).The GMM can be considered a generative model in the sense that it is straightforward to generate new data using ancestral sampling (Bishop, 2006).For given GMM parameters π k , µ k , Σ k , k = 1, . . ., K, we sample an index k from the probability vector [π 1 , . . ., π K ] ⊤ and then sample a data point x ∼ N µ k , Σ k .If we repeat this N times, we obtain a dataset that has been generated by a GMM. Figure 11.1 was generated using this procedure.Throughout this chapter, we assumed that the number of components K is known.In practice, this is often not the case.However, we could use nested cross-validation, as discussed in Section 8.6.1, to find good models.Gaussian mixture models are closely related to the K-means clustering algorithm.K-means also uses the EM algorithm to assign data points to clusters.If we treat the means in the GMM as cluster centers and ignore the covariances (or set them to I), we arrive at K-means.As also nicely described by MacKay ( 2003), K-means makes a "hard" assignment of data points to cluster centers µ k , whereas a GMM makes a "soft" assignment via the responsibilities.We only touched upon the latent-variable perspective of GMMs and the EM algorithm.Note that EM can be used for parameter learning in general latent-variable models, e.g., nonlinear state-space models (Ghahramani and Roweis, 1999;Roweis and Ghahramani, 1999) and for reinforcement learning as discussed by Barber (2012).Therefore, the latent-variable perspective of a GMM is useful to derive the corresponding EM algorithm in a principled way (Bishop, 2006;Barber, 2012;Murphy, 2012).We only discussed maximum likelihood estimation (via the EM algorithm) for finding GMM parameters.The standard criticisms of maximum likelihood also apply here:As in linear regression, maximum likelihood can suffer from severe overfitting.In the GMM case, this happens when the mean of a mixture component is identical to a data point and the covariance tends to 0. Then, the likelihood approaches infinity.Bishop (2006) and Barber (2012) discuss this issue in detail.We only obtain a point estimate of the parameters π k , µ k , Σ k for k = 1, . . ., K, which does not give any indication of uncertainty in the parameter values.A Bayesian approach would place a prior on the parameters, which can be used to obtain a posterior distribution on the parameters.This posterior allows us to compute the model evidence (marginal likelihood), which can be used for model comparison, which gives us a principled way to determine the number of mixture components.Unfortunately, closed-form inference is not possible in this setting because there is no conjugate prior for this model.However, approximations, such as variational inference, can be used to obtain an approximate posterior (Bishop, 2006).In this chapter, we discussed mixture models for density estimation.There is a plethora of density estimation techniques available.In practice, we often use histograms and kernel density estimation.histogram Histograms provide a nonparametric way to represent continuous densities and have been proposed by Pearson (1895).A histogram is constructed by "binning" the data space and count, how many data points fall into each bin.Then a bar is drawn at the center of each bin, and the height of the bar is proportional to the number of data points within that bin.The bin size is a critical hyperparameter, and a bad choice can lead to overfitting and underfitting.Cross-validation, as discussed in Section 8.2.4,can be used to determine a good bin size.Kernel density estimation, independently proposed by Rosenblatt (1956) and Parzen (1962), is a nonparametric way for density estimation.Given N i.i.d.samples, the kernel density estimator represents the underlying distribution aswhere k is a kernel function, i.e., a nonnegative function that integrates to 1 and h > 0 is a smoothing/bandwidth parameter, which plays a similar role as the bin size in histograms.Note that we place a kernel on every single data point x n in the dataset.Commonly used kernel functions are the uniform distribution and the Gaussian distribution.Kernel density estimates are closely related to histograms, but by choosing a suitable kernel, we can guarantee smoothness of the density estimate.Figure 11.13 illustrates the difference between a histogram and a kernel density estimator (with a Gaussian-shaped kernel) for a given dataset of 250 data points.In many situations, we want our machine learning algorithm to predict one of a number of (discrete) outcomes.For example, an email client sorts mail into personal mail and junk mail, which has two outcomes.Another example is a telescope that identifies whether an object in the night sky is a galaxy, star, or planet.There are usually a small number of outcomes, and more importantly there is usually no additional structure on these outcomes.In this chapter, we consider predictors that output binary val- we defer a survey of other approaches to Section 12.6.We present an approach known as the support vector machine (SVM), which solves the binary classification task.As in regression, we have a supervised learning task, where we have a set of examples x n ∈ R D along with their corresponding (binary) labels y n ∈ {+1, −1}.Given a training data set consisting of example-label pairs {(x 1 , y 1 ), . . ., (x N , y N )}, we would like to estimate parameters of the model that will give the smallest classification error.Similar to Chapter 9, we consider a linear model, and hide away the nonlinearity in a transformation ϕ of the examples (9.13).We will revisit ϕ in Section 12.4.The SVM provides state-of-the-art results in many applications, with sound theoretical guarantees (Steinwart and Christmann, 2008).There are two main reasons why we chose to illustrate binary classification using x (1) x (2) SVMs.First, the SVM allows for a geometric way to think about supervised machine learning.While in Chapter 9 we considered the machine learning problem in terms of probabilistic models and attacked it using maximum likelihood estimation and Bayesian inference, here we will consider an alternative approach where we reason geometrically about the machine learning task.It relies heavily on concepts, such as inner products and projections, which we discussed in Chapter 3. The second reason why we find SVMs instructive is that in contrast to Chapter 9, the optimization problem for SVM does not admit an analytic solution so that we need to resort to a variety of optimization tools introduced in Chapter 7.The SVM view of machine learning is subtly different from the maximum likelihood view of Chapter 9.The maximum likelihood view proposes a model based on a probabilistic view of the data distribution, from which an optimization problem is derived.In contrast, the SVM view starts by designing a particular function that is to be optimized during training, based on geometric intuitions.We have seen something similar already in Chapter 10, where we derived PCA from geometric principles.In the SVM case, we start by designing a loss function that is to be minimized on training data, following the principles of empirical risk minimization (Section 8.2).Let us derive the optimization problem corresponding to training an SVM on example-label pairs.Intuitively, we imagine binary classification data, which can be separated by a hyperplane as illustrated in Figure 12.1.Here, every example x n (a vector of dimension 2) is a two-dimensional location (x (1) n and x (2) n ), and the corresponding binary label y n is one of two different symbols (orange cross or blue disc)."Hyperplane" is a word that is commonly used in machine learning, and we encountered hyperplanes already in Section 2.8.A hyperplane is an affine subspace of dimension D − 1 (if the corresponding vector space is of dimension D).The examples consist of two classes (there are two possible labels) that have features (the components of the vector representing the example) arranged in such a way as to allow us to separate/classify them by drawing a straight line.In the following, we formalize the idea of finding a linear separator of the two classes.We introduce the idea of the margin and then extend linear separators to allow for examples to fall on the "wrong" side, incurring a classification error.We present two equivalent ways of formalizing the SVM: the geometric view (Section 12.2.4) and the loss function view (Section 12.2.5).We derive the dual version of the SVM using Lagrange multipliers (Section 7.2).The dual SVM allows us to observe a third way of formalizing the SVM: in terms of the convex hulls of the examples of each class (Section 12.3.2).We conclude by briefly describing kernels and how to numerically solve the nonlinear kernel-SVM optimization problem.Given two examples represented as vectors x i and x j , one way to compute the similarity between them is using an inner product ⟨x i , x j ⟩.Recall from Section 3.2 that inner products are closely related to the angle between two vectors.The value of the inner product between two vectors depends on the length (norm) of each vector.Furthermore, inner products allow us to rigorously define geometric concepts such as orthogonality and projections.The main idea behind many classification algorithms is to represent data in R D and then partition this space, ideally in a way that examples with the same label (and no other examples) are in the same partition.In the case of binary classification, the space would be divided into two parts corresponding to the positive and negative classes, respectively.We consider a particularly convenient partition, which is to (linearly) split the space into two halves using a hyperplane.Let example x ∈ R D be an element of the data space.Consider a functionparametrized by w ∈ R D and b ∈ R. Recall from Section 2.8 that hyperplanes are affine subspaces.Therefore, we define the hyperplane that separates the two classes in our binary classification problem asAn illustration of the hyperplane is shown in Figure 12.2, where the vector w is a vector normal to the hyperplane and b the intercept.We can derive that w is a normal vector to the hyperplane in (12.3) by choosing any two examples x a and x b on the hyperplane and showing that the vector between them is orthogonal to w.In the form of an equation, where the second line is obtained by the linearity of the inner product (Section 3.2).Since we have chosen x a and x b to be on the hyperplane, this implies that f (x a ) = 0 and f (x b ) = 0 and hence ⟨w, x a − x b ⟩ = 0. Recall that two vectors are orthogonal when their inner product is zero.w is orthogonal to any vector on the hyperplane.Therefore, we obtain that w is orthogonal to any vector on the hyperplane.Remark.Recall from Chapter 2 that we can think of vectors in different ways.In this chapter, we think of the parameter vector w as an arrow indicating a direction, i.e., we consider w to be a geometric vector.In contrast, we think of the example vector x as a data point (as indicated by its coordinates), i.e., we consider x to be the coordinates of a vector with respect to the standard basis.♢When presented with a test example, we classify the example as positive or negative depending on the side of the hyperplane on which it occurs.Note that (12.3) not only defines a hyperplane; it additionally defines a direction.In other words, it defines the positive and negative side of the hyperplane.Therefore, to classify a test example x test , we calculate the value of the function f (x test ) and classify the example as +1 if f (x test ) ⩾ 0 and −1 otherwise.Thinking geometrically, the positive examples lie "above" the hyperplane and the negative examples "below" the hyperplane.When training the classifier, we want to ensure that the examples with positive labels are on the positive side of the hyperplane, i.e., Equation (12.7) is equivalent to (12.5) and (12.6) when we multiply both sides of (12.5) and (12.6) with y n = 1 and y n = −1, respectively.x 1) x (2)Based on the concept of distances from points to a hyperplane, we now are in a position to discuss the support vector machine.For a dataset {(x 1 , y 1 ), . . ., (x N , y N )} that is linearly separable, we have infinitely many candidate hyperplanes (refer to Figure 12.3), and therefore classifiers, that solve our classification problem without any (training) errors.To find a unique solution, one idea is to choose the separating hyperplane that maximizes the margin between the positive and negative examples.In other words, we want the positive and negative examples to be separated by a large margin (Section 12.2.1).In the following, we compute the dis-A classifier with large margin turns out to generalize well (Steinwart and Christmann, 2008).tance between an example and a hyperplane to derive the margin.Recall that the closest point on the hyperplane to a given point (example x n ) is obtained by the orthogonal projection (Section 3.8).The concept of the margin is intuitively simple: It is the distance of the margin separating hyperplane to the closest examples in the dataset, assumingThere could be two or more closest examples to a hyperplane.that the dataset is linearly separable.However, when trying to formalize this distance, there is a technical wrinkle that may be confusing.The technical wrinkle is that we need to define a scale at which to measure the distance.A potential scale is to consider the scale of the data, i.e., the raw values of x n .There are problems with this, as we could change the units of measurement of x n and change the values in x n , and, hence, change the distance to the hyperplane.As we will see shortly, we define the scale based on the equation of the hyperplane (12.3) itself.Consider a hyperplane ⟨w, x⟩ + b, and an example x a as illustrated in Figure 12.4.Without loss of generality, we can consider the example x a to be on the positive side of the hyperplane, i.e., ⟨w, x a ⟩ + b > 0. We would like to compute the distance r > 0 of x a from the hyperplane.We do so by considering the orthogonal projection (Section 3.8) of x a onto the hyperplane, which we denote by x ′ a .Since w is orthogonal to the x ′ a r hyperplane, we know that the distance r is just a scaling of this vector w.If the length of w is known, then we can use this scaling factor r factor to work out the absolute distance between x a and x ′ a .For convenience, we choose to use a vector of unit length (its norm is 1) and obtain this by dividing w by its norm, w ∥w∥ .Using vector addition (Section 2.4), we obtainAnother way of thinking about r is that it is the coordinate of x a in the subspace spanned by w/ ∥w∥.We have now expressed the distance of x a from the hyperplane as r, and if we choose x a to be the point closest to the hyperplane, this distance r is the margin.Recall that we would like the positive examples to be further than r from the hyperplane, and the negative examples to be further than distance r (in the negative direction) from the hyperplane.Analogously to the combination of (12.5) and (12.6) (12.7), we formulate this objective as(12.9)In other words, we combine the requirements that examples are at least r away from the hyperplane (in the positive and negative direction) into one single inequality.Since we are interested only in the direction, we add an assumption to our model that the parameter vector w is of unit length, i.e., ∥w∥ = 1, where we use the Euclidean norm ∥w∥ = √ w ⊤ w (Section 3.1).This We will see other choices of inner products (Section 3.2) in Section 12.4.assumption also allows a more intuitive interpretation of the distance r (12.8) since it is the scaling factor of a vector of length 1.Remark.A reader familiar with other presentations of the margin would notice that our definition of ∥w∥ = 1 is different from the standard presentation if the SVM was the one provided by Schölkopf and Smola (2002), for example.In Section 12.2.3,we will show the equivalence of both approaches.♢  which says that we want to maximize the margin r while ensuring that the data lies on the correct side of the hyperplane.Remark.The concept of the margin turns out to be highly pervasive in machine learning.It was used by Vladimir Vapnik and Alexey Chervonenkis to show that when the margin is large, the "complexity" of the function class is low, and hence learning is possible (Vapnik, 2000).It turns out that the concept is useful for various different approaches for theoretically analyzing generalization error (Steinwart and Christmann, 2008;Shalev-Shwartz and Ben-David, 2014).♢In the previous section, we derived (12.10) by making the observation that we are only interested in the direction of w and not its length, leading to the assumption that ∥w∥ = 1.In this section, we derive the margin maximization problem by making a different assumption.Instead of choosing that the parameter vector is normalized, we choose a scale for the data.We choose this scale such that the value of the predictor ⟨w, x⟩ + b is 1 at the closest example.Let us also denote the example in the dataset that isRecall that we currently consider linearly separable data.closest to the hyperplane by x a .Figure 12.5 is identical to Figure 12.4, except that now we rescaled the axes, such that the example x a lies exactly on the margin, i.e., ⟨w, x a ⟩ + b = 1.Since x ′ a is the orthogonal projection of x a onto the hyperplane, it must by definition lie on the hyperplane, i.e., ⟨w, x ′ a ⟩ + b = 0 .(12.11) seem to have derived the distance from the hyperplane in terms of the length of the vector w, but we do not yet know this vector.One way to think about it is to consider the distance r to be a temporary variable that we only use for this derivation.Therefore, for the rest of this section we will denote the distance to the hyperplane by 1 ∥w∥ .In Section 12.2.3,we will see that the choice that the margin equals 1 is equivalent to our previous assumption of ∥w∥ = 1 in Section 12.2.1.Similar to the argument to obtain (12.9), we want the positive and negative examples to be at least 1 away from the hyperplane, which yields the condition "hard" condition can be relaxed to accommodate violations if the data is not linearly separable.In Section 12.2.1,we argued that we would like to maximize some value r, which represents the distance of the closest example to the hyperplane.In Section 12.2.2,we scaled the data such that the closest example is of distance 1 to the hyperplane.In this section, we relate the two derivations, and show that they are equivalent.Proof Consider (12.20).Since the square is a strictly monotonic transformation for non-negative arguments, the maximum stays the same if we consider r 2 in the objective.Since ∥w∥ = 1 we can reparametrize the equation with a new weight vector w ′ that is not normalized by explicitly using w ′ ∥w ′ ∥ .We obtain(12.22) Equation (12.22) explicitly states that the distance r is positive.Therefore, we can divide the first constraint by r, which yields Note that r > 0 because we assumed linear separability, and hence there is no issue to divide by r.Draft (2024-01-15) of "Mathematics for Machine Learning".Feedback: https://mml-book.com.x 1) x (2)(a) Linearly separable data, with a large marginx (2) (b) Non-linearly separable data renaming the parameters to w ′′ and b ′′ .Since w ′′ = w ′ ∥w ′ ∥r , rearranging for r gives .24)By substituting this result into (12.23),we obtainThe final step is to observe that maximizing 1 ∥w ′′ ∥ 2 yields the same solution as minimizing 1 2 ∥w ′′ ∥ 2 , which concludes the proof of Theorem 12.1.In the case where data is not linearly separable, we may wish to allow some examples to fall within the margin region, or even to be on the wrong side of the hyperplane as illustrated in Figure 12.6.The model that allows for some classification errors is called the soft soft margin SVM margin SVM.In this section, we derive the resulting optimization problem using geometric arguments.In Section 12.2.5, we will derive an equivalent optimization problem using the idea of a loss function.Using Lagrange multipliers (Section 7.2), we will derive the dual optimization problem of the SVM in Section 12.3.This dual optimization problem allows us to observe a third interpretation of the SVM: as a hyperplane that bisects the line between convex hulls corresponding to the positive and negative data examples (Section 12.3.2).The key geometric idea is to introduce a slack variable ξ n corresponding slack variable to each example-label pair (x n , y n ) that allows a particular example to be within the margin or even on the wrong side of the hyperplane (refer to .We subtract the value of ξ n from the margin, constraining ξ n to be non-negative.To encourage correct classification of the samples, we add ξ n to the objectivefor n = 1, . . ., N .In contrast to the optimization problem (12.18) for the hard margin SVM, this one is called the soft margin SVM.The parameter soft margin SVM C > 0 trades off the size of the margin and the total amount of slack that we have.This parameter is called the regularization parameter since, as regularization parameter we will see in the following section, the margin term in the objective function (12.26a) is a regularization term.The margin term ∥w∥ 2 is called the regularizer, and in many books on numerical optimization, the regregularizer ularization parameter is multiplied with this term (Section 8.2.3).This is in contrast to our formulation in this section.Here a large value of C implies low regularization, as we give the slack variables larger weight, hence giving more priority to examples that do not lie on the correct side of the margin.There are alternative parametrizations of this regularization, which is why (12.26a) is also often referred to as the C-SVM.Remark.In the formulation of the soft margin SVM (12.26a) w is regularized, but b is not regularized.We can see this by observing that the regularization term does not contain b.The unregularized term b complicates theoretical analysis (Steinwart and Christmann, 2008, chapter 1) and decreases computational efficiency (Fan et al., 2008).♢Let us consider a different approach for deriving the SVM, following the principle of empirical risk minimization (Section 8.2).For the SVM, we Draft (2024-01-15) of "Mathematics for Machine Learning".Feedback: https://mml-book.com.choose hyperplanes as the hypothesis class, that is(12.27)We will see in this section that the margin corresponds to the regularization term.The remaining question is, what is the loss function?In conloss function trast to Chapter 9, where we consider regression problems (the output of the predictor is a real number), in this chapter, we consider binary classification problems (the output of the predictor is one of two labels {+1, −1}).Therefore, the error/loss function for each single examplelabel pair needs to be appropriate for binary classification.For example, the squared loss that is used for regression (9.10b) is not suitable for binary classification.Remark.The ideal loss function between binary labels is to count the number of mismatches between the prediction and the label.This means that for a predictor f applied to an example x n , we compare the output f (x n ) with the label y n .We define the loss to be zero if they match, and one if they do not match.This is denoted by 1(f (x n ) ̸ = y n ) and is called the zero-one loss.Unfortunately, the zero-one loss results in a combinatorial zero-one loss optimization problem for finding the best parameters w, b.Combinatorial optimization problems (in contrast to continuous optimization problems discussed in Chapter 7) are in general more challenging to solve.♢ What is the loss function corresponding to the SVM?Consider the error between the output of a predictor f (x n ) and the label y n .The loss describes the error that is made on the training data.An equivalent way to derive (12.26a) is to use the hinge loss hinge loss ℓ(t) = max{0, 1 − t} where t = yf (x) = y(⟨w, x⟩ + b) .(12.28)If f (x) is on the correct side (based on the corresponding label y) of the hyperplane, and further than distance 1, this means that t ⩾ 1 and the hinge loss returns a value of zero.If f (x) is on the correct side but too close to the hyperplane (0 < t < 1), the example x is within the margin, and the hinge loss returns a positive value.When the example is on the wrong side of the hyperplane (t < 0), the hinge loss returns an even larger value, which increases linearly.In other words, we pay a penalty once we are closer than the margin to the hyperplane, even if the prediction is correct, and the penalty increases linearly.An alternative way to express the hinge loss is by considering it as two linear pieces (12.29) as illustrated in Figure 12.8.The loss corresponding to the hard margin SVM 12.18 is defined as  (12.31)The first term in (12.31) is called the regularization term or the regularizer regularizer (see Section 8.2.3), and the second term is called the loss term or the error loss term error term term.Recall from Section 12.2.4 that the term 1 2 ∥w∥ 2 arises directly from the margin.In other words, margin maximization can be interpreted as regularization.regularizationIn principle, the unconstrained optimization problem in (12.31) can be directly solved with (sub-)gradient descent methods as described in Section 7.1.To see that (12.31) and (12.26a) are equivalent, observe that the hinge loss (12.28) essentially consists of two linear parts, as expressed in (12.29).Consider the hinge loss for a single example-label pair (12.28).We can equivalently replace minimization of the hinge loss over t with a minimization of a slack variable ξ with two constraints.In equation form, (12.33)By substituting this expression into (12.31)and rearranging one of the constraints, we obtain exactly the soft margin SVM (12.26a).Remark.Let us contrast our choice of the loss function in this section to the loss function for linear regression in Chapter 9. Recall from Section 9.2.1 that for finding maximum likelihood estimators, we usually minimize the Draft (2024-01-15) of "Mathematics for Machine Learning".Feedback: https://mml-book.com.negative log-likelihood.Furthermore, since the likelihood term for linear regression with Gaussian noise is Gaussian, the negative log-likelihood for each example is a squared error function.The squared error function is the loss function that is minimized when looking for the maximum likelihood solution.♢The description of the SVM in the previous sections, in terms of the variables w and b, is known as the primal SVM.Recall that we consider inputs x ∈ R D with D features.Since w is of the same dimension as x, this means that the number of parameters (the dimension of w) of the optimization problem grows linearly with the number of features.In the following, we consider an equivalent optimization problem (the so-called dual view), which is independent of the number of features.Instead, the number of parameters increases with the number of examples in the training set.We saw a similar idea appear in Chapter 10, where we expressed the learning problem in a way that does not scale with the number of features.This is useful for problems where we have more features than the number of examples in the training dataset.The dual SVM also has the additional advantage that it easily allows kernels to be applied, as we shall see at the end of this chapter.The word "dual" appears often in mathematical literature, and in this particular case it refers to convex duality.The following subsections are essentially an application of convex duality, which we discussed in Section 7.2.Recall the primal soft margin SVM (12.26a).We call the variables w, b, and ξ corresponding to the primal SVM the primal variables.We use α n ⩾ In Chapter 7, we used λ as Lagrange multipliers.In this section, we follow the notation commonly chosen in SVM literature, and use α and γ. 0 as the Lagrange multiplier corresponding to the constraint (12.26b) that the examples are classified correctly and γ n ⩾ 0 as the Lagrange multiplier corresponding to the non-negativity constraint of the slack variable; see (12.26c).The Lagrangian is then given by L(w, b, ξ, α, γ) = 1 2 ∥w∥ 2 + C   (12.38) which is a particular instance of the representer theorem (Kimeldorf and representer theorem Wahba, 1970).Equation (12.38) states that the optimal weight vector in the primal is a linear combination of the examples x n .Recall from Section 2.6.1 that this means that the solution of the optimization problem lies in the span of training data.Additionally, the constraint obtained by setting (12.36) to zero implies that the optimal weight vector is an affine combination of the examples.The representer theorem turns out to hold for very general settings of regularized empirical risk minimization (Hofmann et al., 2008;Argyriou and Dinuzzo, 2014).The theorem has more general versions (Schölkopf et al., 2001), and necessary and sufficient conditions on its existence can be found in Yu et al. (2013).Remark.The representer theorem (12.38) also provides an explanation of the name "support vector machine.(12.39)Note that there are no longer any terms involving the primal variable w.By setting (12.36) to zero, we obtain bilinear (see Section 3.2).Therefore, the first two terms in (12.39) are over the same objects.These terms (colored blue) can be simplified, and we obtain the Lagrangian (12.40)The last term in this equation is a collection of all terms that contain slack variables ξ i .By setting (12.37) to zero, we see that the last term in (12.40) is also zero.Furthermore, by using the same equation and recalling that the Lagrange multiplers γ i are non-negative, we conclude that α i ⩽ C. We now obtain the dual optimization problem of the SVM, which is expressed exclusively in terms of the Lagrange multipliers α i .Recall from Lagrangian duality (Definition 7.1) that we maximize the dual problem.This is equivalent to minimizing the negative dual problem, such that we end up with the dual SVM (12.41)The equality constraint in (12.41) is obtained from setting (12.36) to zero.The inequality constraint α i ⩾ 0 is the condition imposed on Lagrange multipliers of inequality constraints (Section 7.2).The inequality constraint α i ⩽ C is discussed in the previous paragraph.The set of inequality constraints in the SVM are called "box constraints" because they limit the vector α = [α 1 , • • • , α N ] ⊤ ∈ R N of Lagrange multipliers to be inside the box defined by 0 and C on each axis.These axis-aligned boxes are particularly efficient to implement in numerical solvers (Dostál, 2009, chapter 5).It turns out that examples that lie exactly on the margin are examples whose dual parameters lie strictly inside the box constraints, 0 < α i < C.This is derived using the Karush Kuhn Tucker conditions, for example in Schölkopf and Smola (2002).Once we obtain the dual parameters α, we can recover the primal parameters w by using the representer theorem (12.38).Let us call the optimal primal parameter w * .However, there remains the question on how to obtain the parameter b * .Consider an example x n that lies exactly on the margin's boundary, i.e., ⟨w * , x n ⟩ + b = y n .Recall that y n is either +1 or −1.Therefore, the only unknown is b, which can be computed by b * = y n − ⟨w * , x n ⟩ .(12.42)Remark.In principle, there may be no examples that lie exactly on the margin.In this case, we should compute |y n − ⟨w * , x n ⟩ | for all support vectors and take the median value of this absolute value difference to be the value of b * .A derivation of this can be found in http://fouryears.eu/2012/06/07/the-svm-bias-term-conspiracy/.♢Another approach to obtain the dual SVM is to consider an alternative geometric argument.Consider the set of examples x n with the same label.We would like to build a convex set that contains all the examples such that it is the smallest possible set.This is called the convex hull and is illustrated in Figure 12.9.Let us first build some intuition about a convex combination of points.Consider two points x 1 and x 2 and corresponding non-negative weights α 1 , α 2 ⩾ 0 such that α 1 +α 2 = 1.The equation α 1 x 1 +α 2 x 2 describes each point on a line between x 1 and x 2 .Consider what happens when we add a third point x 3 along with a weight α 3 ⩾ 0 such that 3 n=1 α n = 1.The convex combination of these three points x 1 , x 2 , x 3 spans a twodimensional area.The convex hull of this area is the triangle formed by convex hull the edges corresponding to each pair of of points.As we add more points, and the number of points becomes greater than the number of dimensions, some of the points will be inside the convex hull, as we can see in Figure 12.9(a).In general, building a convex convex hull can be done by introducing non-negative weights α n ⩾ 0 corresponding to each example x n .Then the convex hull can be described as the set conv (X) = Let α be the set of all coefficients, i.e., the concatenation of α + and α − .Recall that we require that for each convex hull that their coefficients sum to one, The objective function (12.48) and the constraint (12.50), along with the assumption that α ⩾ 0, give us a constrained (convex) optimization problem.This optimization problem can be shown to be the same as that of the dual hard margin SVM (Bennett and Bredensteiner, 2000a).Remark.To obtain the soft margin dual, we consider the reduced hull.The reduced hull is similar to the convex hull but has an upper bound to the reduced hull size of the coefficients α.The maximum possible value of the elements of α restricts the size that the convex hull can take.In other words, the bound on α shrinks the convex hull to a smaller volume (Bennett and Bredensteiner, 2000b).♢Consider the formulation of the dual SVM (12.41).Notice that the inner product in the objective occurs only between examples x i and x j .There are no inner products between the examples and the parameters.Therefore, if we consider a set of features ϕ(x i ) to represent x i , the only change in the dual SVM will be to replace the inner product.This modularity, where the choice of the classification method (the SVM) and the choice of the feature representation ϕ(x) can be considered separately, provides flexibility for us to explore the two problems independently.In this section, we discuss the representation ϕ(x) and briefly introduce the idea of kernels, but do not go into the technical details.Since ϕ(x) could be a non-linear function, we can use the SVM (which assumes a linear classifier) to construct classifiers that are nonlinear in the examples x n .This provides a second avenue, in addition to the soft margin, for users to deal with a dataset that is not linearly separable.It turns out that there are many algorithms and statistical methods that have this property that we observed in the dual SVM: the only inner products are those that occur between examples.Instead of explicitly defining a non-linear feature map ϕ(•) and computing the resulting inner product between examples x i and x j , we define a similarity function k(x i , x j ) between x i and x j .For a certain class of similarity functions, called kernels, kernel the similarity function implicitly defines a non-linear feature map ϕ(•).Kernels are by definition functions k : X × X → R for which there existsThe inputs X of the kernel function can be very general and are not necessarily restricted to R D .a Hilbert space H and ϕ : X → H a feature map such that k(x i , x j ) = ⟨ϕ(x i ), ϕ(x j )⟩ H .(12.52)There is a unique reproducing kernel Hilbert space associated with every kernel k (Aronszajn, 1950;Berlinet and Thomas-Agnan, 2004).In this unique association, ϕ(x) = k(•, x) is called the canonical feature map.canonical feature mapThe generalization from an inner product to a kernel function (12.52) is known as the kernel trick (Schölkopf and Smola, 2002; Shawe-Taylor and kernel trick Cristianini, 2004), as it hides away the explicit non-linear feature map.The matrix K ∈ R N ×N , resulting from the inner products or the application of k(•, •) to a dataset, is called the Gram matrix, and is often just Gram matrix referred to as the kernel matrix.Kernels must be symmetric and positive kernel matrix semidefinite functions so that every kernel matrix K is symmetric and positive semidefinite (Section 3. and Williams, 2006).Figure 12.10 illustrates the effect of different kernels on separating hyperplanes on an example dataset.Note that we are still solving for hyperplanes, that is, the hypothesis class of functions are still linear.The non-linear surfaces are due to the kernel function.Remark.Unfortunately for the fledgling machine learner, there are multiple meanings of the word "kernel."In this chapter, the word "kernel" comes from the idea of the reproducing kernel Hilbert space (RKHS) (Aronszajn, 1950;Saitoh, 1988).We have discussed the idea of the kernel in linear algebra (Section 2.7.3),where the kernel is another word for the null space.The third common use of the word "kernel" in machine learning is the smoothing kernel in kernel density estimation (Section 11.5).♢Since the explicit representation ϕ(x) is mathematically equivalent to the kernel representation k(x i , x j ), a practitioner will often design the kernel function such that it can be computed more efficiently than the inner product between explicit feature maps.For example, consider the polynomial kernel (Schölkopf and Smola, 2002), where the number of terms in the explicit expansion grows very quickly (even for polynomials of low degree) when the input dimension is large.The kernel function only requires one multiplication per input dimension, which can provide significant computational savings.Another example is the Gaussian radial basis function kernel (Schölkopf and Smola, 2002;Rasmussen and Williams, 2006), where the corresponding feature space is infinite dimensional.In this case, we cannot explicitly represent the feature space but can still compute similarities between a pair of examples using the kernel.The choice of kernel, as well as the parameters of the kernel, is often chosen using nested cross-validation (Section 8.6.1).Another useful aspect of the kernel trick is that there is no need for the original data to be already represented as multivariate real-valued data.Note that the inner product is defined on the output of the function ϕ(•), but does not restrict the input to real numbers.Hence, the function ϕ(•) and the kernel function k(•, •) can be defined on any object, e.g., sets, sequences, strings, graphs, and distributions (Ben-Hur et al., 2008;Gärtner, 2008;Shi et al., 2009;Sriperumbudur et al., 2010;Vishwanathan et al., 2010).We conclude our discussion of SVMs by looking at how to express the problems derived in this chapter in terms of the concepts presented in Chapter 7. We consider two different approaches for finding the optimal solution for the SVM.First we consider the loss view of SVM 8.2.2 and express this as an unconstrained optimization problem.Then we express the constrained versions of the primal and dual SVMs as quadratic programs in standard form 7.3.2.Consider the loss function view of the SVM (12.31).This is a convex unconstrained optimization problem, but the hinge loss (12.28) is not dif-Draft (2024-01-15) of "Mathematics for Machine Learning".Feedback: https://mml-book.com.ferentiable.Therefore, we apply a subgradient approach for solving it.However, the hinge loss is differentiable almost everywhere, except for one single point at the hinge t = 1.At this point, the gradient is a set of possible values that lie between 0 and −1.Therefore, the subgradient g of the hinge loss is given by(12.54)Using this subgradient, we can apply the optimization methods presented in Section 7.1.Both the primal and the dual SVM result in a convex quadratic programming problem (constrained optimization).Note that the primal SVM in (12.26a) has optimization variables that have the size of the dimension D of the input examples.The dual SVM in (12.41) has optimization variables that have the size of the number N of examples.To express the primal SVM in the standard form (7.45) for quadratic programming, let us assume that we use the dot product (3.5) as the inner product.We rearrange the equation for the primal SVM (12.26a),Recall from Section 3.2 that we use the phrase dot product to mean the inner product on Euclidean vector space.such that the optimization variables are all on the right and the inequality of the constraint matches the standard form.This yields the optimization is an N by N matrix where the elements of the diagonal are from y, and X ∈ R N ×D is the matrix obtained by concatenating all the examples.We can similarly perform a collection of terms for the dual version of the SVM (12.41).To express the dual SVM in standard form, we first have to express the kernel matrix K such that each entry is K ij = k(x i , x j ).If we have an explicit feature representation x i then we define K ij = ⟨x i , x j ⟩.For convenience of notation we introduce a matrix with zeros everywhere except on the diagonal, where we store the labels, that is, Y = diag(y).The dual SVM can be written as(12.57)Remark.In Sections 7.3.1 and 7.3.2,we introduced the standard forms of the constraints to be inequality constraints.We will express the dual SVM's equality constraint as two inequality constraints, i.e., Since there are many different possible views of the SVM, there are many approaches for solving the resulting optimization problem.The approach presented here, expressing the SVM problem in standard convex optimization form, is not often used in practice.The two main implementations of SVM solvers are Chang and Lin (2011) (which is open source) and Joachims (1999).Since SVMs have a clear and well-defined optimization problem, many approaches based on numerical optimization techniques (Nocedal and Wright, 2006) can be applied (Shawe-Taylor and Sun, 2011).The SVM is one of many approaches for studying binary classification.Other approaches include the perceptron, logistic regression, Fisher discriminant, nearest neighbor, naive Bayes, and random forest (Bishop, 2006;Murphy, 2012).A short tutorial on SVMs and kernels on discrete sequences can be found in Ben-Hur et al. (2008).The development of SVMs is closely linked to empirical risk minimization, discussed in Section 8.2.Hence, the SVM has strong theoretical properties (Vapnik, 2000;Steinwart and Christmann, 2008).The book about kernel methods (Schölkopf and Smola, 2002) includes many details of support vector machines and Draft (2024-01-15) of "Mathematics for Machine Learning".Feedback: https://mml-book.com.how to optimize them.A broader book about kernel methods (Shawe-Taylor and Cristianini, 2004) also includes many linear algebra approaches for different machine learning problems.An alternative derivation of the dual SVM can be obtained using the idea of the Legendre-Fenchel transform (Section 7.3.3).The derivation considers each term of the unconstrained formulation of the SVM (12.31) separately and calculates their convex conjugates (Rifkin and Lippert, 2007).Readers interested in the functional analysis view (also the regularization methods view) of SVMs are referred to the work by Wahba (1990).Theoretical exposition of kernels (Aronszajn, 1950;Schwartz, 1964;Saitoh, 1988;Manton and Amblard, 2015) requires a basic grounding in linear operators (Akhiezer and Glazman, 1993).The idea of kernels have been generalized to Banach spaces (Zhang et al., 2009) and Kreȋn spaces (Ong et al., 2004;Loosli et al., 2016).Observe that the hinge loss has three equivalent representations, as shown in (12.28) and (12.29), as well as the constrained optimization problem in (12.33).The formulation (12.28) is often used when comparing the SVM loss function with other loss functions (Steinwart, 2007).The two-piece formulation (12.29) is convenient for computing subgradients, as each piece is linear.The third formulation (12.33), as seen in Section 12.5, enables the use of convex quadratic programming (Section 7.3.2) tools.Since binary classification is a well-studied task in machine learning, other words are also sometimes used, such as discrimination, separation, and decision.Furthermore, there are three quantities that can be the output of a binary classifier.First is the output of the linear function itself (often called the score), which can take any real value.This output can be used for ranking the examples, and binary classification can be thought of as picking a threshold on the ranked examples (Shawe-Taylor and Cristianini, 2004).The second quantity that is often considered the output of a binary classifier is the output determined after it is passed through a non-linear function to constrain its value to a bounded range, for example in the interval [0, 1].A common non-linear function is the sigmoid function (Bishop, 2006).When the non-linearity results in well-calibrated probabilities (Gneiting and Raftery, 2007;Reid and Williamson, 2011), this is called class probability estimation.The third output of a binary classifier is the final binary decision {+1, −1}, which is the one most commonly assumed to be the output of the classifier.The SVM is a binary classifier that does not naturally lend itself to a probabilistic interpretation.There are several approaches for converting the raw output of the linear function (the score) into a calibrated class probability estimate (P (Y = 1|X = x)) that involve an additional calibration step (Platt, 2000;Zadrozny and Elkan, 2001;Lin et al., 2007).From the training perspective, there are many related probabilistic approaches.We mentioned at the end of Section 12.2.5 that there is a re-lationship between loss function and the likelihood (also compare Sections 8.2 and 8.3).The maximum likelihood approach corresponding to a well-calibrated transformation during training is called logistic regression, which comes from a class of methods called generalized linear models.Details of logistic regression from this point of view can be found in Agresti (2002, chapter 5) and McCullagh and Nelder (1989, chapter 4).Naturally, one could take a more Bayesian view of the classifier output by estimating a posterior distribution using Bayesian logistic regression.The Bayesian view also includes the specification of the prior, which includes design choices such as conjugacy (Section 6.6.1) with the likelihood.Additionally, one could consider latent functions as priors, which results in Gaussian process classification (Rasmussen and Williams, 2006, chapter 3).Draft (2024-01-15) of "Mathematics for Machine Learning".Feedback: https://mml-book.com.