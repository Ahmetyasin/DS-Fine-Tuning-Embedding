Analyzing time-oriented data and forecasting future values of a time series are among the most important problems that analysts face in many fields, ranging from finance and economics to managing production operations, to the analysis of political and social policy sessions, to investigating the impact of humans and the policy decisions that they make on the environment.Consequently, there is a large group of people in a variety of fields, including finance, economics, science, engineering, statistics, and public policy who need to understand some basic concepts of time series analysis and forecasting.Unfortunately, most basic statistics and operations management books give little if any attention to time-oriented data and little guidance on forecasting.There are some very good high level books on time series analysis.These books are mostly written for technical specialists who are taking a doctoral-level course or doing research in the field.They tend to be very theoretical and often focus on a few specific topics or techniques.We have written this book to fill the gap between these two extremes.We have made a number of changes in this revision of the book.New material has been added on data preparation for forecasting, including dealing with outliers and missing values, use of the variogram and sections on the spectrum, and an introduction to Bayesian methods in forecasting.We have added many new exercises and examples, including new data sets in Appendix B, and edited many sections of the text to improve the clarity of the presentation.Like the first edition, this book is intended for practitioners who make real-world forecasts.We have attempted to keep the mathematical level modest to encourage a variety of users for the book.Our focus is on shortto medium-term forecasting where statistical methods are useful.Since many organizations can improve their effectiveness and business results by making better short-to medium-term forecasts, this book should be useful to a wide variety of professionals.The book can also be used as a textbook for an applied forecasting and time series analysis course at the advanced undergraduate or first-year graduate level.Students in this course could come from engineering, business, statistics, operations research, mathematics, computer science, and any area of application where making forecasts is important.Readers need a background in basic statistics (previous exposure to linear regression would be helpful, but not essential), and some knowledge of matrix algebra, although matrices appear mostly in the chapter on regression, and if one is interested mainly in the results, the details involving matrix manipulation can be skipped.Integrals and derivatives appear in a few places in the book, but no detailed working knowledge of calculus is required.Successful time series analysis and forecasting requires that the analyst interact with computer software.The techniques and algorithms are just not suitable to manual calculations.We have chosen to demonstrate the techniques presented using three packages: Minitab ® , JMP ® , and R, and occasionally SAS ® .We have selected these packages because they are widely used in practice and because they have generally good capability for analyzing time series data and generating forecasts.Because R is increasingly popular in statistics courses, we have included a section in each chapter showing the R code necessary for working some of the examples in the chapter.We have also added a brief appendix on the use of R. The basic principles that underlie most of our presentation are not specific to any particular software package.Readers can use any software that they like or have available that has basic statistical forecasting capability.While the text examples do utilize these particular software packages and illustrate some of their features and capability, these features or similar ones are found in many other software packages.There are three basic approaches to generating forecasts: regressionbased methods, heuristic smoothing methods, and general time series models.Because all three of these basic approaches are useful, we give an introduction to all of them.Chapter 1 introduces the basic forecasting problem, defines terminology, and illustrates many of the common features of time series data.Chapter 2 contains many of the basic statistical tools used in analyzing time series data.Topics include plots, numerical PREFACE xiii summaries of time series data including the autocovariance and autocorrelation functions, transformations, differencing, and decomposing a time series into trend and seasonal components.We also introduce metrics for evaluating forecast errors and methods for evaluating and tracking forecasting performance over time.Chapter 3 discusses regression analysis and its use in forecasting.We discuss both crosssection and time series regression data, least squares and maximum likelihood model fitting, model adequacy checking, prediction intervals, and weighted and generalized least squares.The first part of this chapter covers many of the topics typically seen in an introductory treatment of regression, either in a stand-alone course or as part of another applied statistics course.It should be a reasonable review for many readers.Chapter 4 presents exponential smoothing techniques, both for time series with polynomial components and for seasonal data.We discuss and illustrate methods for selecting the smoothing constant(s), forecasting, and constructing prediction intervals.The explicit time series modeling approach to forecasting that we have chosen to emphasize is the autoregressive integrated moving average (ARIMA) model approach.Chapter 5 introduces ARIMA models and illustrates how to identify and fit these models for both nonseasonal and seasonal time series.Forecasting and prediction interval construction are also discussed and illustrated.Chapter 6 extends this discussion into transfer function models and intervention modeling and analysis.Chapter 7 surveys several other useful topics from time series analysis and forecasting, including multivariate time series problems, ARCH and GARCH models, and combinations of forecasts.We also give some practical advice for using statistical approaches to forecasting and provide some information about realistic expectations.The last two chapters of the book are somewhat higher in level than the first five.Each chapter has a set of exercises.Some of these exercises involve analyzing the data sets given in Appendix B. These data sets represent an interesting cross section of real time series data, typical of those encountered in practical forecasting problems.Most of these data sets are used in exercises in two or more chapters, an indication that there are usually several approaches to analyzing, modeling, and forecasting a time series.There are other good sources of data for practicing the techniques given in this book.Some of the ones that we have found very interesting and useful include the U.S. Department of Labor-Bureau of Labor Statistics (http:// www.bls.gov/data/home.htm),the U.S. Department of Agriculture-National Agricultural Statistics Service, Quick Stats Agricultural Statistics Data (http://www.nass.usda.gov/Data_and_Statistics/Quick_Stats/index.asp), the U.S. Census Bureau (http://www.census.gov),and the U.S.Forecasting is an important problem that spans many fields including business and industry, government, economics, environmental sciences, medicine, social science, politics, and finance.Forecasting problems are often classified as short-term, medium-term, and long-term.Short-term forecasting problems involve predicting events only a few time periods (days, weeks, and months) into the future.Medium-term forecasts extend from 1 to 2 years into the future, and long-term forecasting problems can extend beyond that by many years.Short-and medium-term forecasts are required for activities that range from operations management to budgeting and selecting new research and development projects.Long-term forecasts impact issues such as strategic planning.Short-and medium-term forecasting is typically based on identifying, modeling, and extrapolating the patterns found in historical data.Because these historical data usually exhibit inertia and do not change dramatically very quickly, statistical methods are very useful for short-and medium-term forecasting.This book is about the use of these statistical methods.Most forecasting problems involve the use of time series data.A time series is a time-oriented or chronological sequence of observations on a variable of interest.For example, Figure 1 series plot.The rate variable is collected at equally spaced time periods, as is typical in most time series and forecasting applications.Many business applications of forecasting utilize daily, weekly, monthly, quarterly, or annual data, but any reporting interval may be used.Furthermore, the data may be instantaneous, such as the viscosity of a chemical product at the point in time where it is measured; it may be cumulative, such as the total sales of a product during the month; or it may be a statistic that in some way reflects the activity of the variable during the time period, such as the daily closing price of a specific stock on the New York Stock Exchange.The reason that forecasting is so important is that prediction of future events is a critical input into many types of planning and decision-making processes, with application to areas such as the following:1. Operations Management.Business organizations routinely use forecasts of product sales or demand for services in order to schedule production, control inventories, manage the supply chain, determine staffing requirements, and plan capacity.Forecasts may also be used to determine the mix of products or services to be offered and the locations at which products are to be produced.2. Marketing.Forecasting is important in many marketing decisions.Forecasts of sales response to advertising expenditures, new promotions, or changes in pricing polices enable businesses to evaluate their effectiveness, determine whether goals are being met, and make adjustments.3. Finance and Risk Management.Investors in financial assets are interested in forecasting the returns from their investments.These assets include but are not limited to stocks, bonds, and commodities; other investment decisions can be made relative to forecasts of interest rates, options, and currency exchange rates.Financial risk management requires forecasts of the volatility of asset returns so that the risks associated with investment portfolios can be evaluated and insured, and so that financial derivatives can be properly priced.4. Economics.Governments, financial institutions, and policy organizations require forecasts of major economic variables, such as gross domestic product, population growth, unemployment, interest rates, inflation, job growth, production, and consumption.These forecasts are an integral part of the guidance behind monetary and fiscal policy, and budgeting plans and decisions made by governments.They are also instrumental in the strategic planning decisions made by business organizations and financial institutions.5. Industrial Process Control.Forecasts of the future values of critical quality characteristics of a production process can help determine when important controllable variables in the process should be changed, or if the process should be shut down and overhauled.Feedback and feedforward control schemes are widely used in monitoring and adjustment of industrial processes, and predictions of the process output are an integral part of these schemes.6. Demography.Forecasts of population by country and regions are made routinely, often stratified by variables such as gender, age, and race.Demographers also forecast births, deaths, and migration patterns of populations.Governments use these forecasts for planning policy and social service actions, such as spending on health care, retirement programs, and antipoverty programs.Many businesses use forecasts of populations by age groups to make strategic plans regarding developing new product lines or the types of services that will be offered.These are only a few of the many different situations where forecasts are required in order to make good decisions.Despite the wide range of problem situations that require forecasts, there are only two broad types of forecasting techniques-qualitative methods and quantitative methods.Qualitative forecasting techniques are often subjective in nature and require judgment on the part of experts.Qualitative forecasts are often used in situations where there is little or no historical data on which to base the forecast.An example would be the introduction of a new product, for which there is no relevant history.In this situation, the company might use the expert opinion of sales and marketing personnel to subjectively estimate product sales during the new product introduction phase of its life cycle.Sometimes qualitative forecasting methods make use of marketing tests, surveys of potential customers, and experience with the sales performance of other products (both their own and those of competitors).However, although some data analysis may be performed, the basis of the forecast is subjective judgment.Perhaps the most formal and widely known qualitative forecasting technique is the Delphi Method.This technique was developed by the RAND Corporation (see Dalkey [1967]).It employs a panel of experts who are assumed to be knowledgeable about the problem.The panel members are physically separated to avoid their deliberations being impacted either by social pressures or by a single dominant individual.Each panel member responds to a questionnaire containing a series of questions and returns the information to a coordinator.Following the first questionnaire, subsequent questions are submitted to the panelists along with information about the opinions of the panel as a group.This allows panelists to review their predictions relative to the opinions of the entire group.After several rounds, it is hoped that the opinions of the panelists converge to a consensus, although achieving a consensus is not required and justified differences of opinion can be included in the outcome.Qualitative forecasting methods are not emphasized in this book.Quantitative forecasting techniques make formal use of historical data and a forecasting model.The model formally summarizes patterns in the data and expresses a statistical relationship between previous and current values of the variable.Then the model is used to project the patterns in the data into the future.In other words, the forecasting model is used to extrapolate past and current behavior into the future.There are several types of forecasting models in general use.The three most widely used are regression models, smoothing models, and general time series models.Regression models make use of relationships between the variable of interest and one or more related predictor variables.Sometimes regression models are called causal forecasting models, because the predictor variables are assumed to describe the forces that cause or drive the observed values of the variable of interest.An example would be using data on house purchases as a predictor variable to forecast furniture sales.The method of least squares is the formal basis of most regression models.Smoothing models typically employ a simple function of previous observations to provide a forecast of the variable of interest.These methods may have a formal statistical basis, but they are often used and justified heuristically on the basis that they are easy to use and produce satisfactory results.General time series models employ the statistical properties of the historical data to specify a formal model and then estimate the unknown parameters of this model (usually) by least squares.In subsequent chapters, we will discuss all three types of quantitative forecasting models.The form of the forecast can be important.We typically think of a forecast as a single number that represents our best estimate of the future value of the variable of interest.Statisticians would call this a point estimate or point forecast.Now these forecasts are almost always wrong; that is, we experience forecast error.Consequently, it is usually a good practice to accompany a forecast with an estimate of how large a forecast error might be experienced.One way to do this is to provide a prediction interval (PI) to accompany the point forecast.The PI is a range of values for the future observation, and it is likely to prove far more useful in decision-making than a single number.We will show how to obtain PIs for most of the forecasting methods discussed in the book.Other important features of the forecasting problem are the forecast horizon and the forecast interval.The forecast horizon is the number of future periods for which forecasts must be produced.The horizon is often dictated by the nature of the problem.For example, in production planning, forecasts of product demand may be made on a monthly basis.Because of the time required to change or modify a production schedule, ensure that sufficient raw material and component parts are available from the supply chain, and plan the delivery of completed goods to customers or inventory facilities, it would be necessary to forecast up to 3 months ahead.The forecast horizon is also often called the forecast lead time.The forecast interval is the frequency with which new forecasts are prepared.For example, in production planning, we might forecast demand on a monthly basis, for up to 3 months in the future (the lead time or horizon), and prepare a new forecast each month.Thus the forecast interval is 1 month, the same as the basic period of time for which each forecast is made.If the forecast lead time is always the same length, say, T periods, and the forecast is revised each time period, then we are employing a rolling or moving horizon forecasting approach.This system updates or revises the forecasts for T−1 of the periods in the horizon and computes a forecast for the newest period T.This rolling horizon approach to forecasting is widely used when the lead time is several periods long.Time series plots can reveal patterns such as random, trends, level shifts, periods or cycles, unusual observations, or a combination of patterns.Patterns commonly found in time series data are discussed next with examples of situations that drive the patterns.The sales of a mature pharmaceutical product may remain relatively flat in the absence of unchanged marketing or manufacturing strategies.Weekly sales of a generic pharmaceutical product shown in Figure 1.2 appear to be constant over time, at about 10,400 × 10 3 units, in a random sequence with no obvious patterns (data in Appendix B, Table B.2).To assure conformance with customer requirements and product specifications, the production of chemicals is monitored by many characteristics.These may be input variables such as temperature and flow rate, and output properties such as viscosity and purity.Due to the continuous nature of chemical manufacturing processes, output properties often are positively autocorrelated; that is, a value above the long-run average tends to be followed by other values above the average, while a value below the average tends to be followed by other values below the average.The viscosity readings plotted in Figure 1.3 exhibit autocorrelated behavior, tending to a long-run average of about 85 centipoises (cP), but with a structured, not completely random, appearance (data in Appendix B, Table B.3).Some methods for describing and analyzing autocorrelated data will be described in Chapter 2.The USDA National Agricultural Statistics Service publishes agricultural statistics for many commodities, including the annual production of dairy products such as butter, cheese, ice cream, milk, yogurt, and whey.These statistics are used for market analysis and intelligence, economic indicators, and identification of emerging issues.Blue and gorgonzola cheese is one of 32 categories of cheese for which data are published.The annual US production of blue and gorgonzola cheeses (in 10 3 lb) is shown in Figure 1.4 (data in Appendix B, Table B.4). Production quadrupled from 1950 to 1997, and the linear trend has a constant positive slope with random, year-to-year variation.The US Census Bureau publishes historic statistics on manufacturers' shipments, inventories, and orders.The statistics are based on North American Industry Classification System (NAICS) code and are utilized for purposes such as measuring productivity and analyzing relationships between employment and manufacturing output.The manufacture of beverage and tobacco products is reported as part of the nondurable subsector.The plot of monthly beverage product shipments (Figure 1.5) reveals an overall increasing trend, with a distinct cyclic pattern that is repeated within each year.January shipments appear to be the lowest, with highs in May and June (data in Appendix B, Table B.5).This monthly, or seasonal, variation may be attributable to some cause  such as the impact of weather on the demand for beverages.Techniques for making seasonal adjustments to data in order to better understand general trends will be discussed in Chapter 2.To determine whether the Earth is warming or cooling, scientists look at annual mean temperatures.At a single station, the warmest and the coolest temperatures in a day are averaged.Averages are then calculated at stations all over the Earth, over an entire year.The change in global annual mean surface air temperature is calculated from a base established from 1951 to 1980, and the result is reported as an "anomaly."The plot of the annual mean anomaly in global surface air temperature (Figure 1.6) shows an increasing trend since 1880; however, the slope, or rate of change, varies with time periods (data in Appendix B, Table B.6).While the slope in earlier time periods appears to be constant, slightly increasing, or slightly decreasing, the slope from about 1975 to the present appears much steeper than the rest of the plot.Business data such as stock prices and interest rates often exhibit nonstationary behavior; that is, the time series has no natural mean.The daily closing price adjusted for stock splits of Whole Foods Market (WFMI) stock in 2001 (Figure 1.7) exhibits a combination of patterns for both mean level and slope (data in Appendix B,Table B.7).While the price is constant in some short time periods, there is no consistent mean level over time.In other time periods, the price changes at different rates, including occasional abrupt shifts in level.This is an example of nonstationary behavior, which will be discussed in Chapter 2.The Current Population Survey (CPS) or "household survey" prepared by the US Department of Labor, Bureau of Labor Statistics, contains national data on employment, unemployment, earnings, and other labor market topics by demographic characteristics.The data are used to report on the employment situation, for projections with impact on hiring and training, and for a multitude of other business planning activities.The data are reported unadjusted and with seasonal adjustment to remove the effect of regular patterns that occur each year.The plot of monthly unadjusted unemployment rates (Figure 1.8) exhibits a mixture of patterns, similar to Figure 1.5 (data in Appendix B, Table B.8).There is a distinct cyclic pattern within a year; January, February, and March generally have the highest unemployment rates.The overall level is also changing, from a gradual decrease, to a steep increase, followed by a gradual decrease.The use of seasonal adjustments as described in Chapter 2 makes it easier to observe the nonseasonal movements in time series data.Solar activity has long been recognized as a significant source of noise impacting consumer and military communications, including satellites, cell phone towers, and electric power grids.The ability to accurately forecast solar activity is critical to a variety of fields.The International Sunspot Number R is the oldest solar activity index.The number incorporates both the number of observed sunspots and the number of observed sunspot groups.In Figure 1.9, the plot of annual sunspot numbers reveals cyclic patterns of varying magnitudes (data in Appendix B, Table B.9).In addition to assisting in the identification of steady-state patterns, time series plots may also draw attention to the occurrence of atypical events.Weekly sales of a generic pharmaceutical product dropped due to limited availability resulting from a fire at one of the four production facilities.The 5-week reduction is apparent in the time series plot of weekly sales shown in Figure 1.10.Another type of unusual event may be the failure of the data measurement or collection system.After recording a vastly different viscosity reading at time period 70 (Figure 1.11), the measurement system was checked with a standard and determined to be out of calibration.The cause was determined to be a malfunctioning sensor.A process is a series of connected activities that transform one or more inputs into one or more outputs.All work activities are performed in processes, and forecasting is no exception.The activities in the forecasting process are: Problem definition involves developing understanding of how the forecast will be used along with the expectations of the "customer" (the user of the forecast).Questions that must be addressed during this phase include the desired form of the forecast (e.g., are monthly forecasts required), the forecast horizon or lead time, how often the forecasts need to be revised (the forecast interval), and what level of forecast accuracy is required in order to make good business decisions.This is also an opportunity to introduce the decision makers to the use of prediction intervals as a measure of the risk associated with forecasts, if they are unfamiliar with this approach.Often it is necessary to go deeply into many aspects of the business system that requires the forecast to properly define the forecasting component of the entire problem.For example, in designing a forecasting system for inventory control, information may be required on issues such as product shelf life or other aging considerations, the time required to manufacture or otherwise obtain the products (production lead time), and the economic consequences of having too many or too few units of product available to meet customer demand.When multiple products are involved, the level of aggregation of the forecast (e.g., do we forecast individual products or families consisting of several similar products) can be an important consideration.Much of the ultimate success of the forecasting model in meeting the customer expectations is determined in the problem definition phase.Data collection consists of obtaining the relevant history for the variable(s) that are to be forecast, including historical information on potential predictor variables.The key here is "relevant"; often information collection and storage methods and systems change over time and not all historical data are useful for the current problem.Often it is necessary to deal with missing values of some variables, potential outliers, or other data-related problems that have occurred in the past.During this phase, it is also useful to begin planning how the data collection and storage issues in the future will be handled so that the reliability and integrity of the data will be preserved.Data analysis is an important preliminary step to the selection of the forecasting model to be used.Time series plots of the data should be constructed and visually inspected for recognizable patterns, such as trends and seasonal or other cyclical components.A trend is evolutionary movement, either upward or downward, in the value of the variable.Trends may be long-term or more dynamic and of relatively short duration.Seasonality is the component of time series behavior that repeats on a regular basis, such as each year.Sometimes we will smooth the data to make identification of the patterns more obvious (data smoothing will be discussed in Chapter 2).Numerical summaries of the data, such as the sample mean, standard deviation, percentiles, and autocorrelations, should also be computed and evaluated.Chapter 2 will provide the necessary background to do this.If potential predictor variables are available, scatter plots of each pair of variables should be examined.Unusual data points or potential outliers should be identified and flagged for possible further study.The purpose of this preliminary data analysis is to obtain some "feel" for the data, and a sense of how strong the underlying patterns such as trend and seasonality are.This information will usually suggest the initial types of quantitative forecasting methods and models to explore.Model selection and fitting consists of choosing one or more forecasting models and fitting the model to the data.By fitting, we mean estimating the unknown model parameters, usually by the method of least squares.In subsequent chapters, we will present several types of time series models and discuss the procedures of model fitting.We will also discuss methods for evaluating the quality of the model fit, and determining if any of the underlying assumptions have been violated.This will be useful in discriminating between different candidate models.Model validation consists of an evaluation of the forecasting model to determine how it is likely to perform in the intended application.This must go beyond just evaluating the "fit" of the model to the historical data and must examine what magnitude of forecast errors will be experienced when the model is used to forecast "fresh" or new data.The fitting errors will always be smaller than the forecast errors, and this is an important concept that we will emphasize in this book.A widely used method for validating a forecasting model before it is turned over to the customer is to employ some form of data splitting, where the data are divided into two segments-a fitting segment and a forecasting segment.The model is fit to only the fitting data segment, and then forecasts from that model are simulated for the observations in the forecasting segment.This can provide useful guidance on how the forecasting model will perform when exposed to new data and can be a valuable approach for discriminating between competing forecasting models.Forecasting model deployment involves getting the model and the resulting forecasts in use by the customer.It is important to ensure that the customer understands how to use the model and that generating timely forecasts from the model becomes as routine as possible.Model maintenance, including making sure that data sources and other required information will continue to be available to the customer is also an important issue that impacts the timeliness and ultimate usefulness of forecasts.Monitoring forecasting model performance should be an ongoing activity after the model has been deployed to ensure that it is still performing satisfactorily.It is the nature of forecasting that conditions change over time, and a model that performed well in the past may deteriorate in performance.Usually performance deterioration will result in larger or more systematic forecast errors.Therefore monitoring of forecast errors is an essential part of good forecasting system design.Control charts of forecast errors are a simple but effective way to routinely monitor the performance of a forecasting model.We will illustrate approaches to monitoring forecast errors in subsequent chapters.Developing time series models and using them for forecasting requires data on the variables of interest to decision-makers.The data are the raw materials for the modeling and forecasting process.The terms data and information are often used interchangeably, but we prefer to use the term data as that seems to reflect a more raw or original form, whereas we think of information as something that is extracted or synthesized from data.The output of a forecasting system could be thought of as information, and that output uses data as an input.In most modern organizations data regarding sales, transactions, company financial and business performance, supplier performance, and customer activity and relations are stored in a repository known as a data warehouse.Sometimes this is a single data storage system; but as the volume of data handled by modern organizations grows rapidly, the data warehouse has become an integrated system comprised of components that are physically and often geographically distributed, such as cloud data storage.The data warehouse must be able to organize, manipulate, and integrate data from multiple sources and different organizational information systems.The basic functionality required includes data extraction, data transformation, and data loading.Data extraction refers to obtaining data from internal sources and from external sources such as third party vendors or government entities and financial service organizations.Once the data are extracted, the transformation stage involves applying rules to prevent duplication of records and dealing with problems such as missing information.Sometimes we refer to the transformation activities as data cleaning.We will discuss some of the important data cleaning operations subsequently.Finally, the data are loaded into the data warehouse where they are available for modeling and analysis.Data quality has several dimensions.Five important ones that have been described in the literature are accuracy, timeliness, completeness, representativeness, and consistency.Accuracy is probably the oldest dimension of data quality and refers to how close that data conform to its "real" values.Real values are alternative sources that can be used for verification purposes.For example, do sales records match payments to accounts receivable records (although the financial records may occur in later time periods because of payment terms and conditions, discounts, etc.)?Timeliness means that the data are as current as possible.Infrequent updating of data can seriously impact developing a time series model that is going to be used for relatively short-term forecasting.In many time series model applications the time between the occurrence of the real-world event and its entry into the data warehouse must be as short as possible to facilitate model development and use.Completeness means that the data content is complete, with no missing data and no outliers.As an example of representativeness, suppose that the end use of the time series model is to forecast customer demand for a product or service, but the organization only records booked orders and the date of fulfillment.This may not accurately reflect demand, because the orders can be booked before the desired delivery period and the date of fulfillment can take place in a different period than the one required by the customer.Furthermore, orders that are lost because of product unavailability or unsatisfactory delivery performance are not recorded.In these situations demand can differ dramatically from sales.Data cleaning methods can often be used to deal with some problems of completeness.Consistency refers to how closely data records agree over time in format, content, meaning, and structure.In many organizations how data are collected and stored evolves over time; definitions change and even the types of data that are collected change.For example, consider monthly data.Some organizations define "months" that coincide with the traditional calendar definition.But because months have different numbers of days that can induce patterns in monthly data, some organizations prefer to define a year as consisting of 13 "months" each consisting of 4 weeks.It has been suggested that the output data that reside in the data warehouse are similar to the output of a manufacturing process, where the raw data are the input.Just as in manufacturing and other service processes, the data production process can benefit by the application of quality management and control tools.Jones-Farmer et al. (2014) describe how statistical quality control methods, specifically control charts, can be used to enhance data quality in the data production process.Data cleaning is the process of examining data to detect potential errors, missing data, outliers or unusual values, or other inconsistencies and then correcting the errors or problems that are found.Sometimes errors are the result of recording or transmission problems, and can be corrected by working with the original data source to correct the problem.Effective data cleaning can greatly improve the forecasting process.Before data are used to develop a time series model, it should be subjected to several different kinds of checks, including but not necessarily limited to the following:1. Is there missing data? 2. Does the data fall within an expected range?3. Are there potential outliers or other unusual values?These types of checks can be automated fairly easily.If this aspect of data cleaning is automated, the rules employed should be periodically evaluated to ensure that they are still appropriate and that changes in the data have not made some of the procedures less effective.However, it is also extremely useful to use graphical displays to assist in identifying unusual data.Techniques such as time series plots, histograms, and scatter diagrams are extremely useful.These and other graphical methods will be described in Chapter 2.Data imputation is the process of correcting missing data or replacing outliers with an estimation process.Imputation replaces missing or erroneous values with a "likely" value based on other available information.This enables the analysis to work with statistical techniques which are designed to handle the complete data sets.Mean value imputation consists of replacing a missing value with the sample average calculated from the nonmissing observations.The big advantage of this method is that it is easy, and if the data does not have any specific trend or seasonal pattern, it leaves the sample mean of the complete data set unchanged.However, one must be careful if there are trends or seasonal patterns, because the sample mean of all of the data may not reflect these patterns.A variation of this is stochastic mean value imputation, in which a random variable is added to the mean value to capture some of the noise or variability in the data.The random variable could be assumed to follow a normal distribution with mean zero and standard deviation equal to the standard deviation of the actual observed data.A variation of mean value imputation is to use a subset of the available historical data that reflects any trend or seasonal patterns in the data.For example, consider the time series y 1 , y 2 , … , y T and suppose that one observation y j is missing.We can impute the missing value aswhere k would be based on the seasonal variability in the data.It is usually chosen as some multiple of the smallest seasonal cycle in the data.So, if the data are monthly and exhibit a monthly cycle, k would be a multiple of 12. Regression imputation is a variation of mean value imputation where the imputed value is computed from a model used to predict the missing value.The prediction model does not have to be a linear regression model.For example, it could be a time series model.Hot deck imputation is an old technique that is also known as the last value carried forward method.The term "hot deck" comes from the use of computer punch cards.The deck of cards was "hot" because it was currently in use.Cold deck imputation uses information from a deck of cards not currently in use.In hot deck imputation, the missing values are imputed by using values from similar complete observations.If there are several variables, sort the data by the variables that are most related to the missing observation and then, starting at the top, replace the missing values with the value of the immediately preceding variable.There are many variants of this procedure.There are a variety of good resources that can be helpful to technical professionals involved in developing forecasting models and preparing forecasts.There are three professional journals devoted to forecasting: r Journal of Forecasting r International Journal of Forecasting r Journal of Business Forecasting Methods and Systems These journals publish a mixture of new methodology, studies devoted to the evaluation of current methods for forecasting, and case studies and applications.In addition to these specialized forecasting journals, there are several other mainstream statistics and operations research/management science journals that publish papers on forecasting, including: There are several books that are good complements to this one.We recommend Box, Jenkins, and Reinsel (1994);Chatfield (1996);Fuller (1995); Abraham and Ledolter (1983);Montgomery, Johnson, and Gardiner (1990);Wei (2006);andBrockwell andDavis (1991, 2002).Some of these books are more specialized than this one, in that they focus on a specific type of forecasting model such as the autoregressive integrated moving average [ARIMA] model, and some also require more background in statistics and mathematics.Many statistics software packages have very good capability for fitting a variety of forecasting models.Minitab ® Statistical Software, JMP ® , the Statistical Analysis System (SAS) and R are the packages that we utilize and illustrate in this book.At the end of most chapters we provide R code for working some of the examples in the chapter.Matlab and S-Plus are also two packages that have excellent capability for solving forecasting problems.1.1 Why is forecasting an essential part of the operation of any organization or business?1.2 What is a time series?Explain the meaning of trend effects, seasonal variations, and random error.Explain the difference between a point forecast and an interval forecast.What do we mean by a causal forecasting technique?total demand for a product during period t, or an instantaneous quantity, such as the daily closing price of a specific stock on the New York Stock Exchange.Generally, we will need to distinguish between a forecast or predicted value of y t that was made at some previous time period, say, t − 𝜏, and a fitted value of y t that has resulted from estimating the parameters in a time series model to historical data.Note that 𝜏 is the forecast lead time.The forecast made at time period t − 𝜏 is denoted by ŷt (t − 𝜏).There is a lot of interest in the lead − 1 forecast, which is the forecast of the observation in period t, y t , made one period prior, ŷt (t − 1).We will denote the fitted value of y t by ŷt .We will also be interested in analyzing forecast errors.The forecast error that results from a forecast of y t that was made at time period t − 𝜏 is the lead − 𝝉 forecast error e t (𝜏) = y t − ŷt (t − 𝜏).(2.1)For example, the lead − 1 forecast error isThe difference between the observation y t and the value obtained by fitting a time series model to the data, or a fitted value ŷt defined earlier, is called a residual, and is denoted byThe reason for this careful distinction between forecast errors and residuals is that models usually fit historical data better than they forecast.That is, the residuals from a model-fitting process will almost always be smaller than the forecast errors that are experienced when that model is used to forecast future observations.Developing a forecasting model should always begin with graphical display and analysis of the available data.Many of the broad general features of a time series can be seen visually.This is not to say that analytical tools are not useful, because they are, but the human eye can be a very sophisticated data analysis tool.To paraphrase the great New York Yankees catcher Yogi Berra, "You can observe a lot just by watching."The basic graphical display for time series data is the time series plot, illustrated in Chapter 1.This is just a graph of y t versus the time period, t, for t = 1, 2, … , T. Features such as trend and seasonality are usually easy to see from the time series plot.It is interesting to observe that some of the classical tools of descriptive statistics, such as the histogram and the stem-and-leaf display, are not particularly useful for time series data because they do not take time order into account.Example 2.1 Figures 2.1 and 2.2 show time series plots for viscosity readings and beverage production shipments (originally shown in Figures 1.3 and 1.5,respectively).At the right-hand side of each time series plot is a histogram of the data.Note that while the two time series display very different characteristics, the histograms are remarkably similar.Essentially, the histogram summarizes the data across the time dimension, and in so doing, the key time-dependent features of the data are lost.Stem-andleaf plots and boxplots would have the same issues, losing time-dependent features.J a n -1 9 9 3 J a n -1 9 9 4 J a n -1 9 9 5 J a n -1 9 9 6 J a n -1 9 9 7 J a n -1 9 9 8 J a n -1 9 9 9 J a n -2 0 0 0 J a n -2 0 0 1 J a n -2 0 0 2 J a n -2 0 0 3 J a n -2 0 0 4 J a n -2 0 0 5 J a n -2 0 0 6 D e c -2 0 0 6 low concentrations of CO 2 are usually accompanied by negative anomalies, and higher concentrations of CO 2 tend to be accompanied by positive anomalies.Note that this does not imply that higher concentrations of CO 2 actually cause higher temperatures.The scatter plot cannot establish a causal relationship between two variables (neither can naive statistical modeling techniques, such as regression), but it is useful in displaying how the variables have varied together in the historical data set.There are many variations of the time series plot and other graphical displays that can be constructed to show specific features of a time series.For example, Figure 2.4 displays daily price information for Whole Foods Market stock during the first quarter of 2001 (the trading days from January 2, 2001 through March 30, 2001).This chart, created in Excel ® , shows the opening, closing, highest, and lowest prices experienced within a trading day for the first quarter.If the opening price was higher than the closing price, the box is filled, whereas if the closing price was higher than the opening price, the box is open.This type of plot is potentially more useful than a time series plot of just the closing (or opening) prices, because it shows the volatility of the stock within a trading day.The volatility of an asset is often of interest to investors because it is a measure of the inherent risk associated with the asset. 1 / 2 / 2 0 0 1 1 / 9 / 2 0 0 1 1 / 1 6 / 2 0 0 1 1 / 2 3 / 2 0 0 1 1 / 3 0 / 2 0 0 1 2 / 6 / 2 0 0 1 2 / 1 3 / 2 0 0 1 2 / 2 0 / 2 0 0 1 2 / 2 7 / 2 0 0 1 3 / 6 / 2 0 0 1 3 / 1 3 / 2 0 0 1 3 / 2 0 / 2 0 0 1 3 / 2 7 / 2 0 0 1 Date Price, $/Share FIGURE 2.4 Open-high/close-low chart of Whole Foods Market stock price.Source: finance.yahoo.com.Sometimes it is useful to overlay a smoothed version of the original data on the original time series plot to help reveal patterns in the original data.There are several types of data smoothers that can be employed.One of the simplest and most widely used is the ordinary or simple moving average.A simple moving average of span N assigns weights 1/N to the most recent N observations y T , y T−1 , … , y T−N+1 , and weight zero to all other observations.If we let M T be the moving average, then the N-span moving average at time period T isClearly, as each new observation becomes available it is added into the sum from which the moving average is computed and the oldest observation is discarded.The moving average has less variability than the original observations; in fact, if the variance of an individual observation y t is 𝜎 2 , then assuming that the observations are uncorrelated the variance of the moving average isSometimes a "centered" version of the moving average is used, such as inwhere the span of the centered moving average isExample 2.2 Figure 2.5 plots the annual global mean surface air temperature anomaly data along with a five-period (a period is 1 year) moving average of the same data.Note that the moving average exhibits less variability than found in the original series.It also makes some features of the data easier to see; for example, it is now more obvious that the global air temperature decreased from about 1940 until about 1975.Plots of moving averages are also used by analysts to evaluate stock price trends; common MA periods are 5, 10, 20, 50, 100, and 200  average is shown in Figure 2.6.The moving average plot smoothes the day-to-day noise and shows a generally increasing trend.The simple moving average is a linear data smoother, or a linear filter, because it replaces each observation y t with a linear combination of the other data points that are near to it in time.The weights in the linear combination are equal, so the linear combination here is an average.Of course, unequal weights could be used.For example, the Hanning filter is a weighted, centered moving averageJulius von Hann, a nineteenth century Austrian meteorologist, used this filter to smooth weather data.An obvious disadvantage of a linear filter such as a moving average is that an unusual or erroneous data point or an outlier will dominate the moving averages that contain that observation, contaminating the moving averages for a length of time equal to the span of the filter.For example, consider the sequence of observations 15,18,13,12,16,14,16,17,18,15,18,200,19,14,21,24,19,25 which increases reasonably steadily from 15 to 25, except for the unusual value 200.Any reasonable smoothed version of the data should also increase steadily from 15 to 25 and not emphasize the value 200.Now even if the value 200 is a legitimate observation, and not the result of a data recording or reporting error (perhaps it should be 20!), it is so unusual that it deserves special attention and should likely not be analyzed along with the rest of the data.Odd-span moving medians (also called running medians) are an alternative to moving averages that are effective data smoothers when the time series may be contaminated with unusual values or outliers.The moving median of span N is defined aswhere N = 2u + 1.The median is the middle observation in rank order (or order of value).The moving median of span 3 is a very popular and effective data smoother, whereThis smoother would process the data three values at a time, and replace the three original observations by their median.If we apply this smoother to the data above, we obtain , 15, 13, 13, 14, 16, 17, 17, 18, 18, 19, 19, 19, 21, 21, 24, .This smoothed data are a reasonable representation of the original data, but they conveniently ignore the value 200.The end values are lost when using the moving median, and they are represented by " ".In general, a moving median will pass monotone sequences of data unchanged.It will follow a step function in the data, but it will eliminate a spike or more persistent upset in the data that has duration of at most u consecutive observations.Moving medians can be applied more than once if desired to obtain an even smoother series of observations.For example, applying the moving median of span 3 to the smoothed data above results in , , 13, 13, 14, 16, 17, 17, 18, 18, 19, 19, 19, 21, 21, , .These data are now as smooth as it can get; that is, repeated application of the moving median will not change the data, apart from the end values.If there are a lot of observations, the information loss from the missing end values is not serious.However, if it is necessary or desirable to keep the lengths of the original and smoothed data sets the same, a simple way to do this is to "copy on" or add back the end values from the original data.This would result in the smoothed data: 15,18,13,13,14,16,17,17,18,18,19,19,19,21,21,19,25 There are also methods for smoothing the end values.Tukey (1979) is a basic reference on this subject and contains many other clever and useful techniques for data analysis.The chemical process viscosity readings shown in Figure 1.11 are an example of a time series that benefits from smoothing to evaluate patterns.The selection of a moving median over a moving average, as shown in Figure 2.7, minimizes the impact of the invalid measurements, such as the one at time period 70.A very important type of time series is a stationary time series.A time series is said to be strictly stationary if its properties are not affected by a change in the time origin.That is, if the joint probability distribution of the observations y t , y t+1 , … , y t+n is exactly the same as the joint probability distribution of the observations y t+k , y t+k+1 , … , y t+k+n then the time series is strictly stationary.When n = 0 the stationarity assumption means that the probability distribution of y t is the same for all time periods Foods Market stock price data in Figure 1.7 tends to wander around or drift, with no obvious fixed level.This is behavior typical of a nonstationary time series.Stationary implies a type of statistical equilibrium or stability in the data.Consequently, the time series has a constant mean defined in the usual way asand constant variance defined as(2.7)The sample mean and sample variance are used to estimate these parameters.If the observations in the time series are y 1 , y 2 , … , y T , then the sample mean isand the sample variance is(2.9)Note that the divisor in Eq. (2.9) is T rather than the more familiar T − 1.This is the common convention in many time series applications, and because T is usually not small, there will be little difference between using T instead of T − 1.If a time series is stationary this means that the joint probability distribution of any two observations, say, y t and y t+k , is the same for any two time periods t and t + k that are separated by the same interval k.Useful information about this joint distribution, and hence about the nature of the time series, can be obtained by plotting a scatter diagram of all of the data pairs y t , y t+k that are separated by the same interval k.The interval k is called the lag.Example 2.4 Figure 2.10 is a scatter diagram for the pharmaceutical product sales for lag k = 1 and Figure 2.11 is a scatter diagram for the chemical viscosity readings for lag k = 1.Both scatter diagrams were constructed by plotting y t+1 versus y t .Figure 2.10 exhibits little structure; the plotted pairs of adjacent observations y t , y t+1 seem to be uncorrelated.That is, the value of y in the current period does not provide any useful information about the value of y that will be observed in the next period.A different story is revealed in Figure 2.11, where we observe that the pairs of adjacent observations y t+1 , y t are positively correlated.That is, a small value of y tends to be followed in the next time period by another small value of y, and a large value of y tends to be followed immediately by another large value of y.Note from inspection of Figures 2.10 and 2.11 that the behavior inferred from inspection of the scatter diagrams is reflected in the observed time series.The covariance between y t and its value at another time period, say, y t+k is called the autocovariance at lag k, defined by(2.10)The collection of the values of 𝛾 k , k = 0, 1, 2, … is called the autocovariance function.Note that the autocovariance at lag k = 0 is just the variance of the time series; that is, 𝛾 0 = 𝜎 2 y ,which is constant for a stationary time series.The autocorrelation coefficient at lag k for a stationary time series isThe collection of the values of 𝜌 k , k = 0, 1, 2, … is called the autocorrelation function (ACF).Note that by definition 𝜌 0 = 1.Also, the ACF is independent of the scale of measurement of the time series, so it is a dimensionless quantity.Furthermore, 𝜌 k = 𝜌 −k ; that is, the ACF is symmetric around zero, so it is only necessary to compute the positive (or negative) half.If a time series has a finite mean and autocovariance function it is said to be second-order stationary (or weakly stationary of order 2).If, in addition, the joint probability distribution of the observations at all times is multivariate normal, then that would be sufficient to result in a time series that is strictly stationary.It is necessary to estimate the autocovariance and ACFs from a time series of finite length, say, y 1 , y 2 , … , y T .The usual estimate of the autocovariance function isand the ACF is estimated by the sample autocorrelation function (or sample ACF)A good general rule of thumb is that at least 50 observations are required to give a reliable estimate of the ACF, and the individual sample autocorrelations should be calculated up to lag K, where K is about T/4.Often we will need to determine if the autocorrelation coefficient at a particular lag is zero.This can be done by comparing the sample autocorrelation coefficient at lag k, r k , to its standard error.If we make the assumption that the observations are uncorrelated, that is, 𝜌 k = 0 for all k, then the variance of the sample autocorrelation coefficient isand the standard error is  Note the rate of decrease or decay in ACF values in Figure 2.12 from 0.78 to 0, followed by a sinusoidal pattern about 0. This ACF pattern is typical of stationary time series.The importance of ACF estimates exceeding the 5% significance limits will be discussed in Chapter 5.In contrast, the plot of sample ACFs for a time series of random values with constant mean has a much different appearance.The sample ACFs for pharmaceutical product sales plotted in Figure 2.14 appear randomly positive or negative, with values near zero.While the ACF is strictly speaking defined only for a stationary time series, the sample ACF can be computed for any time series, so a logical question is: What does the sample ACF of a nonstationary time series look like?Consider the daily closing price for Whole Foods Market stock in Figure 1.7.The sample ACF of this time series is shown in Figure 2.15.Note that this sample ACF plot behaves quite differently than the ACF plots in Figures 2.12 and 2.14.Instead of cutting off or tailing off near zero after a few lags, this sample ACF is very persistent; that is, it decays very slowly and exhibits sample autocorrelations that are still rather large even at long lags.This behavior is characteristic of a nonstationary time series.Generally, if the sample ACF does not dampen out within about 15 to 20 lags, the time series is nonstationary.We have discussed two techniques for determining if a time series is nonstationary, plotting a reasonable long series of the data to see if it drifts or wanders away from its mean for long periods of time, and computing the sample ACF.However, often in practice there is no clear demarcation between a stationary and a nonstationary process for many real-world time series.An additional diagnostic tool that is very useful is the variogram.Suppose that the time series observations are represented by y t .The variogram G k measures variances of the differences between observations that are k lags apart, relative to the variance of the differences that are one time unit apart (or at lag 1).The variogram is defined mathematically asand the values of G k are plotted as a function of the lag k.If the time series is stationary, it turns out thatbut for a stationary time series 𝜌 k → 0 as k increases, so when the variogram is plotted against lag k, G k will reach an asymptote 1∕(1 − 𝜌 1 ).However, if the time series is nonstationary, G k will increase monotonically.Estimating the variogram is accomplished by simply applying the usual sample variance to the differences, taking care to account for the changing sample sizes when the differences are taken (see Haslett (1997)).LetThen an estimate of Var (y t+k − y t ) isTherefore the sample variogram is given by ĜkTo illustrate the use of the variogram, consider the chemical process viscosity data plotted in Figure 2.9.Both the data plot and the sample ACF in Start by computing the successive differences of the time series for a number of lags and then find their sample variances.The ratios of these sample variances to the sample variance of the first differences will produce the sample variogram.The JMP calculations of the sample variogram are shown in Figure 2.16 and a plot is given in Figure 2.17.Notice that the sample variogram generally converges to a stable level and then fluctuates around it.This is consistent with a stationary time series, and it provides additional evidence that the chemical process viscosity data are stationary.Notice that the sample variogram in Figure 2.19 increases monotonically for all 25 lags.This is a strong indication that the time series is nonstationary.Data transformations are useful in many aspects of statistical work, often for stabilizing the variance of the data.Nonconstant variance is quite common in time series data.For example, the International Sunspot Numbers plotted in Figure 2.20a show cyclic patterns of varying magnitudes.The variability from about 1800 to 1830 is smaller than that from about 1830 to 1880; other small periods of constant, but different, variances can also be identified.A very popular type of data transformation to deal with nonconstant variance is the power family of transformations, given by where ̇y = exp[(1∕T)∑ T t=1 ln y t ] is the geometric mean of the observations.If 𝜆 = 1, there is no transformation.Typical values of 𝜆 used with time series data are 𝜆 = 0.5 (a square root transformation), 𝜆 = 0 (the log transformation), 𝜆 = −0.5 (reciprocal square root transformation), and 𝜆 = −1 (inverse transformation).The divisor ̇y𝜆−1 is simply a scale factor that ensures that when different models are fit to investigate the utility of different transformations (values of 𝜆), the residual sum of squares for these models can be meaningfully compared.The reason that 𝜆 = 0 implies a log transformation is that (y 𝜆 − 1)∕𝜆 approaches the log of y as 𝜆 approaches zero.Often an appropriate value of 𝜆 is chosen empirically by fitting a model to y (𝜆) for various values of 𝜆 and then selecting the transformation that produces the minimum residual sum of squares.The log transformation is used frequently in situations where the variability in the original time series increases with the average level of the series.When the standard deviation of the original series increases linearly with the mean, the log transformation is in fact an optimal variancestabilizing transformation.The log transformation also has a very nice physical interpretation as percentage change.To illustrate this, let the time series be y 1 , y 2 , … , y T and suppose that we are interested in the percentage change in y t , say,= 100 lnThe application of a natural logarithm transformation to the International Sunspot Number, as shown in Figure 2.20b, tends to stabilize the variance and leaves just a few unusual values.In addition to transformations, there are also several types of adjustments that are useful in time series modeling and forecasting.Two of the most widely used are trend adjustments and seasonal adjustments.Sometimes these procedures are called trend and seasonal decomposition.A time series that exhibits a trend is a nonstationary time series.Modeling and forecasting of such a time series is greatly simplified if we can eliminate the trend.One way to do this is to fit a regression model describing the trend component to the data and then subtracting it out of the original observations, leaving a set of residuals that are free of trend.The trend models that are usually considered are the linear trend, in which the mean of y t is expected to change linearly with time as inor as a quadratic function of timeor even possibly as an exponential function of time such as  Another approach to removing trend is by differencing the data; that is, applying the difference operator to the original time series to obtain a new time series, say,where ∇ is the (backward) difference operator.Another way to write the differencing operation is in terms of a backshift operator B, defined as By t = y t−1 , soDifferencing can be performed successively if necessary until the trend is removed; for example, the second difference isIn general, powers of the backshift operator and the backward difference operator are defined asDifferencing has two advantages relative to fitting a trend model to the data.First, it does not require estimation of any parameters, so it is a more parsimonious (i.e., simpler) approach; and second, model fitting assumes that the trend is fixed throughout the time series history and will remain so in the (at least immediate) future.In other words, the trend component, once estimated, is assumed to be deterministic.Differencing can allow the trend component to change through time.The first difference accounts for a trend that impacts the change in the mean of the time series, the second difference accounts for changes in the slope of the time series, and so forth.Usually, one or two differences are all that is required in practice to remove an underlying trend in the data.Reconsider the blue and gorgonzola cheese production data.A difference of one applied to this time series removes the increasing trend (Figure 2.23) and also improves the appearance of the residuals plotted versus fitted value and observation order when a linear model is fitted to the detrended time series (Figure 2.24).This illustrates that differencing may be a very good alternative to detrending a time series by using a regression model.Seasonal, or both trend and seasonal, components are present in many time series.Differencing can also be used to eliminate seasonality.Define a lag-d seasonal difference operator as(2.26)For example, if we had monthly data with an annual season (a very common situation), we would likely use d = 12, so the seasonally differenced data would beWhen both trend and seasonal components are simultaneously present, we can sequentially difference to eliminate these effects.That is, first seasonally difference to remove the seasonal component and then difference one or more times using the regular difference operator to remove the trend.The beverage shipment data shown in Figure 2.2 appear to have a strong monthly pattern-January consistently has the lowest shipments in a year while the peak shipments are in May and June.There is also an overall increasing trend from year to year that appears to be the same regardless of month.A seasonal difference of twelve followed by a trend difference of one was applied to the beverage shipments, and the results are shown in Regression models can also be used to eliminate seasonal (or trend and seasonal components) from time series data.A simple but useful model iswhere d is the period (or length) of the season and 2𝜋∕d is expressed in radians.For example, if we had monthly data and an annual season, then d = 12.This model describes a simple, symmetric seasonal pattern thatD e c -2 0 0 6 J a n -2 0 0 6 J a n -2 0 0 5 J a n -2 0 0 4 J a n -2 0 0 3 J a n -2 0 0 2 J a n -2 0 0 1 J a n -2 0 0 0 J a n -1 9 9 9 J a n -1 9 9 8 J a n -1 9 9 7 J a n -1 9 9 6 J a n -1 9 9 5 J a n -1 9 9 4 J a n -1 9 9 3 J a n -1 9 9 2 e c -2 0 0 6 J a n -2 0 0 6 J a n -2 0 0 5 J a n -2 0 0 4 J a n -2 0 0 3 J a n -2 0 0 2 J a n -2 0 0 1 J a n -2 0 0 0 J a n -1 9 9 9 J a n -1 9 9 8 J a n -1 9 9 7 J a n -1 9 9 6 J a n -1 9 9 5 J a n -1 9 9 4 J a n -1 9 9 3 J a n -1 9 9 2 repeats every 12 periods.The model is actually a sine wave.To see this, recall that a sine wave with amplitude 𝛽, phase angle or origin 𝜃, and period or cycle length 𝜔 can be written as(2.28) Equation (2.27) was obtained by writing Eq. (2.28) as a sine-cosine pair using the trigonometric identity sin(u + v) = cos u sin v + sin u cos v and adding an intercept term 𝛽 0 :where 𝛽 1 = 𝛽 cos 𝜔𝜃 and 𝛽 2 = 𝛽 sin 𝜔𝜃.Setting 𝜔 = 2𝜋∕12 and adding the intercept term 𝛽 0 produces Eq. (2.27).This model is very flexible; for example, if we set 𝜔 = 2𝜋∕52 we can model a yearly seasonal pattern that is observed weekly, if we set 𝜔 = 2𝜋∕4 we can model a yearly seasonal pattern observed quarterly, and if we set 𝜔 = 2𝜋∕13 we can model an annual seasonal pattern observed in 13 four-week periods instead of the usual months.Equation (2.27) incorporates a single sine wave at the fundamental frequency 𝜔 = 2𝜋∕12.In general, we could add harmonics of the fundamental frequency to the model in order to model more complex seasonal patterns.For example, a very general model for monthly data and an annual season that uses the fundamental frequency and the first three harmonics is) .(2.29)If the data are observed in 13 four-week periods, the model would be(There is also a "classical" approach to decomposition of a time series into trend and seasonal components (actually, there are a lot of different decomposition algorithms; here we explain a very simple but useful approach).The general mathematical model for this decomposition iswhere S t is the seasonal component, T t is the trend effect (sometimes called the trend-cycle effect), and 𝜀 t is the random error component.There are usually two forms for the function f ; an additive modeland a multiplicative modelThe additive model is appropriate if the magnitude (amplitude) of the seasonal variation does not vary with the level of the series, while the multiplicative version is more appropriate if the amplitude of the seasonal fluctuations increases or decreases with the average level of the time series.Decomposition is useful for breaking a time series down into these component parts.For the additive model, it is relatively easy.First, we would model and remove the trend.A simple linear model could be used to do this, say, T t = 𝛽 0 + 𝛽 1 t.Other methods could also be used.Moving averages can be used to isolate a trend and remove it from the original data, as could more sophisticated regression methods.These techniques might be appropriate when the trend is not a straight line over the history of the time series.Differencing could also be used, although it is not typically in the classical decomposition approach.Once the trend or trend-cycle component is estimated, the series is detrended:Now a seasonal factor can be calculated for each period in the season.For example, if the data are monthly and an annual season is anticipated, we would calculate a season effect for each month in the data set.Then the seasonal indices are computed by taking the average of all of the seasonal factors for each period in the season.In this example, all of the January seasonal factors are averaged to produce a January season index; all of the February seasonal factors are averaged to produce a February season index; and so on.Sometimes medians are used instead of averages.In multiplicative decomposition, ratios are used, so that the data are detrended byThe seasonal indices are estimated by taking the averages over all of the detrended values for each period in the season.Example 2.9 The decomposition approach can be applied to the beverage shipment data.Examining the time series plot in Figure 2.2, there is both a strong positive trend as well as month-to-month variation, so the model should include both a trend and a seasonal component.It also appears that the magnitude of the seasonal variation does not vary with the level of the series, so an additive model is appropriate.Results of a time series decomposition analysis from Minitab of the beverage shipments are in Figure 2.27, showing the original data (labeled "Actual"); along with the fitted trend line ("Trend") and the predicted values ("Fits") from the additive model with both the trend and seasonal components.Details of the seasonal analysis are shown in  J a n -1 9 9 3 J a n -1 9 9 4 J a n -1 9 9 5 J a n -1 9 9 6 J a n -1 9 9 7 J a n -1 9 9 8 J a n -1 9 9 9 J a n -2 0 0 0 J a n -2 0 0 1 J a n -2 0 0 2 J a n -2 0 0 3 J a n -2 0 0 4 J a n -2 0 0 5 J a n -2 0 0 6 D e c -2 0 0 6  1 2 3 4 5 6 7 8 9 10 11 12   1 2 3 4 5 6 7 8 9 10 11 12  1 2 3 4 5 6 7 8 9 10 11 12   1 2 3 4 5 6 7 8 9   Looking at the normal probability plot and histogram of residuals (Figure 2.30a,c), there does not appear to be an issue with the normality assumption.Figure 2.30d is the same plot as Figure 2.29d.However, variance does seem to increase as the predicted value increases; there is a funnel shape to the residuals plotted in Figure 2.30b.A natural logarithm transformation of the data may stabilize the variance and allow a useful decomposition model to be fit.Results from the decomposition analysis of the natural log-transformed beverage shipment data are plotted in Figure 2.31, with the transformed data, fitted trend line, and predicted values.J a n -1 9 9 2 J a n -1 9 9 3 J a n -1 9 9 4 J a n -1 9 9 5 J a n -1 9 9 6 J a n -1 9 9 7 J a n -1 9 9 8 J a n -1 9 9 9 J a n -2 0 0 0 J a n -2 0 0 1 J a n -2 0 0 2 J a n -2 0 0 3 J a n -2 0 0 4 J a n -2 0 0 5 J a n -2 0 0 6 D e c -2 0 0 6   Another technique for seasonal adjustment that is widely used in modeling and analyzing economic data is the X-11 method.Much of the development work on this method was done by Julian Shiskin and others at the US Bureau of the Census beginning in the mid-1950s and culminating into the X-11 Variant of the Census Method II Seasonal Adjustment Program.References for this work during this period include Shiskin (1958), andMarris (1961).Authoritative documentation for the X-11 procedure is in Shiskin, Young, and Musgrave (1967).The X-11 method uses symmetric moving averages in an iterative approach to estimating the trend and seasonal components.At the end of the series, however, these symmetric weights cannot be applied.Asymmetric weights have to be used.JMP (V12 and higher) provides the X-11 technique.Figure 2.34 shows the JMP X-11 output for the beverage shipment data from Figure 2.2.The upper part of the output contains a plot of the original time series, followed by the sample ACF and PACF.Then Display D10 in the figure shows the final estimates of the seasonal factors by month followed in Display D13 by the irregular or deseasonalized series.The final display is a plot of the original and adjusted time series.While different variants of the X-11 technique have been proposed, the most important method to date has been the X-11-ARIMA method developed at Statistics Canada.This method uses Box-Jenkins autoregressive integrated moving average models (which are discussed in Chapter 5) to extend the series.The use of ARIMA models will result in differences in the final component estimates.Details of this method are in Dagum (1980Dagum ( , 1983Dagum ( , 1988.The techniques that we have been describing form the basis of a general approach to modeling and forecasting time series data.We now give a broad overview of the approach.This should give readers a general understanding of the connections between the ideas we have presented in this chapter and guidance in understanding how the topics in subsequent chapters form a collection of useful techniques for modeling and forecasting time series.The basic steps in modeling and forecasting a time series are as follows:1. Plot the time series and determine its basic features, such as whether trends or seasonal behavior or both are present.Look for possible outliers or any indication that the time series has changed with respect to its basic features (such as trends or seasonality) over the time period history.by fitting an appropriate model to the data.Also consider using data transformations, particularly if the variability in the time series seems to be proportional to the average level of the series.The objective of these operations is to produce a set of stationary residuals.3. Develop a forecasting model for the residuals.It is not unusual to find that there are several plausible models, and additional analysis will have to be performed to determine the best one to deploy.Sometimes potential models can be eliminated on the basis of their fit to the historical data.It is unlikely that a model that fits poorly will produce good forecasts.4. Validate the performance of the model (or models) from the previous step.This will probably involve some type of split-sample or crossvalidation procedure.The objective of this step is to select a model to use in forecasting.We will discuss this more in the next section and illustrate these techniques throughout the book. 5. Also of interest are the differences between the original time series y t and the values that would be forecast by the model on the original scale.To forecast values on the scale of the original time series y t , reverse the transformations and any differencing adjustments made to remove trends or seasonal effects.6.For forecasts of future values in period T + 𝜏 on the original scale, if a transformation was used, say, x t = ln y t , then the forecast made at the end of period T for T + 𝜏 would be obtained by reversing the transformation.For the natural log this would be7. If prediction intervals are desired for the forecast (and we recommend doing this), construct prediction intervals for the residuals and then reverse the transformations made to produce the residuals as described earlier.We will discuss methods for finding prediction intervals for most of the forecasting methods presented in this book.8. Develop and implement a procedure for monitoring the forecast to ensure that deterioration in performance will be detected reasonably quickly.Forecast monitoring is usually done by evaluating the stream of forecast errors that are experienced.We will present methods for monitoring forecast errors with the objective of detecting changes in performance of the forecasting model.We now consider how to evaluate the performance of a forecasting technique for a particular time series or application.It is important to carefully define the meaning of performance.It is tempting to evaluate performance on the basis of the fit of the forecasting or time series model to historical data.There are many statistical measures that describe how well a model fits a given sample of data, and several of these will be described in subsequent chapters.This goodness-of-fit approach often uses the residuals and does not really reflect the capability of the forecasting technique to successfully predict future observations.The user of the forecasts is very concerned about the accuracy of future forecasts, not model goodness of fit, so it is important to evaluate this aspect of any recommended technique.Sometimes forecast accuracy is called "out-of-sample" forecast error, to distinguish it from the residuals that arise from a model-fitting process.Measure of forecast accuracy should always be evaluated as part of a model validation effort (see step 4 in the general approach to forecasting in the previous section).When more than one forecasting technique seems reasonable for a particular application, these forecast accuracy measures can also be used to discriminate between competing models.We will discuss this more in Section 2.6.2.It is customary to evaluate forecasting model performance using the one-step-ahead forecast errors (2.34)The mean forecast error in Eq. (2.32) is an estimate of the expected value of forecast error, which we would hope to be zero; that is, the forecasting technique produces unbiased forecasts.If the mean forecast error differs appreciably from zero, bias in the forecast is indicated.If the mean forecast error drifts away from zero when the forecasting technique is in use, this can be an indication that the underlying time series has changed in some fashion, the forecasting technique has not tracked this change, and now biased forecasts are being generated.Both the mean absolute deviation (MAD) in Eq. (2.33) and the mean squared error (MSE) in Eq. (2.34) measure the variability in forecast errors.Obviously, we want the variability in forecast errors to be small.The MSE is a direct estimator of the variance of the one-step-ahead forecast errors:If the forecast errors are normally distributed (this is usually not a bad assumption, and one that is easily checked), the MAD is related to the standard deviation of forecast errors byThe one-step-ahead forecast error and its summary measures, the ME, MAD, and MSE, are all scale-dependent measures of forecast accuracy; that is, their values are expressed in terms of the original units of measurement (or in the case of MSE, the square of the original units).So, for example, if we were forecasting demand for electricity in Phoenix during the summer, the units would be megawatts (MW).If the MAD for the forecast error during summer months was 5 MW, we might not know whether this was a large forecast error or a relatively small one.Furthermore, accuracy measures that are scale dependent do not facilitate comparisons of a single forecasting technique across different time series, or comparisons across different time periods.To accomplish this, we need a measure of relative forecast error.Define the relative forecast error (in percent) asand the mean absolute percent error is computed from Eq. (2.39) asThere is much empirical evidence (and even some theoretical justification) that the distribution of forecast errors can be well approximated by a normal distribution.This can easily be checked by constructing a normal probability plot of the forecast errors in Table 2.2, as shown in Figure 2.35.The forecast errors deviate somewhat from the straight line, indicating that the normal distribution is not a perfect model for the distribution of forecast errors, but it is not unreasonable.Minitab calculates the Anderson-Darling statistic, a widely used test statistic for normality.The P-value is 0.088, so the hypothesis of normality of the forecast errors would not be rejected at the 0.05 level.This test assumes that the observations (in this case the forecast errors) are uncorrelated.Minitab also reports the standard deviation of the forecast errors to be 4.947, a slightly larger value than we computed from the MSE, because Minitab uses the standard method for calculating sample standard deviations.Note that Eq. (2.31) could have been written as Error = Observation − Forecast.Hopefully, the forecasts do a good job of describing the structure in the observations.In an ideal situation, the forecasts would adequately model all of the structure in the data, and the sequence of forecast errors would be structureless.If they are, the sample ACF of the forecast error should look like the ACF of random data; that is, there should not be any large "spikes" on the sample ACF at low lag.Any systematic or nonrandom pattern in the forecast errors will tend to show up as significant spikes on the sample ACF.If the sample ACF suggests that the forecast errors are not random, then this is evidence that the forecasts can be improved by refining the forecasting model.Essentially, this would consist of taking the structure out of the forecast errors and putting it into the forecasts, resulting in forecasts that are better prediction of the data.Example 2.11 Table 2.3 presents a set of 50 one-step-ahead errors from a forecasting model, and Table 2.4 shows the sample ACF of these forecast errors.The sample ACF is plotted in Figure 2.36.This sample ACF was obtained from Minitab.Note that sample autocorrelations for the first 13 lags are computed.This is consistent with our guideline indicating that for T observations only the first T/4 autocorrelations should be computed.The sample ACF does not provide any strong evidence to support a claim that there is a pattern in the forecast errors.If a time series is white noise, the distribution of the sample autocorrelation coefficient at lag k in large samples is approximately normal with mean zero and variance 1/T; that is,) .Therefore we could test the hypothesis H 0 : 𝜌 k = 0 using the test statistic(2.40)Minitab calculates this Z-statistic (calling it a t-statistic), and it is reported in Table 2.4 for the one-step-ahead forecast errors of This procedure is a one-at-a-time test; that is, the significance level applies to the autocorrelations considered individually.We are often interested in evaluating a set of autocorrelations jointly to determine if they indicate that the time series is white noise.Box and Pierce (1970) have suggested such a procedure.Consider the square of the test statistic Z 0 in Eq. (2.40).The distribution of Z 2 0 = r 2 k T is approximately chi-square with one degree of freedom.The Box-Pierce statisticis distributed approximately as chi-square with Kdegrees of freedom under the null hypothesis that the time series is white noise.Therefore, if Q BP > 𝜒 2𝛼,K we would reject the null hypothesis and conclude that the time series is not white noise because some of the autocorrelations are not zero.A P-value approach could also be used.When this test statistic is applied to a set of residual autocorrelations the statistic Q BP ∼ 𝜒 2 𝛼,K−p , where p is the number of parameters in the model, so the number of degrees of freedom in the chi-square distribution becomes K − p. Box and Pierce call this procedure a "Portmanteau" or general goodness-of-fit statistic (it is testing the goodness of fit of the ACF to the ACF of white noise).A modification of this test that works better for small samples was devised by Ljung and Box (1978).The Ljung-Box goodness-of-fit statistic is(2.42)Note that the Ljung-Box goodness-of-fit statistic is very similar to the original Box-Pierce statistic, the difference being that the squared sample autocorrelation at lag k is weighted by (T + 2)∕(T − k).For large values of T, these weights will be approximately unity, and so the Q LB and Q BP statistics will be very similar.Minitab calculates the Ljung-Box goodness-of-fit statistic Q LB , and the values for the first 13 sample autocorrelations of the one-step-ahead forecast errors of Table 2.3 are shown in the last column of Table 2.4.At lag 13, the value Q LB = 10.5363, and since 𝜒 2 0.05,13 = 22.36, there is no strong evidence to indicate that the first 13 autocorrelations of the forecast errors considered jointly differ from zero.If we calculate the P-value for this test statistic, we find that P = 0.65.This is a good indication that the forecast errors are white noise.Note that Figure 2.13 also gave values for the Ljung-Box statistic.There are often several competing models that can be used for forecasting a particular time series.For example, there are several ways to model and forecast trends.Consequently, selecting an appropriate forecasting model is of considerable practical importance.In this section we discuss some general principles of model selection.In subsequent chapters, we will illustrate how these principles are applied in specific situations.Selecting the model that provides the best fit to historical data generally does not result in a forecasting method that produces the best forecasts of new data.Concentrating too much on the model that produces the best historical fit often results in overfitting, or including too many parameters or terms in the model just because these additional terms improve the model fit.In general, the best approach is to select the model that results in the smallest standard deviation (or mean squared error) of the one-step-ahead forecast errors when the model is applied to data that were not used in the fitting process.Some authors refer to this as an outof-sample forecast error standard deviation (or mean squared error).A standard way to measure this out-of-sample performance is by utilizing some form of data splitting; that is, divide the time series data into two segments-one for model fitting and the other for performance testing.Sometimes data splitting is called cross-validation.It is somewhat arbitrary as to how the data splitting is accomplished.However, a good rule of thumb is to have at least 20 or 25 observations in the performance testing data set.When evaluating the fit of the model to historical data, there are several criteria that may be of value.The mean squared error of the residuals iswhere T periods of data have been used to fit a model with p parameters and e t is the residual from the model-fitting process in period t.The mean squared error s 2 is just the sample variance of the residuals and it is an estimator of the variance of the model errors.Another criterion is the R-squared statistic(2.44)The denominator of Eq. (2.44) is just the total sum of squares of the observations, which is constant (not model dependent), and the numerator is just the residual sum of squares.Therefore, selecting the model that maximizes R 2 is equivalent to selecting the model that minimizes the sum of the squared residuals.Large values of R 2 suggest a good fit to the historical data.Because the residual sum of squares always decreases when parameters are added to a model, relying on R 2 to select a forecasting model encourages overfitting or putting in more parameters than are really necessary to obtain good forecasts.A large value of R 2 does not ensure that the out-of-sample one-step-ahead forecast errors will be small.A better criterion is the "adjusted" R 2 statistic, defined as.(2.45)The adjustment is a "size" adjustment-that is, adjust for the number of parameters in the model.Note that a model that maximizes the adjusted R 2 statistic is also the model that minimizes the residual mean square.Two other important criteria are the Akaike Information Criterion (AIC) (see Akaike (1974)) and the Schwarz Bayesian Information Criterion (abbreviated as BIC or SIC by various authors) (see Schwarz (1978)):(2.47)These two criteria penalize the sum of squared residuals for including additional parameters in the model.Models that have small values of the AIC or BIC are considered good models.One way to evaluate model selection criteria is in terms of consistency.A model selection criterion is consistent if it selects the true model when the true model is among those considered with probability approaching unity as the sample size becomes large, and if the true model is not among those considered, it selects the best approximation with probability approaching unity as the sample size becomes large.It turns out that s 2 , the adjusted R 2 , and the AIC are all inconsistent, because they do not penalize for adding parameters heavily enough.Relying on these criteria tends to result in overfitting.The BIC, which caries a heavier "size adjustment" penalty, is consistent.Consistency, however, does not tell the complete story.It may turn out that the true model and any reasonable approximation to it are very complex.An asymptotically efficient model selection criterion chooses a sequence of models as T(the amount of data available) gets large for which the one-step-ahead forecast error variances approach the one-stepahead forecast error variance for the true model at least as fast as any other criterion.The AIC is asymptotically efficient but the BIC is not.There are a number of variations and extensions of these criteria.The AIC is a biased estimator of the discrepancy between all candidate models and the true model.This has led to developing a "corrected" version of AIC:Sometimes we see the first term in the AIC, AICc, or BIC written as −2 ln L(𝜷, 𝜎 2 ), where L(𝜷, 𝜎 2 ) is the likelihood function for the fitted model evaluated at the maximum likelihood estimates of the unknown parameters 𝜷 and 𝜎 2 .In this context, AIC, AICc, and SIC are called penalized likelihood criteria.Many software packages evaluate and print model selection criteria, such as those discussed here.When both AIC and SIC are available, we prefer using SIC.It generally results in smaller, and hence simpler, models, and so its use is consistent with the time-honored model-building principle of parsimony (all other things being equal, simple models are preferred to complex ones).We will discuss and illustrate model selection criteria again in subsequent chapters.However, remember that the best way to evaluate a candidate model's potential predictive performance is to use data splitting.This will provide a direct estimate of the one-step-ahead forecast error variance, and this method should always be used, if possible, along with the other criteria that we have discussed here.Developing and implementing procedures to monitor the performance of the forecasting model is an essential component of good forecasting system design.No matter how much effort has been expended in developing the forecasting model, and regardless of how well the model works initially, over time it is likely that its performance will deteriorate.The underlying pattern of the time series may change, either because the internal inertial forces that drive the process may evolve through time, or because of external events such as new customers entering the market.For example, a level change or a slope change could occur in the variable that is being forecasted.It is also possible for the inherent variability in the data to increase.Consequently, performance monitoring is important.The one-step-ahead forecast errors e t (1) are typically used for forecast monitoring.The reason for this is that changes in the underlying time series will also typically be reflected in the forecast errors.For example, if a level change occurs in the time series, the sequence of forecast errors will no longer fluctuate around zero; that is, a positive or negative bias will be introduced.There are several ways to monitor forecasting model performance.The simplest way is to apply Shewhart control charts to the forecast errors.A Shewhart control chart is a plot of the forecast errors versus time containing a center line that represents the average (or the target value) of the forecast errors and a set of control limits that are designed to provide an indication that the forecasting model performance has changed.The center line is usually taken as either zero (which is the anticipated forecast error for an unbiased forecast) or the average forecast error (ME from Eq. (2.32)), and the control limits are typically placed at three standard deviations of the forecast errors above and below the center line.If the forecast errors plot within the control limits, we assume that the forecasting model performance is satisfactory (or in control), but if one or more forecast errors exceed the control limits, that is a signal that something has happened and the forecast errors are no longer fluctuating around zero.In control chart terminology, we would say that the forecasting process is out of control and some analysis is required to determine what has happened.The most familiar Shewhart control charts are those applied to data that have been collected in subgroups or samples.The one-step-ahead forecast errors e t (1) are individual observations.Therefore the Shewhart control chart for individuals would be used for forecast monitoring.On this control chart it is fairly standard practice to estimate the standard deviation of the individual observations using a moving range method.The moving range is defined as the absolute value of the difference between any two successive one-step-ahead forecast errors, say, |e t (1) − e t−1 (1)|, and the moving range based on n observations is(2.49)The estimate of the standard deviation of the one-step-ahead forecast errors is based on the average of the moving rangeswhere MR is the average of the moving ranges.This estimate of the standard deviation would be used to construct the control limits on the control chart for forecast errors.For more details on constructing and interpreting control charts, see Montgomery (2013).Example 2.12 Minitab can be used to construct Shewhart control charts for individuals.Figure 2.38 shows the Minitab control charts for the onestep-ahead forecast errors in Table 2.3.Note that both an individuals control chart of the one-step-ahead forecast errors and a control chart of the moving ranges of these forecast errors are provided.On the individuals control chart the center line is taken to be the average of the forecast errors ME defined in Eq. (2.30) (denoted X in Figure 2.38) and the upper and lower three-sigma control limits are abbreviated as UCL and LCL, respectively.The center line on the moving average control chart is at the average of the moving ranges MR = MR∕(n − 1), the three-sigma upper control limit UCL is at 3.267MR∕(n − 1), and the lower control limit is at zero (for details on how the control limits are derived, see Montgomery ( 2013)).All of the one-step-ahead forecast errors plot within the control limits (and the moving range also plot within their control limits).Thus there is no reason to suspect that the forecasting model is performing inadequately, at least from the statistical stability viewpoint.Forecast errors that plot outside the control limits would indicate model inadequacy, or possibly the presence of unusual observations such as outliers in the data.An investigation would be required to determine why these forecast errors exceed the control limits.Because the control charts in Figure 2.38 exhibit statistical control, we would conclude that there is no strong evidence of statistical inadequacy in the forecasting model.Therefore, these control limits would be retained and used to judge the performance of future forecasts (in other words, we do not recalculate the control limits with each new forecast).However, the stable control chart does not imply that the forecasting performance is satisfactory in the sense that the model results in small forecast errors.In the quality control literature, these two aspects of process performance are referred to as control and capability, respectively.It is possible for the forecasting process to be stable or in statistical control but not capablethat is, produce forecast errors that are unacceptably large.Two other types of control charts, the cumulative sum (or CUSUM) control chart and the exponentially weighted moving average (or EWMA) control chart, can also be useful for monitoring the performance of a forecasting model.These charts are more effective at detecting smaller changes or disturbances in the forecasting model performance than the individuals control chart.The CUSUM is very effective in detecting level changes in the monitored variable.It works by accumulating deviations of the forecast errors that are above the desired target value T 0 (usually either zero or the average forecast error) with one statistic C + and deviations that are below the target with another statistic C − .The statistics C + and C − are called the upper and lower CUSUMs, respectively.They are computed as follows:where the constant K, usually called the reference value, is usually chosen as K = 0.5𝜎 using Minitab with a target value of T = 0 and 𝜎 e(1) was estimated using the moving range method described previously, resulting in H = 5 σe(1) = 5(0.8865)MR∕(T− 1) = 5(0.8865)3.24= 14.36.Minitab labels H and −H as UCL and LCL, respectively.The CUSUM control chart reveals no obvious forecasting model inadequacies.A control chart based on the EWMA is also useful for monitoring forecast errors.The EWMA applied to the one-step-ahead forecast errors iswhere 0 < 𝜆 < 1 is a constant (usually called the smoothing constant) and the starting value of the EWMA (required at the first observation) is either ē0 (1) = 0 or the average of the forecast errors.Typical values of the smoothing constant for an EWMA control chart are 0.05 < 𝜆 < 0.2.The EWMA is a weighted average of all current and previous forecast errors, and the weights decrease geometrically with the "age" of the forecast error.To see this, simply substitute recursively for ēt−1 (1), then ēt−2 (1), then ēt−j (1) j for j = 3, 4, …, until we obtainand note that the weights sum to unity becauseThe standard deviation of the EWMA isSo an EWMA control chart for the one-step-ahead forecast errors with a center line of T (the target for the forecast errors) is defined as follows:(2.53)Example 2.14 Minitab can be used to construct EWMA control charts.errors exceeds the control limits so there is no indication of a problem with the forecasting model.Note from Eq. (2.51) and Figure 2.40 that the control limits on the EWMA control chart increase in width for the first few observations and then stabilize at a constant value because the term [1 − (1 − 𝜆) 2t ] approaches unity as t increases.Therefore steady-state limits for the EWMA control chart are(2.54)In addition to control charts, other statistics have been suggested for monitoring the performance of a forecasting model.The most common of these are tracking signals.The cumulative error tracking signal (CETS) is based on the cumulative sum of all current and previous forecast errors, say,If the forecasts are unbiased, we would expect Y(n) to fluctuate around zero.If it differs from zero by very much, it could be an indication that the forecasts are biased.The standard deviation of Y(n), say, 𝜎 Y(n) , will provide a measure of how far Y(n) can deviate from zero due entirely to random variation.Therefore, we would conclude that the forecast is biased if |Y(n)| exceeds some multiple of its standard deviation.To operationalize this, suppose that we have an estimate σY(n) of 𝜎 Y(n) and form the cumulative error tracking signal(2.55)If the CETS exceeds a constant, say, K 1 , we would conclude that the forecasts are biased and that the forecasting model may be inadequate.It is also possible to devise a smoothed error tracking signal based on the smoothed one-step-ahead forecast errors in Eq. (2.52).This would lead to a ratio(2.56)If the SETS exceeds a constant, say, K 2 , this is an indication that the forecasts are biased and that there are potentially problems with the forecasting model.Note that the CETS is very similar to the CUSUM control chart and that the SETS is essentially equivalent to the EWMA control chart.Furthermore, the CUSUM and EWMA are available in standard statistics software (such as Minitab) and the tracking signal procedures are not.So, while tracking signals have been discussed extensively and recommended by some authors, we are not going to encourage their use.Plotting and periodically visually examining a control chart of forecast errors is also very informative, something that is not typically done with tracking signals.Example 2.15 The data are in the second column of the array called gms.data in which the first column is the year.For moving averages, we use functions from package "zoo."plot(gms.data,type="l",xlab='Year',ylab='AverageAmount of Anomaly, • C') points(gms.data,pch=16,cex=.5)lines(gms.data[5:125,1],rollmean(gms.data[,2],5),col="red")points(gms.data[5:125,1],rollmean(gms.data[,2],5),col="red",pch=15,cex=.5)legend(1980,-.3,c("Actual","Fits"),pch=c(16,15),lwd=c(.5,.5), cex=.55,col=c("black","red"))Example 2. 16 The data are in the second column of the array called vis.data in which the first column is the time period (or index).# Moving Average plot(vis.data,type="l",xlab='TimePeriod',ylab='Viscosity, cP') points(vis.data,pch=16,cex=.5)lines(vis.data[5:100,1],rollmean(vis.data[,2],5),col="red")points(vis.data[5:100,1],rollmean(vis.data[,2],5),col="red",pch=15,cex=.5)legend(1,61,c("Actual","Fits"), pch=c(16,15),lwd=c(.5,.5),cex=.55,col=c("black","red")) Example 2.17 The pharmaceutical sales data are in the second column of the array called pharma.data in which the first column is the week.The viscosity data are in the second column of the array called vis.data in which the first column is the year (Note that the 70th observation is corrected).par(mfrow=c(2,2),oma=c(0,0,0,0)) qqnorm(fit.cheese$res,datax=TRUE,pch=16,xlab='Residual',main='')qqline(fit.cheese$res,datax=TRUE)plot(fit.cheese$fit,fit.cheese$res,pch=16,xlab='Fitted Value', ylab='Residual') abline( h=0) hist(fit.cheese$res,col="gray",xlab='Residual',main='')plot(fit.cheese$res,type="l",xlab='ObservationOrder', ylab='Residual') points(fit.cheese$res,pch=16,cex=.5) abline(h=0)    Example 2.23 Functions used to fit a time series model often also provide summary statistics.However, in this example we provide some calculations for a given set of forecast errors as provided in the text.# original data and forecast errors yt<-c (47,46,51,44,54,47,52,45,50,51,49,41,48,50,51,55,52,53,48,52) fe<-c(-4.1,-6.9,2.2,-4.1,4.3,-.5,.8,-8.1,-4.4,-.2,-4.3,-5.5,-5.1,-2.1,4.2,7.3,6.6,5.9,-3.35 Consider an N-span moving average where each observation is weighted by a constant, say, a j ≥ 0. Therefore the weighted moving average at the end of period T isa. Why would you consider using a weighted moving average? b.Show that the variance of the weighted moving average is VarConsider the Hanning filter.This is a weighted moving average.a. Find the variance of the weighted moving average for the Hanning filter.Is this variance smaller than the variance of a simple span-3 moving average with equal weights?b.Find the autocorrelation function for the Hanning filter.Compare this with the autocorrelation function for a simple span-3 moving average with equal weights.Suppose that a simple moving average of span N is used to forecast a time series that varies randomly around a constant, that is, y t = 𝜇 + 𝜀 t , where the variance of the error term is 𝜎 2 .The forecast error at lead one is e T+1 (1) = y T+1 − M T .What is the variance of this lead-one forecast error?2.38 Suppose that a simple moving average of span N is used to forecast a time series that varies randomly around a constant, that is,2.40 Suppose that a simple moving average of span N is used to forecast a time series that varies randomly around a constant mean, that is, y t = 𝜇 + 𝜀 t .At the start of period t 1 the process experiences a transient; that is, it shifts to a new mean level, say, 𝜇 + 𝛿, but it reverts to its original level 𝜇 at the start of period t 1 + 1. Show that the expected value of the moving average isIf a simple N−span moving average is applied to a time series that has a linear trend, say, y t = 𝛽 0 + 𝛽 1 t + 𝜀 t , the moving average will lag behind the observations.Assume that the observations are uncorrelated and have constant variance.Show that at time T the expected value of the moving average is2.42 Use a three-period moving average to smooth the champagne sales data in Table B.11. Plot the moving average on the same axes as the original data.What impact has this smoothing procedure had on the data?Weather forecast for tonight: dark GEORGE CARLIN, American comedianRegression analysis is a statistical technique for modeling and investigating the relationships between an outcome or response variable and one or more predictor or regressor variables.The end result of a regression analysis study is often to generate a model that can be used to forecast or predict future values of the response variable, given specified values of the predictor variables.The simple linear regression model involves a single predictor variable and is written aswhere y is the response, x is the predictor variable, 𝛽 0 and 𝛽 1 are unknown parameters, and 𝜀 is an error term.coefficients 𝛽 0 and 𝛽 1 have a physical interpretation as the intercept and slope of a straight line, respectively.The slope 𝛽 1 measures the change in the mean of the response variable y for a unit change in the predictor variable x.These parameters are typically unknown and must be estimated from a sample of data.The error term 𝜀 accounts for deviations of the actual data from the straight line specified by the model equation.We usually think of 𝜀 as a statistical error, so we define it as a random variable and will make some assumptions about its distribution.For example, we typically assume that 𝜀 is normally distributed with mean zero and variance 𝜎 2 , abbreviated N(0, 𝜎 2 ).Note that the variance is assumed constant; that is, it does not depend on the value of the predictor variable (or any other variable).Regression models often include more than one predictor or regressor variable.If there are k predictors, the multiple linear regression model isThe parameters 𝛽 0 , 𝛽 1 , … , 𝛽 k in this model are often called partial regression coefficients because they convey information about the effect on y of the predictor that they multiply, given that all of the other predictors in the model do not change.The regression models in Eqs.(3.1) and (3.2) are linear regression models because they are linear in the unknown parameters (the 𝛽's), and not because they necessarily describe linear relationships between the response and the regressors.For example, the modelis a linear regression model because it is linear in the unknown parameters 𝛽 0 , 𝛽 1 , and 𝛽 2 , although it describes a quadratic relationship between y and x.As another example, consider the regression modelwhich describes the relationship between a response variable y that varies cyclically with time (hence the subscript t) and the nature of this cyclic variation can be described as a simple sine wave.Regression models such as Eq. ( 3.3) can be used to remove seasonal effects from time series data (refer to Section 2.4.2where models like this were introduced).If the period d of the cycle is specified (such as d = 12 for monthly data with an annual cycle), then sin (2𝜋/d)t and cos (2𝜋/d)t are just numbers for each observation on the response variable and Eq.(3.3) is a standard linear regression model.We will discuss the use of regression models for forecasting or making predictions in two different situations.The first of these is the situation where all of the data are collected on y and the regressors in a single time period (or put another way, the data are not time oriented).For example, suppose that we wanted to develop a regression model to predict the proportion of consumers who will redeem a coupon for purchase of a particular brand of milk (y) as a function of the amount of the discount or face value of the coupon (x).These data are collected over some specified study period (such as a month) and the data do not explicitly vary with time.This type of regression data is called cross-section data.The regression model for cross-section data is written aswhere the subscript i is used to denote each individual observation (or case) in the data set and n represents the number of observations.In the other situation the response and the regressors are time series, so the regression model involves time series data.For example, the response variable might be hourly CO 2 emissions from a chemical plant and the regressor variables might be the hourly production rate, hourly changes in the concentration of an input raw material, and ambient temperature measured each hour.All of these are time-oriented or time series data.The regression model for time series data is written asIn comparing Eq. (3.5) to Eq. (3.4), note that we have changed the observation or case subscript from i to t to emphasize that the response and the predictor variables are time series.Also, we have used T instead of n to denote the number of observations in keeping with our convention that, when a time series is used to build a forecasting model, T represents the most recent or last available observation.Equation (3.3) is a specific example of a time series regression model.The unknown parameters 𝛽 0 , 𝛽 1 , … , 𝛽 k in a linear regression model are typically estimated using the method of least squares.We illustrated least squares model fitting in Chapter 2 for removing trend and seasonal effects from time series data.This is an important application of regression models in forecasting, but not the only one.Section 3.1 gives a formal description of the least squares estimation procedure.Subsequent sections deal with statistical inference about the model and its parameters, and with model adequacy checking.We will also describe and illustrate several ways in which regression models are used in forecasting.We begin with the situation where the regression model is used with crosssection data.The model is given in Eq. (3.4).There are n > k observations on the response variable available, say, y 1 , y 2 , … , y n .Along with each observed response y i , we will have an observation on each regressor or predictor variable and x ij denotes the ith observation or level of variable x j .The data will appear as in Table 3.1.We assume that the error term 𝜀 in the model has expected value E(𝜀) = 0 and variance Var (𝜀) = 𝜎 2 , and that the errors 𝜀 i , i = 1, 2, … , n are uncorrelated random variables.The method of least squares chooses the model parameters (the 𝛽's) in Eq. (3.4) so that the sum of the squares of the errors, 𝜀 i , is minimized.The least squares function isThis function is to be minimized with respect to 𝛽 0 , 𝛽 1 , … , 𝛽 k .Therefore the least squares estimators, say, β0 , β1 , … , βk , must satisfySimplifying Eqs.(3.7) and (3.8), we obtainThese equations are called the least squares normal equations.Note that there are p = k + 1 normal equations, one for each of the unknown regression coefficients.The solutions to the normal equations will be the least squares estimators of the model regression coefficients.It is simpler to solve the normal equations if they are expressed in matrix notation.We now give a matrix development of the normal equations that parallels the development of Eq. (3.10).The multiple linear regression model may be written in matrix notation as y = X𝜷 + 𝜺, (3.11)whereIn general, y is an (n × 1) vector of the observations, X is an (n × p) matrix of the levels of the regressor variables, 𝜷 is a ( p × 1) vector of the regression coefficients, and 𝜺 is an (n × 1) vector of random errors.X is usually called the model matrix, because it is the original data table for the problem expanded to the form of the regression model that you desire to fit.The vector of least squares estimators minimizesWe can expand the right-hand side of L and obtainbecause 𝜷 ′ X ′ y is a (1×1) matrix, or a scalar, and its transpose (𝜷 ′ X ′ y) ′ = y ′ X𝜷 is the same scalar.The least squares estimators must satisfyIn Eq. (3.12) X ′ X is a (p × p) symmetric matrix and X ′ y is a (p × 1) column vector.Equation (3.12) is just the matrix form of the least squares normal equations.It is identical to Eq. (3.10).To solve the normal equations, multiply both sides of Eq. (3.12) by the inverse of X ′ X (we assume that this inverse exists).Thus the least squares estimator of β isThe fitted values of the response variable from the regression model are computed fromor in scalar notation,The difference between the actual observation y i and the corresponding fitted value is the residual e i = y i − ŷi , i = 1, 2, … , n.The n residuals can be written as an (n × 1) vector denoted byIn addition to estimating the regression coefficients 𝛽 0 , 𝛽 1 , … , 𝛽 k , it is also necessary to estimate the variance of the model errors, 𝜎 2 .The estimator of this parameter involves the sum of squares of the residualsWe can show that E(SS E ) = (n − p)𝜎 2 , so the estimator of 𝜎 2 is the residual or mean square errorThe method of least squares is not the only way to estimate the parameters in a linear regression model, but it is widely used, and it results in estimates of the model parameters that have nice properties.If the model is correct (it has the right form and includes all of the relevant predictors), the least squares estimator β is an unbiased estimator of the model parameters 𝜷; that is,The variances and covariances of the estimators β are contained in a (p × p) covariance matrixThe variances of the regression coefficients are on the main diagonal of this matrix and the covariances are on the off-diagonals.Example 3.1 A hospital is implementing a program to improve quality and productivity.As part of this program, the hospital is attempting to measure and evaluate patient satisfaction.Table 3.2 contains some of the data that have been collected for a random sample of 25 recently discharged patients.The "severity" variable is an index that measures the severity of the patient's illness, measured on an increasing scale (i.e., more severe illnesses have higher values of the index), and the response satisfaction is also measured on an increasing scale, with larger values indicating greater satisfaction.We will fit a multiple linear regression model to the patient satisfaction data.The model iswhere y = patient satisfaction, x 1 = patient age, and x 2 = illness severity.To solve the least squares normal equations, we will need to set up the X ′ X matrix and the X ′ y vector.The model matrix X and observation vector y areThe X ′ X matrix and the X ′ y vector areUsing Eq. (3.13), we can find the least squares estimates of the parameters in the regression model asTherefore the regression model iswhere x 1 = patient age and x 2 = severity of illness, and we have reported the regression coefficients to three decimal places.Table 3.3 shows the output from the JMP regression routine for the patient satisfaction data.At the top of the table JMP displays a plot of the actual satisfaction data points versus the fitted values from the regression.If the fit is "perfect" then the actual-predicted and the plotted points would lie on a straight 45 • line.The points do seem to scatter closely along the 45 • line, suggesting that the model is a reasonably good fit to the data.Note that, in addition to the fitted regression model, JMP provides a list of the residuals computed from Eq. (3.16) along with other output that will provide information about the quality of the regression model.This output will be explained in subsequent sections, and we will frequently refer back to Table 3.3.One way to forecast time series data that contain a linear trend is with a trend adjustment procedure.This involves fitting a model with a linear trend term in time, subtracting the fitted values from the original observations to obtain a set of residuals that are trend-free, then forecast the residuals, and compute the forecast by adding the forecast of the residual value(s) to the estimate of trend.WeThe least squares normal equations for this model areBecause there are only two parameters, it is easy to solve the normal equations directly, resulting in the least squares estimatorsMinitab computes these parameter estimates in its trend adjustment procedure, which we illustrated in Example 2.6.The least squares estimates obtained from this trend adjustment model depend on the point in time at which they were computed, that is, T. Sometimes it may be convenient to keep track of the period of computation and denote the estimates as functions of time, say, β0 (T) and β1 (T).The model can be used to predict the next observation by predicting the point on the trend line in period T + 1, which is β0 (T) + β1 (T)(T + 1), and adding to the trend a forecast of the next residual, say, êT+1 (1).If the residuals are structureless and have average value zero, the forecast of the next residual would be zero.Then the forecast of the next observation would beWhen a new observation becomes available, the parameter estimates β0 (T) and β1 (T) could be updated to reflect the new information.This could be done by solving the normal equations again.In some situations it is possible to devise simple updating equations so that new estimates β0 (T + 1) and β1 (T + 1) can be computed directly from the previous ones β0 (T) and β1 (T) without having to directly solve the normal equations.We will show how to do this later.In linear regression problems, certain tests of hypotheses about the model parameters and confidence interval estimates of these parameters are helpful in measuring the usefulness of the model.In this section, we describe several important hypothesis-testing procedures and a confidence interval estimation procedure.These procedures require that the errors 𝜀 i in the model are normally and independently distributed with mean zero and variance 𝜎 2 , abbreviated NID(0, 𝜎 2 ).As a result of this assumption, the observations y i are normally and independently distributed with mean 𝛽 0 + ∑ k j=1 𝛽 j x ij and variance 𝜎 2 .The test for significance of regression is a test to determine whether there is a linear relationship between the response variable y and a subset of the predictor or regressor variables x 1 , x 2 , … , x k .The appropriate hypotheses areRejection of the null hypothesis H 0 in Eq. (3.19) implies that at least one of the predictor variables x 1 , x 2 , … , x k contributes significantly to the model.The test procedure involves an analysis of variance partitioning of the total sum of squaresinto a sum of squares due to the model (or to regression) and a sum of squares due to residual (or error), say,Now if the null hypothesis in Eq. (3.19) is true and the model errors are normally and independently distributed with constant variance as assumed, then the test statistic for significance of regression isand one rejects H 0 if the test statistic F 0 exceeds the upper tail point of the F distribution with k numerator degrees of freedom and n − p denominator degrees of freedom, F 𝛼,k,n−p .Table A.4 in Appendix A contains these upper tail percentage points of the F distribution.Alternatively, we could use the P-value approach to hypothesis testing and thus reject the null hypothesis if the P-value for the statistic F 0 isThe quantities in the numerator and denominator of the test statistic F 0 are called mean squares.Recall that the mean square for error or residual estimates 𝜎 2 .The test for significance of regression is usually summarized in an analysis of variance (ANOVA) table such as Table 3.4.Computational formulas for the sums of squares in the ANOVA areRegression model ANOVA computations are almost always performed using a computer software package.The JMP output in Table 3.3 shows the ANOVA test for significance of regression for the regression model for the patient satisfaction data.The hypotheses in this problem areThe reported value of the F-statistic from Eq. and the P-value is reported as <0.0001.The actual P-value is approximately 1.44 × 10 −11 , a very small value, so there is strong evidence to reject the null hypothesis and we conclude that either patient age or severity are useful predictors for patient satisfaction.Table 3.3 also reports the coefficient of multiple determination R 2 , first introduced in Section 2.6.2 in the context of choosing between competing forecasting models.Recall thatFor the regression model for the patient satisfaction data, we haveSo this model explains about 89.7% of the variability in the data.The statistic R 2 is a measure of the amount of reduction in the variability of y obtained by using the predictor variables x 1 , x 2 , … , x k in the model.It is a measure of how well the regression model fits the data sample.However, as noted in Section 2.6.2, a large value of R 2 does not necessarily imply that the regression model is a good one.Adding a variable to the model will never cause a decrease in R 2 , even in situations where the additional variable is not statistically significant.In almost all cases, when a variable is added to the regression model R 2 increases.As a result, over reliance on R 2 as a measure of model adequacy often results in overfitting; that is, putting too many predictors in the model.In Section 2.6.2 we introduced the adjusted R 2 statisticIn general, the adjusted R 2 statistic will not always increase as variables are added to the model.In fact, if unnecessary regressors are added, the value of the adjusted R 2 statistic will often decrease.Consequently, models with a large value of the adjusted R 2 statistic are usually considered good regression models.Furthermore, the regression model that maximizes the adjusted R 2 statistic is also the model that minimizes the residual mean square.JMP reports both R 2 and R 2 Adj in Table 3.4.The value of R 2 = 0.897 (or 89.7%), and the adjusted R 2 statistic isBoth R 2 and R 2 Adj are very similar, usually a good sign that the regression model does not contain unnecessary predictor variables.It seems reasonable to conclude that the regression model involving patient age and severity accounts for between about 88% and 90% of the variability in the patient satisfaction data.We are frequently interested in testing hypotheses on the individual regression coefficients.These tests would be useful in determining the value or contribution of each predictor variable in the regression model.For example, the model might be more effective with the inclusion of additional variables or perhaps with the deletion of one or more of the variables already in the model.Adding a variable to the regression model always causes the sum of squares for regression to increase and the error sum of squares to decrease.We must decide whether the increase in the regression sum of squares is sufficient to warrant using the additional variable in the model.Furthermore, adding an unimportant variable to the model can actually increase the mean squared error, thereby decreasing the usefulness of the model.The hypotheses for testing the significance of any individual regression coefficient, say, 𝛽 j , areIf the null hypothesis H 0 : 𝛽 j = 0 is not rejected, then this indicates that the predictor variable x j can be deleted from the model.The test statistic for this hypothesis iswhere C jj is the diagonal element of the (X ′ X) −1 matrix corresponding to the regression coefficient βj (in numbering the elements of the matrix C = (X ′ X) −1 , it is necessary to number the first row and column as zero so that the first diagonal element C 00 will correspond to the subscript number on the intercept).The regression coefficient for x 1 = patient age is β1 = −1.0311.The standard error of this estimated regression coefficient iswhich when rounded agrees with the JMP output.(Often manual calculations will differ slightly from those reported by the computer, because the computer carries more decimal places.For instance, in this example if the mean squared error is computed to four decimal places as.6612 instead of the two places reported in the JMP output, and this value of the MS E is used as the estimate σ2 in calculating the standard error, then the standard error of β1 will match the JMP output.)The test statistic is computed from Eq.(3.29) asThis is agrees with the results reported by JMP.Because the P-value reported is small, we would conclude that patient age is statistically significant; that is, it is an important predictor variable, given that severity is also in the model.Similarly, because the t-test statistic for x 2 = severity is large, we would conclude that severity is a significant predictor, given that patient age is in the model.We may also directly examine the contribution to the regression sum of squares for a particular predictor, say, x j , or a group of predictors, given that other predictors x i (i ≠ j) are included in the model.The procedure for doing this is the general regression significance test or, as it is more often called, the extra sum of squares method.This procedure can also be used to investigate the contribution of a subset involving several regressor or predictor variables to the model.Consider the regression model with k regressor variableswhere y is (n × 1), X is (n × p), 𝜷 is ( p × 1), 𝜺 is (n × 1), and p = k + 1.We would like to determine if a subset of the predictor variables x 1 , x 2 , … , x r (r < k) contributes significantly to the regression model.Let the vector of regression coefficients be partitioned as follows:where 𝛽 1 is (r × 1) and 𝛽 2 is [(p − r) × 1].We wish to test the hypothesesThe model may be written aswhere X 1 represents the columns of X (or the predictor variables) associated with 𝜷 1 and X 2 represents the columns of X (predictors) associated with 𝜷 2 .For the full model (including both 𝜷 1 and 𝜷 2 ), we know that β = (X ′ X) −1 X ′ y.Also, the regression sum of squares for all predictor variables including the intercept isand the estimate of 𝜎 2 based on this full model isSS R (𝜷) is called the regression sum of squares due to 𝜷.To find the contribution of the terms in 𝜷 1 to the regression, we fit the model assuming that the null hypothesis H 0 : 𝜷 1 = 0 is true.The reduced model is found from Eq. (3.32) with 𝜷 1 = 0:The least squares estimator of 𝜷 2 is β2 = (X ′ 2 X 2 ) −1 X ′ 2 y and the regression sum of squares for the reduced model isThe regression sum of squares due to 𝜷 1 , given that 𝜷 2 is already in the model isThis sum of squares has r degrees of freedom.It is the "extra sum of squares" due to 𝜷 1 .Note that SS R (𝜷 1 | 𝜷 2 ) is the increase in the regression sum of squares due to including the predictor variables x 1 , x 2 , … , x r in the model.Now SS R (𝜷 1 | 𝜷 2 ) is independent of the estimate of 𝜎 2 based on the full model from Eq. (3.34), so the null hypothesis H 0 : 𝜷 1 = 0 may be tested by the statisticwhere σ2 is computed from Eq. (3.34).If F 0 > F 𝛼,r,n−p we reject H 0 , concluding that at least one of the parameters in 𝜷 1 is not zero, and, consequently, at least one of the predictor variables x 1 , x 2 , … , x r in X 1 contributes significantly to the regression model.A P-value approach could also be used in testing this hypothesis.Some authors call the test in Eq.(3.38) a partial F test.The partial F test is very useful.We can use it to evaluate the contribution of an individual predictor or regressor x j as if it were the last variable added to the model by computingThis is the increase in the regression sum of squares due to adding x j to a model that already includes x 1 , … , x j−1 , x j+1 , … , x k .The partial F test on a single variable x j is equivalent to the t-test in Equation (3.27).The computed value of F 0 will be exactly equal to the square of the t-test statistic t 0 .However, the partial F test is a more general procedure in that we can evaluate simultaneously the contribution of more than one predictor variable to the model.To illustrate this procedure, consider again the patient satisfaction data from Table 3.2.Suppose that we wish to consider fitting a more elaborate model to this data; specifically, consider the second-order polynomial where x 1 = patient age and x 2 = severity.To fit the model, the model matrix would need to be expanded to include columns for the second-order terms x 1 x 2 , x 2 1 , and x 2 2 .The results of fitting this model using JMP are shown in Table 3.5.Suppose that we want to test the significance of the additional secondorder terms.That is, the hypotheses are H 0 : 𝛽 12 = 𝛽 11 = 𝛽 22 = 0 H 1 : at least one of the parameters 𝛽 12 , 𝛽 11 , or 𝛽 22 ≠ 0In the notation used in this section, these second-order terms are the parameters in the vector 𝜷 1 .Since the quadratic model is the full model, we can find SS R (𝜷) directly from the JMP output in Table 3.5 as SS R (𝜷) = 9708.738with 5 degrees of freedom (because there are five predictors in this model).The reduced model is the model with all of the predictors in the vector 𝜷 1 equal to zero.This reduced model is the original regression model that we fit to the data in Table 3.3.From Table 3.3, we can find the regression sum of squares for the reduced model as SS R (𝜷 2 ) = 9663.694and this sum of squares has 2 degrees of freedom (the model has two predictors).Therefore the extra sum of squares for testing the significance of the quadratic terms is just the difference between the regression sums of squares for the full and reduced models, or= 9708.738− 9663.694= 45.044 with 5 − 2 = 3 degrees of freedom.These three degrees of freedom correspond to the three additional terms in the second-order model.The test statistic from Eq. (3.38) isThis F-statistic is very small, so there is no evidence against the null hypothesis.Furthermore, from Table 3.5, we observe that the individual t-statistics for the second-order terms are very small and have large P-values, so there is no reason to believe that the model would be improved by adding any of the second-order terms.It is also interesting to compare the R 2 and R 2 Adj statistics for the two models.From Table 3.3, we find that R 2 = 0.897 and R 2 Adj = 0.887 for the original two-variable model, and from Table 3.5, we find that R 2 = 0.901 and R 2 Adj = 0.875 for the quadratic model.Adding the quadratic terms caused the ordinary R 2 to increase slightly (it will never decrease when additional predictors are inserted into the model), but the adjusted R 2 statistic decreased.This decrease in the adjusted R 2 is an indication that the additional variables did not contribute to the explanatory power of the model.It is often necessary to construct confidence interval (CI) estimates for the parameters in a linear regression and for other quantities of interest from the regression model.The procedure for obtaining these confidence intervals requires that we assume that the model errors are normally and independently distributed with mean zero and variance 𝜎 2 , the same assumption made in the two previous sections on hypothesis testing.Because the least squares estimator β is a linear combination of the observations, it follows that β is normally distributed with mean vector 𝜷 and covariance matrixis distributed as t with n − p degrees of freedom, where C jj is the (jj)th element of the (X ′ X) −1 matrix, and σ2 is the estimate of the error variance, obtained from Eq. (3.34).Therefore a 100(1 − 𝛼) percent confidence interval for an individual regression coefficientThis CI could also be written asExample 3.4 We will find a 95% CI on the regression for patient age in the patient satisfaction data regression model.From the JMP output in Table 3.3, we find that β1 = −1.0311and se( β1 ) = 0.1156.Therefore the 95% CI isThis confidence interval does not include zero; this is equivalent to rejecting (at the 0.05 level of significance) the null hypothesis that the regression coefficient 𝛽 1 = 0.We may also obtain a confidence interval on the mean response at a particular combination of the predictor or regressor variables, say, x 01 , x 02 , … , x 0k .We first define a vector that represents this point expanded to model form.Since the standard multiple linear regression model contains the k predictors and an intercept term, this vector isThe mean response at this point isThe estimator of the mean response at this point is found by substitutingThis estimator is normally distributed because β is normally distributed and it is also unbiased because β is an unbiased estimator of 𝜷.The variance of ŷ(x 0 ) isTherefore, a 100(1 − 𝛼) percent CI on the mean response at the point x 01 ,where σ2 is the estimate of the error variance, obtained from Eq. (3.34).Note that the length of this confidence interval will depend on the location of the point x 0 through the term x ′ 0 (X ′ X) −1 x 0 in the confidence interval formula.Generally, the length of the CI will increase as the point x 0 moves further from the center of the predictor variable data.The quantity √ Var [ŷ(x 0 )] = √ σ2 x ′ 0 (X ′ X) −1 x 0 used in the confidence interval calculations in Eq. (3.43) is sometimes called the standard error of the fitted response.JMP will calculate and display these standard errors for each individual observation in the sample used to fit the model and for other non-sample points of interest.The next-to-last column of Table 3.6 displays the standard error of the fitted response for the patient satisfaction data.These standard errors can be used to compute the CI in Eq. (3.43).Example 3.5 Suppose that we want to find a confidence interval on mean patient satisfaction for the point where x 1 = patient age = 55 and x 2 = severity = 50.This is the first observation in the sample, so refer to Table 3.6, the JMP output for the patient satisfaction regression model.For this observation, JMP reports that the "SE Fit" is 1.51 rounded to two decimal places, or in our notation, √ Var [ŷ(x 0 )] = 1.51.Therefore, if we want to find a 95% CI on the mean patient satisfaction for the case where x 1 = patient age = 55 and x 2 = severity = 50, we would proceed as follows: From inspection of Table 3.6, note that the standard errors for each observation are different.This reflects the fact that the length of the CI on the mean response depends on the location of the observation.Generally, the standard error increases as the distance of the point from the center of the predictor variable data increases.In the case where the point of interest x 0 is not one of the observations in the sample, it is necessary to calculate the standard error for that point √ Var [ŷ(x 0 )] = √ σ2 x ′ 0 (X ′ X) −1 x 0 , which involves finding x ′ 0 (X ′ X) −1 x 0 for the observation x 0 .This is not too difficult (you can do it in Excel), but it is not necessary, because JMP will provide the CI at any point that you specify.For example, if you want to find a 95% CI on the mean patient satisfaction for the point where x 1 = patient age = 60 and x 2 = severity = 60 (this is not a sample observation), then in the last row of Table 3.6 JMP reports that the estimate of the mean patient satisfaction at the point x 1 = patient age = 60 and x 2 = severity = 60 as ŷ(x 0 ) = 48.25, and the standard error of the fitted response as12. Consequently, the 95% CI on the mean patient satisfaction at that point is 43.85 ≤ 𝜇 y|x 0 ≤ 52.65.A regression model can be used to predict future observations on the response y corresponding to a particular set of values of the predictor or regressor variables, say, x 01 , x 02 , … , x 0k .Let x 0 represent this point, expanded to model form.That is, if the regression model is the standard multiple regression model, then x 0 contains the coordinates of the point of interest and unity to account for the intercept term, soThe prediction error in using ŷ(x 0 ) to estimate y(x 0 ) is y(x 0 ) − ŷ(x 0 ).Because ŷ(x 0 ) and y(x 0 ) are independent, the variance of this prediction error isIf we use σ2 from Eq. (3.34) to estimate the error variance 𝜎 2 , then the ratiohas a t distribution with n − p degrees of freedom.Consequently, we can write the following probability statement:This probability statement can be rearranged as follows:Therefore, the probability is 1 − 𝛼 that the future observation falls in the intervalThis statement is called a 100(1 -𝛼) percent prediction interval (PI) for the future observation y(x 0 ) at the point x 01 , x 02 , … , x 0k .The expression in the square tool in Eq. (3.46) is often called the standard error of the predicted response.The PI formula in Eq. (3.46) looks very similar to the formula for the CI on the mean, Eq. (3.43).The difference is the "1" in the variance of the prediction error under the square root.This will make PI longer than the corresponding CI at the same point.It is reasonable that the PI should be longer, as the CI is an interval estimate on the mean of the response distribution at a specific point, while the PI is an interval estimate on a single future observation from the response distribution at that point.There should be more variability associated with an individual observation than with an estimate of the mean, and this is reflected in the additional length of the PI.Example 3.6 JMP will compute the standard errors of the predicted response so it is easy to construct the prediction interval in Eq. (3.46).To illustrate, suppose that we want a 95% PI on a future observation of patient satisfaction for a patient whose age is 75 and with severity of illness 60.In the next to last row of Table 3.6 JMP predicted value of satisfaction at this new observation as ŷ(x 0 ) = 32.78, and the standard error of the predicted response is 7.65.Then from Eq. (3.46) the prediction interval is 16.93 ≤ y(x 0 ) ≤ 48.64.This example provides us with an opportunity to compare prediction and confidence intervals.First, note that from Table 3.6 the standard error of the fit at this point is smaller than the standard error of the prediction.Therefore, the PI is longer than the corresponding CI.Now compare the length of the CI and the PI for this point with the length of the CI and the PI for the point x 1 = patient age = 60 and x 2 = severity = 60 from Example 3.4.The intervals are longer for the point in this example because this point with x 1 = patient age = 75 and x 2 = severity = 60 is further from the center of the predictor variable data than the point in Example 3.4, where x 1 = patient age = 60 and x 2 = severity = 60.An important part of any data analysis and model-building procedure is checking the adequacy of the model.We know that all models are wrong, but a model that is a reasonable fit to the data used to build it and that does not seriously ignore or violate any of the underlying model-building assumptions can be quite useful.Model adequacy checking is particularly important in building regression models for purposes of forecasting, because forecasting will almost always involve some extrapolation or projection of the model into the future, and unless the model is reasonable the forecasting process is almost certainly doomed to failure.Regression model residuals, originally defined in Eq. (2.2), are very useful in model adequacy checking and to get some sense of how well the regression model assumptions of normally and independently distributed model errors with constant variance are satisfied.Recall that if y i is the observed value of the response variable and if the corresponding fitted value from the model is ŷi , then the residuals areResidual plots are the primary approach to model adequacy checking.The simplest way to check the adequacy of the normality assumption on the model errors is to construct a normal probability plot of the residuals.In Section 2.6.1 we introduced and used the normal probability plot of forecast errors to check for the normality of forecast errors.The use of the normal probability plot for regression residuals follows the same approach.To check the assumption of constant variance, plot the residuals versus the fitted values from the model.If the constant variance assumption is satisfied, this plot should exhibit a random scatter of residuals around zero.Problems with the equal variance assumption usually show up as a pattern on this plot.The most common pattern is an outward-opening funnel or megaphone pattern, indicating that the variance of the observations is increasing as the mean increases.Data transformations (see Section 2.4.1) are useful in stabilizing the variance.The log transformation is frequently useful in forecasting applications.It can also be helpful to plot the residuals against each of the predictor or regressor variables in the model.Any deviation from random scatter on these plots can indicate how well the model fits a particular predictor.When the data are a time series, it is also important to plot the residuals versus time order.As usual, the anticipated pattern on this plot is random scatter.Trends, cycles, or other patterns in the plot of residuals versus time indicate model inadequacies, possibly due to missing terms or some other model specification issue.A funnel-shaped pattern that increases in width with time is an indication that the variance of the time series is increasing with time.This happens frequently in economic time series data, and in data that span a long period of time.Log transformations are often useful in stabilizing the variance of these types of time series.Example 3.7 Table 3.3 presents the residuals for the regression model for the patient satisfaction data from Example 3.1.Figure 3.1 presents plots of these residuals.The plot in the upper left-hand portion of the display is a normal probability plot of the residuals.The residuals lie generally along a straight line, so there is no obvious reason to be concerned with the normality assumption.There is a very mild indication that one of the residuals (in the lower tail) may be slightly larger than expected, so this could be an indication of an outlier (a very mild one).The lower left plot is a histogram of the residuals.Histograms are more useful for large samples of data than small ones, so since there are only 25 residuals, this display is probably not as reliable as the normal probability plot.However, the histogram does not give any serious indication of nonnormality.The upper right is a plot of residuals versus the fitted values.This plot indicates essentially random scatter in the residuals, the ideal pattern.If this plot had exhibited a funnel shape, it could indicate problems with the equality of variance assumption.The lower right is a plot of the observations in the order of the data.If this was the order in which the data were collected, or if the data were a time series, this plot could reveal information about how the data may be changing over time.For example, a funnel shape on this plot might indicate that the variability of the observations was changing with time.In addition to residual plots, other model diagnostics are frequently useful in regression.The following sections introduce and briefly illustrate some of these procedures.For more complete presentations, see Montgomery, Peck, and Vining (2012) and Myers (1990).Standardized Residuals Many regression model builders prefer to work with scaled residuals in contrast to the ordinary least squares (OLS) residuals.These scaled residuals frequently convey more information than do the ordinary residuals.One type of scaled residual is the standardized residual,where we generally use σ = √ MS E in the computation.The standardized residuals have mean zero and approximately unit variance; consequently, they are useful in looking for outliers.Most of the standardized residuals should lie in the interval −3 ≤ d i ≤ +3, and any observation with a standardized residual outside this interval is potentially unusual with respect to its observed response.These outliers should be carefully examined because they may represent something as simple as a data-recording error or something of more serious concern, such as a region of the predictor or regressor variable space where the fitted model is a poor approximation to the true response.The standardizing process in Eq. (3.47) scales the residuals by dividing them by their approximate average standard deviation.In some data sets, residuals may have standard deviations that differ greatly.We now present a scaling that takes this into account.The vector of fitted values ŷi that corresponds to the observed values yThe n × n matrix H = X(X ′ X) −1 X ′ is usually called the "hat" matrix because it maps the vector of observed values into a vector of fitted values.The hat matrix and its properties play a central role in regression analysis.The residuals from the fitted model may be conveniently written in matrix notation as The matrix I − H is in general not diagonal, so the residuals from a linear regression model have different variances and are correlated.The variance of the ith residual iswhere h ii is the ith diagonal element of the hat matrix H.Because 0 ≤ h ii ≤ 1 using the mean squared error MS E to estimate the variance of the residuals actually overestimates the true variance.Furthermore, it turns out that h ii is a measure of the location of the ith point in the predictor variable or x-space; the variance of the residual e i depends on where the point x i lies.As h ii increases, the observation x i lies further from the center of the region containing the data.Therefore residuals near the center of the x-space have larger variance than do residuals at more remote locations.Violations of model assumptions are more likely at remote points, so these violations may be hard to detect from inspection of the ordinary residuals e i (or the standardized residuals d i ) because their residuals will usually be smaller.We recommend taking this inequality of variance into account when scaling the residuals.We suggest plotting the studentized residuals as:with σ2 = MS E instead of the ordinary residuals or the standardized residuals.The studentized residuals have unit variance (i.e., V(r i ) = 1) regardless of the location of the observation x i when the form of the regression model is correct.In many situations the variance of the residuals stabilizes, particularly for large data sets.In these cases, there may be little difference between the standardized and studentized residuals.Thus standardized and studentized residuals often convey equivalent information.However, because any point with a large residual and a large hat diagonal h ii is potentially highly influential on the least squares fit, examination of the studentized residuals is generally recommended.Table 3.7 displays the residuals, the studentized residuals, hat diagonals h ii , and several other diagnostics for the regression model for the patient satisfaction data in Example 3.1.These quantities were computer generated using JMP.To illustrate the calculations, consider the first observation.The studentized residual is calculated as follows:which agrees approximately with the value reported by JMP in Table 3.7.Large values of the studentized residuals are usually an indication of potential unusual values or outliers in the data.Absolute values of the studentized residuals that are larger than three or four indicate potentially problematic observations.Note that none of the studentized residuals in Table 3.7 is this large.The largest studentized residual, −2.65767, is associated with observation 9.This observation does show up on the normal probability plot of residuals in Figure 3.1 as a very mild outlier, but there is no indication of a significant problem with this observation.PRESS Another very useful residual scaling can be based on the prediction error sum of squares or PRESS.To calculate PRESS, we select an observation-for example, i.We fit the regression model to the remaining n − 1 observations and use this equation to predict the withheld observation y i .Denoting this predicted value by ŷ(i) , we may now find the prediction error for the ith observation asThe prediction error is often called the ith PRESS residual.Note that the prediction error for the ith observation differs from the ith residual because observation i was not used in calculating the ith prediction value ŷ(i) .This procedure is repeated for each observation i = 1, 2, … , n, producing a set of n PRESS residuals e (1) , e (2) , … , e (n) .Then the PRESS statistic is defined as the sum of squares of the n PRESS residuals orThus PRESS is a form of data splitting (discussed in Chapter 2), since it uses each possible subset of n − 1 observations as an estimation data set, and every observation in turn is used to form a prediction data set.Generally, small values of PRESS imply that the regression model will be useful in predicting new observations.To get an idea about how well the model will predict new data, we can calculate an R 2 -like statistic called the R 2 for predictionNow PRESS will always be larger than the residual sum of squares and, because the ordinary R 2 = 1 − (SS E ∕SS T ), if the value of the R 2 Prediction is not much smaller than the ordinary R 2 , this is a good indication about potential model predictive performance.It would initially seem that calculating PRESS requires fitting n different regressions.However, it is possible to calculate PRESS from the results of a single least squares fit to all n observations.It turns out that the ith PRESS residual iswhere e i is the OLS residual.The hat matrix diagonals are directly calculated as a routine part of solving the least squares normal equations.Therefore PRESS is easily calculated asJMP will calculate the PRESS statistic for a regression model and the R 2 for prediction based on PRESS from Eq. (3.54).The value of PRESS is PRESS = 1484.93and the R 2 for prediction isThat is, this model would be expected to account for about 86.22% of the variability in new data.The studentized residual r i discussed earlier is often considered an outlier diagnostic.It is customary to use the mean squared error MS E as an estimate of 𝜎 2 in computing r i .This is referred to as internal scaling of the residual because MS E is an internally generated estimate of 𝜎 2 obtained from fitting the model to all n observations.Another approach would be to use an estimate of 𝜎 2 based on a data set with the ith observation removed.We denote the estimate of 𝜎 2 so obtained by S 2 (i) .We can show thatThe estimate of 𝜎 2 in Eq. (3.57) is used instead of MS E to produce an externally studentized residual, usually called R-student, given byIn many situations, t i will differ little from the studentized residual r i .However, if the ith observation is influential, then S 2 (i) can differ significantly from MS E , and consequently the R-student residual will be more sensitive to this observation.Furthermore, under the standard assumptions, the Rstudent residual t i has a t-distribution with n -p -1 degrees of freedom.Thus R-student offers a more formal procedure for investigating potential outliers by comparing the absolute magnitude of the residual t i to an appropriate percentage point of t n−p−1 .JMP will compute the R-student residuals.They are shown in Table 3.7 for the regression model for the patient satisfaction data.The largest value of R-student is for observation 9, t 9 = −3.15124.This is another indication that observation 9 is a very mild outlier.In building regression models, we occasionally find that a small subset of the data exerts a disproportionate influence on the fitted model.That is, estimates of the model parameters or predictions may depend more on the influential subset than on the majority of the data.We would like to locate these influential points and assess their impact on the model.If these influential points really are "bad" values, they should be eliminated.On the other hand, there may be nothing wrong with these points, but if they control key model properties, we would like to know it because it could affect the use of the model.In this section we describe and illustrate some useful measures of influence.The disposition of points in the predictor variable space is important in determining many properties of the regression model.In particular, remote observations potentially have disproportionate leverage on the parameter estimates, predicted values, and the usual summary statistics.The hat matrix H = X(X ′ X) −1 X ′ is very useful in identifying influential observations.As noted earlier, H determines the variances and covariances of the predicted response and the residuals becauseThe elements h ij of the hat matrix H may be interpreted as the amount of leverage exerted by the observation y j on the predicted value ŷi .Thus inspection of the elements of H can reveal points that are potentially influential by virtue of their location in x-space.Attention is usually focused on the diagonal elements of the hat matrix h ii .It can be shown that ∑ n i=1 h ii = rank(H) = rank(X) = p, so the average size of the diagonal elements of the H matrix is p/n.A widely used rough guideline is to compare the diagonal elements h ii to twice their average value 2p/n, and if any hat diagonal exceeds this value to consider that observation as a high-leverage point.JMP will calculate and save the values of the hat diagonals.Table 3.7 displays the hat diagonals for the regression model for the patient satisfaction data in Example 3.1.Since there are p = 3 parameters in the model and n = 25 observations, twice the average size of a hat diagonal for this problem is 2p∕n = 2(3)∕25 = 0.24.The largest hat diagonal, 0.212456, is associated with observation 23.This does not exceed twice the average size of a hat diagonal, so there are no high-leverage observations in these data.The hat diagonals will identify points that are potentially influential due to their location in x-space.It is desirable to consider both the location of the point and the response variable in measuring influence.Cook (1977Cook ( , 1979 has suggested using a measure of the squared distance between the least squares estimate based on all n points β and the estimate obtained by deleting the ith point, say, β(i) .This distance measure can be expressed asA reasonable cutoff for D i is unity.That is, we usually consider observations for which D i > 1 to be influential.Cook's distance statistic D i is actually calculated fromNote that, apart from the constant p, D i is the product of the square of the ith studentized residual and the ratio h ii ∕(1 − h ii ).This ratio can be shown to be the distance from the vector x i to the centroid of the remaining data.Thus D i is made up of a component that reflects how well the regression model fits the ith observation y i and a component that measures how far that point is from the rest of the data.Either component (or both) may contribute to a large value of D i .JMP will calculate and save the values of Cook's distance statistic D i .Table 3.7 displays the values of Cook's distance statistic for the regression model for the patient satisfaction data in Example 3.1.The largest value, 0.467041, is associated with observation 9.This value was calculated from Eq. (3.60) as follows:This does not exceed twice the cutoff of unity, so there are no influential observations in these data.In our treatment of regression we have concentrated on fitting the full regression model.Actually, in most applications of regression the analyst will have a very good idea about the general form of the model he/she wishes to fit, but there may be uncertainty about the exact structure of the model.For example, we may not know if all of the predictor variables are really necessary.These applications of regression frequently involve a moderately large or large set of candidate predictors, and the objective of the analyst here is to fit a regression model to the "best subset" of these candidates.This can be a complex problem, as these data sets frequently have outliers, strong correlations between subsets of the variables, and other complicating features.There are several techniques that have been developed for selecting the best subset regression model.Generally, these methods are either stepwisetype variable selection methods or all possible regressions.Stepwise-type methods build a regression model by either adding or removing a predictor variable to the basic model at each step.The forward selection version of the procedure begins with a model containing none of the candidate predictor variables and sequentially inserts variables into the model oneat-a-time until a final equation is produced.The criterion for entering a variable into the equation is that the t-statistic for that variable must be significant.The process is continued until there are no remaining candidate predictors that qualify for entry into the equation.In backward elimination, the procedure begins with all of the candidate predictor variables in the equation, and then variables are removed one-at-a-time to produce a final equation.The criterion for removing a variable is usually based on the t-statistic, with the variable having the smallest t-statistic considered for removal first.Variables are removed until all of the predictors remaining in the model have significant t-statistics.Stepwise regression usually consists of a combination of forward and backward stepping.There are many variations of the basic procedures.In all possible regressions with K candidate predictor variables, the analyst examines all 2 K possible regression equations to identify the ones with potential to be a useful model.Obviously, as K becomes even moderately large, the number of possible regression models quickly becomes formidably large.Efficient algorithms have been developed that implicitly rather than explicitly examine all of these equations.Typically, only the equations that are found to be "best" according to some criterion (such as minimum MS E or AICc) at each subset size are displayed.For more discussion of variable selection methods, see textbooks on regression such as Montgomery, Peck, and Vining (2012) or Myers (1990).Example 3.8 Table 3.8 contains an expanded set of data for the hospital patient satisfaction data introduced in Example 3.1.In addition to the patient age and illness severity data, there are two additional regressors, an indicator of whether the patent is a surgical patient (1) or a medical patient (0), and an index indicating the patient's anxiety level.We will use these data to illustrate how variable selection methods in regression can be used to help the analyst build a regression model.We will illustrate the forward selection procedure first.The JMP output that results from applying forward selection to these data is shown in Table 3.9.We used the AICc criterion for selecting the best model.The forward selection algorithm inserted the predictor patient age first, then severity, then anxiety, and finally surg-med was inserted into the equation.The best model based on the minimum value of AICc contained age and severity.Table 3.10 presents the results of applying the JMP backward elimination procedure to the patient satisfaction data.Once again the AICc criterion was chosen to select the final model.The procedure begins with all four predictors in the model, then the surgical-medical indicator variable was removed, followed by the anxiety predictor, followed by severity.However, removing severity causes an increase in AICc so it is added back to the model.The algorithm concluded with both patient age and severity in the model.Note that in this example, the forward selection procedure produced the same model as the backward elimination procedure.This does not always happen, so it is usually a good idea to investigate different model-building techniques for a problem.Table 3.11 is the JMP stepwise regression algorithm applied to the patient satisfaction data, JMP calls the stepwise option "mixed" variable selection.The default significance levels of 0.25 to enter or remove variables from the model were used.At the first step, patient age is entered in the model.Then severity is entered as the second variable.This is followed by anxiety as the third variable.At that point, none of the remaining predictors met the 0.25 significance level criterion to enter the model, so stepwise regression terminated with age, severity and anxiety as the model predictors.This is not the same model found by backwards elimination and forward selection.Table 3.12 shows the results of applying the JMP all possible regressions algorithm to the patient satisfaction data.Since there are k = 4 predictors, there are 16 possible regression equations.JMP shows the best four of each subset size, along with the full (four-variable) model.For each model, JMP presents the value of R 2 , the square root of the mean squared error (RMSE), and the AICc and BIC statistics.The model with the smallest value of AICc and BIC is the two-variable model with age and severity.The model with the smallest value of the mean squared error (or its square root, RMSE) is the three-variable model with age, severity, and anxiety.Both of these models were found using the stepwise-type algorithms.Either one of these models is likely to be a good regression model describing the effects of the predictor variables on patient satisfaction.In Section 3.4 we discussed methods for checking the adequacy of a linear regression model.Analysis of the model residuals is the basic methodology.A common defect that shows up in fitting regression models is nonconstant variance.That is, the variance of the observations is not constant but changes in some systematic way with each observation.This problem is often identified from a plot of residuals versus the fitted values.Transformation of the response variable is a widely used method for handling the inequality of variance problem.Another technique for dealing with nonconstant error variance is to fit the model using the method of weighted least squares (WLS).In this method of estimation the deviation between the observed and expected values of y i is multiplied by a weight w i that is inversely proportional to the variance of y i .For the case of simple linear regression, the WLS function is (3.61)where w i = 1∕𝜎 2 i and 𝜎 2 i is the variance of the ith observation y i .The resulting least squares normal equations areSolving Eq. (3.62) will produce WLS estimates of the model parameters 𝛽 0 and 𝛽 1 .In this section we give a development of WLS for the multiple regression model.We begin by considering a slightly more general situation concerning the structure of the model errors.The assumptions that we have made concerning the linear regression model y = X𝜷 + 𝜺 are that E(𝜺) = 0 and Var (𝜺) = 𝜎 2 I; that is, the errors have expected value zero and constant variance, and they are uncorrelated.For testing hypotheses and constructing confidence and prediction intervals we also assume that the errors are normally distributed, in which case they are also independent.As we have observed, there are situations where these assumptions are unreasonable.We will now consider the modifications that are necessary to the OLS procedure when E(𝜺) = 0 and Var (𝜺) = 𝜎 2 V, where V is a known n × n matrix.This situation has a simple interpretation; if V is diagonal but with unequal diagonal elements, then the observations y are uncorrelated but have unequal variances, while if some of the offdiagonal elements of V are nonzero, then the observations are correlated.When the model isthe OLS estimator β = (X ′ X) −1 X ′ y is no longer appropriate.The OLS estimator is unbiased becausePractically, this implies that the variances of the regression coefficients are larger than we expect them to be.This problem can be avoided if we estimate the model parameters with a technique that takes the correct variance structure in the errors into account.We will develop this technique by transforming the model to a new set of observations that satisfy the standard least squares assumptions.Then we will use OLS on the transformed observations.Because 𝜎 2 V is the covariance matrix of the errors, V must be nonsingular and positive definite, so there exists an n × n nonsingular symmetric matrix K defined such thatThe matrix K is often called the square root of V. Typically, the error variance 𝜎 2 is unknown, in which case V represents the known (or assumed) structure of the variances and covariances among the random errors apart from the constant 𝜎 2 .Define the new variables z = K −1 y, B = K −1 X, and 𝜹 = K −1 𝜺 (3.64) so that the regression model y = X𝜷 + 𝜺 becomes, upon multiplication by K −1 ,The errors in the transformed model Eq.(3.65) have zero expectation becauseThus the elements of the vector of errors 𝜹 have mean zero and constant variance and are uncorrelated.Since the errors 𝜹 in the model in Eq. (3.65) satisfy the usual assumptions, we may use OLS to estimate the parameters.The least squares function isThe corresponding normal equations areIn Equation (3.66) βGLS is the generalized least squares (GLS) estimator of the model parameters 𝜷.The solution to the GLS normal equations isThe GLS estimator is an unbiased estimator for the model parameters 𝜷, and the covariance matrix of βGLS isThe GLS estimator is a best linear unbiased estimator of the model parameters 𝜷, where "best" means minimum variance.Weighted least squares or WLS is a special case of GLS where the n response observations y i do not have the same variances but are uncorrelated.Therefore the matrix V iswhere 𝜎 2 i is the variance of the ith observation y i , i =1, 2, … , n.Because the weight for each observation should be the reciprocal of the variance of that observation, it is convenient to define a diagonal matrix of weights W = V −1 .Clearly, the weights are the main diagonals of the matrix W. Therefore the WLS criterion isand the WLS normal equations are (X ′ WX) βWLS = X ′ Wy.(3.70)The WLS estimator is βWLS = (X ′ WX) −1 X ′ Wy.(3.71)The WLS estimator is an unbiased estimator for the model parameters 𝜷, and the covariance matrix of βWLS isTo use WLS, the weights w i must be known.Sometimes prior knowledge or experience or information from an underlying theoretical model can be used to determine the weights.For example, suppose that a significant source of error is measurement error and different observations are measured by different instruments of unequal but known or well-estimated accuracy.Then the weights could be chosen inversely proportional to the variances of measurement error.In most practical situations, however, the analyst learns about the inequality of variance problem from the residual analysis for the original model that was fit using OLS.For example, the plot of the OLS residuals e i versus the fitted values ŷi may exhibit an outward-opening funnel shape, suggesting that the variance of the observations is increasing with the mean of the response variable y.Plots of the OLS residuals versus the predictor variables may indicate that the variance of the observations is a function of one of the predictors.In these situations we can often use estimates of the weights.There are several approaches that could be used to estimate the weights.We describe two of the most widely used methods.In the first method, suppose that analysis of the OLS residuals indicates that the variance of the ith observation is a function of one or more predictors or the mean of y.The squared OLS residual e 2 i is an estimator of the variance of the ith observation 𝜎 2 i if the form of the regression model is correct.Furthermore, the absolute value of the residual |e i | is an estimator of the standard deviation 𝜎 i (because. Consequently, we can find a variance equation or a regression model relating 𝜎 2 i to appropriate predictor variables by the following process:1. Fit the model relating y to the predictor variables using OLS and find the OLS residuals.2. Use residual analysis to determine potential relationships between 𝜎 2 i and either the mean of y or some of the predictor variables.3. Regress the squared OLS residuals on the appropriate predictors to obtain an equation for predicting the variance of each observation, say, ŝ2 i = f (x) or ŝ2 i = f (y). 4. Use the fitted values from the estimated variance function to obtain estimates of the weights, w i = 1∕ŝ 2 i , i = 1, 2, … , n. 5. Use the estimated weights as the diagonal elements of the matrix W in the WLS procedure.As an alternative to estimating a variance equation in step 3 above, we could use the absolute value of the OLS residual and fit an equation that relates the standard deviation of each observation to the appropriate regressors.This is the preferred approach if there are potential outliers in the data, because the absolute value of the residuals is less affected by outliers than the squared residuals.When using the five-step procedure outlined above, it is a good idea to compare the estimates of the model parameters obtained from the WLS fit to those obtained from the original OLS fit.Because both methods produce unbiased estimators, we would expect to find that the point estimates of the parameters from both analyses are very similar.If the WLS estimates differ significantly from their OLS counterparts, it is usually a good idea to use the new WLS residuals and reestimate the variance equation to produce a new set of weights and a revised set of WLS estimates using these new weights.This procedure is called iteratively reweighted least squares (IRLS).Usually one or two iterations are all that is required to produce stable estimates of the model parameters.The second approach to estimating the weights makes use of replicate observations or nearest neighbors.Exact replicates are sample observations that have exactly the same values of the predictor variables.Suppose that there are replicate observations at each of the combination of levels of the predictor variables.The weights w i can be estimated directly as the reciprocal of the sample variances at each combination of these levels.Each observation in a replicate group would receive the same weight.This method works best when there are a moderately large number of observations in each replicate group, because small samples do not produce reliable estimates of the variance.Unfortunately, it is fairly unusual to find groups of replicate observations in most regression-modeling situations.It is especially unusual to find them in time series data.An alternative is to look for observations with similar x-levels, which can be thought of as a nearest-neighbor group of observations.The observations in a nearest-neighbor group can be considered as pseudoreplicates and the sample variance for all of the observations in each nearest-neighbor group can be computed.The reciprocal of a sample variance would be used as the weight for all observations in the nearestneighbor group.Sometimes these nearest-neighbor groups can be identified visually by inspecting the scatter plots of y versus the predictor variables or from plots of the predictor variables versus each other.Analytical methods can also be used to find these nearest-neighbor groups.One nearest-neighbor algorithm is described in Montgomery, Peck, and Vining (2012).These authors also present a complete example showing how the nearest-neighbor approach can be used to estimate the weights for a WLS analysis.In WLS the variances 𝜎 2 i are almost always unknown and must be estimated.Since statistical inference on the model parameters as well as confidence intervals and prediction intervals on the response are usually necessary, we should consider the effect of using estimated weights on these procedures.Recall that the covariance matrix of the model parameters in WLS was given in Eq. (3.72).This covariance matrix plays a central role in statistical inference.Obviously, when estimates of the weights are substituted into Eq.(3.72) an estimated covariance matrix is obtained.Generally, the impact of using estimated weights is modest, provided that the sample size is not very small.In these situations, statistical tests, confidence intervals, and prediction intervals should be considered as approximate rather than exact.Example 3.9 Table 3.13 contains 28 observations on the strength of a connector and the age in weeks of the glue used to bond the components of the connector together.A scatter plot of the strength versus age, shown in Figure 3.2, suggests that there may be a linear relationship between strength and age, but there may also be a problem with nonconstant variance in the data.The regression model that was fit to these data is ŷ = 25.936+ 0.3759x, where x = weeks.The residuals from this model are shown in Table 3.13.Figure 3.3 is a plot of the residuals versus weeks.The pronounced outward-opening funnel shape on this plot confirms the inequality of variance problem. Figure 3.4 is a plot of the absolute value of the residuals from this model versus week.There is an indication that a linear relationship may exist between the absolute value of the residuals and weeks, although there is evidence of one outlier in the data.Therefore it seems reasonable to fit a model relating the absolute value of the residuals to weeks.Since the absolute value of a residual is the residual standard deviation, the predicted values from this equation could be used to determine weights for the regression model relating strength to weeks.This regression model is ŝi = −5.854+ 0.29852x.The weights would be equal to the inverse of the square of the fitted value for each s i .These weights are shown in Table 3.13.Using these weights to fit a new regression model to strength using WLS results in ŷ = 27.545+ 0.32383x Note that the weighted least squares model does not differ very much from the OLS model.Because the parameter estimates did not change very much, this is an indication that it is not necessary to iteratively reestimate the standard deviation model and obtain new weights.3.12.Weighted least squares is typically used in situations where the variance of the observations is not constant.We now consider a different situation where a WLS-type procedure is also appropriate.Suppose that the predictor variables in the regression model are only functions of time.As an illustration, consider the linear regression model with a linear trend in time:This model was introduced to illustrate trend adjustment in a time series in Section 2.4.2 and Example 3.2.As another example, the regression model (3.74) describes the relationship between a response variable y that varies cyclically or periodically with time where the cyclic variation is modeled as a simple sine wave.A very general model for these types of situations could be written as (3.75)where the predictors x 1 (t), x 2 (t), … , x k (t) are mathematical functions of time, t.In these types of models it is often logical to believe that older observations are of less value in predicting the future observations at periods T + 1, T + 2, … , than are the observations that are close to the current time period, T. In other words, if you want to predict the value of y at time T + 1 given that you are at the end of time period T (or ŷT+1 (T)), it is logical to assume that the more recent observations such as y T , y T−1 , and y T−2 carry much more useful information than do older observations such as y T−20 .Therefore it seems reasonable to weight the observations in the regression model so that recent observations are weighted more heavily than older observations.A very useful variation of WLS, called discounted least squares, can be used to do this.Discounted least squares also lead to a relatively simple way to update the estimates of the model parameters after each new observation in the time series.Suppose that the model for observation y t is given by Eq. (3.75): where y is a T × 1 vector of the observations, 𝜷 is a p × 1 vector of the model parameters, 𝜺 is a T × 1 vector of the errors, and X(T) is the T × p matrixNote that the tth row of X(T) contains the values of the predictor variables that correspond to the tth observation of the response, y t .We will estimate the parameters in Eq. (3.76) using WLS.However, we are going to choose the weights so that they decrease in magnitude with time.Specifically, let the weight for observation y T−j be 𝜃 j , where 0 < 𝜃 < 1.We are also going to shift the origin of time with each new observation so that T is the current time period.Therefore the WLS criterion is (3.77)where 𝜷(T) indicates that the vector of regression coefficients is estimated at the end of time period T, and x(−j) indicates that the predictor variables, which are just mathematical functions of time, are evaluated at −j.This is just WLS with a T × T diagonal weight matrix In many important applications, the discounted least squares estimator can be simplified considerably.Assume that the predictor variables x i (t) in the model are functions of time that have been chosen so that their values at time period t + 1 are linear combinations of their values at the previous time period.That is, Consequently, the inverse of G would only need to be computed once.The right-hand side of the normal equations can also be simplified.We can writeSo the discounted least squares estimator can be written asThis can also be simplified.Note thatorwhereandThe right-hand side of Eq. (3.85) can still be simplified becauseand letting k = j + 1,Substituting for L −1 G on the right-hand side of Eq. (3.87) results inNow the vector of discounted least squares parameter estimates at the end of time period T in Eq. (3.85) isBut x(1) ′ 𝜷(T − 1) = ŷT (T − 1) is the forecast of y T computed at the end of the previous time period, T -1, so the discounted least squares vector of parameter estimates computed at the end of time period t is(3.88)The last line in Eq. (3.88) is an extremely important result; it states that in discounted least squares the vector of parameter estimates computed at the end of time period T can be computed as a simple linear combination of the estimates made at the end of the previous time period T − 1 and the one-step-ahead forecast error for the observation in period T. Note that there are really two things going on in estimating 𝜷 by discounted least squares: the origin of time is being shifted to the end of the current period, and the estimates of the model parameters are being modified to reflect the forecast error in the current time period.The first and second terms on the right-hand side of Eq. (3.88) accomplish these objectives, respectively.When discounted least squares estimation is started up, an initial estimate of the parameters is required at time period zero, say, β(0).This could be found by a standard least squares (or WLS) analysis of historical data.Because the origin of time is shifted to the end of the current time period, forecasting is easy with discounted least squares.The forecast of the observation at a future time period T + 𝜏, made at the end of time period T, is(3.89)To illustrate the discounted least squares procedure, let us consider the linear trend model:To write the parameter estimation equations in Eq. (3.88), we need the transition matrix L. For the linear trend model, this matrix isTherefore the parameter estimation equations areThe elements of the vector h are found from Eq. (3.86):The steady-state matrix G is found as follows:The steady-state value of G(T) is found by taking the limit as T → ∞, which results in] .Substituting the elements of the vector h into Eq.(3.90) we obtain the parameter estimating equations for the linear trend model asInspection of these equations illustrates the twin aspects of discounted least squares; shifting the origin of time, and updating the parameter estimates.In the first equation, the updated intercept at time T consists of the old intercept plus the old slope (this shifts the origin of time to the end of the current period T), plus a fraction of the current forecast error (this revises or updates the estimate of the intercept).The second equation revises the slope estimate by adding a fraction of the current period forecast error to the previous estimate of the slope.To illustrate the computations, suppose that we are forecasting a time series with a linear trend and we have initial estimates of the slope and intercept at time t = 0 as β0 (0) = 50 and β1 (0) = 1.5These estimates could have been obtained by regression analysis of historical data.Assume that 𝜃 = 0.9, so that 1 − 𝜃 2 = 1 − (0.9) 2 = 0.19 and (1 − 𝜃) 2 = (1 − 0.9) 2 = 0.01.The forecast for time period t = 1, made at the end of time period t = 0, is computed from Eq. (3.89):Suppose that the actual observation in time period 1 is y 1 = 52.The forecast error in time period 1 is The origin of time is now T = 1.Therefore the forecast for time period 2 made at the end of period 1 isIf the observation in period 2 is y 2 = 55, we would update the parameter estimates exactly as we did at the end of time period 1.First, calculate the forecast error: The forecast for period 3, made at the end of period 2, isSuppose that a forecast at a longer lead time than one period is required.If a forecast for time period 5 is required at the end of time period 2, then because the forecast lead time is 𝜏 = 5 − 2 = 3, the desired forecast isIn general, the forecast for any lead time 𝜏, computed at the current origin of time (the end of time period 2), isWhen the discounted least squares procedure is applied to a linear trend model as in Example 3.9, the resulting forecasts are equivalent to the forecasts produced by a method called double exponential smoothing.Exponential smoothing is a popular and very useful forecasting technique and will be discussed in detail in Chapter 4.Discounted least squares can be applied to more complex models.For example, suppose that the model is a polynomial of degree k.The transition matrix for this model is a square (k + 1) × (k + 1) matrix in which the diagonal elements are unity, the elements immediately to the left of the diagonal are also unity, and all other elements are zero.In this polynomial, the term of degree r is written asIn the next example we illustrate discounted least squares for a simple seasonal model.Suppose that a time series can be modeled as a linear trend with a superimposed sine wave to represent a seasonal pattern that is observed monthly.The model is a variation of the one shown in Eq. (3.3):The transition matrix L for this model, which contains a mixture of polynomial and trigonometric terms, isNote that L has a block diagonal structure, with the first block containing the elements for the polynomial portion of the model and the second block containing the elements for the trigonometric terms, and the remaining elements of the matrix are zero.The parameter estimation equations for this model are The steady-state G matrix for this model iswhere we have let 𝜔 = 2𝜋∕12.Because G is symmetric, we only need to show the upper half of the matrix.It turns out that there are closed-form expressions for all of the entries in G.We will evaluate these expressions for 𝜃 = 0.9.This gives the following:for the polynomial terms and1 − (0.9)0.866 1 − 2(0.9)0.866+ (0.9) 2 = 0.8824[ 1 − 0.9(0.5) 1 − 2(0.9)0.5 + (0.9) 2 − 1 − 0.9(1) 1 − 2(0.9)(1) + (0.91 2 [ 0.9(0.866) 1 − 2(0.9)0.5 + (0.9) 2 + 0.9(0) 1 − 2(0.9)1 + (0.9[ 1 − 0.9(0.5) 1 − 2(0.9)0.5 + (0.9) 2 + 1 − 0.9(1) 1 − 2(0.9)(1) + (0.9) 2 ] = 5.3022 for the trignometric terms.Therefore the G matrix is0.214401 0.01987 0.075545 −0.02264 0.01987 0.001138 0.003737 −0.00081 0.075545 0.003737 0.238595 0.009066 −0.02264 −0.00081 0.009066 0.192591where we have shown the entire matrix.The h vector is0.01987 0.075545 −0.02264 0.01987 0.001138 0.003737 −0.00081 0.075545 0.003737 0.238595 0.009066 −0.02264 −0.00081 0.009066 0.192591Therefore the discounted least squares parameter estimation equations areMany applications of regression in forecasting involve both predictor and response variables that are time series.Regression models using time series data occur relatively often in economics, business, and many fields of engineering.The assumption of uncorrelated or independent errors that is typically made for cross-section regression data is often not appropriate for time series data.Usually the errors in time series data exhibit some type of autocorrelated structure.You might find it useful at this point to review the discussion of autocorrelation in time series data from Chapter 2. There are several sources of autocorrelation in time series regression data.In many cases, the cause of autocorrelation is the failure of the analyst to include one or more important predictor variables in the model.For example, suppose that we wish to regress the annual sales of a product in a particular region of the country against the annual advertising expenditures for that product.Now the growth in the population in that region over the period of time used in the study will also influence the product sales.If population size is not included in the model, this may cause the errors in the model to be positively autocorrelated, because if the per capita demand for the product is either constant or increasing with time, population size is positively correlated with product sales.The presence of autocorrelation in the errors has several effects on the OLS regression procedure.These are summarized as follows:1.The OLS regression coefficients are still unbiased, but they are no longer minimum-variance estimates.We know this from our study of GLS in Section 3.7.2. When the errors are positively autocorrelated, the residual mean square may seriously underestimate the error variance 𝜎 2 .Consequently, the standard errors of the regression coefficients may be too small.As a result, confidence and prediction intervals are shorter than they really should be, and tests of hypotheses on individual regression coefficients may be misleading, in that they may indicate that one or more predictor variables contribute significantly to the model when they really do not.Generally, underestimating the error variance 𝜎 2 gives the analyst a false impression of precision of estimation and potential forecast accuracy.3. The confidence intervals, prediction intervals, and tests of hypotheses based on the t and Fdistributions are, strictly speaking, no longer exact procedures.There are three approaches to dealing with the problem of autocorrelation.If autocorrelation is present because of one or more omitted predictors and if those predictor variable(s) can be identified and included in the model, the observed autocorrelation should disappear.Alternatively, the WLS or GLS methods discussed in Section 3.7 could be used if there were sufficient knowledge of the autocorrelation structure.Finally, if these approaches cannot be used, the analyst must turn to a model that specifically incorporates the autocorrelation structure.These models usually require special parameter estimation techniques.We will provide an introduction to these procedures in Section 3.8.2.Residual plots can be useful for the detection of autocorrelation.The most useful display is the plot of residuals versus time.If there is positive autocorrelation, residuals of identical sign occur in clusters: that is, there are not enough changes of sign in the pattern of residuals.On the other hand, if there is negative autocorrelation, the residuals will alternate signs too rapidly.Various statistical tests can be used to detect the presence of autocorrelation.The test developed by Durbin and Watson (1950, 1951, 1971) is a very widely used procedure.This test is based on the assumption that the errors in the regression model are generated by a first-order autoregressive process observed at equally spaced time periods; that is, (3.93)where 𝜀 t is the error term in the model at time period t, a t is an NID(0, 𝜎 2 a ) random variable, and 𝜙 is a parameter that defines the relationship between successive values of the model errors 𝜀 t and 𝜀 t−1 .We will require that |𝜙| < 1, so that the model error term in time period t is equal to a fraction of the error experienced in the immediately preceding period plus a normally and independently distributed random shock or disturbance that is unique to the current period.In time series regression models 𝜙 is sometimes called the autocorrelation parameter.Thus a simple linear regression model with first-order autoregressive errors would be (3.94)where y t and x t are the observations on the response and predictor variables at time period t.When the regression model errors are generated by the first-order autoregressive process in Eq. (3.93), there are several interesting properties of these errors.By successively substituting for 𝜀 t , 𝜀 t−1 , … on the right-hand side of Eq. (3.93) we obtainIn other words, the error term in the regression model for period t is just a linear combination of all of the current and previous realizations of the NID(0, 𝜎 2 ) random variables a t .Furthermore, we can show thatThat is, the errors have zero mean and constant variance but have a nonzero covariance structure unless 𝜙 = 0.The autocorrelation between two errors that are one period apart, or the lag one autocorrelation, isThe autocorrelation between two errors that are k periods apart is Situations where negative autocorrelation occurs are not often encountered.However, if a test for negative autocorrelation is desired, one can use the statistic 4 −d, where d is defined in Eq. (3.97).Then the decision rules for testing the hypotheses H 0 : 𝜙 = 0 versus H 1 : 𝜙 < 0 are the same as those used in testing for positive autocorrelation.It is also possible to test a two-sided alternative hypothesis (H 0 : 𝜙 = 0 versus H 1 : 𝜙 ≠ 0 ) by using both of the one-sided tests simultaneously.If this is done, the two-sided procedure has type I error 2𝛼, where 𝛼 is the type I error used for each individual one-sided test.Example 3.12 Montgomery, Peck, and Vining (2012) present an example of a regression model used to relate annual regional advertising expenses to annual regional concentrate sales for a soft drink company.Table 3.14 presents the 20 years of these data used by Montgomery, Peck, and Vining (2012).The authors assumed that a straight-line relationship was appropriate and fit a simple linear regression model by OLS.The Minitab output for this model is shown in Table 3.15 and the residuals are shown in the last column of Table 3.14.Because these are time series data, there is a possibility that autocorrelation may be present.The plot of residuals versus time, shown in Figure 3.5, has a pattern indicative of potential autocorrelation; there is a definite upward trend in the plot, followed by a downward trend.A significant value of the Durbin-Watson statistic or a suspicious residual plot indicates a potential problem with auto correlated model errors.This could be the result of an actual time dependence in the errors or an "artificial" time dependence caused by the omission of one or more important predictor variables.If the apparent autocorrelation results from missing predictors and if these missing predictors can be identified and incorporated into the model, the apparent autocorrelation problem may be eliminated.This is illustrated in the following example.Example 3.13 Table 3.16 presents an expanded set of data for the soft drink concentrate sales problem introduced in Example 3.12.Because it is reasonably likely that regional population affects soft drink sales, Montgomery, Peck, and Vining (2012) provided data on regional population for each of the study years.Table 3.17 is the Minitab output for a regression model that includes as the predictor variables advertising expenditures and population.Both of these predictor variables are highly significant.The last column of Table 3.16 shows the residuals from this model.Minitab calculates the Durbin-Watson statistic for this model as d = 3.05932, and the 5% critical values are d L = 1.10 and d U = 1.54, and since d is greater than d U , we conclude that there is no evidence to reject the null hypothesis.That is, there is no indication of autocorrelation in the errors.Figure 3.6 is a plot of the residuals from this regression model in time order.This plot shows considerable improvement when compared to the plot of residuals from the model using only advertising expenditures as the predictor.Therefore, we conclude that adding the new predictor population size to the original model has eliminated an apparent problem with autocorrelation in the errors.The Cochrane-Orcutt Method When the observed autocorrelation in the model errors cannot be removed by adding one or more new predictor variables to the model, it is necessary to take explicit account of the autocorrelative structure in the model and use an appropriate parameter We will describe the Cochrane-Orcutt method for the simple linear regression model with first-order autocorrelated errors given in Eq. (3.94).The procedure is based on transforming the response variable so that y ′ t = y t − 𝜙y t−1 .Substituting for y t and y t−1 , the model becomes (3.98)where 𝛽 ′ 0 = 𝛽 0 (1 − 𝜙) and x ′ t = x t − 𝜙x t−1 .Note that the error terms a t in the transformed or reparameterized model are independent random variables.Unfortunately, this new reparameterized model contains an unknown parameter 𝜙 and it is also no longer linear in the unknown parameters because it involves products of 𝜙, 𝛽 0 , and 𝛽 1 .However, the first-order autoregressive process 𝜀 t = 𝜙𝜀 t−1 + a t can be viewed as a simple linear regression through the origin and the parameter 𝜙 can be estimated by obtaining the residuals of an OLS regression of y t on x t and then regressing e t on e t−1 .The OLS regression of e t on e t−1 results in φ = Using φ as an estimate of 𝜙, we can calculate the transformed response and predictor variables asNow apply OLS to the transformed data.This will result in estimates of the transformed slope β′ 0 , the intercept β1 , and a new set of residuals.The Durbin-Watson test can be applied to these new residuals from the reparameterized model.If this test indicates that the new residuals are uncorrelated, then no additional analysis is required.However, if positive autocorrelation is still indicated, then another iteration is necessary.In the second iteration 𝜙 is estimated with new residuals that are obtained by using the regression coefficients from the reparameterized model with the original regressor and response variables.This iterative procedure may be continued as necessary until the residuals indicate that the error terms in the reparameterized model are uncorrelated.Usually only one or two iterations are sufficient to produce uncorrelated errors.Example 3.14 Montgomery, Peck, and Vining (2012) give data on the market share of a particular brand of toothpaste for 30 time periods and the corresponding selling price per pound.These data are shown in 1.20 and d U = 1.41, so there is evidence to support the conclusion that the residuals are positively autocorrelated.We will use the Cochrane-Orcutt method to estimate the model parameters.The autocorrelation coefficient can be estimated using the residuals in Table 3.The slope in the transformed model 𝛽 ′ 1 is equal to the slope in the original model, 𝛽 1 .A comparison of the slopes in the two models in Tables 3.19 and 3.20 shows that the two estimates are very similar.However, if the standard errors are compared, the Cochrane-Orcutt method produces an estimate of the slope that has a larger standard error than the standard error of the OLS estimate.This reflects the fact that if the errors are autocorrelated and OLS is used, the standard errors of the model coefficients are likely to be underestimated.There are other alternatives to the Cochrane-Orcutt method.A popular approach is to use the method of maximum likelihood to estimate the parameters in a time series regression model.We will concentrate on the simple linear regression model with first-order autoregressive errors y t = 𝛽 0 + 𝛽 1 x t + 𝜀 t , 𝜀 t = 𝜙𝜀 t−1 + a t (3.100)One reason that the method of maximum likelihood is so attractive is that, unlike the Cochrane-Orcutt method, it can be used in situations where the autocorrelative structure of the errors is more complicated than first-order autoregressive.For readers unfamiliar with maximum likelihood estimation, we will present a simple example.Consider the time series model (3.101)where a t is N(0, 𝜎 2 ) and 𝜇 is unknown.This is a time series model for a process that varies randomly around a fixed level (𝜇) and for which there is no autocorrelation.We will estimate the unknown parameter 𝜇 using the method of maximum likelihood.Suppose that there are T observations available, y 1 , y 2 , … , y T .The probability distribution of any observation is normal, that is,The likelihood function is just the joint probability density function of the sample.Because the observations y 1 , y 2 , … , y T are independent, the likelihood function is just the product of the individual density functions, orThe maximum likelihood estimator of 𝜇 is the value of the parameter that maximizes the likelihood function.It is often easier to work with the log-likelihood, and this causes no problems because the value of 𝜇 that maximizes the likelihood function also maximizes the log-likelihood.Note that this is just the error sum of squares from the model in Eq. (3.101).So, in the case of normally distributed errors, the maximum likelihood estimator of 𝜇 is identical to the least squares estimator of 𝜇.It is easy to show that this estimator is just the sample average; that is,Suppose that the mean of the model in Eq. (3.101) is a linear regression function of time, say, 𝜇 = 𝛽 0 + 𝛽 1 t so that the model is y t = 𝜇 + a t = 𝛽 0 + 𝛽 1 t + a t with independent and normally distributed errors.The likelihood function for this model is identical to Eq. (3.102), and, once again, the maximum likelihood estimators of the model parameters 𝛽 0 and 𝛽 1 are found by minimizing the error sum of squares from the model.Thus when the errors are normally and independently distributed, the maximum likelihood estimators of the model parameters 𝛽 0 and 𝛽 1 in the linear regression model are identical to the least squares estimators.Now let us consider the simple linear regression model with first-order autoregressive errors, first introduced in Eq. (3.94), and repeated for convenience below:Recall that the a's are normally and independently distributed with mean zero and variance 𝜎 2 a and 𝜙 is the autocorrelation parameter.Write this equation for y t−1 and subtract 𝜙y t−1 from y t .This results in y t − 𝜙y t−1 = (1 − 𝜙)𝛽 0 + 𝛽 1 (x t − 𝜙x t−1 ) + a t or y t = 𝜙y t−1 + (1 − 𝜙)𝛽 0 + 𝛽 1 (x t − 𝜙x t−1 ) + a t = 𝜇(z t , 𝜽) + a t , (3.103)where z ′ t = [y t−1 , x t ] and 𝜽 ′ = [𝜙, 𝛽 0 , 𝛽 1 ].We can think of z t as a vector of predictor variables and 𝜽 as the vector of regression model parameters.Since y t−1 appears on the right-hand side of the model in Eq. (3.103), the index of time must run from 2, 3, … , T. At time period t = 2, we treat y 1 as an observed predictor.Because the a's are normally and independently distributed, the joint probability density of the a's is f (a 2 , a 3 , … , a T ) = which is the error sum of squares for the model.Therefore the maximum likelihood estimators of 𝜙, 𝛽 0 , and 𝛽 1 are also least squares estimators.There are two important points about the maximum likelihood (or least squares) estimators.First, the sum of squares in Eq. (3.104) is conditional on the initial value of the time series, y 1 .Therefore the maximum likelihood (or least squares) estimators found by minimizing this conditional sum of squares are conditional maximum likelihood (or conditional least squares) estimators.Second, because the model involves products of the parameters 𝜙 and 𝛽 0 , the model is no longer linear in the unknown parameters.That is, it is not a linear regression model and consequently we cannot give an explicit closed-form solution for the parameter estimators.Iterative methods for fitting nonlinear regression models must be used.These procedures work by linearizing the model about a set of initial guesses for the parameters, solving the linearized model to obtain improved parameter estimates, then using the improved estimates to define a new linearized model, which leads to new parameter estimates and so on.The details of fitting nonlinear models by least squares are discussed in Montgomery, Peck, and Vining (2012).Suppose that we have obtained a set of parameter estimates, say, θ′ = [ φ, β0 , β1 ].The maximum likelihood estimate of 𝜎 2 a is computed aswhere SS E ( θ) is the error sum of squares in Eq. (3.104) evaluated at the conditional maximum likelihood (or conditional least squares) parameter estimates θ′ = [ φ, β0 , β1 ].Some authors (and computer programs) use an adjusted number of degrees of freedom in the denominator to account for the number of parameters that have been estimated.If there are k predictors, then the number of estimated parameters will be p = k + 3, and the formula for estimating 𝜎 2 a is(3.106) structure is necessary to adequately model the data.The model with firstorder autoregressive errors is satisfactory.We now consider how to obtain forecasts at any lead time using a time series model.It is very tempting to ignore the autocorrelation in the data when forecasting, and simply substitute the conditional maximum likelihood estimates into the regression equation:Now suppose that we are at the end of the current time period, T, and we wish to obtain a forecast for period T + 1.Using the above equation, this results in ŷT+1 (T) = β0 + β1 x T+1 , assuming that the value of the predictor variable in the next time period x T+1 is known.reasonable forecast of the observation in time period T+1 that we can make at the end of the current time period T is ŷT+1 (T) = φy T + (1 − φ) β0 + β1 (x T+1 − φx T ) (3.108)Note that this forecast is likely to be very different from the naive forecast obtained by ignoring the autocorrelation.To find a prediction interval on the forecast, we need to find the variance of the prediction error.The one-step-ahead forecast error is Assume that the future value of the regressor variables x T+1 and x T+2 are known.At the end of the current time period, both y T and x T are known.The random errors at time T + 1 and T + 2 have not been observed yet, and because we have assumed that the expected value of the errors is zero, the best estimate we can make of both a T+1 and a T+2 is zero.This suggests that the forecast of the observation in time period T + 2 made at the end of the current time period T is where 𝜎 2x (1) is the variance of the one-step-ahead forecast error for the predictor variable x and we have assumed that the random error a T+1 in period T+1 is independent of the error in forecasting the predictor variable.Using the variance of the one-step-ahead forecast error from Eq. (3.115), we can construct a 100(1 − 𝛼) percent prediction interval for the lead-one forecast from Eq. (3.114).The PI iswhere z 𝛼∕2 is the upper 𝛼/2 percentage point of the standard normal distribution.To actually compute an interval, we must replace the parameters 𝛽 1 , 𝜎 2 a , and 𝜎 2 x (1) by estimates, resulting in = (1 + 𝜙 2 + ⋯ + 𝜙 2(𝜏−1) )𝜎 2 a + 𝛽 2 1 𝜎 2 x (𝜏)where 𝜎 2 x (𝜏) is the variance of the 𝜏-step-ahead forecast error for the predictor variable x.A 100(1 − 𝛼) percent PI for the lead-𝜏 forecast from Eq. (3.117) isReplacing all of the unknown parameters by estimates, the approximate 100(1 − 𝛼) percent PI is actually computed from Often just including the lagged value of the response variable is sufficient and Eq.(3.120) will be satisfactory.The choice between models should always be a data-driven decision.The different models can be fit to the available data, and model selection can be based on the criteria that we have discussed previously, such as model adequacy checking and residual analysis, and (if enough data are available to do some data splitting) forecasting performance over a test or trial period of data.Example 3.16 Reconsider the toothpaste market share data originally presented in Example 3.14 and modeled with a time series regression model with first-order autoregressive errors in Example 3.15.First we will try fitting the model in Eq. (3.119).This model simply relaxes the restriction that the regression coefficient for the lagged predictor variable x t−1 (price in this example) be equal to −𝛽 1 𝜙.Since this is just a linear regression model, we can fit it using Minitab.Table 3.23 contains the Minitab results.This model is a good fit to the data.The Durbin-Watson statistic is d = 2.04203, which indicates no problems with autocorrelation in the residuals.However, note that the t-statistic for the lagged predictor variable (price) is not significant (P = 0.217), indicating that this variable could be removed from the model.If x t−1 is removed, the model becomes the one in Eq. (3.120).The Minitab output for this model is in Table 3.24.This model is also a good fit to the data.Both predictors, the lagged variable y t−1 and x t , are significant.The Durbin-Watson statistic does not indicate any significant problems with autocorrelation.It seems that either of these models would be reasonable for the toothpaste market share data.The advantage of these models relative to the time series regression model with autocorrelated errors is that they can be fit by OLS.In this example, including a lagged response variable and a lagged predictor variable has essentially eliminated any problems with autocorrelated errors.The field of econometrics involves the unified study of economics, economic data, mathematics, and statistical models.The term econometrics is generally credited to the Norwegian economist Ragnar Frisch  who was one of the founders of the Econometric Society and the founding editor of the important journal Econometrica in 1933.Frisch was a co-winner of the first Nobel Prize in Economic Sciences in 1969.For  (2011).Econometric models assume that the quantities being studied are random variables and regression modeling techniques are widely used in the field to describe the relationships between these quantities.Typically, an analyst may want to quantify the impact of one set of variables on another variable.For example, one may want to investigate the effect of education on income; that is, what is the change in earnings that result from increasing a worker's education, while holding other variables such as age and gender constant.Large-scale, comprehensive econometric models of macroeconomic relationships are used by government agencies and central banks to evaluate economic activity and to provide guidance on economic policies.For example, the United States Federal Reserve Bank has maintained macroeconometric models for forecasting and quantitative policy and macroeconomic analysis for over 40 years.The Fed focuses on both the US economy and the global economy.There are several types of data used in econometric modeling.Timeseries data are used in many applications.Typical examples include aggregates of economic quantities, such as GDP, asset or commodity prices, and interest rates.As we have discussed earlier in this chapter, time series such as these are characterized by serial correlation.A lot of aggregate economic data are only available at a relatively low sampling frequency, such as monthly, quarterly, or in some cases annually.One exception is financial data, which may be available at very high frequency, such as hourly, daily, or even by individual transaction.Cross-sectional data consist of observations taken at the same point in time.In econometric work, surveys are a typical source of cross-sectional data.In typical applications, the surveys