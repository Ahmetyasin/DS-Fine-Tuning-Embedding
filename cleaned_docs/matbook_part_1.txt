We are grateful to many people who looked at early drafts of the book and suffered through painful expositions of concepts.We tried to implement their ideas that we did not vehemently disagree with.We would like to especially acknowledge Christfried Webers for his careful reading of many parts of the book, and his detailed suggestions on structure and presentation.Many friends and colleagues have also been kind enough to provide their time and energy on different versions of each chapter.We have been lucky to benefit from the generosity of the online community, who have suggested improvements via https://github.com,which greatly improved the book.The following people have found bugs, proposed clarifications and suggested relevant literature, either via https://github.comor personal communication.Their names are sorted alphabetically.Machine learning is the latest in a long line of attempts to distill human knowledge and reasoning into a form that is suitable for constructing machines and engineering automated systems.As machine learning becomes more ubiquitous and its software packages become easier to use, it is natural and desirable that the low-level technical details are abstracted away and hidden from the practitioner.However, this brings with it the danger that a practitioner becomes unaware of the design decisions and, hence, the limits of machine learning algorithms.The enthusiastic practitioner who is interested to learn more about the magic behind successful machine learning algorithms currently faces a daunting set of pre-requisite knowledge:Programming languages and data analysis tools Large-scale computation and the associated frameworks Mathematics and statistics and how machine learning builds on it At universities, introductory courses on machine learning tend to spend early parts of the course covering some of these pre-requisites.For historical reasons, courses in machine learning tend to be taught in the computer science department, where students are often trained in the first two areas of knowledge, but not so much in mathematics and statistics.Current machine learning textbooks primarily focus on machine learning algorithms and methodologies and assume that the reader is competent in mathematics and statistics.Therefore, these books only spend one or two chapters on background mathematics, either at the beginning of the book or as appendices.We have found many people who want to delve into the foundations of basic machine learning methods who struggle with the mathematical knowledge required to read a machine learning textbook.Having taught undergraduate and graduate courses at universities, we find that the gap between high school mathematics and the mathematics level required to read a standard machine learning textbook is too big for many people.This book brings the mathematical foundations of basic machine learning concepts to the fore and collects the information in a single place so that this skills gap is narrowed or even closed.2 Foreword Why Another Book on Machine Learning?Machine learning builds upon the language of mathematics to express concepts that seem intuitively obvious but that are surprisingly difficult to formalize.Once formalized properly, we can gain insights into the task we want to solve.One common complaint of students of mathematics around the globe is that the topics covered seem to have little relevance to practical problems.We believe that machine learning is an obvious and direct motivation for people to learn mathematics.This book is intended to be a guidebook to the vast mathematical literature that forms the foundations of modern machine learning.We mo-"Math is linked in the popular mind with phobia and anxiety.You'd think we're discussing spiders."(Strogatz, 2014, page 281) tivate the need for mathematical concepts by directly pointing out their usefulness in the context of fundamental machine learning problems.In the interest of keeping the book short, many details and more advanced concepts have been left out.Equipped with the basic concepts presented here, and how they fit into the larger context of machine learning, the reader can find numerous resources for further study, which we provide at the end of the respective chapters.For readers with a mathematical background, this book provides a brief but precisely stated glimpse of machine learning.In contrast to other books that focus on methods and models of machine learning (MacKay, 2003;Bishop, 2006;Alpaydin, 2010;Barber, 2012;Murphy, 2012;Shalev-Shwartz and Ben-David, 2014;Rogers and Girolami, 2016) or programmatic aspects of machine learning (Müller and Guido, 2016;Raschka and Mirjalili, 2017;Chollet and Allaire, 2018), we provide only four representative examples of machine learning algorithms.Instead, we focus on the mathematical concepts behind the models themselves.We hope that readers will be able to gain a deeper understanding of the basic questions in machine learning and connect practical questions arising from the use of machine learning with fundamental choices in the mathematical model.We do not aim to write a classical machine learning book.Instead, our intention is to provide the mathematical background, applied to four central machine learning problems, to make it easier to read other machine learning textbooks.As applications of machine learning become widespread in society, we believe that everybody should have some understanding of its underlying principles.This book is written in an academic mathematical style, which enables us to be precise about the concepts behind machine learning.We encourage readers unfamiliar with this seemingly terse style to persevere and to keep the goals of each topic in mind.We sprinkle comments and remarks throughout the text, in the hope that it provides useful guidance with respect to the big picture.The book assumes the reader to have mathematical knowledge commonly Draft (2024-01-15) of "Mathematics for Machine Learning".Feedback: https://mml-book.com.Foreword 3 covered in high school mathematics and physics.For example, the reader should have seen derivatives and integrals before, and geometric vectors in two or three dimensions.Starting from there, we generalize these concepts.Therefore, the target audience of the book includes undergraduate university students, evening learners and learners participating in online machine learning courses.In analogy to music, there are three types of interaction that people have with machine learning:Astute Listener The democratization of machine learning by the provision of open-source software, online tutorials and cloud-based tools allows users to not worry about the specifics of pipelines.Users can focus on extracting insights from data using off-the-shelf tools.This enables nontech-savvy domain experts to benefit from machine learning.This is similar to listening to music; the user is able to choose and discern between different types of machine learning, and benefits from it.More experienced users are like music critics, asking important questions about the application of machine learning in society such as ethics, fairness, and privacy of the individual.We hope that this book provides a foundation for thinking about the certification and risk management of machine learning systems, and allows them to use their domain expertise to build better machine learning systems.Experienced Artist Skilled practitioners of machine learning can plug and play different tools and libraries into an analysis pipeline.The stereotypical practitioner would be a data scientist or engineer who understands machine learning interfaces and their use cases, and is able to perform wonderful feats of prediction from data.This is similar to a virtuoso playing music, where highly skilled practitioners can bring existing instruments to life and bring enjoyment to their audience.Using the mathematics presented here as a primer, practitioners would be able to understand the benefits and limits of their favorite method, and to extend and generalize existing machine learning algorithms.We hope that this book provides the impetus for more rigorous and principled development of machine learning methods.Fledgling Composer As machine learning is applied to new domains, developers of machine learning need to develop new methods and extend existing algorithms.They are often researchers who need to understand the mathematical basis of machine learning and uncover relationships between different tasks.This is similar to composers of music who, within the rules and structure of musical theory, create new and amazing pieces.We hope this book provides a high-level overview of other technical books for people who want to become composers of machine learning.There is a great need in society for new researchers who are able to propose and explore novel approaches for attacking the many challenges of learning from data.We are also very grateful to Parameswaran Raman and the many anonymous reviewers, organized by Cambridge University Press, who read one or more chapters of earlier versions of the manuscript, and provided constructive criticism that led to considerable improvements.A special mention goes to Dinesh Singh Negi, our L A T E X support, for detailed and prompt advice about L A T E X-related issues.Last but not least, we are very grateful to our editor Lauren Cowles, who has been patiently guiding us through the gestation process of this book.Total derivative of f with respect to xThe smallest function value of f x * ∈ arg min x f (x) The value x * that minimizes f (note: arg min returns a set of values) L Lagrangian L Negative log-likelihoodVariance of x with respect to the random variable XExpectation of x with respect to the random variable X Cov X,Y [x, y] Covariance between x and y.Random variable X is distributed according to p N µ, ΣMachine learning is about designing algorithms that automatically extract valuable information from data.The emphasis here is on "automatic", i.e., machine learning is concerned about general-purpose methodologies that can be applied to many datasets, while producing something that is meaningful.There are three concepts that are at the core of machine learning: data, a model, and learning.Since machine learning is inherently data driven, data is at the core data of machine learning.The goal of machine learning is to design generalpurpose methodologies to extract valuable patterns from data, ideally without much domain-specific expertise.For example, given a large corpus of documents (e.g., books in many libraries), machine learning methods can be used to automatically find relevant topics that are shared across documents (Hoffman et al., 2010).To achieve this goal, we design models that are typically related to the process that generates data, similar to model the dataset we are given.For example, in a regression setting, the model would describe a function that maps inputs to real-valued outputs.To paraphrase Mitchell (1997): A model is said to learn from data if its performance on a given task improves after the data is taken into account.The goal is to find good models that generalize well to yet unseen data, which we may care about in the future.Learning can be understood as a learning way to automatically find patterns and structure in data by optimizing the parameters of the model.While machine learning has seen many success stories, and software is readily available to design and train rich and flexible machine learning systems, we believe that the mathematical foundations of machine learning are important in order to understand fundamental principles upon which more complicated machine learning systems are built.Understanding these principles can facilitate creating new machine learning solutions, understanding and debugging existing approaches, and learning about the inherent assumptions and limitations of the methodologies we are working with.A challenge we face regularly in machine learning is that concepts and words are slippery, and a particular component of the machine learning system can be abstracted to different mathematical concepts.For example, the word "algorithm" is used in at least two different senses in the context of machine learning.In the first sense, we use the phrase "machine learning algorithm" to mean a system that makes predictions based on input data.We refer to these algorithms as predictors.In the second sense, predictor we use the exact same phrase "machine learning algorithm" to mean a system that adapts some internal parameters of the predictor so that it performs well on future unseen input data.Here we refer to this adaptation as training a system.training This book will not resolve the issue of ambiguity, but we want to highlight upfront that, depending on the context, the same expressions can mean different things.However, we attempt to make the context sufficiently clear to reduce the level of ambiguity.The first part of this book introduces the mathematical concepts and foundations needed to talk about the three main components of a machine learning system: data, models, and learning.We will briefly outline these components here, and we will revisit them again in Chapter 8 once we have discussed the necessary mathematical concepts.While not all data is numerical, it is often useful to consider data in a number format.In this book, we assume that data has already been appropriately converted into a numerical representation suitable for reading into a computer program.Therefore, we think of data as vectors.As data as vectors another illustration of how subtle words are, there are (at least) three different ways to think about vectors: a vector as an array of numbers (a computer science view), a vector as an arrow with a direction and magnitude (a physics view), and a vector as an object that obeys addition and scaling (a mathematical view).A model is typically used to describe a process for generating data, simmodel ilar to the dataset at hand.Therefore, good models can also be thought of as simplified versions of the real (unknown) data-generating process, capturing aspects that are relevant for modeling the data and extracting hidden patterns from it.A good model can then be used to predict what would happen in the real world without performing real-world experiments.We now come to the crux of the matter, the learning component of learning machine learning.Assume we are given a dataset and a suitable model.Training the model means to use the data available to optimize some parameters of the model with respect to a utility function that evaluates how well the model predicts the training data.Most training methods can be thought of as an approach analogous to climbing a hill to reach its peak.In this analogy, the peak of the hill corresponds to a maximum of some Draft (2024-01-15) of "Mathematics for Machine Learning".Feedback: https://mml-book.com.13 desired performance measure.However, in practice, we are interested in the model to perform well on unseen data.Performing well on data that we have already seen (training data) may only mean that we found a good way to memorize the data.However, this may not generalize well to unseen data, and, in practical applications, we often need to expose our machine learning system to situations that it has not encountered before.Let us summarize the main concepts of machine learning that we cover in this book:We represent data as vectors.We choose an appropriate model, either using the probabilistic or optimization view.We learn from available data by using numerical optimization methods with the aim that the model performs well on data not used for training.We can consider two strategies for understanding the mathematics for machine learning:Bottom-up: Building up the concepts from foundational to more advanced.This is often the preferred approach in more technical fields, such as mathematics.This strategy has the advantage that the reader at all times is able to rely on their previously learned concepts.Unfortunately, for a practitioner many of the foundational concepts are not particularly interesting by themselves, and the lack of motivation means that most foundational definitions are quickly forgotten.Top-down: Drilling down from practical needs to more basic requirements.This goal-driven approach has the advantage that the readers know at all times why they need to work on a particular concept, and there is a clear path of required knowledge.The downside of this strategy is that the knowledge is built on potentially shaky foundations, and the readers have to remember a set of words that they do not have any way of understanding.We decided to write this book in a modular way to separate foundational (mathematical) concepts from applications so that this book can be read in both ways.The book is split into two parts, where Part I lays the mathematical foundations and Part II applies the concepts from Part I to a set of fundamental machine learning problems, which form four pillars of machine learning as illustrated in Figure 1.1: regression, dimensionality reduction, density estimation, and classification.Chapters in Part I mostly build upon the previous ones, but it is possible to skip a chapter and work backward if necessary.Chapters in Part II are only loosely coupled and can be read in any order.There are many pointers forward and backward  between the two parts of the book to link mathematical concepts with machine learning algorithms.Of course there are more than two ways to read this book.Most readers learn using a combination of top-down and bottom-up approaches, sometimes building up basic mathematical skills before attempting more complex concepts, but also choosing topics based on applications of machine learning.The four pillars of machine learning we cover in this book (see Figure 1.1) require a solid mathematical foundation, which is laid out in Part I.We represent numerical data as vectors and represent a table of such data as a matrix.The study of vectors and matrices is called linear algebra, which we introduce in Chapter 2. The collection of vectors as a matrix is linear algebra also described there.Given two vectors representing two objects in the real world, we want to make statements about their similarity.The idea is that vectors that are similar should be predicted to have similar outputs by our machine learning algorithm (our predictor).To formalize the idea of similarity between vectors, we need to introduce operations that take two vectors as input and return a numerical value representing their similarity.The construction of similarity and distances is central to analytic geometry and is analytic geometry discussed in Chapter 3.In Chapter 4, we introduce some fundamental concepts about matrices and matrix decomposition.Some operations on matrices are extremely matrix decomposition useful in machine learning, and they allow for an intuitive interpretation of the data and more efficient learning.We often consider data to be noisy observations of some true underlying signal.We hope that by applying machine learning we can identify the signal from the noise.This requires us to have a language for quantifying what "noise" means.We often would also like to have predictors that Draft (2024-01-15) of "Mathematics for Machine Learning".Feedback: https://mml-book.com.15 allow us to express some sort of uncertainty, e.g., to quantify the confidence we have about the value of the prediction at a particular test data point.Quantification of uncertainty is the realm of probability theory and probability theory is covered in Chapter 6.To train machine learning models, we typically find parameters that maximize some performance measure.Many optimization techniques require the concept of a gradient, which tells us the direction in which to search for a solution.Chapter 5 is about vector calculus and details the vector calculus concept of gradients, which we subsequently use in Chapter 7, where we talk about optimization to find maxima/minima of functions.optimizationThe second part of the book introduces four pillars of machine learning as shown in Figure 1.1.We illustrate how the mathematical concepts introduced in the first part of the book are the foundation for each pillar.Broadly speaking, chapters are ordered by difficulty (in ascending order).In Chapter 8, we restate the three components of machine learning (data, models, and parameter estimation) in a mathematical fashion.In addition, we provide some guidelines for building experimental set-ups that guard against overly optimistic evaluations of machine learning systems.Recall that the goal is to build a predictor that performs well on unseen data.In Chapter 9, we will have a close look at linear regression, where our linear regression objective is to find functions that map inputs x ∈ R D to corresponding observed function values y ∈ R, which we can interpret as the labels of their respective inputs.We will discuss classical model fitting (parameter estimation) via maximum likelihood and maximum a posteriori estimation, as well as Bayesian linear regression, where we integrate the parameters out instead of optimizing them.Chapter 10 focuses on dimensionality reduction, the second pillar in Figdimensionality reduction ure 1.1, using principal component analysis.The key objective of dimensionality reduction is to find a compact, lower-dimensional representation of high-dimensional data x ∈ R D , which is often easier to analyze than the original data.Unlike regression, dimensionality reduction is only concerned about modeling the data -there are no labels associated with a data point x.In Chapter 11, we will move to our third pillar: density estimation.The density estimation objective of density estimation is to find a probability distribution that describes a given dataset.We will focus on Gaussian mixture models for this purpose, and we will discuss an iterative scheme to find the parameters of this model.As in dimensionality reduction, there are no labels associated with the data points x ∈ R D .However, we do not seek a low-dimensional representation of the data.Instead, we are interested in a density model that describes the data.Chapter 12 concludes the book with an in-depth discussion of the fourthpillar: classification.We will discuss classification in the context of support classification vector machines.Similar to regression (Chapter 9), we have inputs x and corresponding labels y.However, unlike regression, where the labels were real-valued, the labels in classification are integers, which requires special care.We provide some exercises in Part I, which can be done mostly by pen and paper.For Part II, we provide programming tutorials (jupyter notebooks) to explore some properties of the machine learning algorithms we discuss in this book.We appreciate that Cambridge University Press strongly supports our aim to democratize education and learning by making this book freely available for download at https://mml-book.comwhere tutorials, errata, and additional materials can be found.Mistakes can be reported and feedback provided using the preceding URL.When formalizing intuitive concepts, a common approach is to construct a set of objects (symbols) and a set of rules to manipulate these objects.This is known as an algebra.Linear algebra is the study of vectors and certain algebra rules to manipulate vectors.The vectors many of us know from school are called "geometric vectors", which are usually denoted by a small arrow above the letter, e.g., − → x and − → y .In this book, we discuss more general concepts of vectors and use a bold letter to represent them, e.g., x and y.In general, vectors are special objects that can be added together and multiplied by scalars to produce another object of the same kind.From an abstract mathematical viewpoint, any object that satisfies these two properties can be considered a vector.Here are some examples of such vector objects:1. Geometric vectors.This example of a vector may be familiar from high school mathematics and physics.Geometric vectors -see Figure 2.1(a) -are directed segments, which can be drawn (at least in two dimensions).Two geometric vectors x, λ ∈ R, is also a geometric vector.In fact, it is the original vector scaled by λ.Therefore, geometric vectors are instances of the vector concepts introduced previously.Interpreting vectors as geometric vectors enables us to use our intuitions about direction and magnitude to reason about mathematical operations.2. Polynomials are also vectors; see  Linear Algebra be added together, which results in another polynomial; and they can be multiplied by a scalar λ ∈ R, and the result is a polynomial as well.Therefore, polynomials are (rather unusual) instances of vectors.Note that polynomials are very different from geometric vectors.While geometric vectors are concrete "drawings", polynomials are abstract concepts.However, they are both vectors in the sense previously described.3. Audio signals are vectors.Audio signals are represented as a series of numbers.We can add audio signals together, and their sum is a new audio signal.If we scale an audio signal, we also obtain an audio signal.Therefore, audio signals are a type of vector, too.4. Elements of R n (tuples of n real numbers) are vectors.R n is more abstract than polynomials, and it is the concept we focus on in this book.For instance,is an example of a triplet of numbers.Adding two vectors a, b ∈ R n component-wise results in another vector: a + b = c ∈ R n .Moreover, multiplying a ∈ R n by λ ∈ R results in a scaled vector λa ∈ R n .Considering vectors as elements of R n has an additional benefit that Be careful to check whether array operations actually perform vector operations when implementing on a computer.it loosely corresponds to arrays of real numbers on a computer.Many programming languages support array operations, which allow for convenient implementation of algorithms that involve vector operations.Linear algebra focuses on the similarities between these vector concepts.We can add them together and multiply them by scalars.We will largely focus on vectors in R n since most algorithms in linear algebra are formulated in R n .We will see in Chapter 8 that we often consider data to be represented as vectors in R n .In this book, we will focus on finitedimensional vector spaces, in which case there is a 1:1 correspondence between any kind of vector and R n .When it is convenient, we will use intuitions about geometric vectors and consider array-based algorithms.One major idea in mathematics is the idea of "closure".This is the question: What is the set of all things that can result from my proposed operations?In the case of vectors: What is the set of vectors that can result by starting with a small set of vectors, and adding them to each other and scaling them?This results in a vector space (Section 2.4).The concept of a vector space and its properties underlie much of machine learning.The concepts introduced in this chapter are summarized in Figure 2.2.This chapter is mostly based on the lecture notes and books by Drumm and Weil (2001), Strang (2003), Hogben (2013, Liesen and Mehrmann (2015), as well as Pavel Grinfeld's Linear Algebra series.Other excellent Draft (2024-01-15) of "Mathematics for Machine Learning".Feedback: https://mml-book.com.Linear algebra plays an important role in machine learning and general mathematics.The concepts introduced in this chapter are further expanded to include the idea of geometry in Chapter 3. In Chapter 5, we will discuss vector calculus, where a principled knowledge of matrix operations is essential.In Chapter 10, we will use projections (to be introduced in Section 3.8) for dimensionality reduction with principal component analysis (PCA).In Chapter 9, we will discuss linear regression, where linear algebra plays a central role for solving least-squares problems.Systems of linear equations play a central part of linear algebra.Many problems can be formulated as systems of linear equations, and linear algebra gives us the tools for solving them.A company produces products N 1 , . . ., N n for which resources R 1 , . . ., R m are required.To produce a unit of product N j , a ij units of resource R i are needed, where i = 1, . . ., m and j = 1, . . ., n.The objective is to find an optimal production plan, i.e., a plan of how many units x j of product N j should be produced if a total of b i units of resource R i are available and (ideally) no resources are left over.If we produce x 1 , . . ., x n units of the corresponding products, we need Linear Algebra a total of2) many units of resource R i .An optimal production plan (x 1 , . . ., x n ) ∈ R n , therefore, has to satisfy the following system of equations:whereEquation (2.3) is the general form of a system of linear equations, and system of linear equationsx 1 , . . ., x n are the unknowns of this system.Every n-tuple (x 1 , . . ., x n ) ∈ R n that satisfies (2.3) is a solution of the linear equation system.solutionThe system of linear equationshas no solution: Adding the first two equations yields 2x 1 +3x 3 = 5, which contradicts the third equation (3).Let us have a look at the system of linear equations.(2.5)From the first and third equation, it follows that x 1 = 1.From (1)+(2), we get 2x 1 + 3x 3 = 5, i.e., x 3 = 1.From (3), we then get that x 2 = 1.Therefore, (1, 1, 1) is the only possible and unique solution (verify that (1, 1, 1) is a solution by plugging in).As a third example, we consider.(2.6)Since (1)+(2)=(3), we can omit the third equation (redundancy).From (1) and (2), we get 2x 1 = 5−3x 3 and 2x 2 = 1+x 3 .We define x 3 = a ∈ R as a free variable, such that any tripletis a solution of the system of linear equations, i.e., we obtain a solution set that contains infinitely many solutions.In general, for a real-valued system of linear equations we obtain either no, exactly one, or infinitely many solutions.Linear regression (Chapter 9) solves a version of Example 2.1 when we cannot solve the system of linear equations.Remark (Geometric Interpretation of Systems of Linear Equations).In a system of linear equations with two variables x 1 , x 2 , each linear equation defines a line on the x 1 x 2 -plane.Since a solution to a system of linear equations must satisfy all equations simultaneously, the solution set is the intersection of these lines.This intersection set can be a line (if the linear equations describe the same line), a point, or empty (when the lines are parallel).An illustration is given in Figure 2.3 for the systemwhere the solution space is the point (x 1 , x 2 ) = (1, 1 4 ).Similarly, for three variables, each linear equation determines a plane in three-dimensional space.When we intersect these planes, i.e., satisfy all linear equations at the same time, we can obtain a solution set that is a plane, a line, a point or empty (when the planes have no common intersection).♢For a systematic approach to solving systems of linear equations, we will introduce a useful compact notation.We collect the coefficients a ij into vectors and collect the vectors into matrices.In other words, we write the system from (2.3) in the following form: 22(2.10)In the following, we will have a close look at these matrices and define computation rules.We will return to solving linear equations in Section 2.3.Matrices play a central role in linear algebra.They can be used to compactly represent systems of linear equations, but they also represent linear functions (linear mappings) as we will see later in Section 2.7.Before we discuss some of these interesting topics, let us first define what a matrix is and what kind of operations we can do with matrices.We will see more properties of matrices in Chapter 4.. ., m, j = 1, . . ., n, which is ordered according to a rectangular scheme consisting of m rows and n columns:R m×n is the set of all real-valued (m, n)-matrices.A ∈ R m×n can be equivalently represented as a ∈ R mn by stacking all n columns of the matrix into a long vector; see Figure 2.4.The sum of two matrices A ∈ R m×n , B ∈ R m×n is defined as the elementwise sum, i.e.,(2.12)For matrices A ∈ R m×n , B ∈ R n×k , the elements c ij of the product Note the size of the matrices.C = AB ∈ R m×k are computed as(2.13)This means, to compute element c ij we multiply the elements of the ith There are n columns in A and n rows in B so that we can compute a il b lj for l = 1, . . ., n.Commonly, the dot product between two vectors a, b is denoted by a ⊤ b or ⟨a, b⟩.row of A with the jth column of B and sum them up.Later in Section 3.2, we will call this the dot product of the corresponding row and column.In cases, where we need to be explicit that we are performing multiplication, we use the notation A • B to denote multiplication (explicitly showing "•").Remark.Matrices can only be multiplied if their "neighboring" dimensions match.For instance, an n × k-matrix A can be multiplied with a k × mmatrix B, but only from the left side:The product BA is not defined if m ̸ = n since the neighboring dimensions do not match.♢Remark.Matrix multiplication is not defined as an element-wise operation on matrix elements, i.e., c ij ̸ = a ij b ij (even if the size of A, B was chosen appropriately).This kind of element-wise multiplication often appears in programming languages when we multiply (multi-dimensional) arrays with each other, and is called a Hadamard product.Example 2.3(2.16) From this example, we can already see that matrix multiplication is not commutative, i.e., AB ̸ = BA; see also Figure 2.5 for an illustration.Definition 2.2 (Identity Matrix).In R n×n , we define the identity matrix identity matrixas the n × n-matrix containing 1 on the diagonal and 0 everywhere else.Now that we defined matrix multiplication, matrix addition and the identity matrix, let us have a look at some properties of matrices: associativity Associativity:(2.18) distributivity Distributivity:Multiplication with the identity matrix: a 12 a 21 is the determinant of a 2×2-matrix.Furthermore, we can generally use the determinant to check whether a matrix is invertible.♢The matricesare inverse to each other since AB = I = BA.transposeThe main diagonal (sometimes called "principal diagonal", "primary diagonal", "leading diagonal", or "major diagonal") of a matrix A is the collection of entries A ij where i = j.In general, A ⊤ can be obtained by writing the columns of A as the rows of A ⊤ .The following are important properties of inverses and transposes:The scalar case of (2.28) isNote that only (n, n)-matrices can be symmetric.Generally, we call (n, n)-matrices also square matrices because they possess the same numsquare matrix ber of rows and columns.Moreover, if A is invertible, then so is A ⊤ , andRemark (Sum and Product of Symmetric Matrices).The sum of symmetric matrices A, B ∈ R n×n is always symmetric.However, although their product is always defined, it is generally not symmetric:Let us look at what happens to matrices when they are multiplied by a scalar λ ∈ R. Let A ∈ R m×n and λ ∈ R.Practically, λ scales each element of A. For λ, ψ ∈ R, the following holds:Linear Algebra associativity Associativity:Note that this allows us to move scalar values around.(λCdistributivity Distributivity:Example 2.5 (Distributivity) If we definethen for any λ, ψ ∈ R we obtain(2.34b)If we consider the system of linear equationsand use the rules for matrix multiplication, we can write this equation system in a more compact form as(2.36)Note that x 1 scales the first column, x 2 the second one, and x 3 the third one.Generally, a system of linear equations can be compactly represented in their matrix form as Ax = b; see (2.3), and the product Ax is a (linear) combination of the columns of A. We will discuss linear combinations in more detail in Section 2.5.Draft (2024-01-15) of "Mathematics for Machine Learning".Feedback: https://mml-book.com.In (2.3), we introduced the general form of an equation system, i.e.,where a ij ∈ R and b i ∈ R are known constants and x j are unknowns, i = 1, . . ., m, j = 1, . . ., n.Thus far, we saw that matrices can be used as a compact way of formulating systems of linear equations so that we can write Ax = b, see (2.10).Moreover, we defined basic matrix operations, such as addition and multiplication of matrices.In the following, we will focus on solving systems of linear equations and provide an algorithm for finding the inverse of a matrix.Before discussing how to generally solve systems of linear equations, let us have a look at an example.Consider the system of equations(2.38)The system has two equations and four unknowns.Therefore, in general we would expect infinitely many solutions.This system of equations is in a particularly easy form, where the first two columns consist of a 1 and a 0. Remember that we want to find scalars x 1 , . . ., x 4 , such that 4 i=1 x i c i = b, where we define c i to be the ith column of the matrix and b the right-hand-side of (2.38).A solution to the problem in (2.38) can be found immediately by taking 42 times the first column and 8 times the second column so thatTherefore, a solution is [42, 8, 0, 0] ⊤ .This solution is called a particular particular solution solution or special solution.However, this is not the only solution of this special solution system of linear equations.To capture all the other solutions, we need to be creative in generating 0 in a non-trivial way using the columns of the matrix: Adding 0 to our special solution does not change the special solution.To do so, we express the third column using the first two columns (which are of this very simple form)Linear Algebra so that 0 = 8c 1 + 2c 2 − 1c 3 + 0c 4 and (x 1 , x 2 , x 3 , x 4 ) = (8, 2, −1, 0).In fact, any scaling of this solution by λ 1 ∈ R produces the 0 vector, i.e.,(2.41)Following the same line of reasoning, we express the fourth column of the matrix in (2.38) using the first two columns and generate another set of non-trivial versions of 0 asfor any λ 2 ∈ R. Putting everything together, we obtain all solutions of the equation system in (2.38), which is called the general solution, as the set general solutionRemark.The general approach we followed consisted of the following three steps:Exchange of two equations (rows in the matrix representing the system of equations) Multiplication of an equation (row) with a constant λ ∈ R\{0} Addition of two equations (rows)For a ∈ R, we seek all solutions of the following system of equations:(2.44)We start by converting this system of equations into the compact matrix notation Ax = b.We no longer mention the variables x explicitly and build the augmented matrix (in the formwhere we used the vertical line to separate the left-hand side from the right-hand side in (2.44).We use ⇝ to indicate a transformation of the augmented matrix using elementary transformations.Swapping Rows 1 and 3 leads toWhen we now apply the indicated transformations (e.g., subtract Row 1 four times from Row 2), we obtain Linear AlgebraThis (augmented) matrix is in a convenient form, the row-echelon form row-echelon form (REF). Reverting this compact notation back into the explicit notation with the variables we seek, we obtain(2.45)Only for a = −1 this system can be solved.A particular solution is particular solutionThe general solution, which captures the set of all possible solutions, is general solutionIn the following, we will detail a constructive way to obtain a particular and general solution of a system of linear equations.Remark (Pivots and Staircase Structure).The leading coefficient of a row (first nonzero number from the left) is called the pivot and is always pivot strictly to the right of the pivot of the row above it.Therefore, any equation system in row-echelon form always has a "staircase" structure.♢ Definition 2.6 (Row-Echelon Form).A matrix is in row-echelon form if row-echelon form All rows that contain only zeros are at the bottom of the matrix; correspondingly, all rows that contain at least one nonzero element are on top of rows that contain only zeros.Looking at nonzero rows only, the first nonzero number from the left (also called the pivot or the leading coefficient) is always strictly to the pivot leading coefficient right of the pivot of the row above it.In other texts, it is sometimes required that the pivot is 1.our lives easier when we need to determine a particular solution.To do this, we express the right-hand side of the equation system using the pivot columns, such that b = P i=1 λ i p i , where p i , i = 1, . . ., P , are the pivot columns.The λ i are determined easiest if we start with the rightmost pivot column and work our way to the left.In the previous example, we would try to find λ 1 , λ 2 , λ 3 so that(2.48)From here, we find relatively directly that λ 3 = 1, λ 2 = −1, λ 1 = 2.When we put everything together, we must not forget the non-pivot columns for which we set the coefficients implicitly to 0. Therefore, we get the particular solution x = [2, 0, −1, 1, 0] ⊤ .♢ Remark (Reduced Row Echelon Form).An equation system is in reduced reduced row-echelon form row-echelon form (also: row-reduced echelon form or row canonical form) ifIt is in row-echelon form.Every pivot is 1.The pivot is the only nonzero entry in its column.The reduced row-echelon form will play an important role later in Section 2.3.3 because it allows us to determine the general solution of a system of linear equations in a straightforward way.Remark (Gaussian Elimination).Gaussian elimination is an algorithm that performs elementary transformations to bring a system of linear equations into reduced row-echelon form.♢Verify that the following matrix is in reduced row-echelon form (the pivots are in bold):(2.49)The key idea for finding the solutions of Ax = 0 is to look at the nonpivot columns, which we will need to express as a (linear) combination of the pivot columns.The reduced row echelon form makes this relatively straightforward, and we express the non-pivot columns in terms of sums and multiples of the pivot columns that are on their left: The second column is 3 times the first column (we can ignore the pivot columns on the right of the second column).Therefore, to obtain 0, we need to subtract the second column from three times the first column.Now, we look at the fifth column, which is our second non-pivot column.The fifth column can be expressed as 3 times the first pivot column, 9 times the second pivot column, and −4 times the third pivot column.We need to keep track of the indices of the pivot columns and translate this into 3 times the first column, 0 times the second column (which is a non-pivot column), 9 times the third column (which is our second pivot column), and −4 times the fourth column (which is the third pivot column).Then we need to subtract the fifth column to obtain 0. In the end, we are still solving a homogeneous equation system.To summarize, all solutions of Ax = 0, x ∈ R 5 are given by.(2.50)In the following, we introduce a practical trick for reading out the solutions x of a homogeneous system of linear equations Ax = 0, whereTo start, we assume that A is in reduced row-echelon form without any rows that just contain zeros, i.e.,where * can be an arbitrary real number, with the constraints that the first nonzero entry per row must be 1 and all other entries in the corresponding column must be 0. The columns j 1 , . . ., j k with the pivots (marked in bold) are the standard unit vectors e 1 , . . ., e k ∈ R k .We extend this matrix to an n × n-matrix Ã by adding n − k rows of the formso that the diagonal of the augmented matrix Ã contains either 1 or −1.Then, the columns of Ã that contain the −1 as pivots are solutions of Draft (2024-01-15) of "Mathematics for Machine Learning".Feedback: https://mml-book.com.the homogeneous equation system Ax = 0. To be more precise, these columns form a basis (Section 2.6.1) of the solution space of Ax = 0, which we will later call the kernel or null space (see Section 2.7.3).Example 2.8 (Minus-1 Trick) Let us revisit the matrix in (2.49), which is already in reduced REF:(2.53)We now augment this matrix to a 5 × 5 matrix by adding rows of the form (2.52) at the places where the pivots on the diagonal are missing and obtainFrom this form, we can immediately read out the solutions of Ax = 0 by taking the columns of Ã, which contain −1 on the diagonal:which is identical to the solution in (2.50) that we obtained by "insight".To compute the inverse A −1 of A ∈ R n×n , we need to find a matrix X that satisfies AX = I n .Then, X = A −1 .We can write this down as a set of simultaneous linear equations AX = I n , where we solve forWe use the augmented matrix notation for a compact representation of this set of systems of linear equations and obtain (2.56)This means that if we bring the augmented equation system into reduced row-echelon form, we can read out the inverse on the right-hand side of the equation system.Hence, determining the inverse of a matrix is equivalent to solving systems of linear equations.Linear AlgebraTo determine the inverse of(2.57)we write down the augmented matrixand use Gaussian elimination to bring it into reduced row-echelon formsuch that the desired inverse is given as its right-hand side:(2.58)We can verify that (2.58) is indeed the inverse by performing the multiplication AA −1 and observing that we recover I 4 .In the following, we briefly discuss approaches to solving a system of linear equations of the form Ax = b.We make the assumption that a solution exists.Should there be no solution, we need to resort to approximate solutions, which we do not cover in this chapter.One way to solve the approximate problem is using the approach of linear regression, which we discuss in detail in Chapter 9.In special cases, we may be able to determine the inverse A −1 , such that the solution of Ax = b is given as x = A −1 b.However, this is only possible if A is a square matrix and invertible, which is often not the case.Otherwise, under mild assumptions (i.e., A needs to have linearly independent columns) we can use the transformationDraft (2024-01-15) of "Mathematics for Machine Learning".Feedback: https://mml-book.com.and use the Moore-Penrose pseudo-inverse (A ⊤ A) −1 A ⊤ to determine the Moore-Penrose pseudo-inverse solution (2.59) that solves Ax = b, which also corresponds to the minimum norm least-squares solution.A disadvantage of this approach is that it requires many computations for the matrix-matrix product and computing the inverse of A ⊤ A. Moreover, for reasons of numerical precision it is generally not recommended to compute the inverse or pseudo-inverse.In the following, we therefore briefly discuss alternative approaches to solving systems of linear equations.Gaussian elimination plays an important role when computing determinants (Section 4.1), checking whether a set of vectors is linearly independent (Section 2.5), computing the inverse of a matrix (Section 2.2.2), computing the rank of a matrix (Section 2.6.2), and determining a basis of a vector space (Section 2.6.1).Gaussian elimination is an intuitive and constructive way to solve a system of linear equations with thousands of variables.However, for systems with millions of variables, it is impractical as the required number of arithmetic operations scales cubically in the number of simultaneous equations.In practice, systems of many linear equations are solved indirectly, by either stationary iterative methods, such as the Richardson method, the Jacobi method, the Gauß-Seidel method, and the successive over-relaxation method, or Krylov subspace methods, such as conjugate gradients, generalized minimal residual, or biconjugate gradients.We refer to the books by Stoer and Burlirsch (2002), Strang (2003), and Liesen and Mehrmann (2015 for further details.Let x * be a solution of Ax = b.The key idea of these iterative methods is to set up an iteration of the formfor suitable C and d that reduces the residual error ∥x (k+1) − x * ∥ in every iteration and converges to x * .We will introduce norms ∥ • ∥, which allow us to compute similarities between vectors, in Section 3.1.Thus far, we have looked at systems of linear equations and how to solve them (Section 2.3).We saw that systems of linear equations can be compactly represented using matrix-vector notation (2.10).In the following, we will have a closer look at vector spaces, i.e., a structured space in which vectors live.In the beginning of this chapter, we informally characterized vectors as objects that can be added together and multiplied by a scalar, and they remain objects of the same type.Now, we are ready to formalize this, and we will start by introducing the concept of a group, which is a set of elements and an operation defined on these elements that keeps some structure of the set intact.Groups play an important role in computer science.Besides providing a fundamental framework for operations on sets, they are heavily used in cryptography, coding theory, and graphics.Remark.The inverse element is defined with respect to the operation ⊗ and does not necessarily meanExample 2.10 (Groups) Let us have a look at some examples of sets with associated operations and see whether they are groups:(Z, +) is an Abelian group.(N 0 , +) is not a group: Although (N 0 , +) possesses a neutral element(0), the inverse elements are missing.(Z, •) is not a group: Although (Z, •) contains a neutral element (1), the inverse elements for any z ∈ Z, z ̸ = ±1, are missing.(R, •) is not a group since 0 does not possess an inverse element.(R m×n , +), the set of m × n-matrices is Abelian (with componentwise addition as defined in (2.61)).Let us have a closer look at (R n×n , •), i.e., the set of n × n-matrices with matrix multiplication as defined in (2.13).-Closure and associativity follow directly from the definition of matrix multiplication.-Neutral element: The identity matrix I n is the neutral element with respect to matrix multiplication "•" in (R n×n , •).-Inverse element: If the inverse exists (A is regular), then A −1 is the inverse element of A ∈ R n×n , and in exactly this case (R n×n , •) is a group, called the general linear group.Definition 2.8 (General Linear Group).The set of regular (invertible) matrices A ∈ R n×n is a group with respect to matrix multiplication as defined in (2.13) and is called general linear group GL(n, R).However, general linear group since matrix multiplication is not commutative, the group is not Abelian.When we discussed groups, we looked at sets G and inner operations on G, i.e., mappings G × G → G that only operate on elements in G.In the following, we will consider sets that in addition to an inner operation + also contain an outer operation •, the multiplication of a vector x ∈ G by a scalar λ ∈ R. We can think of the inner operation as a form of addition, and the outer operation as a form of scaling.Note that the inner/outer operations have nothing to do with inner/outer products.Definition 2.9 (Vector Space).A real-valued vector space V = (V, +, •) is vector space a set V with two operationswhere 1. (V, +) is an Abelian group 2. Distributivity:Neutral element with respect to the outer operation: ∀x ∈ V : 1•x = xThe elements x ∈ V are called vectors.The neutral element of (V, +) is vector the zero vector 0 = [0, . . ., 0] ⊤ , and the inner operation + is called vector vector addition addition.The elements λ ∈ R are called scalars and the outer operation scalar • is a multiplication by scalars.Note that a scalar product is something multiplication by scalars different, and we will get to this in Section 3.2.Remark.A "vector multiplication" ab, a, b ∈ R n , is not defined.Theoretically, we could define an element-wise multiplication, such that c = ab with c j = a j b j .This "array multiplication" is common to many programming languages but makes mathematically limited sense using the standard rules for matrix multiplication: By treating vectors as n × 1 matrices Linear Algebra(which we usually do), we can use the matrix multiplication as defined in (2.13).However, then the dimensions of the vectors do not match.Only the following multiplications for vectors are defined: ab ⊤ ∈ R n×n (outer outer product product), a ⊤ b ∈ R (inner/scalar/dot product).♢Let us have a look at some important examples:, n ∈ N is a vector space with operations defined as follows:-Section 2.2.Remember that R m×n is equivalent to R mn .V = C, with the standard definition of addition of complex numbers.Remark.In the following, we will denote a vector space (V, +, •) by V when + and • are the standard vector addition and scalar multiplication.Moreover, we will use the notation x ∈ V for vectors in V to simplify notation.♢Remark.The vector spaces R n , R n×1 , R 1×n are only different in the way we write vectors.In the following, we will not make a distinction between R n and R n×1 , which allows us to write n-tuples as column vectorsThis simplifies the notation regarding vector space operations.However, we do distinguish between R n×1 and R 1×n (the row vectors) to avoid conrow vector fusion with matrix multiplication.By default, we write x to denote a column vector, and a row vector is denoted by x ⊤ , the transpose of x. ♢ transpose Draft (2024-01-15) of "Mathematics for Machine Learning".Feedback: https://mml-book.com.39In the following, we will introduce vector subspaces.Intuitively, they are sets contained in the original vector space with the property that when we perform vector space operations on elements within this subspace, we will never leave it.In this sense, they are "closed".Vector subspaces are a key idea in machine learning.For example, Chapter 10 demonstrates how to use vector subspaces for dimensionality reduction.Definition 2.10 (Vector Subspace).Let V = (V, +, •) be a vector space and U ⊆ V, U ̸ = ∅.Then U = (U, +, •) is called vector subspace of V (or vector subspace linear subspace) if U is a vector space with the vector space operations + linear subspace and • restricted to U × U and R × U. We write U ⊆ V to denote a subspace U of V .If U ⊆ V and V is a vector space, then U naturally inherits many properties directly from V because they hold for all x ∈ V, and in particular for all x ∈ U ⊆ V.This includes the Abelian group properties, the distributivity, the associativity and the neutral element.To determine whether (U, +, •) is a subspace of V we still do need to show 1. U ̸ = ∅, in particular: 0 ∈ U 2. Closure of U : a.With respect to the outer operation: ∀λ ∈ R ∀x ∈ U : λx ∈ U. b.With respect to the inner operation: ∀x, y ∈ U : x + y ∈ U.Example 2.12 (Vector Subspaces) Let us have a look at some examples:For every vector space V , the trivial subspaces are V itself and {0}.Only example D in Figure 2.6 is a subspace of R 2 (with the usual inner/ outer operations).In A and C, the closure property is violated; B does not contain 0. The solution set of a homogeneous system of linear equations Ax = 0 with n unknowns x = [x 1 , . . ., x n ] ⊤ is a subspace of R n .The solution of an inhomogeneous system of linear equations Ax = b, b ̸ = 0 is not a subspace of R n .The intersection of arbitrarily many subspaces is a subspace itself.Remark.Every subspace U ⊆ (R n , +, •) is the solution space of a homogeneous system of linear equations Ax = 0 for x ∈ R n .♢In the following, we will have a close look at what we can do with vectors (elements of the vector space).In particular, we can add vectors together and multiply them with scalars.The closure property guarantees that we end up with another vector in the same vector space.It is possible to find a set of vectors with which we can represent every vector in the vector space by adding them together and scaling them.This set of vectors is a basis, and we will discuss them in Section 2.6.1.Before we get there, we will need to introduce the concepts of linear combinations and linear independence.Definition 2.11 (Linear Combination).Consider a vector space V and a finite number of vectors x 1 , . . .,linear combinationThe 0-vector can always be written as the linear combination of k vectors x 1 , . . ., x k because 0 = k i=1 0x i is always true.In the following, we are interested in non-trivial linear combinations of a set of vectors to represent 0, i.e., linear combinations of vectors x 1 , . . ., x k , where not all coefficients λ i in (2.65) are 0. Definition 2.12 (Linear (In)dependence).Let us consider a vector space V with k ∈ N and x 1 , . . ., x k ∈ V .If there is a non-trivial linear combination, such that 0 = k i=1 λ i x i with at least one λ i ̸ = 0, the vectors x 1 , . . ., x k are linearly dependent.If only the trivial solution exists, i.e., linearly dependent λ 1 = . . .= λ k = 0 the vectors x 1 , . . ., x k are linearly independent.linearly independent Linear independence is one of the most important concepts in linear algebra.Intuitively, a set of linearly independent vectors consists of vectors that have no redundancy, i.e., if we remove any of those vectors from the set, we will lose something.Throughout the next sections, we will formalize this intuition more.A geographic example may help to clarify the concept of linear independence.A person in Nairobi (Kenya) describing where Kigali (Rwanda) is might say ,"You can get to Kigali by first going 506 km Northwest to Kampala (Uganda) and then 374 km Southwest.".This is sufficient information to describe the location of Kigali because the geographic coordinate system may be considered a two-dimensional vector space (ignoring altitude and the Earth's curved surface).The person may add, "It is about 751 km West of here."Although this last statement is true, it is not necessary to find Kigali given the previous information (see Figure 2.7 for an illustration).In this example, the "506 km Northwest" vector (blue) and the "374 km Southwest" vector (purple) are linearly independent.This means the Southwest vector cannot be described in terms of the Northwest vector, and vice versa.However, the third "751 km West" vector (black) is a linear combination of the other two vectors, and it makes the set of vectors linearly dependent.Equivalently, given "751 km West" and "374 km Southwest" can be linearly combined to obtain "506 km Northwest".k vectors are either linearly dependent or linearly independent.There is no third option.If at least one of the vectors x 1 , . . ., x k is 0 then they are linearly dependent.The same holds if two vectors are identical.The vectors {x 1 , . . ., x k : x i ̸ = 0, i = 1, . . ., k}, k ⩾ 2, are linearly dependent if and only if (at least) one of them is a linear combination of the others.In particular, if one vector is a multiple of another vector, i.e., x i = λx j , λ ∈ R then the set {x 1 , . . ., x k : x i ̸ = 0, i = 1, . . ., k} is linearly dependent.A practical way of checking whether vectors x 1 , . . ., x k ∈ V are linearly independent is to use Gaussian elimination: Write all vectors as columns of a matrix A and perform Gaussian elimination until the matrix is in row echelon form (the reduced row-echelon form is unnecessary here):Linear Algebra -The pivot columns indicate the vectors, which are linearly independent of the vectors on the left.Note that there is an ordering of vectors when the matrix is built.-The non-pivot columns can be expressed as linear combinations of the pivot columns on their left.For instance, the row-echelon formtells us that the first and third columns are pivot columns.The second column is a non-pivot column because it is three times the first column.All column vectors are linearly independent if and only if all columns are pivot columns.If there is at least one non-pivot column, the columns (and, therefore, the corresponding vectors) are linearly dependent.Example 2.14 Consider R 4 with(2.67)To check whether they are linearly dependent, we follow the general approach and solvefor λ 1 , . . ., λ 3 .We write the vectors x i , i = 1, 2, 3, as the columns of a matrix and apply elementary row operations until we identify the pivot columns:(2.69)Here, every column of the matrix is a pivot column.Therefore, there is no non-trivial solution, and we require λ 1 = 0, λ 2 = 0, λ 3 = 0 to solve the equation system.Hence, the vectors x 1 , x 2 , x 3 are linearly independent.Remark.Consider a vector space V with k linearly independent vectors b 1 , . . ., b k and m linear combinations(2.70)as the matrix whose columns are the linearly independent vectors b 1 , . . ., b k , we can writein a more compact form.We want to test whether x 1 , . . ., x m are linearly independent.For this purpose, we follow the general approach of testing when m j=1 ψ j x j = 0.With (2.71), we obtain((2.73)Are the vectors x 1 , . . ., x 4 ∈ R n linearly independent?To answer this question, we investigate whether the column vectors are linearly independent.The reduced row-echelon form of the corresponding linear equation system with coefficient matrixis given as(2.76)We see that the corresponding linear equation system is non-trivially solvable: The last column is not a pivot column, andTherefore, x 1 , . . ., x 4 are linearly dependent as x 4 can be expressed as a linear combination of x 1 , . . ., x 3 .In a vector space V , we are particularly interested in sets of vectors A that possess the property that any vector v ∈ V can be obtained by a linear combination of vectors in A. These vectors are special vectors, and in the following, we will characterize them.Definition 2.13 (Generating Set and Span).Consider a vector space V = (V, +, •) and set of vectors A = {x 1 , . . ., x k } ⊆ V.If every vector v ∈ V can be expressed as a linear combination of x 1 , . . ., x k , A is called a generating set of V .The set of all linear combinations of vectors in A is generating set called the span of A. If A spans the vector space V , we writeGenerating sets are sets of vectors that span vector (sub)spaces, i.e., every vector can be represented as a linear combination of the vectors in the generating set.Now, we will be more specific and characterize the smallest generating set that spans a vector (sub)space.Definition 2.14 (Basis).Consider a vector space V = (V, +, •) and A ⊆ V.A generating set A of V is called minimal if there exists no smaller set minimal Ã ⊊ A ⊆ V that spans V .Every linearly independent generating set of V is minimal and is called a basis of V .basis Draft (2024-01-15) of "Mathematics for Machine Learning".Feedback: https://mml-book.com.Let V = (V, +, •) be a vector space and B ⊆ V, B ̸ = ∅.Then, the following statements are equivalent:A basis is a minimal generating set and a maximal linearly independent set of vectors.B is a basis of V .B is a minimal generating set.B is a maximal linearly independent set of vectors in V , i.e., adding any other vector to this set will make it linearly dependent.Every vector x ∈ V is a linear combination of vectors from B, and every linear combination is unique, i.e., withIn R 3 , the canonical/standard basis is canonical basisThe setis linearly independent, but not a generating set (and no basis) of R 4 : For instance, the vector [1, 0, 0, 0] ⊤ cannot be obtained by a linear combination of elements in A.Remark.Every vector space V possesses a basis B. The preceding examples show that there can be many bases of a vector space V , i.e., there is no unique basis.However, all bases possess the same number of elements, the basis vectors.We only consider finite-dimensional vector spaces V .In this case, the dimension of V is the number of basis vectors of V , and we write dim(V ).dimension If U ⊆ V is a subspace of V , then dim(U ) ⩽ dim(V ) and dim(U ) = Linear Algebra dim(V ) if and only if U = V .Intuitively, the dimension of a vector space can be thought of as the number of independent directions in this vector space.The dimension of a vector space corresponds to the number of its basis vectors.Remark.The dimension of a vector space is not necessarily the number of elements in a vector.For instance, the vector space V = span[ 0 1 ] is one-dimensional, although the basis vector possesses two elements.♢Remark.A basis of a subspace U = span[x 1 , . . ., x m ] ⊆ R n can be found by executing the following steps:1. Write the spanning vectors as columns of a matrix A 2. Determine the row-echelon form of A.3. The spanning vectors associated with the pivot columns are a basis of U .Example 2.17 (Determining a Basis) For a vector subspace U ⊆ R 5 , spanned by the vectorswe are interested in finding out which vectors x 1 , . . ., x 4 are a basis for U .For this, we need to check whether x 1 , . . ., x 4 are linearly independent.Therefore, we need to solvewhich leads to a homogeneous system of equations with matrix(2.83)With the basic transformation rules for systems of linear equations, we obtain the row-echelon form.Draft (2024-01-15) of "Mathematics for Machine Learning".Feedback: https://mml-book.com.Since the pivot columns indicate which set of vectors is linearly independent, we see from the row-echelon form that x 1 , x 2 , x 4 are linearly independent (because the system of linear equations λ 1 x 1 + λ 2 x 2 + λ 4 x 4 = 0 can only be solved with λ 1 = λ 2 = λ 4 = 0).Therefore, {x 1 , x 2 , x 4 } is a basis of U .The number of linearly independent columns of a matrix A ∈ R m×n equals the number of linearly independent rows and is called the rank rank of A and is denoted by rk(A).Remark.The rank of a matrix has some important properties: rk(A) = rk(A ⊤ ), i.e., the column rank equals the row rank.The columns of A ∈ R m×n span a subspace U ⊆ R m with dim(U ) = rk(A).Later we will call this subspace the image or range.A basis of U can be found by applying Gaussian elimination to A to identify the pivot columns.The rows of A ∈ R m×n span a subspace W ⊆ R n with dim(W ) = rk(A).A basis of W can be found by applying Gaussian elimination to A ⊤ .For all A ∈ R n×n it holds that A is regular (invertible) if and only if rk(A) = n.For all A ∈ R m×n and all b ∈ R m it holds that the linear equation system Ax = b can be solved if and only if rk(A) = rk(A|b), where A|b denotes the augmented system.For A ∈ R m×n the subspace of solutions for Ax = 0 possesses dimension n − rk(A).Later, we will call this subspace the kernel or the null kernel null spaceA matrix A ∈ R m×n has full rank if its rank equals the largest possible full rank rank for a matrix of the same dimensions.This means that the rank of a full-rank matrix is the lesser of the number of rows and columns, i.e., rk(A) = min(m, n).A matrix is said to be rank deficient if it does not rank deficient have full rank.Example 2.18 (Rank)A has two linearly independent rows/columns so that rk(A) = 2. 48We use Gaussian elimination to determine the rank:(2.84)Here, we see that the number of linearly independent rows and columns is 2, such that rk(A) = 2.In the following, we will study mappings on vector spaces that preserve their structure, which will allow us to define the concept of a coordinate.In the beginning of the chapter, we said that vectors are objects that can be added together and multiplied by a scalar, and the resulting object is still a vector.We wish to preserve this property when applying the mapping: Consider two real vector spaces V, W .A mapping Φ : V → W preserves the structure of the vector space if(2.86) for all x, y ∈ V and λ ∈ R. We can summarize this in the following definition:Definition 2.15 (Linear Mapping).For vector spaces V, W , a mapping Φ : V → W is called a linear mapping (or vector space homomorphism/ linear mapping vector space homomorphism linear transformation) if linear transformation ∀x, y ∈ V ∀λ, ψ ∈ R : Φ(λx + ψy) = λΦ(x) + ψΦ(y) .(2.87)It turns out that we can represent linear mappings as matrices (Section 2.7.1). Recall that we can also collect a set of vectors as columns of a matrix.When working with matrices, we have to keep in mind what the matrix represents: a linear mapping or a collection of vectors.We will see more about linear mappings in Chapter 4. Before we continue, we will briefly introduce special mappings.If Φ is surjective, then every element in W can be "reached" from V using Φ.A bijective Φ can be "undone", i.e., there exists a mapping Ψ : W → V so that Ψ • Φ(x) = x.This mapping Ψ is then called the inverse of Φ and normally denoted by Φ −1 .With these definitions, we introduce the following special cases of linear mappings between vector spaces V and W : isomorphism Isomorphism: Φ : V → W linear and bijective endomorphism Endomorphism: Φ : V → V linear automorphism Automorphism: Φ : V → V linear and bijective We define id V : V → V , x → x as the identity mapping or identity identity mapping identity automorphism automorphism in V .The mapping Φ : R 2 → C, Φ(x) = x 1 + ix 2 , is a homomorphism:(2.88)This also justifies why complex numbers can be represented as tuples in R 2 : There is a bijective linear mapping that converts the elementwise addition of tuples in R 2 into the set of complex numbers with the corresponding addition.Note that we only showed linearity, but not the bijection.Theorem 2.17 (Theorem 3.59 in Axler (2015)).Finite-dimensional vector spaces V and W are isomorphic if and only if dim(V ) = dim(W ).Theorem 2.17 states that there exists a linear, bijective mapping between two vector spaces of the same dimension.Intuitively, this means that vector spaces of the same dimension are kind of the same thing, as they can be transformed into each other without incurring any loss.Theorem 2.17 also gives us the justification to treat R m×n (the vector space of m × n-matrices) and R mn (the vector space of vectors of length mn) the same, as their dimensions are mn, and there exists a linear, bijective mapping that transforms one into the other.Remark.Consider vector spaces V, W, X. Then:For linear mappings Φ : V → W and Ψ : W → X, the mapping x♢Any n-dimensional vector space is isomorphic to R n (Theorem 2.17).We consider a basis {b 1 , . . ., b n } of an n-dimensional vector space V .In the following, the order of the basis vectors will be important.Therefore, we writeand call this n-tuple an ordered basis of V .is the coordinate vector/coordinate representation of x with respect to the coordinate vector coordinate representation ordered basis B.Draft (2024-01-15) of "Mathematics for Machine Learning".Feedback: https://mml-book.com.A basis effectively defines a coordinate system.We are familiar with the Cartesian coordinate system in two dimensions, which is spanned by the canonical basis vectors e 1 , e 2 .In this coordinate system, a vector x ∈ R 2 has a representation that tells us how to linearly combine e 1 and e 2 to obtain x.However, any basis of R 2 defines a valid coordinate system, and the same vector x from before may have a different coordinate representation in the (b 1 , b 2 ) basis.In Figure 2.8, the coordinates of x with respect to the standard basis (e 1 , e 2 ) is [2, 2] ⊤ .However, with respect to the basis (b 1 , b 2 ) the same vector x is represented as [1.09, 0.72] ⊤ , i.e., x = 1.09b 1 + 0.72b 2 .In the following sections, we will discover how to obtain this representation.Let us have a look at a geometric vectorDifferent coordinate representations of a vector x, depending on the choice of basis.with respect to the standard basis (e 1 , e 2 ) of R 2 .This means, we can write x = 2e 1 + 3e 2 .However, we do not have to choose the standard basis to represent this vector.If we use the basis vectors bRemark.For an n-dimensional vector space V and an ordered basis B of V , the mapping Φ : R n → V , Φ(e i ) = b i , i = 1, . . ., n, is linear (and because of Theorem 2.17 an isomorphism), where (e 1 , . . ., e n ) is the standard basis of R n .♢Now we are ready to make an explicit connection between matrices and linear mappings between finite-dimensional vector spaces.Moreover, we consider a linear mapping Φ : V → W .For j ∈ {1, . . ., n},is the unique representation of Φ(b j ) with respect to C.Then, we call the m × n-matrix A Φ , whose elements are given bytransformation matrix A Φ .If x is the coordinate vector of x ∈ V with respect to B and ŷ the coordinate vector of y = Φ(x) ∈ W with respect to C, then(2.94)This means that the transformation matrix can be used to map coordinates with respect to an ordered basis in V to coordinates with respect to an ordered basis in W .Consider a homomorphism Φ :where the α j , j = 1, 2, 3, are the coordinate vectors of Φ(b j ) with respect to C. We consider three linear transformations of a set of vectors in R 2 with the transformation matricesDraft (2024-01-15) of "Mathematics for Machine Learning".Feedback: https://mml-book.com.shows the original square from Figure 2.10(a) when linearly transformed using A 3 , which is a combination of a reflection, a rotation, and a stretch.In the following, we will have a closer look at how transformation matrices of a linear mapping Φ : V → W change if we change the bases in V and W . Consider two ordered basesof V and two ordered basesis the transformation matrix of the linear mapping Φ : V → W with respect to the bases B and C, and ÃΦ ∈ R m×n is the corresponding transformation mapping with respect to B and C.In the following, we will investigate how A and Ã are related, i.e., how/ whether we can transform A Φ into ÃΦ if we choose to perform a basis change from B, C to B, C.Remark.We effectively get different coordinate representations of the identity mapping id V .In the context of Figure 2.9, this would mean to map coordinates with respect to (e 1 , e 2 ) onto coordinates with respect to (b 1 , b 2 ) without changing the vector x.By changing the basis and correspondingly the representation of vectors, the transformation matrix with respect to this new basis can have a particularly simple form that allows for straightforward computation.♢Consider a transformation matrixwith respect to the canonical basis in R 2 .If we define a new basisLinear Algebrawe obtain a diagonal transformation matrixwith respect to B, which is easier to work with than A.In the following, we will look at mappings that transform coordinate vectors with respect to one basis into coordinate vectors with respect to a different basis.We will state our main result first and then provide an explanation.Theorem 2.20 (Basis Change).For a linear mapping Φ : V → W , ordered basesof W , and a transformation matrix A Φ of Φ with respect to B and C, the corresponding transformation matrix ÃΦ with respect to the bases B and C is given as(2.105)Here, S ∈ R n×n is the transformation matrix of id V that maps coordinates with respect to B onto coordinates with respect to B, and T ∈ R m×m is the transformation matrix of id W that maps coordinates with respect to C onto coordinates with respect to C.Proof Following Drumm and Weil (2001), we can write the vectors of the new basis B of V as a linear combination of the basis vectors of B,(2.106)Similarly, we write the new basis vectors C of W as a linear combination of the basis vectors of C, which yields(2.107)We define S = ((s ij )) ∈ R n×n as the transformation matrix that maps coordinates with respect to B onto coordinates with respect to B and T = ((t lk )) ∈ R m×m as the transformation matrix that maps coordinates with respect to C onto coordinates with respect to C. In particular, the jth column of S is the coordinate representation of bj with respect to B and Draft (2024-01-15) of "Mathematics for Machine Learning".Feedback: https://mml-book.com.the kth column of T is the coordinate representation of ck with respect to C. Note that both S and T are regular.We are going to look at Φ( bj ) from two perspectives.First, applying the mapping Φ, we get that for all j = 1, . . ., nwhere we first expressed the new basis vectors ck ∈ W as linear combinations of the basis vectors c l ∈ W and then swapped the order of summation.Alternatively, when we express the bj ∈ V as linear combinations of b j ∈ V , we arrive atwhere we exploited the linearity of Φ. Comparing (2.108) and (2.109b), it follows for all j = 1, . . ., n and l = 1, . . ., m that m k=1and, therefore,which proves Theorem 2.20.Theorem 2.20 tells us that with a basis change in V (B is replaced with B) and W (C is replaced with C), the transformation matrix A Φ of a linear mapping Φ : V → W is replaced by an equivalent matrix ÃΦ with(2.113)with respect to the bases in the subscripts.The corresponding transformation matrices are in red.VOrdered bases corresponding transformation matrix ÃΦ as follows: First, we find the matrix representation of the linear mapping Ψ B B : V → V that maps coordinates with respect to the new basis B onto the (unique) coordinates with respect to the "old" basis B (in V ).Then, we use the transformation matrix A Φ of Φ CB : V → W to map these coordinates onto the coordinates with respect to C in W . Finally, we use a linear mapping Ξ CC : W → W to map the coordinates with respect to C onto coordinates with respect to C. Therefore, we can express the linear mapping Φ C B as a composition of linear mappings that involve the "old" basis:(2.114)Concretely, we use, the identity mappings that map vectors onto themselves, but with respect to a different basis.Remark.Similar matrices are always equivalent.However, equivalent matrices are not necessarily similar.♢Remark.Consider vector spaces V, W, X.From the remark that follows Theorem 2.17, we already know that for linear mappings Φ : V → W and Ψ : W → X the mapping Ψ • Φ : V → X is also linear.With transformation matrices A Φ and A Ψ of the corresponding mappings, the overall transformation matrix isIn light of this remark, we can look at basis changes from the perspective of composing linear mappings:A Φ is the transformation matrix of a linear mapping Φ CB : V → W with respect to the bases B, C. ÃΦ is the transformation matrix of the linear mapping Φ C B : V → W with respect to the bases B, C. S is the transformation matrix of a linear mappingT is the transformation matrix of a linear mapping Ξ C C : W → W (automorphism) that represents C in terms of C. Normally, Ξ = id W is the identity mapping in W .If we (informally) write down the transformations just in terms of bases, then(2.116)Note that the execution order in (2.116) is from right to left because vectors are multiplied at the right-hand side so thatConsider a linear mapping Φ : R 3 → R 4 whose transformation matrix iswith respect to the standard bases(2.118)We seek the transformation matrix ÃΦ of Φ with respect to the new baseswhere the ith column of S is the coordinate representation of bi in terms of the basis vectors of B. Since B is the standard basis, the coordinate representation is straightforward to find.For a general basis B, we would need to solve a linear equation system to find the λ i such that i=1 λ i b i = bj , j = 1, . . ., 3. Similarly, the jth column of T is the coordinate representation of cj in terms of the basis vectors of C.Therefore, we obtain(2.121b)In Chapter 4, we will be able to exploit the concept of a basis change to find a basis with respect to which the transformation matrix of an endomorphism has a particularly simple (diagonal) form.In Chapter 10, we will look at a data compression problem and find a convenient basis onto which we can project the data while minimizing the compression loss.The image and kernel of a linear mapping are vector subspaces with certain important properties.In the following, we will characterize them more carefully.For Φ : V → W , we define the kernel/null spaceand the image/range image rangeWe also call V and W also the domain and codomain of Φ, respectively.domain codomainIntuitively, the kernel is the set of vectors v ∈ V that Φ maps onto the neutral element 0 W ∈ W .The image is the set of vectors w ∈ W that can be "reached" by Φ from any vector in V .An illustration is given in Figure 2.12.Remark.Consider a linear mapping Φ : V → W , where V, W are vector spaces.It always holds that Φ(0 V ) = 0 W and, therefore, 0 V ∈ ker(Φ).In particular, the null space is never empty.Im(Φ) ⊆ W is a subspace of W , and ker(Φ) ⊆ V is a subspace of V .Draft (2024-01-15) of "Mathematics for Machine Learning".Feedback: https://mml-book.com.Φ is injective (one-to-one) if and only if ker(Φ) = {0}.Remark (Null Space and Column Space).Let us consider A ∈ R m×n and a linear mapping Φ : R n → R m , x → Ax.where a i are the columns of A, we obtaini.e., the image is the span of the columns of A, also called the column column space space.Therefore, the column space (image) is a subspace of R m , where m is the "height" of the matrix.rk(A) = dim(Im(Φ)).The kernel/null space ker(Φ) is the general solution to the homogeneous system of linear equations Ax = 0 and captures all possible linear combinations of the elements in R n that produce 0 ∈ R m .The kernel is a subspace of R n , where n is the "width" of the matrix.The kernel focuses on the relationship among the columns, and we can use it to determine whether/how we can express a column as a linear combination of other columns.The mapping 60is linear.To determine Im(Φ), we can take the span of the columns of the transformation matrix and obtainTo compute the kernel (null space) of Φ, we need to solve Ax = 0, i.e., we need to solve a homogeneous equation system.To do this, we use Gaussian elimination to transform A into reduced row-echelon form:(2.127)This matrix is in reduced row-echelon form, and we can use the Minus-1 Trick to compute a basis of the kernel (see Section 2.3.3).Alternatively, we can express the non-pivot columns (columns 3 and 4) as linear combinations of the pivot columns (columns 1 and 2).The third column a 3 is equivalent to − 1 2 times the second column a 2 .Therefore, 0 = a 3 + 1 2 a 2 .In the same way, we see that a 4 = a 1 − 1 2 a 2 and, therefore, 0 = a 1 − 1 2 a 2 −a 4 .Overall, this gives us the kernel (null space) as(2.128) rank-nullity theorem Theorem 2.24 (Rank-Nullity Theorem).For vector spaces V, W and a linear mapping Φ :(2.129)The rank-nullity theorem is also referred to as the fundamental theorem fundamental theorem of linear mappings of linear mappings (Axler, 2015, theorem 3.22).The following are direct consequences of Theorem 2.24:, the kernel contains more than 0 V and dim(ker(Φ)) ⩾ 1.If A Φ is the transformation matrix of Φ with respect to an ordered basis and dim(Im(Φ)) < dim(V ), then the system of linear equations A Φ x = 0 has infinitely many solutions.If dim(V ) = dim(W ), then the three-way equivalenceDraft (2024-01-15) of "Mathematics for Machine Learning".Feedback: https://mml-book.com.In the following, we will take a closer look at spaces that are offset from the origin, i.e., spaces that are no longer vector subspaces.Moreover, we will briefly discuss properties of mappings between these affine spaces, which resemble linear mappings.Remark.In the machine learning literature, the distinction between linear and affine is sometimes not clear so that we can find references to affine spaces/mappings as linear spaces/mappings.♢Definition 2.25 (Affine Subspace).Let V be a vector space, x 0 ∈ V and U ⊆ V a subspace.Then the subsetis called affine subspace or linear manifold of V .U is called direction or affine subspace linear manifold direction direction space, and x 0 is called support point.In Chapter 12, we refer to direction space support point such a subspace as a hyperplane.hyperplane Note that the definition of an affine subspace excludes 0 if x 0 / ∈ U .Therefore, an affine subspace is not a (linear) subspace (vector subspace) of V for x 0 / ∈ U .Examples of affine subspaces are points, lines, and planes in R 3 , which do not (necessarily) go through the origin.Remark.Consider two affine subspaces L = x 0 + U and L = x0 + Ũ of a vector space V .Then, L ⊆ L if and only if U ⊆ Ũ and x 0 − x0 ∈ Ũ .Affine subspaces are often described by parameters:) is an ordered basis of U , then every element x ∈ L can be uniquely described asOne-dimensional affine subspaces are called lines and can be written line asThis means that a line is defined by a support point x 0 and a vector b 1 that defines the direction.See Figure 2.13 for an illustration.Two-dimensional affine subspaces of R n are called planes.The paraplane metric equation for planes is yThis means that a plane is defined by a support point x 0 and two linearly independent vectors b 1 , b 2 that span the direction space.In R n , the (n − 1)-dimensional affine subspaces are called hyperplanes, hyperplane and the corresponding parametric equation is yThis means that a hyperplane is defined by a support point x 0 and (n − 1) linearly independent vectors b 1 , . . ., b n−1 that span the direction space.In R 2 , a line is also a hyperplane.In R 3 , a plane is also a hyperplane.Remark (Inhomogeneous systems of linear equations and affine subspaces).For A ∈ R m×n and x ∈ R m , the solution of the system of linear equations Aλ = x is either the empty set or an affine subspace of R n of dimension n − rk(A).In particular, the solution of the linear equationIn R n , every k-dimensional affine subspace is the solution of an inhomogeneous system of linear equations Ax = b, where A ∈ R m×n , b ∈ R m and rk(A) = n − k.Recall that for homogeneous equation systems Ax = 0 the solution was a vector subspace, which we can also think of as a special affine space with support point x 0 = 0. ♢Similar to linear mappings between vector spaces, which we discussed in Section 2.7, we can define affine mappings between two affine spaces.Linear and affine mappings are closely related.Therefore, many properties that we already know from linear mappings, e.g., that the composition of linear mappings is a linear mapping, also hold for affine mappings.Definition 2.26 (Affine Mapping).For two vector spaces V, W , a linear mapping Φ : V → W , and a ∈ W , the mappingis an affine mapping from V to W .The vector a is called the translation affine mapping translation vector vector of ϕ.Every affine mapping ϕ : V → W is also the composition of a linear mapping Φ : V → W and a translation τIf ϕ is bijective, affine mappings keep the geometric structure invariant.They then also preserve the dimension and parallelism.There are many resources for learning linear algebra, including the textbooks by Strang (2003), Golan (2007), Axler (2015), and Liesen and Exercises 2.1 We consider (R\{−1}, ⋆), wherein the Abelian group (R\{−1}, ⋆), where ⋆ is defined in (2.134).2.2 Let n be in N\{0}.Let k, x be in Z.We define the congruence class k of the integer k as the setWe now define Z/nZ (sometimes written Zn) as the set of all congruence classes modulo n.Euclidean division implies that this set is a finite set containing n elements:a. Show that (Zn, ⊕) is a group.Is it Abelian?b.We now define another operation ⊗ for all a and b in Zn aswhere a × b represents the usual multiplication in Z.Let n = 5.Draw the times table of the elements of Z 5 \{0} under ⊗, i.e., calculate the products a ⊗ b for all a and b in Z 5 \{0}.Hence, show that Z 5 \{0} is closed under ⊗ and possesses a neutral element for ⊗.Display the inverse of all elements in Z 5 \{0} under ⊗.Conclude that (Z 5 \{0}, ⊗) is an Abelian group.c.Show that (Z 8 \{0}, ⊗) is not a group.d.We recall that the Bézout theorem states that two integers a and b are relatively prime (i.e., gcd(a, b) = 1) if and only if there exist two integers u and v such that au + bv = 1.Show that (Zn\{0}, ⊗) is a group if and only if n ∈ N\{0} is prime.Consider the set G of 3 × 3 matrices defined as follows:We define • as the standard matrix multiplication.Is (G, •) a group?If yes, is it Abelian?Justify your answer.Draft (2024-01-15) of "Mathematics for Machine Learning".Feedback: https://mml-book.com.2.5 Find the set S of all solutions in x of the following inhomogeneous linear systems Ax = b, where A and b are defined as follows:a.2.6 Using Gaussian elimination, find all solutions of the inhomogeneous equation system Ax = b withwhere2.8 Determine the inverses of the following matrices if possible:a.2.9 Which of the following sets are subspaces of R 3 ?2.10 Are the following sets of vectors linearly independent?a. 2024-01-15) of "Mathematics for Machine Learning".Feedback: https://mml-book.com.Exercises 67 2.12 Consider two subspaces of R 4 :Determine a basis of U 1 ∩ U 2 .2.13 Consider two subspaces U 1 and U 2 , where U 1 is the solution space of the homogeneous equation system A 1 x = 0 and U 2 is the solution space of the homogeneous equation system A 2 x = 0 with2.14 Consider two subspaces U 1 and U 2 , where U 1 is spanned by the columns of A 1 and U 2 is spanned by the columns of A 2 witha. Show that F and G are subspaces of R 3 .b. Calculate F ∩ G without resorting to any basis vector.c.Find one basis for F and one for G, calculate F ∩G using the basis vectors previously found and check your result with the previous question.2.16 Are the following mappings linear?2.17 Consider the linear mappingCompute the kernel and image of Φ.What are dim(ker(Φ)) and dim(Im(Φ))?2.18 Let E be a vector space.Let f and g be two automorphisms on E such that f • g = id E (i.e., f • g is the identity mapping id E ).Show that ker(f ) = ker(g • f ), Im(g) = Im(g • f ) and that ker(f ) ∩ Im(g) = {0 E }. 2.19 Consider an endomorphism Φ : R 3 → R 3 whose transformation matrix (with respect to the standard basis in R 3 ) isa. Determine ker(Φ) and Im(Φ).b.Determine the transformation matrix ÃΦ with respect to the basisi.e., perform a basis change toward the new basis B.and let us define two ordered basesDraft (2024-01-15) of "Mathematics for Machine Learning".Feedback: https://mml-book.com.a. Show that B and B ′ are two bases of R 2 and draw those basis vectors.b.Compute the matrix P 1 that performs a basis change from B ′ to B. c.We consider c 1 , c 2 , c 3 , three vectors of R 3 defined in the standard basis of R 3 as(i) Show that C is a basis of R 3 , e.g., by using determinants (see Section 4.1).3 ) the standard basis of R 3 .Determine the matrix P 2 that performs the basis change from C to C ′ .d.We consider a homomorphism Φ : R 2 −→ R 3 , such that c 3 ) are ordered bases of R 2 and R 3 , respectively.Determine the transformation matrix A Φ of Φ with respect to the ordered bases B and C. e. Determine A ′ , the transformation matrix of Φ with respect to the basesIn other words,(ii) Based on that, compute the coordinates of Φ(x) expressed in C. (iii) Then, write Φ(x) in terms of c ′ In Chapter 2, we studied vectors, vector spaces, and linear mappings at a general but abstract level.In this chapter, we will add some geometric interpretation and intuition to all of these concepts.In particular, we will look at geometric vectors and compute their lengths and distances or angles between two vectors.To be able to do this, we equip the vector space with an inner product that induces the geometry of the vector space.Inner products and their corresponding norms and metrics capture the intuitive notions of similarity and distances, which we use to develop the support vector machine in Chapter 12.We will then use the concepts of lengths and angles between vectors to discuss orthogonal projections, which will play a central role when we discuss principal component analysis in Chapter 10 and regression via maximum likelihood estimation in Chapter 9. Figure 3.1 gives an overview of how concepts in this chapter are related and how they are connected to other chapters of the book.When we think of geometric vectors, i.e., directed line segments that start at the origin, then intuitively the length of a vector is the distance of the "end" of this directed line segment from the origin.In the following, we will discuss the notion of the length of vectors using the concept of a norm.which assigns each vector x its length ∥x∥ ∈ R, such that for all λ ∈ R length and x, y ∈ V the following hold: In geometric terms, the triangle inequality states that for any triangle, the sum of the lengths of any two sides must be greater than or equal to the length of the remaining side; see Figure 3.2 for an illustration.Definition 3.1 is in terms of a general vector space V (Section 2.4), but in this book we will only consider a finite-dimensional vector space R n .Recall that for a vector x ∈ R n we denote the elements of the vector using a subscript, that is, x i is the i th element of the vector x.The Manhattan norm on R n is defined for x ∈ R n as Manhattan normwhere | • | is the absolute value.The left panel of Figure 3.3 shows all vectors x ∈ R 2 with ∥x∥ 1 = 1.The Manhattan norm is also called ℓ 1 ℓ 1 norm norm.The Euclidean norm of x ∈ R n is defined as Euclidean normand computes the Euclidean distance of x from the origin.The right panel Euclidean distance of Figure 3.3 shows all vectors x ∈ R 2 with ∥x∥ 2 = 1.The Euclidean norm is also called ℓ 2 norm.Remark.Throughout this book, we will use the Euclidean norm (3.4) by default if not stated otherwise.♢Inner products allow for the introduction of intuitive geometrical concepts, such as the length of a vector and the angle or distance between two vectors.A major purpose of inner products is to determine whether vectors are orthogonal to each other.We may already be familiar with a particular type of inner product, the scalar product/dot product in R n , which is given by scalar product dot productx ⊤ y = n i=1x i y i .(3.5)We will refer to this particular inner product as the dot product in this book.However, inner products are more general concepts with specific properties, which we will now introduce.Recall the linear mapping from Section 2.7, where we can rearrange the mapping with respect to addition and multiplication with a scalar.A bibilinear mapping linear mapping Ω is a mapping with two arguments, and it is linear in each argument, i.e., when we look at a vector space V then it holds that for all x, y, z ∈ V, λ, ψ ∈ R that(3.7)Here, (3.6) asserts that Ω is linear in the first argument, and (3.7) asserts that Ω is linear in the second argument (see also (2.87)).Definition 3.2.Let V be a vector space and Ω : V × V → R be a bilinear mapping that takes two vectors and maps them onto a real number.Then Ω is called symmetric if Ω(x, y) = Ω(y, x) for all x, y ∈ V , i.e., the symmetric order of the arguments does not matter.Definition 3.3.Let V be a vector space and Ω : V × V → R be a bilinear mapping that takes two vectors and maps them onto a real number.Then A positive definite, symmetric bilinear mapping Ω : V ×V → R is called an inner product on V .We typically write ⟨x, y⟩ instead of Ω(x, y).inner productThe pair (V, ⟨•, •⟩) is called an inner product space or (real) vector space inner product space vector space with inner product with inner product.If we use the dot product defined in (3.5), we call (V, ⟨•, •⟩) a Euclidean vector space.We will refer to these spaces as inner product spaces in this book.then ⟨•, •⟩ is an inner product but different from the dot product.The proof will be an exercise.Symmetric, positive definite matrices play an important role in machine learning, and they are defined via the inner product.In Section 4.3, we will return to symmetric, positive definite matrices in the context of matrix decompositions.The idea of symmetric positive semidefinite matrices is key in the definition of kernels (Section 12.4).Consider an n-dimensional vector space V with an inner product ⟨•, •⟩ :Recall from Section 2.6.1 that any vectors x, y ∈ V can be written as linear combinations of the basis vectors so thatwhere A ij := ⟨b i , b j ⟩ and x, ŷ are the coordinates of x and y with respect to the basis B. This implies that the inner product ⟨•, •⟩ is uniquely determined through A. The symmetry of the inner product also means that A is symmetric.Furthermore, the positive definiteness of the inner product implies that ∀x ∈ V \{0} : x ⊤ Ax > 0 .(3.11) (3.12)A 1 is positive definite because it is symmetric anddefines an inner product with respect to an ordered basis B, where x and ŷ are the coordinate representations of x, y ∈ V with respect to B.Theorem 3.5.For a real-valued, finite-dimensional vector space V and an ordered basis The following properties hold if A ∈ R n×n is symmetric and positive definite:The null space (kernel) of A consists only of 0 because x ⊤ Ax > 0 for all x ̸ = 0.This implies that Ax ̸ = 0 if x ̸ = 0.The diagonal elements a ii of A are positive because a ii = e ⊤ i Ae i > 0, where e i is the ith vector of the standard basis in R n .In Section 3.1, we already discussed norms that we can use to compute the length of a vector.Inner products and norms are closely related in the sense that any inner product induces a norm Inner products induce norms.in a natural way, such that we can compute lengths of vectors using the inner product.However, not every norm is induced by an inner product.The Manhattan norm (3.3) is an example of a norm without a corresponding inner product.In the following, we will focus on norms that are induced by inner products and introduce geometric concepts, such as lengths, distances, and angles.Remark (Cauchy-Schwarz Inequality).For an inner product vector spaceIn geometry, we are often interested in lengths of vectors.We can now use an inner product to compute them using (3.16).Let us takeIf we use the dot product as the inner product, with (3.16) we obtainas the length of x.Let us now choose a different inner product:If we compute the norm of a vector, then this inner product returns smaller values than the dot product if x 1 and x 2 have the same sign (and x 1 x 2 > 0); otherwise, it returns greater values than the dot product.With this inner product, we obtainsuch that x is "shorter" with this inner product than with the dot product.The mappingis called a metric.metric Remark.Similar to the length of a vector, the distance between vectors does not require an inner product: a norm is sufficient.If we have a norm induced by an inner product, the distance may vary depending on the choice of the inner product.♢A metric d satisfies the following:Remark.At first glance, the lists of properties of inner products and metrics look very similar.However, by comparing Definition 3.3 with Definition 3.6 we observe that ⟨x, y⟩ and d(x, y) behave in opposite directions.Very similar x and y will result in a large value for the inner product and a small value for the metric.♢ In addition to enabling the definition of lengths of vectors, as well as the distance between two vectors, inner products also capture the geometry of a vector space by defining the angle ω between two vectors.We use the Cauchy-Schwarz inequality (3.17) to define angles ω in inner product spaces between two vectors x, y, and this notion coincides with our intuition in R 2 and R 3 .Assume that x ̸ = 0, y ̸ = 0. ThenTherefore, there exists a unique ω ∈ [0, π], illustrated in Figure 3.4, withThe number ω is the angle between the vectors x and y.Intuitively, the angle angle between two vectors tells us how similar their orientations are.For example, using the dot product, the angle between x and y = 4x, i.e., y is a scaled version of x, is 0: Their orientation is the same.Example 3.6 (Angle between Vectors) Let us compute the angle betweenThe angle ω between two vectors x, y is computed using the inner product.A key feature of the inner product is that it also allows us to characterize vectors that are orthogonal.Definition 3.7 (Orthogonality).Two vectors x and y are orthogonal if and orthogonal only if ⟨x, y⟩ = 0, and we write x ⊥ y.If additionally ∥x∥ = 1 = ∥y∥, i.e., the vectors are unit vectors, then x and y are orthonormal.orthonormal An implication of this definition is that the 0-vector is orthogonal to every vector in the vector space.Remark.Orthogonality is the generalization of the concept of perpendicularity to bilinear forms that do not have to be the dot product.In our context, geometrically, we can think of orthogonal vectors as having a right angle with respect to a specific inner product.♢ Example 3.7 (Orthogonal Vectors)Figure 3.6 The angle ω between two vectors x, y can change depending on the inner product.We are interested in determining the angle ω between them using two different inner products.Using the dot product as the inner product yields an angle ω between x and y of 90 • , such that x ⊥ y.However, if we choose the inner productwe get that the angle ω between x and y is given byand x and y are not orthogonal.Therefore, vectors that are orthogonal with respect to one inner product do not have to be orthogonal with respect to a different inner product.Definition 3.8 (Orthogonal Matrix).A square matrix A ∈ R n×n is an orthogonal matrix if and only if its columns are orthonormal so that orthogonal matrixi.e., the inverse is obtained by simply transposing the matrix.It is convention to call these matrices "orthogonal" but a more precise description would be "orthonormal".Transformations by orthogonal matrices are special because the length of a vector x is not changed when transforming it using an orthogonal matrix A. For the dot product, we obtain Transformations with orthogonal matrices preserve distances and angles.Moreover, the angle between any two vectors x, y, as measured by their inner product, is also unchanged when transforming both of them using an orthogonal matrix A. Assuming the dot product as the inner product, the angle of the images Ax and Ay is given aswhich gives exactly the angle between x and y.This means that orthogonal matrices A with A ⊤ = A −1 preserve both angles and distances.It turns out that orthogonal matrices define transformations that are rotations (with the possibility of flips).In Section 3.9, we will discuss more details about rotations.In Section 2.6.1, we characterized properties of basis vectors and found that in an n-dimensional vector space, we need n basis vectors, i.e., n vectors that are linearly independent.In Sections 3.3 and 3.4, we used inner products to compute the length of vectors and the angle between vectors.In the following, we will discuss the special case where the basis vectors are orthogonal to each other and where the length of each basis vector is 1.We will call this basis then an orthonormal basis.Draft (2024-01-15) of "Mathematics for Machine Learning".Feedback: https://mml-book.com.Let us introduce this more formally.Definition 3.9 (Orthonormal Basis).Consider an n-dimensional vector space V and a basis {b 1 , . . ., b n } of V .Iffor all i, j = 1, . . ., n then the basis is called an orthonormal basis (ONB).orthonormal basis ONB If only (3.33) is satisfied, then the basis is called an orthogonal basis.Note orthogonal basis that (3.34) implies that every basis vector has length/norm 1.Recall from Section 2.6.1 that we can use Gaussian elimination to find a basis for a vector space spanned by a set of vectors.Assume we are given a set { b1 , . . ., bn } of non-orthogonal and unnormalized basis vectors.We concatenate them into a matrix B = [ b1 , . . ., bn ] and apply Gaussian elimination to the augmented matrix (Section 2.3.2) [ B B⊤ | B] to obtain an orthonormal basis.This constructive way to iteratively build an orthonormal basis {b 1 , . . ., b n } is called the Gram-Schmidt process (Strang, 2003).The canonical/standard basis for a Euclidean vector space R n is an orthonormal basis, where the inner product is the dot product of vectors.In R 2 , the vectorsform an orthonormal basis since b ⊤ 1 b 2 = 0 and ∥b 1 ∥ = 1 = ∥b 2 ∥.We will exploit the concept of an orthonormal basis in Chapter 12 and Chapter 10 when we discuss support vector machines and principal component analysis.Having defined orthogonality, we will now look at vector spaces that are orthogonal to each other.This will play an important role in Chapter 10, when we discuss linear dimensionality reduction from a geometric perspective.Consider a D-dimensional vector space V and an M -dimensional subspace U ⊆ V .Then its orthogonal complement U ⊥ is a (D−M )-dimensional orthogonal complement subspace of V and contains all vectors in V that are orthogonal to every vector in U .Furthermore, U ∩ U ⊥ = {0} so that any vector x ∈ V can beTherefore, the orthogonal complement can also be used to describe a plane U (two-dimensional subspace) in a three-dimensional vector space.More specifically, the vector w with ∥w∥ = 1, which is orthogonal to the plane U , is the basis vector of U ⊥ .Figure 3.7 illustrates this setting.All vectors that are orthogonal to w must (by construction) lie in the plane U .The vector w is called the normal vector of U .Generally, orthogonal complements can be used to describe hyperplanes in n-dimensional vector and affine spaces.Thus far, we looked at properties of inner products to compute lengths, angles and distances.We focused on inner products of finite-dimensional vectors.In the following, we will look at an example of inner products of a different type of vectors: inner products of functions.The inner products we discussed so far were defined for vectors with a finite number of entries.We can think of a vector x ∈ R n as a function with n function values.The concept of an inner product can be generalized to vectors with an infinite number of entries (countably infinite) and also continuous-valued functions (uncountably infinite).Then the sum over individual components of vectors (see Equation (3.5) for example) turns into an integral.An inner product of two functions u : R → R and v : R → R can be defined as the definite integralDraft (2024-01-15) of "Mathematics for Machine Learning".Feedback: https://mml-book.com.for lower and upper limits a, b < ∞, respectively.As with our usual inner product, we can define norms and orthogonality by looking at the inner product.If (3.37) evaluates to 0, the functions u and v are orthogonal.To make the preceding inner product mathematically precise, we need to take care of measures and the definition of integrals, leading to the definition of a Hilbert space.Furthermore, unlike inner products on finite-dimensional vectors, inner products on functions may diverge (have infinite value).All this requires diving into some more intricate details of real and functional analysis, which we do not cover in this book.Example 3.9 (Inner Product of Functions)of (3.37), is shown in Figure 3.8.We see that this function is odd, i.e., f (−x) = −f (x).Therefore, the integral with limits a = −π, b = π of this product evaluates to 0. Therefore, sin and cos are orthogonal functions.Remark.It also holds that the collection of functionsis orthogonal if we integrate from −π to π, i.e., any pair of functions are orthogonal to each other.The collection of functions in (3.38) spans a large subspace of the functions that are even and periodic on [−π, π), and projecting functions onto this subspace is the fundamental idea behind Fourier series.♢In Section 6.4.6, we will have a look at a second type of unconventional inner products: the inner product of random variables.Projections are an important class of linear transformations (besides rotations and reflections) and play an important role in graphics, coding theory, statistics and machine learning.In machine learning, we often deal with data that is high-dimensional.High-dimensional data is often hard to analyze or visualize.However, high-dimensional data quite often possesses the property that only a few dimensions contain most information, and most other dimensions are not essential to describe key properties of the data.When we compress or visualize high-dimensional data, we will lose information.To minimize this compression loss, we ideally find the most informative dimensions in the data.As discussed in Chapter 1, "Feature" is a common expression for data representation.data can be represented as vectors, and in this chapter, we will discuss some of the fundamental tools for data compression.More specifically, we can project the original high-dimensional data onto a lower-dimensional feature space and work in this lower-dimensional space to learn more about the dataset and extract relevant patterns.For example, machinelearning algorithms, such as principal component analysis (PCA) by Pearson (1901) and Hotelling (1933) and deep neural networks (e.g., deep auto-encoders (Deng et al., 2010)), heavily exploit the idea of dimensionality reduction.In the following, we will focus on orthogonal projections, which we will use in Chapter 10 for linear dimensionality reduction and in Chapter 12 for classification.Even linear regression, which we discuss in Chapter 9, can be interpreted using orthogonal projections.For a given lower-dimensional subspace, orthogonal projections of high-dimensional data retain as much information as possible and minimize the difference/ error between the original data and the corresponding projection.An illustration of such an orthogonal projection is given in Figure 3.9.Before we detail how to obtain these projections, let us define what a projection actually is.Definition 3.10 (Projection).Let V be a vector space and U ⊆ V a subspace of V .A linear mapping π :Since linear mappings can be expressed by transformation matrices (see Section 2.7), the preceding definition applies equally to a special kind of transformation matrices, the projection matrices P π , which exhibit the projection matrix property that P 2 π = P π .In the following, we will derive orthogonal projections of vectors in the inner product space (R n , ⟨•, •⟩) onto subspaces.We will start with onedimensional subspaces, which are also called lines.If not mentioned othline erwise, we assume the dot product ⟨x, y⟩ = x ⊤ y as the inner product.Assume we are given a line (one-dimensional subspace) through the origin with basis vector b ∈ R n .The line is a one-dimensional subspace U ⊆ R n spanned by b.When we project x ∈ R n onto U , we seek the vector π U (x) ∈ U that is closest to x.Using geometric arguments, let us characterize some properties of the projection π U (x) (Figure 3.10(a) serves as an illustration):The projection π U (x) is closest to x, where "closest" implies that the distance ∥x − π U (x)∥ is minimal.It follows that the segment π U (x) − x from π U (x) to x is orthogonal to U , and therefore the basis vector b of U .The orthogonality condition yields ⟨π U (x) − x, b⟩ = 0 since angles between vectors are defined via the inner product.λ is then the coordinate of π U (x) with respect to b.The projection π U (x) of x onto U must be an element of U and, therefore, a multiple of the basis vector b that spans U .Hence, π U (x) = λb, for some λ ∈ R.In the following three steps, we determine the coordinate λ, the projection π U (x) ∈ U , and the projection matrix P π that maps any x ∈ R n onto U :1. Finding the coordinate λ.The orthogonality condition yields(3.39)We can now exploit the bilinearity of the inner product and arrive at With a general inner product, we getIn the last step, we exploited the fact that inner products are symmetric.If we choose ⟨•, •⟩ to be the dot product, we obtain 2. Finding the projection point π U (x) ∈ U .Since π U (x) = λb, we immediately obtain with (3.40) thatwhere the last equality holds for the dot product only.We can also compute the length of π U (x) by means of Definition 3.1 asHence, our projection is of length |λ| times the length of b.This also adds the intuition that λ is the coordinate of π U (x) with respect to the basis vector b that spans our one-dimensional subspace U .If we use the dot product as an inner product, we getHere, ω is the angle between x and b.This equation should be familiar from trigonometry: If ∥x∥ = 1, then x lies on the unit circle.It follows that the projection onto the horizontal axis spanned by b is exactlyThe horizontal axis is a one-dimensional subspace.cos ω, and the length of the corresponding vector π U (x) = |cos ω|.An illustration is given in Figure 3.10(b).3. Finding the projection matrix P π .We know that a projection is a linear mapping (see Definition 3.10).Therefore, there exists a projection matrix P π , such that π U (x) = P π x.With the dot product as inner product andwe immediately see thatNote that bb ⊤ (and, consequently, P π ) is a symmetric matrix (of rank Projection matrices are always symmetric.1), and ∥b∥ 2 = ⟨b, b⟩ is a scalar.The projection matrix P π projects any vector x ∈ R n onto the line through the origin with direction b (equivalently, the subspace U spanned by b).Remark.The projection π U (x) ∈ R n is still an n-dimensional vector and not a scalar.However, we no longer require n coordinates to represent the projection, but only a single one if we want to express it with respect to the basis vector b that spans the subspace U : λ. ♢ With (3.46), we obtainLet us now choose a particular x and see whether it lies in the subspace spanned by b.For x = 1 1 1 ⊤ , the projection isNote that the application of P π to π U (x) does not change anything, i.e., P π π U (x) = π U (x).This is expected because according to Definition 3.10, we know that a projection matrix P π satisfies P 2 π x = P π x for all x.Remark.With the results from Chapter 4, we can show that π U (x) is an eigenvector of P π , and the corresponding eigenvalue is 1. ♢If U is given by a set of spanning vectors, which are not a basis, make sure you determine a basis b 1 , . . ., bm before proceeding.In the following, we look at orthogonal projections of vectors x ∈ R n onto lower-dimensional subspaces U ⊆ R n with dim(U ) = m ⩾ 1.An illustration is given in Figure 3.As in the 1D case, we follow a three-step procedure to find the projection π U (x) and the projection matrix P π :1. Find the coordinates λ 1 , . . ., λ m of the projection (with respect to the basis of U ), such that the linear combinationAs in the 1D case, "closest" means "minimum distance", which implies that the vector connecting π U (x) ∈ U and x ∈ R n must be orthogonal to all basis vectors of U .Therefore, we obtain m simultaneous conditions (assuming the dot product as the inner product)such that we obtain a homogeneous linear equation system The matrix (B ⊤ B) −1 B ⊤ is also called the pseudo-inverse of B, which pseudo-inverse can be computed for non-square matrices B. It only requires that B ⊤ B is positive definite, which is the case if B is full rank.In practical applications (e.g., linear regression), we often add a "jitter term" ϵI to B ⊤ B to guarantee increased numerical stability and positive definiteness.This "ridge" can be rigorously derived using Bayesian inference.See Chapter 9 for details. 2. Find the projection π U (x) ∈ U .We already established that π U (x) = Bλ.Therefore, with (3.57)(3.58)3. Find the projection matrix P π .From (3.58), we can immediately see that the projection matrix that solves P π x = π U (x) must beRemark.The solution for projecting onto general subspaces includes the 1D case as a special case: If dim(U ) = 1, then B ⊤ B ∈ R is a scalar and we can rewrite the projection matrix in (3.59)Second, we compute the matrix B ⊤ B and the vector B ⊤ x as(3.60)Third, we solve the normal equation B ⊤ Bλ = B ⊤ x to find λ:Fourth, the projection π U (x) of x onto U , i.e., into the column space of B, can be directly computed via 88The corresponding projection error is the norm of the difference vector projection error between the original vector and its projection onto U , i.e.,The projection error is also called the reconstruction error.Fifth, the projection matrix (for any x ∈ R 3 ) is given byTo verify the results, we can (a) check whether the displacement vector π U (x) − x is orthogonal to all basis vectors of U , and (b) verify that P π = P 2 π (see Definition 3.10).Remark.The projections π U (x) are still vectors in R n although they lie in an m-dimensional subspace U ⊆ R n .However, to represent a projected vector we only need the m coordinates λ 1 , . . ., λ m with respect to the basis vectors b 1 , . . ., b m of U .♢Remark.In vector spaces with general inner products, we have to pay attention when computing angles and distances, which are defined by means of the inner product.♢We can find approximate solutions to unsolvable linear equation systems using projections.Projections allow us to look at situations where we have a linear system Ax = b without a solution.Recall that this means that b does not lie in the span of A, i.e., the vector b does not lie in the subspace spanned by the columns of A. Given that the linear equation cannot be solved exactly, we can find an approximate solution.The idea is to find the vector in the subspace spanned by the columns of A that is closest to b, i.e., we compute the orthogonal projection of b onto the subspace spanned by the columns of A. This problem arises often in practice, and the solution is called the least-squares solution (assuming the dot product as the inner product) of least-squares solution an overdetermined system.This is discussed further in Section 9.4.Using reconstruction errors (3.63) is one possible approach to derive principal component analysis (Section 10.3).Remark.We just looked at projections of vectors x onto a subspace U with basis vectors {b 1 , . . ., b k }.If this basis is an ONB, i.e., (3.33) and (3.34) are satisfied, the projection equation (3.58) simplifies greatly toThis means that we no longer have to compute the inverse from (3.58), which saves computation time.♢Draft (2024-01-15) of "Mathematics for Machine Learning".Feedback: https://mml-book.com.see also Figure 3.12(a).Using the Gram-Schmidt method, we construct an orthogonal basis (u 1 , u 2 ) of R 2 as follows (assuming the dot product as the inner product): LThese steps are illustrated in Figures 3.12(b) and (c).We immediately see that u 1 and u 2 are orthogonal, i.e., u ⊤ 1 u 2 = 0.Thus far, we discussed how to project a vector onto a lower-dimensional subspace U .In the following, we provide a solution to projecting a vector onto an affine subspace.Consider the setting in Figure 3.13(a).We are given an affine space L = x 0 + U , where b 1 , b 2 are basis vectors of U .To determine the orthogonal projection π L (x) of x onto L, we transform the problem into a problem that we know how to solve: the projection onto a vector subspace.In order to get there, we subtract the support point x 0 from x and from L, so that L − x 0 = U is exactly the vector subspace U .We can now use the orthogonal projections onto a subspace we discussed in Section 3.8.2 and obtain the projection π U (x − x 0 ), which is illustrated in Figure 3.13(b).This projection can now be translated back into L by adding x 0 , such that we obtain the orthogonal projection onto an affine space L aswhere π U (•) is the orthogonal projection onto the subspace U , i.e., the direction space of L; see Figure 3.13(c).From Figure 3.13, it is also evident that the distance of x from the affine space L is identical to the distance of x − x 0 from U , i.e.,(3.73b)We will use projections onto an affine subspace to derive the concept of a separating hyperplane in Section 12.1.Draft (2024-01-15) of "Mathematics for Machine Learning".Feedback: https://mml-book.com.Length and angle preservation, as discussed in Section 3.4, are the two characteristics of linear mappings with orthogonal transformation matrices.In the following, we will have a closer look at specific orthogonal transformation matrices, which describe rotations.A rotation is a linear mapping (more specifically, an automorphism of rotation a Euclidean vector space) that rotates a plane by an angle θ about the origin, i.e., the origin is a fixed point.For a positive angle θ > 0, by common convention, we rotate in a counterclockwise direction.An example is shown in Figure 3Consider the standard basis e 1 = 1 0 , e 2 = 0 1 of R 2 , which defines the standard coordinate system in R 2 .We aim to rotate this coordinate system by an angle θ as illustrated in Figure 3.16.Note that the rotated vectors are still linearly independent and, therefore, are a basis of R 2 .This means that the rotation performs a basis change.Rotations Φ are linear mappings so that we can express them by a rotation matrix R(θ).Trigonometry (see Figure 3 (3.76)In contrast to the R 2 case, in R 3 we can rotate any two-dimensional plane about a one-dimensional axis.The easiest way to specify the general rotation matrix is to specify how the images of the standard basis e 1 , e 2 , e 3 are supposed to be rotated, and making sure these images Re 1 , Re 2 , Re 3 are orthonormal to each other.We can then obtain a general rotation matrix R by combining the images of the standard basis.To have a meaningful rotation angle, we have to define what "counterclockwise" means when we operate in more than two dimensions.We use the convention that a "counterclockwise" (planar) rotation about an axis refers to a rotation about an axis when we look at the axis "head on, from the end toward the origin".In R 3 , there are therefore three (planar) rotations about the three standard basis vectors (see Figure 3.17):Draft (2024-01-15) of "Mathematics for Machine Learning".Feedback: https://mml-book.com.Here, the e 1 coordinate is fixed, and the counterclockwise rotation is performed in the e 2 e 3 plane.Rotation about the e 2 -axis(3.78)If we rotate the e 1 e 3 plane about the e 2 axis, we need to look at the e 2 axis from its "tip" toward the origin.Rotation about the e 3 -axis(3.79)Figure 3.17 illustrates this.The generalization of rotations from 2D and 3D to n-dimensional Euclidean vector spaces can be intuitively described as fixing n − 2 dimensions and restrict the rotation to a two-dimensional plane in the n-dimensional space.As in the three-dimensional case, we can rotate any plane (two-dimensional subspace of R n ).Definition 3.11 (Givens Rotation).Let V be an n-dimensional Euclidean vector space and Φ : V → V an automorphism with transformation ma-trixEssentially, R ij (θ) is the identity matrix I n withIn two dimensions (i.e., n = 2), we obtain (3.76) as a special case.Rotations exhibit a number of useful properties, which can be derived by considering them as orthogonal matrices (Definition 3.8):Rotations preserve distances, i.e., ∥x−y∥ = ∥R θ (x)−R θ (y)∥.In other words, rotations leave the distance between any two points unchanged after the transformation.Rotations preserve angles, i.e., the angle between R θ x and R θ y equals the angle between x and y.Rotations in three (or more) dimensions are generally not commutative.Therefore, the order in which rotations are applied is important, even if they rotate about the same point.Only in two dimensions vector rotations are commutative, such that R(ϕ)R(θ) = R(θ)R(ϕ) for all ϕ, θ ∈ [0, 2π).They form an Abelian group (with multiplication) only if they rotate about the same point (e.g., the origin).In this chapter, we gave a brief overview of some of the important concepts of analytic geometry, which we will use in later chapters of the book.For a broader and more in-depth overview of some of the concepts we presented, we refer to the following excellent books: Axler (2015) and Boyd and Vandenberghe (2018).Inner products allow us to determine specific bases of vector (sub)spaces, where each vector is orthogonal to all others (orthogonal bases) using the Gram-Schmidt method.These bases are important in optimization and numerical algorithms for solving linear equation systems.For instance, Krylov subspace methods, such as conjugate gradients or the generalized minimal residual method (GMRES), minimize residual errors that are orthogonal to each other (Stoer and Burlirsch, 2002).In machine learning, inner products are important in the context of Draft (2024-01-15) of "Mathematics for Machine Learning".Feedback: https://mml-book.com.kernel methods (Schölkopf and Smola, 2002).Kernel methods exploit the fact that many linear algorithms can be expressed purely by inner product computations.Then, the "kernel trick" allows us to compute these inner products implicitly in a (potentially infinite-dimensional) feature space, without even knowing this feature space explicitly.This allowed the "non-linearization" of many algorithms used in machine learning, such as kernel-PCA (Schölkopf et al., 1997) for dimensionality reduction.Gaussian processes (Rasmussen and Williams, 2006) also fall into the category of kernel methods and are the current state of the art in probabilistic regression (fitting curves to data points).The idea of kernels is explored further in Chapter 12.Projections are often used in computer graphics, e.g., to generate shadows.In optimization, orthogonal projections are often used to (iteratively) minimize residual errors.This also has applications in machine learning, e.g., in linear regression where we want to find a (linear) function that minimizes the residual errors, i.e., the lengths of the orthogonal projections of the data onto the linear function (Bishop, 2006).We will investigate this further in Chapter 9. PCA (Pearson, 1901;Hotelling, 1933) also uses projections to reduce the dimensionality of high-dimensional data.We will discuss this in more detail in Chapter 10.96is an inner product.3.2 Consider R 2 with ⟨•, •⟩ defined for all x and y in R 2 as ⟨x, y⟩ := x ⊤ 2 0 1 2Is ⟨•, •⟩ an inner product?3.3 Compute the distance between 3.5 Consider the Euclidean vector space R 5 with the dot product.A subspace U ⊆ R 5 and x ∈ R 5 are given by3.6 Consider R 3 with the inner productFurthermore, we define e 1 , e 2 , e 3 as the standard/canonical basis in R 3 .Draft (2024-01-15) of "Mathematics for Machine Learning".Feedback: https://mml-book.com.Hint: Orthogonality is defined through the inner product.b.Compute the distance d(e 2 , U ). c. Draw the scenario: standard basis vectors and π U (e 2 ) 3.7 Let V be a vector space and π an endomorphism of V .a. Prove that π is a projection if and only if id V − π is a projection, where id V is the identity endomorphism on V .b. Assume now that π is a projection.Calculate Im(id V −π) and ker(idas a function of Im(π) and ker(π).3.8 Using the Gram-Schmidt method, turn the basis3.9 Let n ∈ N and let x 1 , . . ., xn > 0 be n positive real numbers so that x 1 + . . .+ xn = 1.Use the Cauchy-Schwarz inequality and show that a.Hint: Think about the dot product on R n .Then, choose specific vectors x, y ∈ R n and apply the Cauchy-Schwarz inequality.3.10 Rotate the vectorsIn Chapters 2 and 3, we studied ways to manipulate and measure vectors, projections of vectors, and linear mappings.Mappings and transformations of vectors can be conveniently described as operations performed by matrices.Moreover, data is often represented in matrix form as well, e.g., where the rows of the matrix represent different people and the columns describe different features of the people, such as weight, height, and socioeconomic status.In this chapter, we present three aspects of matrices: how to summarize matrices, how matrices can be decomposed, and how these decompositions can be used for matrix approximations.We first consider methods that allow us to describe matrices with just a few numbers that characterize the overall properties of matrices.We will do this in the sections on determinants (Section 4.1) and eigenvalues (Section 4.2) for the important special case of square matrices.These characteristic numbers have important mathematical consequences and allow us to quickly grasp what useful properties a matrix has.From here we will proceed to matrix decomposition methods: An analogy for matrix decomposition is the factoring of numbers, such as the factoring of 21 into prime numbers 7 • 3.For this reason matrix decomposition is also often referred to as matrix factorization.Matrix decompositions are used matrix factorization to describe a matrix by means of a different representation using factors of interpretable matrices.We will first cover a square-root-like operation for symmetric, positive definite matrices, the Cholesky decomposition (Section 4.3).From here we will look at two related methods for factorizing matrices into canonical forms.The first one is known as matrix diagonalization (Section 4.4), which allows us to represent the linear mapping using a diagonal transformation matrix if we choose an appropriate basis.The second method, singular value decomposition (Section 4.5), extends this factorization to non-square matrices, and it is considered one of the fundamental concepts in linear algebra.These decompositions are helpful, as matrices representing numerical data are often very large and hard to analyze.We conclude the chapter with a systematic overview of the types of matrices and the characteristic properties that distinguish them in the form of a matrix taxonomy (Section 4.7).The methods that we cover in this chapter will become important in 98The determinant notation |A| must not be confused with the absolute value.Determinants are important concepts in linear algebra.A determinant is a mathematical object in the analysis and solution of systems of linear equations.Determinants are only defined for square matrices A ∈ R n×n , i.e., matrices with the same number of rows and columns.In this book, we write the determinant as det(A) or sometimes as |A| so thatThus a 1 a = 1 holds, if and only if a ̸ = 0.For 2 × 2 matrices, by the definition of the inverse (Definition 2.3), we know that AA −1 = I.Then, with (2.24), the inverse of A is For a memory aid of the product terms in Sarrus' rule, try tracing the elements of the triple products in the matrix.We call a square matrix T an upper-triangular matrix if T ij = 0 for upper-triangular matrix i > j, i.e., the matrix is zero below its diagonal.Analogously, we define a lower-triangular matrix as a matrix with zeros above its diagonal.For a trilower-triangular matrix angular matrix T ∈ R n×n , the determinant is the product of the diagonal elements, i.e.,The determinant is the signed volume of the parallelepiped formed by the columns of the matrix.The sign of the determinant indicates the orientation of the spanning vectors b, g with respect to the standard basis (e 1 , e 2 ).In our figure, flipping the order to g, b swaps the columns of A and reverses the orientation of the shaded area.This becomes the familiar formula: area = height × length.This intuition extends to higher dimensions.In R 3 , we consider three vectors r, b, g ∈ R 3 spanning the edges of a parallelepiped, i.e., a solid with faces that are parallel parallelograms (see Figure 4.3).The ab-The sign of the determinant indicates the orientation of the spanning vectors.solute value of the determinant of the 3 × 3 matrix [r, b, g] is the volume of the solid.Thus, the determinant acts as a function that measures the signed volume formed by column vectors composed in a matrix.Consider the three linearly independent vectors r, g, b ∈ R 3 given as 102Writing these vectors as the columns of a matrixallows us to compute the desired volume asComputing the determinant of an n × n matrix requires a general algorithm to solve the cases for n > 3, which we are going to explore in the following.Theorem 4.2 below reduces the problem of computing the determinant of an n×n matrix to computing the determinant of (n−1)×(n−1) matrices.By recursively applying the Laplace expansion (Theorem 4.2), we can therefore compute determinants of n × n matrices by ultimately computing determinants of 2 × 2 matrices.Theorem 4.2 (Laplace Expansion).Consider a matrix A ∈ R n×n .Then, for all j = 1, . . ., n:(4.12)(−1) k+j a jk det(A j,k ) .(4.13)Here A k,j ∈ R (n−1)×(n−1) is the submatrix of A that we obtain when deleting row k and column j.Let us compute the determinant ofusing the Laplace expansion along the first row.Applying (4.13) yieldsDraft (2024-01-15) of "Mathematics for Machine Learning".Feedback: https://mml-book.com.We use (4.6) to compute the determinants of all 2 × 2 matrices and obtainFor completeness we can compare this result to computing the determinant using Sarrus' rule (4.7):For A ∈ R n×n the determinant exhibits the following properties:The determinant of a matrix product is the product of the corresponding determinants, det(AB) = det(A)det(B).Determinants are invariant to transposition, i.e., det(A) = det(A ⊤ ).If A is regular (invertible), then det(A −1 ) = 1 det(A) .Similar matrices (Definition 2.22) possess the same determinant.Therefore, for a linear mapping Φ : V → V all transformation matrices A Φ of Φ have the same determinant.Thus, the determinant is invariant to the choice of basis of a linear mapping.Adding a multiple of a column/row to another one does not change det(A).Multiplication of a column/row with λ ∈ R scales det(A) by λ.In particular, det(λA) = λ n det(A).Because of the last three properties, we can use Gaussian elimination (see Section 2.1) to compute det(A) by bringing A into row-echelon form.We can stop Gaussian elimination when we have A in a triangular form where the elements below the diagonal are all 0. Recall from (4.8) that the determinant of a triangular matrix is the product of the diagonal elements.Theorem 4.3.A square matrix A ∈ R n×n has det(A) ̸ = 0 if and only if rk(A) = n.In other words, A is invertible if and only if it is full rank.When mathematics was mainly performed by hand, the determinant calculation was considered an essential way to analyze matrix invertibility.However, contemporary approaches in machine learning use direct numerical methods that superseded the explicit calculation of the determinant.For example, in Chapter 2, we learned that inverse matrices can be computed by Gaussian elimination.Gaussian elimination can thus be used to compute the determinant of a matrix.Determinants will play an important theoretical role for the following sections, especially when we learn about eigenvalues and eigenvectors (Section 4.2) through the characteristic polynomial.i.e. , the trace is the sum of the diagonal elements of A.The trace satisfies the following properties:It can be shown that only one function satisfies these four properties together -the trace (Gohberg et al., 2012).The properties of the trace of matrix products are more general.Specifically, the trace is invariant under cyclic permutations, i.e.,The trace is invariant under cyclic permutations.tr(AKL) = tr(KLA) (4.19)for matrices A ∈ R a×k , K ∈ R k×l , L ∈ R l×a .This property generalizes to products of an arbitrary number of matrices.As a special case of (4.19), it follows that for two vectorsGiven a linear mapping Φ : V → V , where V is a vector space, we define the trace of this map by using the trace of matrix representation of Φ.For a given basis of V , we can describe Φ by means of the transformation matrix A. Then the trace of Φ is the trace of A. For a different basis of V , it holds that the corresponding transformation matrix B of Φ can be obtained by a basis change of the form S −1 AS for suitable S (see Section 2.7.2).For the corresponding trace of Φ, this means tr(B) = tr(S −1 AS)= tr(ASS −1 ) = tr(A) .(4.21)Hence, while matrix representations of linear mappings are basis dependent the trace of a linear mapping Φ is independent of the basis.In this section, we covered determinants and traces as functions characterizing a square matrix.Taking together our understanding of determinants and traces we can now define an important equation describing a matrix A in terms of a polynomial, which we will use extensively in the following sections.Draft (2024-01-15) of "Mathematics for Machine Learning".Feedback: https://mml-book.com.105 c 0 = det(A) , (4.23) c n−1 = (−1) n−1 tr(A) .(4.24)The characteristic polynomial (4.22a) will allow us to compute eigenvalues and eigenvectors, covered in the next section.We will now get to know a new way to characterize a matrix and its associated linear mapping.Recall from Section 2.7.1 that every linear mapping has a unique transformation matrix given an ordered basis.We can interpret linear mappings and their associated transformation matrices by performing an "eigen" analysis.As we will see, the eigenvalues of a lin-Eigen is a German word meaning "characteristic", "self", or "own".ear mapping will tell us how a special set of vectors, the eigenvectors, is transformed by the linear mapping.Definition 4.6.Let A ∈ R n×n be a square matrix.Then λ ∈ R is an eigenvalue of A and x ∈ R n \{0} is the corresponding eigenvector of A if eigenvalue eigenvector Ax = λx .(4.25)We call (4.25) the eigenvalue equation.Remark.In the linear algebra literature and software, it is often a convention that eigenvalues are sorted in descending order, so that the largest eigenvalue and associated eigenvector are called the first eigenvalue and its associated eigenvector, and the second largest called the second eigenvalue and its associated eigenvector, and so on.However, textbooks and publications may have different or no notion of orderings.We do not want to presume an ordering in this book if not stated explicitly.♢The following statements are equivalent:λ is an eigenvalue of A ∈ R n×n .There exists an x ∈ R n \{0} with Ax = λx, or equivalently, (A − λI n )x = 0 can be solved non-trivially, i.e., x ̸ = 0. Remark (Non-uniqueness of eigenvectors).If x is an eigenvector of A associated with eigenvalue λ, then for any c ∈ R\{0} it holds that cx is an eigenvector of A with the same eigenvalue since spectrum If λ is an eigenvalue of A ∈ R n×n , then the corresponding eigenspace E λ is the solution space of the homogeneous system of linear equations (A−λI)x = 0. Geometrically, the eigenvector corresponding to a nonzero eigenvalue points in a direction that is stretched by the linear mapping.The eigenvalue is the factor by which it is stretched.If the eigenvalue is negative, the direction of the stretching is flipped.The identity matrix I ∈ R n×n has characteristic polynomial p I (λ) = det(I − λI) = (1 − λ) n = 0, which has only one eigenvalue λ = 1 that occurs n times.Moreover, Ix = λx = 1x holds for all vectors x ∈ R n \{0}.Because of this, the sole eigenspace E 1 of the identity matrix spans n dimensions, and all n standard basis vectors of R n are eigenvectors of I.A matrix A and its transpose A ⊤ possess the same eigenvalues, but not necessarily the same eigenvectors.The eigenspace E λ is the null space of A − λI sinceSimilar matrices (see Definition 2.22) possess the same eigenvalues.Therefore, a linear mapping Φ has eigenvalues that are independent of the choice of basis of its transformation matrix.This makes eigenvalues, together with the determinant and the trace, key characteristic parameters of a linear mapping as they are all invariant under basis change.Symmetric, positive definite matrices always have positive, real eigenvalues.Step 1: Characteristic Polynomial.From our definition of the eigenvector x ̸ = 0 and eigenvalue λ of A, there will be a vector such that Ax = λx, i.e., (A − λI)x = 0. Since x ̸ = 0, this requires that the kernel (null space) of A − λI contains more elements than just 0. This means that A − λI is not invertible and therefore det(A − λI) = 0. Hence, we need to compute the roots of the characteristic polynomial (4.22a) to find the eigenvalues.Step 2: Eigenvalues.The characteristic polynomial isWe factorize the characteristic polynomial and obtaingiving the roots λ 1 = 2 and λ 2 = 5.Step 3: Eigenvectors and Eigenspaces.We find the eigenvectors that correspond to these eigenvalues by looking at vectors x such thatFor λ = 5 we obtainWe solve this homogeneous system and obtain a solution spaceThis eigenspace is one-dimensional as it possesses a single basis vector.Analogously, we find the eigenvector for λ = 2 by solving the homogeneous system of equations This means any vector x = x 1 x 2 , where x 2 = −x 1 , such as 1 −1 , is an eigenvector with eigenvalue 2. The corresponding eigenspace is given asThe two eigenspaces E 5 and E 2 in Example 4.5 are one-dimensional as they are each spanned by a single vector.However, in other cases we may have multiple identical eigenvalues (see Definition 4.9) and the eigenspace may have more than one dimension.Definition 4.11.Let λ i be an eigenvalue of a square matrix A. Then the geometric multiplicity of λ i is the number of linearly independent eigengeometric multiplicity vectors associated with λ i .In other words, it is the dimensionality of the eigenspace spanned by the eigenvectors associated with λ i .Remark.A specific eigenvalue's geometric multiplicity must be at least one because every eigenvalue has at least one associated eigenvector.An eigenvalue's geometric multiplicity cannot exceed its algebraic multiplicity, but it may be lower.♢The matrix A = 2 1 0 2 has two repeated eigenvalues λ 1 = λ 2 = 2 and an algebraic multiplicity of 2. The eigenvalue has, however, only one distinct unit eigenvector x 1 = 1 0 and, thus, geometric multiplicity 1.Let us gain some intuition for determinants, eigenvectors, and eigenvalues using different linear mappings.Figure 4.4 depicts five transformation matrices A 1 , . . ., A 5 and their impact on a square grid of points, centered at the origin:In geometry, the area-preserving properties of this type of shearing parallel to an axis is also known as Cavalieri's principle of equal areas for parallelograms (Katz, 2004).. The direction of the two eigenvectors correspond to the canonical basis vectors in R 2 , i.e., to two cardinal axes.The vertical axis is extended by a factor of 2 (eigenvalue λ 1 = 2), and the horizontal axis is compressed by factor 1 2 (eigenvalue λ 2 = 1 2 ).The mapping is area preserving (detcorresponds to a shearing mapping , i.e., it shears the points along the horizontal axis to the right if they are on the positive Draft (2024-01-15) of "Mathematics for Machine Learning".Feedback: https://mml-book.com.half of the vertical axis, and to the left vice versa.This mapping is area preserving (det(A 2 ) = 1).The eigenvalue λ 1 = 1 = λ 2 is repeated and the eigenvectors are collinear (drawn here for emphasis in two opposite directions).This indicates that the mapping acts only along one direction (the horizontal axis).The matrix A 3 rotates the points by π 6 rad = 30 • counter-clockwise and has only complex eigenvalues, reflecting that the mapping is a rotation (hence, no eigenvectors are drawn).A rotation has to be volume preserving, and so the determinant is 1.For more details on rotations, we refer to Section 3.9.value is 0, the space in direction of the (blue) eigenvector corresponding to λ 1 = 0 collapses, while the orthogonal (red) eigenvector stretches space by a factor λ 2 = 2. Therefore, the area of the image is 0.1 is a shear-and-stretch mapping that scales space by 75% since | det(A 5 )| = 3 4 .It stretches space along the (red) eigenvector of λ 2 by a factor 1.5 and compresses it along the orthogonal (blue) eigenvector by a factor 0.5.Methods to analyze and learn from network data are an essential component of machine learning methods.The key to understanding networks is the connectivity between network nodes, especially if two nodes are connected to each other or not.In data science applications, it is often useful to study the matrix that captures this connectivity data.We build a connectivity/adjacency matrix A ∈ R 277×277 of the complete neural network of the worm C.Elegans.Each row/column represents one of the 277 neurons of this worm's brain.The connectivity matrix A has a value of a ij = 1 if neuron i talks to neuron j through a synapse, and a ij = 0 otherwise.The connectivity matrix is not symmetric, which implies that eigenvalues may not be real valued.Therefore, we compute a symmetrized version of the connectivity matrix as A sym := A + A ⊤ .This new matrix A sym is shown in Figure 4.5(a) and has a nonzero value a ij if and only if two neurons are connected (white pixels), irrespective of the direction of the connection.In Figure 4.5(b), we show the corresponding eigenspectrum of A sym .The horizontal axis shows the index of the eigenvalues, sorted in descending order.The vertical axis shows the corresponding eigenvalue.The S-like shape of this eigenspectrum is typical for many biological neural networks.The underlying mechanism responsible for this is an area of active neuroscience research.Theorem 4.12.The eigenvectors x 1 , . . ., x n of a matrix A ∈ R n×n with n distinct eigenvalues λ 1 , . . ., λ n are linearly independent.This theorem states that eigenvectors of a matrix with n distinct eigenvalues form a basis of R n .Definition 4.13.A square matrix A ∈ R n×n is defective if it possesses defective fewer than n linearly independent eigenvectors.A non-defective matrix A ∈ R n×n does not necessarily require n distinct eigenvalues, but it does require that the eigenvectors form a basis of R n .Looking at the eigenspaces of a defective matrix, it follows that the sum of the dimensions of the eigenspaces is less than n.Specifically, a defective matrix has at least one eigenvalue λ i with an algebraic multiplicity m > 1 and a geometric multiplicity of less than m.Remark.A defective matrix cannot have n distinct eigenvalues, as distinct eigenvalues have linearly independent eigenvectors (Theorem 4.12).♢ Theorem 4.14.Given a matrix A ∈ R m×n , we can always obtain a symmetric, positive semidefinite matrix S ∈ R n×n by definingUnderstanding why Theorem 4.14 holds is insightful for how we can use symmetrized matrices: Symmetry requires S = S ⊤ , and by inserting (4.36) we obtain S = A ⊤ A = A ⊤ (A ⊤ ) ⊤ = (A ⊤ A) ⊤ = S ⊤ .Moreover, positive semidefiniteness (Section 3.2.3)requires that x ⊤ Sx ⩾ 0 and inserting (4.36) we obtain x ⊤ Sx = x ⊤ A ⊤ Ax = (x ⊤ A ⊤ )(Ax) = (Ax) ⊤ (Ax) ⩾ 0, because the dot product computes a sum of squares (which are themselves non-negative).spectral theorem Theorem 4.15 (Spectral Theorem).If A ∈ R n×n is symmetric, there exists an orthonormal basis of the corresponding vector space V consisting of eigenvectors of A, and each eigenvalue is real.A direct implication of the spectral theorem is that the eigendecomposition of a symmetric matrix A exists (with real eigenvalues), and that we can find an ONB of eigenvectors so that A = P DP ⊤ , where D is diagonal and the columns of P contain the eigenvectors.112The characteristic polynomial of A isso that we obtain the eigenvalues λ 1 = 1 and λ 2 = 7, where λ 1 is a repeated eigenvalue.Following our standard procedure for computing eigenvectors, we obtain the eigenspaces] .(4.39)We see that x 3 is orthogonal to both x 1 and x 2 .However, since x ⊤ 1 x 2 = 1 ̸ = 0, they are not orthogonal.The spectral theorem (Theorem 4.15) states that there exists an orthogonal basis, but the one we have is not orthogonal.However, we can construct one.To construct such a basis, we exploit the fact that x 1 , x 2 are eigenvectors associated with the same eigenvalue λ.Therefore, for any α, β ∈ R it holds thati.e., any linear combination of x 1 and x 2 is also an eigenvector of A associated with λ.The Gram-Schmidt algorithm (Section 3.8.3) is a method for iteratively constructing an orthogonal/orthonormal basis from a set of basis vectors using such linear combinations.Therefore, even if x 1 and x 2 are not orthogonal, we can apply the Gram-Schmidt algorithm and find eigenvectors associated with λ 1 = 1 that are orthogonal to each other (and to x 3 ).In our example, we will obtainwhich are orthogonal to each other, orthogonal to x 3 , and eigenvectors of A associated with λ 1 = 1.Before we conclude our considerations of eigenvalues and eigenvectors it is useful to tie these matrix characteristics together with the concepts of the determinant and the trace.where λ i ∈ C are (possibly repeated) eigenvalues of A.Draft (2024-01-15) of "Mathematics for Machine Learning".Feedback: https://mml-book.com.Let us provide a geometric intuition of these two theorems.Consider a matrix A ∈ R 2×2 that possesses two linearly independent eigenvectors x 1 , x 2 .For this example, we assume (x 1 , x 2 ) are an ONB of R 2 so that they are orthogonal and the area of the square they span is 1; see Figure 4.6.From Section 4.1, we know that the determinant computes the change of area of unit square under the transformation A. In this example, we can compute the change of area explicitly: Mapping the eigenvectors using A gives us vectors v 1 = Ax 1 = λ 1 x 1 and v 2 = Ax 2 = λ 2 x 2 , i.e., the new vectors v i are scaled versions of the eigenvectors x i , and the scaling factors are the corresponding eigenvalues λ i .v 1 , v 2 are still orthogonal, and the area of the rectangle they span is |λ 1 λ 2 |.Given that x 1 , x 2 (in our example) are orthonormal, we can directly compute the perimeter of the unit square as 2(1 + 1).Mapping the eigenvectors using A creates a rectangle whose perimeter is 2(|λ 1 | + |λ 2 |).Therefore, the sum of the absolute values of the eigenvalues tells us how the perimeter of the unit square changes under the transformation matrix A.Google uses the eigenvector corresponding to the maximal eigenvalue of a matrix A to determine the rank of a page for search.The idea for the PageRank algorithm, developed at Stanford University by Larry Page and Sergey Brin in 1996, was that the importance of any web page can be approximated by the importance of pages that link to it.For this, they write down all web sites as a huge directed graph that shows which page links to which.PageRank computes the weight (importance) x i ⩾ 0 of a web site a i by counting the number of pages pointing to a i .Moreover, PageRank takes into account the importance of the web sites that link to a i .The navigation behavior of a user is then modeled by a transition matrix A of this graph that tells us with what (click) probability somebody will end up on a different web site.The matrix A has the property that for any initial rank/importance vector x of a web site the sequence x, Ax, A 2 x, . . .converges to a vector x * .This vector is called the PageRank and satisfies PageRank Ax * = x * , i.e., it is an eigenvector (with corresponding eigenvalue 1) of A. After normalizing x * , such that ∥x * ∥ = 1, we can interpret the entries as probabilities.More details and different perspectives on PageRank can be found in the original technical report (Page et al., 1999).There are many ways to factorize special types of matrices that we encounter often in machine learning.In the positive real numbers, we have the square-root operation that gives us a decomposition of the number into identical components, e.g., 9 = 3 • 3.For matrices, we need to be careful that we compute a square-root-like operation on positive quantities.For symmetric, positive definite matrices (see Section 3.2.3),we can choose from a number of square-root equivalent operations.The CholeskyL is called the Cholesky factor of A, and L is unique.Comparing the left-hand side of (4.45) and the right-hand side of (4.46) shows that there is a simple pattern in the diagonal elements l ii :) .(4.47) Similarly for the elements below the diagonal (l ij , where i > j), there is also a repeating pattern: The Cholesky decomposition is an important tool for the numerical computations underlying machine learning.Here, symmetric positive definite matrices require frequent manipulation, e.g., the covariance matrix of a multivariate Gaussian variable (see Section 6.5) is symmetric, positive definite.The Cholesky factorization of this covariance matrix allows us to generate samples from a Gaussian distribution.It also allows us to perform a linear transformation of random variables, which is heavily exploited when computing gradients in deep stochastic models, such as the variational auto-encoder (Jimenez Rezende et al., 2014;Kingma and Welling, 2014).The Cholesky decomposition also allows us to compute determinants very efficiently.Given the Cholesky decomposition A = LL ⊤ , we know that det(A) = det(L) det(L ⊤ ) = det(L) 2 .Since L is a triangular matrix, the determinant is simply the product of its diagonal entries so that det(A) = i l 2ii .Thus, many numerical software packages use the Cholesky decomposition to make computations more efficient.A diagonal matrix is a matrix that has value zero on all off-diagonal elediagonal matrix ments, i.e., they are of the formThey allow fast computation of determinants, powers, and inverses.The determinant is the product of its diagonal entries, a matrix power D k is given by each diagonal element raised to the power k, and the inverse D −1 is the reciprocal of its diagonal elements if all of them are nonzero.In this section, we will discuss how to transform matrices into diagonal form.This is an important application of the basis change we discussed in Section 2.7.2 and eigenvalues from Section 4.2.Recall that two matrices A, D are similar (Definition 2.22) if there exists an invertible matrix P , such that D = P −1 AP .More specifically, we will look at matrices A that are similar to diagonal matrices D that contain the eigenvalues of A on the diagonal.is similar to a diagonal matrix, i.e., if there exists an invertible matrix P ∈ R n×n such that D = P −1 AP .In the following, we will see that diagonalizing a matrix A ∈ R n×n is a way of expressing the same linear mapping but in another basis (see Section 2.6.1),which will turn out to be a basis that consists of the eigenvectors of A.Let A ∈ R n×n , let λ 1 , . . ., λ n be a set of scalars, and let p 1 , . . ., p n be a set of vectors in R n .We define P := [p 1 , . . ., p n ] and let D ∈ R n×n be a diagonal matrix with diagonal entries λ 1 , . . ., λ n .Then we can show thatThus, (4.50) implies thatTherefore, the columns of P must be eigenvectors of A.Our definition of diagonalization requires that P ∈ R n×n is invertible, i.e., P has full rank (Theorem 4.3).This requires us to have n linearly independent eigenvectors p 1 , . . ., p n , i.e., the p i form a basis of R n .A square matrix A ∈ R n×n can be factored into Theorem 4.20 implies that only non-defective matrices can be diagonalized and that the columns of P are the n eigenvectors of A. For symmetric matrices we can obtain even stronger outcomes for the eigenvalue decomposition.Theorem 4.21.A symmetric matrix S ∈ R n×n can always be diagonalized.Theorem 4.21 follows directly from the spectral theorem 4.15.Moreover, the spectral theorem states that we can find an ONB of eigenvectors of R n .This makes P an orthogonal matrix so that D = P ⊤ AP .Remark.The Jordan normal form of a matrix offers a decomposition that works for defective matrices (Lang, 1987) but is beyond the scope of this book.♢We can interpret the eigendecomposition of a matrix as follows (see also Figure 4.7): Let A be the transformation matrix of a linear mapping with respect to the standard basis e i (blue arrows).P −1 performs a basis change from the standard basis into the eigenbasis.Then, the diagonal D scales the vectors along these axes by the eigenvalues λ i .Finally, P transforms these scaled vectors back into the standard/canonical coordinates yielding λ i p i .Let us compute the eigendecomposition of A = 1 2 5 −2 −2 5 .StepTherefore, the eigenvalues of A are λ 1 = 7 2 and λ 2 = 3 2 (the roots of the characteristic polynomial), and the associated (normalized) eigenvectors are obtained viaThis yieldsStep 2: Check for existence.The eigenvectors p 1 , p 2 form a basis of R 2 .Therefore, A can be diagonalized.Step 3: Construct the matrix P to diagonalize A. We collect the eigenvectors of A in P so thatWe then obtain(4.60)Equivalently, we get (exploiting that P −1 = P ⊤ since the eigenvectors p 1 and p 2 in this example form an ONB)Diagonal matrices D can efficiently be raised to a power.Therefore, we can find a matrix power for a matrix A ∈ R n×n via the eigenvalue decomposition (if it exists) so thatComputing D k is efficient because we apply this operation individually to any diagonal element.Assume that the eigendecomposition A = P DP −1 exists.Then, det(A) = det(P DP −1 ) = det(P ) det(D) det(P −1 ) (4.63a)Draft (2024-01-15) of "Mathematics for Machine Learning".Feedback: https://mml-book.com.allows for an efficient computation of the determinant of A.The eigenvalue decomposition requires square matrices.It would be useful to perform a decomposition on general matrices.In the next section, we introduce a more general matrix decomposition technique, the singular value decomposition.The singular value decomposition (SVD) of a matrix is a central matrix decomposition method in linear algebra.It has been referred to as the "fundamental theorem of linear algebra" (Strang, 1993) because it can be applied to all matrices, not only to square matrices, and it always exists.Moreover, as we will explore in the following, the SVD of a matrix A, which represents a linear mapping Φ : V → W , quantifies the change between the underlying geometry of these two vector spaces.We recommend the work by Kalman (1996) and Roy and Banerjee (2014) for a deeper overview of the mathematics of the SVD. with an orthogonal matrix U ∈ R m×m with column vectors u i , i = 1, . . ., m, and an orthogonal matrix V ∈ R n×n with column vectors v j , j = 1, . . ., n.The diagonal entries σ i , i = 1, . . ., r, of Σ are called the singular values, singular values u i are called the left-singular vectors, and v j are called the right-singular left-singular vectors right-singular vectors vectors.By convention, the singular values are ordered, i.e., σ 1 ⩾ σ 2 ⩾ σ r ⩾ 0.The singular value matrix Σ is unique, but it requires some attention.singular value matrix Observe that the Σ ∈ R m×n is rectangular.In particular, Σ is of the same size as A. This means that Σ has a diagonal submatrix that contains the singular values and needs additional zero padding.Specifically, if m > n, then the matrix Σ has diagonal structure up to row n and then consists of  Remark.The SVD exists for any matrix A ∈ R m×n .♢The SVD offers geometric intuitions to describe a transformation matrix A. In the following, we will discuss the SVD as sequential linear transformations performed on the bases.In Example 4.12, we will then apply transformation matrices of the SVD to a set of vectors in R 2 , which allows us to visualize the effect of each transformation more clearly.The SVD of a matrix can be interpreted as a decomposition of a corresponding linear mapping (recall Section 2.7.1)Φ : R n → R m into three operations; see Figure 4.8.The SVD intuition follows superficially a similar structure to our eigendecomposition intuition, see Figure 4.7: Broadly speaking, the SVD performs a basis change via V ⊤ followed by a scaling and augmentation (or reduction) in dimensionality via the singular Draft (2024-01-15) of "Mathematics for Machine Learning".Feedback: https://mml-book.com.121 value matrix Σ.Finally, it performs a second basis change via U .The SVD entails a number of important details and caveats, which is why we will review our intuition in more detail.It is useful to review basis changes (Section 2.7.2), orthogonal matrices (Definition 3.8) and orthonormal bases (Section 3.5).Assume we are given a transformation matrix of a linear mapping Φ : R n → R m with respect to the standard bases B and C of R n and R m , respectively.Moreover, assume a second basis B of R n and C of R m .Then The SVD expresses a change of basis in both the domain and codomain.This is in contrast with the eigendecomposition that operates within the same vector space, where the same basis change is applied and then undone.What makes the SVD special is that these two different bases are simultaneously linked by the singular value matrix Σ.Example 4.12 (Vectors and the SVD) Consider a mapping of a square grid of vectors X ∈ R 2 that fit in a box of size 2 × 2 centered at the origin.Using the standard basis, we map these vectors usingWe start with a set of vectors X (colored dots; see top-left panel of Figure 4.9) arranged in a grid.We then apply V ⊤ ∈ R 2×2 , which rotates X .The rotated vectors are shown in the bottom-left panel of Figure 4.9.We now map these vectors using the singular value matrix Σ to the codomain R 3 (see the bottom-right panel in Figure 4.9).Note that all vectors lie in the x 1 -x 2 plane.The third coordinate is always 0. The vectors in the x 1 -x 2 plane have been stretched by the singular values.The direct mapping of the vectors X by A to the codomain R 3 equals the transformation of X by U ΣV ⊤ , where U performs a rotation within the codomain R 3 so that the mapped vectors are no longer restricted to the x 1 -x 2 plane; they still are on a plane as shown in the top-right panel of Figure 4.9.x 1 -1.5 -0.5 0.5 1.5x 2 -1.5 -0.5 0.5 1.5x 3 0We will next discuss why the SVD exists and show how to compute it in detail.The SVD of a general matrix shares some similarities with the eigendecomposition of a square matrix.we see that the SVD of SPD matrices is their eigendecomposition.♢In the following, we will explore why Theorem 4.22 holds and how the SVD is constructed.Computing the SVD of A ∈ R m×n is equivalent to finding two sets of orthonormal bases U = (u 1 , . . ., u m ) and V = (v 1 , . . ., v n ) of the codomain R m and the domain R n , respectively.From these ordered bases, we will construct the matrices U and V .Our plan is to start with constructing the orthonormal set of rightsingular vectors v 1 , . . ., v n ∈ R n .We then construct the orthonormal set of left-singular vectors u 1 , . . ., u m ∈ R m .Thereafter, we will link the two and require that the orthogonality of the v i is preserved under the transformation of A. This is important because we know that the images Av i form a set of orthogonal vectors.We will then normalize these images by scalar factors, which will turn out to be the singular values.Let us begin with constructing the right-singular vectors.The spectral theorem (Theorem 4.15) tells us that the eigenvectors of a symmetric matrix form an ONB, which also means it can be diagonalized.Moreover, from Theorem 4.14 we can always construct a symmetric, positive semidefinite matrix A ⊤ A ∈ R n×n from any rectangular matrix A ∈ R m×n .Thus, we can always diagonalize A ⊤ A and obtain (4.71)where P is an orthogonal matrix, which is composed of the orthonormal eigenbasis.The λ i ⩾ 0 are the eigenvalues of A ⊤ A. Let us assume the SVD of A exists and inject (4.64) into (4.71).This yieldswhere U , V are orthogonal matrices.Therefore, with U ⊤ U = I we obtainComparing now (4.71) and (4.73), we identifyTherefore, the eigenvectors of A ⊤ A that compose P are the right-singular vectors V of A (see (4.74)).The eigenvalues of A ⊤ A are the squared singular values of Σ (see (4.75)).To obtain the left-singular vectors U , we follow a similar procedure.We start by computing the SVD of the symmetric matrix AA ⊤ ∈ R m×m (instead of the previous A ⊤ A ∈ R n×n ).The SVD of A yieldsThe spectral theorem tells us that AA ⊤ = SDS ⊤ can be diagonalized and we can find an ONB of eigenvectors of AA ⊤ , which are collected in S. The orthonormal eigenvectors of AA ⊤ are the left-singular vectors U and form an orthonormal basis in the codomain of the SVD.This leaves the question of the structure of the matrix Σ.Since AA ⊤ and A ⊤ A have the same nonzero eigenvalues (see page 106), the nonzero entries of the Σ matrices in the SVD for both cases have to be the same.The last step is to link up all the parts we touched upon so far.We have an orthonormal set of right-singular vectors in V .To finish the construction of the SVD, we connect them with the orthonormal vectors U .To reach this goal, we use the fact the images of the v i under A have to be orthogonal, too.We can show this by using the results from Section 3.4.We require that the inner product between Av i and Av j must be 0 for i ̸ = j.For any two orthogonal eigenvectors v i , v j , i ̸ = j, it holds thatFor the case m ⩾ r, it holds that {Av 1 , . . ., Av r } is a basis of an rdimensional subspace of R m .To complete the SVD construction, we need left-singular vectors that are orthonormal: We normalize the images of the right-singular vectors Av i and obtainwhere the last equality was obtained from (4.75) and (4.76b), showing us that the eigenvalues of AA ⊤ are such that σ 2 i = λ i .Therefore, the eigenvectors of A ⊤ A, which we know are the rightsingular vectors v i , and their normalized images under A, the left-singular vectors u i , form two self-consistent ONBs that are connected through the singular value matrix Σ.Let us rearrange (4.78) to obtain the singular value equation singular value equationThis equation closely resembles the eigenvalue equation (4.25), but the vectors on the left-and the right-hand sides are not the same.For n < m, (4.79) holds only for i ⩽ n, but (4.79) says nothing about the u i for i > n.However, we know by construction that they are orthonormal.Conversely, for m < n, (4.79) holds only for i ⩽ m.For i > m, we have Av i = 0 and we still know that the v i form an orthonormal set.This means that the SVD also supplies an orthonormal basis of the kernel (null space) of A, the set of vectors x with Ax = 0 (see Section 2.7.3).Concatenating the v i as the columns of V and the u i as the columns of U yieldswhere Σ has the same dimensions as A and a diagonal structure for rows 1, . . ., r.Hence, right-multiplying with V ⊤ yields A = U ΣV ⊤ , which is the SVD of A.Let us find the singular value decomposition ofThe SVD requires us to compute the right-singular vectors v j , the singular values σ k , and the left-singular vectors u i .Step 1: Right-singular vectors as the eigenbasis of A ⊤ A.We compute the singular values and right-singular vectors v j through the eigenvalue decomposition of A ⊤ A, which is given asand we obtain the right-singular vectors as the columns of P so thatStep 2: Singular-value matrix.As the singular values σ i are the square roots of the eigenvalues of A ⊤ A we obtain them straight from D. Since rk(A) = 2, there are only two nonzero singular values: σ 1 = √ 6 and σ 2 = 1.The singular value matrix must be the same size as A, and we obtainStep 3: Left-singular vectors as the normalized image of the rightsingular vectors.We find the left-singular vectors by computing the image of the rightsingular vectors under A and normalizing them by dividing them by their corresponding singular value.We obtain, (4.86)Note that on a computer the approach illustrated here has poor numerical behavior, and the SVD of A is normally computed without resorting to the eigenvalue decomposition of A ⊤ A.Let us consider the eigendecomposition A = P DP −1 and the SVD A = U ΣV ⊤ and review the core elements of the past sections.The SVD always exists for any matrix R m×n .The eigendecomposition is only defined for square matrices R n×n and only exists if we can find a basis of eigenvectors of R n .The vectors in the eigendecomposition matrix P are not necessarily orthogonal, i.e., the change of basis is not a simple rotation and scaling.On the other hand, the vectors in the matrices U and V in the SVD are orthonormal, so they do represent rotations.Both the eigendecomposition and the SVD are compositions of three linear mappings:1. Change of basis in the domain 2. Independent scaling of each new basis vector and mapping from domain to codomain 3. Change of basis in the codomain A key difference between the eigendecomposition and the SVD is that in the SVD, domain and codomain can be vector spaces of different dimensions.In the SVD, the left-and right-singular vector matrices U and V are generally not inverse of each other (they perform basis changes in different vector spaces).In the eigendecomposition, the basis change matrices P and P −1 are inverses of each other.In the SVD, the entries in the diagonal matrix Σ are all real and nonnegative, which is not generally true for the diagonal matrix in the eigendecomposition.The SVD and the eigendecomposition are closely related through their projections -The left-singular vectors of A are eigenvectors of AA ⊤ -The right-singular vectors of A are eigenvectors of A ⊤ A.-The nonzero singular values of A are the square roots of the nonzero eigenvalues of both AA ⊤ and A ⊤ A. For symmetric matrices A ∈ R n×n , the eigenvalue decomposition and the SVD are one and the same, which follows from the spectral theorem 4.15.Let us add a practical interpretation of the SVD by analyzing data on people and their preferred movies.Consider three viewers (Ali, Beatrix, Chandra) rating four different movies (Star Wars, Blade Runner, Amelie, Delicatessen).Their ratings are values between 0 (worst) and 5 (best) and encoded in a data matrix A ∈ R 4×3 as shown in Figure 4.10.Each row represents a movie and each column a user.Thus, the column vectors of movie ratings, one for each viewer, are x Ali , x Beatrix , x Chandra .Factoring A using the SVD offers us a way to capture the relationships of how people rate movies, and especially if there is a structure linking which people like which movies.Applying the SVD to our data matrix A makes a number of assumptions:1.All viewers rate movies consistently using the same linear mapping.2. There are no errors or noise in the ratings.3. We interpret the left-singular vectors u i as stereotypical movies and the right-singular vectors v j as stereotypical viewers.We then make the assumption that any viewer's specific movie preferences can be expressed as a linear combination of the v j .Similarly, any movie's like-ability can be expressed as a linear combination of the u i .Therefore, a vector in the domain of the SVD can be interpreted as a viewer in the "space" of stereotypical viewers, and a vector in the codomain of the SVD correspondingly as a movie in the "space" of stereotypical movies.Let us inspect the SVD of our movie-user matrix.The first left-singular vector u 1 has large absolute values for the two science fiction movies and a large first singular value (red shading in Figure 4.10).Thus, this groups a type of users with a specific set of movies (science fiction theme).Similarly, the first right-singular v 1 shows large absolute values for Ali and Beatrix, who give high ratings to science fiction movies (green shading in Figure 4.10).This suggests that v 1 reflects the notion of a science fiction lover.Similarly, u 2 , seems to capture a French art house film theme, and v 2 indicates that Chandra is close to an idealized lover of such movies.An idealized science fiction lover is a purist and only loves science fiction movies, so a science fiction lover v 1 gives a rating of zero to everything but science fiction themed-this logic is implied by the diagonal substructure for the singular value matrix Σ.A specific movie is therefore represented by how it decomposes (linearly) into its stereotypical movies.Likewise, a person would be represented by how they decompose (via linear combination) into movie themes.It is worth to briefly discuss SVD terminology and conventions, as there are different versions used in the literature.While these differences can be confusing, the mathematics remains invariant to them.For convenience in notation and abstraction, we use an SVD notation where the SVD is described as having two square left-and right-singular vector matrices, but a non-square singular value matrix.Our definition (4.64) for the SVD is sometimes called the full SVD.full SVD Some authors define the SVD a bit differently and focus on square singular matrices.Then, for A ∈ R m×n and m ⩾ n,(4.89)Draft (2024-01-15) of "Mathematics for Machine Learning".Feedback: https://mml-book.com.129 Sometimes this formulation is called the reduced SVD (e.g., Datta (2010)) reduced SVD or the SVD (e.g., Press et al. (2007)).This alternative format changes merely how the matrices are constructed but leaves the mathematical structure of the SVD unchanged.The convenience of this alternative formulation is that Σ is diagonal, as in the eigenvalue decomposition.In Section 4.6, we will learn about matrix approximation techniques using the SVD, which is also called the truncated SVD.It is possible to define the SVD of a rank-r matrix A so that U is an m × r matrix, Σ a diagonal matrix r × r, and V an r × n matrix.This construction is very similar to our definition, and ensures that the diagonal matrix Σ has only nonzero entries along the diagonal.The main convenience of this alternative notation is that Σ is diagonal, as in the eigenvalue decomposition.A restriction that the SVD for A only applies to m × n matrices with m > n is practically unnecessary.When m < n, the SVD decomposition will yield Σ with more zero columns than rows and, consequently, the singular values σ m+1 , . . ., σ n are 0.The SVD is used in a variety of applications in machine learning from least-squares problems in curve fitting to solving systems of linear equations.These applications harness various important properties of the SVD, its relation to the rank of a matrix, and its ability to approximate matrices of a given rank with lower-rank matrices.Substituting a matrix with its SVD has often the advantage of making calculation more robust to numerical rounding errors.As we will explore in the next section, the SVD's ability to approximate matrices with "simpler" matrices in a principled manner opens up machine learning applications ranging from dimensionality reduction and topic modeling to data compression and clustering.We considered the SVD as a way to factorize A = U ΣV ⊤ ∈ R m×n into the product of three matrices, where U ∈ R m×m and V ∈ R n×n are orthogonal and Σ contains the singular values on its main diagonal.Instead of doing the full SVD factorization, we will now investigate how the SVD allows us to represent a matrix A as a sum of simpler (low-rank) matrices A i , which lends itself to a matrix approximation scheme that is cheaper to compute than the full SVD.We construct a rank-1 matrix A i ∈ R m×n aswhich is formed by the outer product of the ith orthogonal column vector of U and V .Figure 4.11 shows an image of Stonehenge, which can be represented by a matrix A ∈ R 1432×1910 , and some outer products A i , as defined in (4.90).A matrix A ∈ R m×n of rank r can be written as a sum of rank-1 matrices A i so thatwhere the outer-product matrices A i are weighted by the ith singular value σ i .We can see why (4.91) holds: The diagonal structure of the singular value matrix Σ multiplies only matching left-and right-singular vectors u i v ⊤ i and scales them by the corresponding singular value σ i .All terms Σ ij u i v ⊤ j vanish for i ̸ = j because Σ is a diagonal matrix.Any terms i > r vanish because the corresponding singular values are 0.In (4.90), we introduced rank-1 matrices A i .We summed up the r individual rank-1 matrices to obtain a rank-r matrix A; see (4.91).If the sum does not run over all matrices A i , i = 1, . . ., r, but only up to an intermediate value k < r, we obtain a rank-k approximation.12 shows low-rank approximations A(k) of an original image A of Stonehenge.The shape of the rocks becomes increasingly visible and clearly recognizable in the rank-5 approximation.While the original image requires 1, 432 • 1, 910 = 2, 735, 120 numbers, the rank-5 approximation requires us only to store the five singular values and the five left-and right-singular vectors (1, 432 and 1, 910dimensional each) for a total of 5 • (1, 432 + 1, 910 + 1) = 16, 715 numbers -just above 0.6% of the original.To measure the difference (error) between A and its rank-k approximation A(k), we need the notion of a norm.In Section 3.1, we already used  norms on vectors that measure the length of a vector.By analogy we can also define norms on matrices.Definition 4.23 (Spectral Norm of a Matrix).For x ∈ R n \{0}, the spectral spectral norm norm of a matrix A ∈ R m×n is defined asWe introduce the notation of a subscript in the matrix norm (left-hand side), similar to the Euclidean norm for vectors (right-hand side), which has subscript 2. The spectral norm (4.93) determines how long any vector x can at most become when multiplied by A.Theorem 4.24.The spectral norm of A is its largest singular value σ 1 .We leave the proof of this theorem as an exercise.Theorem 4.25 (Eckart-Young Theorem (Eckart and Young, 1936)).Consider a matrix A ∈ R m×n of rank r and let B ∈ R m×n be a matrix of rank k.For any k ⩽ r withThe Eckart-Young theorem states explicitly how much error we introduce by approximating A using a rank-k approximation.We can interpret the rank-k approximation obtained with the SVD as a projection of the full-rank matrix A onto a lower-dimensional space of rank-at-most-k matrices.Of all possible projections, the SVD minimizes the error (with respect to the spectral norm) between A and any rank-k approximation.We can retrace some of the steps to understand why (4.95) should hold.132We observe that the difference between A − A(k) is a matrix containing the sum of the remaining rank-1 matricesBy Theorem 4.24, we immediately obtain σ k+1 as the spectral norm of the difference matrix.Let us have a closer look at (4.94).If we assume that there is another matrix B with rk(B) ⩽ k, such thatthen there exists an at least (n − k)-dimensional null space Z ⊆ R n , such that x ∈ Z implies that Bx = 0. Then it follows thatand by using a version of the Cauchy-Schwartz inequality (3.17) that encompasses norms of matrices, we obtainHowever, there exists a (k + 1)-dimensional subspace where ∥Ax∥ 2 ⩾ σ k+1 ∥x∥ 2 , which is spanned by the right-singular vectors v j , j ⩽ k + 1 of A. Adding up dimensions of these two spaces yields a number greater than n, as there must be a nonzero vector in both spaces.This is a contradiction of the rank-nullity theorem (Theorem 2.24) in Section 2.7.3.The Eckart-Young theorem implies that we can use SVD to reduce a rank-r matrix A to a rank-k matrix A in a principled, optimal (in the spectral norm sense) manner.We can interpret the approximation of A by a rank-k matrix as a form of lossy compression.Therefore, the low-rank approximation of a matrix appears in many machine learning applications, e.g., image processing, noise filtering, and regularization of ill-posed problems.Furthermore, it plays a key role in dimensionality reduction and principal component analysis, as we will see in Chapter 10.Coming back to our movie-rating example, we can now apply the concept of low-rank approximations to approximate the original data matrix.Recall that our first singular value captures the notion of science fiction theme in movies and science fiction lovers.Thus, by using only the first singular value term in a rank-1 decomposition of the movie-rating matrix, we obtain the predicted ratings This first rank-1 approximation A 1 is insightful: it tells us that Ali and Beatrix like science fiction movies, such as Star Wars and Bladerunner (entries have values > 0.4), but fails to capture the ratings of the other movies by Chandra.This is not surprising, as Chandra's type of movies is not captured by the first singular value.The second singular value gives us a better rank-1 approximation for those movie-theme lovers: In this second rank-1 approximation A 2 , we capture Chandra's ratings and movie types well, but not the science fiction movies.This leads us to consider the rank-2 approximation A(2), where we combine the first two rank-1 approximations (4.102)A( 2) is similar to the original movie ratings tableand this suggests that we can ignore the contribution of A 3 .We can interpret this so that in the data table there is no evidence of a third movietheme/movie-lovers category.This also means that the entire space of movie-themes/movie-lovers in our example is a two-dimensional space spanned by science fiction and French art house movies and lovers.The word "phylogenetic" describes how we capture the relationships among individuals or groups and derived from the Greek words for "tribe" and "source".In Chapters 2 and 3, we covered the basics of linear algebra and analytic geometry.In this chapter, we looked at fundamental characteristics of matrices and linear mappings.Figure 4.13 depicts the phylogenetic tree of relationships between different types of matrices (black arrows indicating "is a subset of") and the covered operations we can perform on them (in blue).We consider all real matrices A ∈ R n×m .For non-square matrices (where n ̸ = m), the SVD always exists, as we saw in this chapter.Focusing on square matrices A ∈ R n×n , the determinant informs us whether a square matrix possesses an inverse matrix, i.e., whether it belongs to the class of regular, invertible matrices.If the square n × n matrix possesses n linearly independent eigenvectors, then the matrix is non-defective and an eigendecomposition exists (Theorem 4.12).We know that repeated eigenvalues may result in defective matrices, which cannot be diagonalized.Non-singular and non-defective matrices are not the same.For example, a rotation matrix will be invertible (determinant is nonzero) but not diagonalizable in the real numbers (eigenvalues are not guaranteed to be real numbers).We dive further into the branch of non-defective square n × n matrices.A is normal if the condition A ⊤ A = AA ⊤ holds.Moreover, if the more restrictive condition holds that A ⊤ A = AA ⊤ = I, then A is called orthogonal (see Definition 3.8).The set of orthogonal matrices is a subset of the regular (invertible) matrices and satisfiesNormal matrices have a frequently encountered subset, the symmetric matrices S ∈ R n×n , which satisfy S = S ⊤ .Symmetric matrices have only real eigenvalues.A subset of the symmetric matrices consists of the positive definite matrices P that satisfy the condition of x ⊤ P x > 0 for all x ∈ R n \{0}.In this case, a unique Cholesky decomposition exists (Theorem 4.18).Positive definite matrices have only positive eigenvalues and are always invertible (i.e., have a nonzero determinant).Another subset of symmetric matrices consists of the diagonal matrices D. Diagonal matrices are closed under multiplication and addition, but do not necessarily form a group (this is only the case if all diagonal entries are nonzero so that the matrix is invertible).A special diagonal matrix is the identity matrix I.Most of the content in this chapter establishes underlying mathematics and connects them to methods for studying mappings, many of which are at the heart of machine learning at the level of underpinning software solutions and building blocks for almost all machine learning theory.Matrix characterization using determinants, eigenspectra, and eigenspaces provides fundamental features and conditions for categorizing and analyzing matrices.This extends to all forms of representations of data and mappings involving data, as well as judging the numerical stability of computational operations on such matrices (Press et al., 2007).Determinants are fundamental tools in order to invert matrices and compute eigenvalues "by hand".However, for almost all but the smallest instances, numerical computation by Gaussian elimination outperforms determinants (Press et al., 2007).Determinants remain nevertheless a powerful theoretical concept, e.g., to gain intuition about the orientation of a basis based on the sign of the determinant.Eigenvectors can be used to perform basis changes to transform data into the coordinates of meaningful orthogonal, feature vectors.Similarly, matrix decomposition methods, such as the Cholesky decomposition, reappear often when we compute or simulate random events (Rubinstein and Kroese, 2016).Therefore, the Cholesky decomposition enables us to compute the reparametrization trick where we want to perform continuous differentiation over random variables, e.g., in variational autoencoders (Jimenez Rezende et al., 2014;Kingma and Welling, 2014).Eigendecomposition is fundamental in enabling us to extract meaningful and interpretable information that characterizes linear mappings.Therefore, the eigendecomposition underlies a general class of machine learning algorithms called spectral methods that perform eigendecomposition of a positive-definite kernel.These spectral decomposition methods encompass classical approaches to statistical data analysis, such as the following:principal component analysis Principal component analysis (PCA (Pearson, 1901), see also Chapter 10), in which a low-dimensional subspace, which explains most of the variability in the data, is sought.Fisher discriminant analysis, which aims to determine a separating hyperplane for data classification (Mika et al., 1999).Malik, 2000).The core computations of these are generally underpinned by low-rank matrix approximation techniques (Belabbas and Wolfe, 2009) as we encountered here via the SVD.The SVD allows us to discover some of the same kind of information as the eigendecomposition.However, the SVD is more generally applicable to non-square matrices and data tables.These matrix factorization methods become relevant whenever we want to identify heterogeneity in data when we want to perform data compression by approximation, e.g., instead of storing n×m values just storing (n+m)k values, or when we want to perform data pre-processing, e.g., to decorrelate predictor variables of a design matrix (Ormoneit et al., 2001).The SVD operates on matrices, which we can interpret as rectangular arrays with two indices (rows and columns).The extension of matrix-like structure to higher-dimensional arrays are called tensors.It turns out that the SVD is the special case of a more general family of decompositions that operate on such tensors (Kolda and Bader, 2009).SVD-like operations and low-rank approximations on tensors are, for example, the Tucker decomposition (Tucker, 1966) Tucker decomposition or the CP decomposition (Carroll and Chang, 1970).The SVD low-rank approximation is frequently used in machine learning for computational efficiency reasons.This is because it reduces the amount of memory and operations with nonzero multiplications we need to perform on potentially very large matrices of data (Trefethen and Bau III, 1997).Moreover, low-rank approximations are used to operate on matrices that may contain missing values as well as for purposes of lossy compression and dimensionality reduction (Moonen and De Moor, 1995;Markovsky, 2011).Draft (2024-01-15) of "Mathematics for Machine Learning".Feedback: https://mml-book.com.4.1 Compute the determinant using the Laplace expansion (using the first row) and the Sarrus rule for4.2 Compute the following determinant efficiently:A4.5 Diagonalizability of a matrix is unrelated to its invertibility.Determine for the following four matrices whether they are diagonalizable and/or invertible4.6 Compute the eigenspaces of the following transformation matrices.Are they diagonalizable?a.For 1384.7 Are the following matrices diagonalizable?If yes, determine their diagonal form and a basis with respect to which the transformation matrices are diagonal.If no, give reasons why they are not diagonalizable.a.4.9 Find the singular value decomposition of4.10 Find the rank-1 approximation of4.11 Show that for any A ∈ R m×n the matrices A ⊤ A and AA ⊤ possess the same nonzero eigenvalues.4.12 Show that for x ̸ = 0 Theorem 4.24 holds, i.e., show thatwhere σ 1 is the largest singular value of A ∈ R m×n .Draft (2024-01-15) of "Mathematics for Machine Learning".Feedback: https://mml-book.com.Many algorithms in machine learning optimize an objective function with respect to a set of desired model parameters that control how well a model explains the data: Finding good parameters can be phrased as an optimization problem (see Sections 8.2 and 8.3).Examples include: (i) linear regression (see Chapter 9), where we look at curve-fitting problems and optimize linear weight parameters to maximize the likelihood; (ii) neural-network auto-encoders for dimensionality reduction and data compression, where the parameters are the weights and biases of each layer, and where we minimize a reconstruction error by repeated application of the chain rule; and (iii) Gaussian mixture models (see Chapter 11) for modeling data distributions, where we optimize the location and shape parameters of each mixture component to maximize the likelihood of the model.Figure 5.1 illustrates some of these problems, which we typically solve by using optimization algorithms that exploit gradient information (Section 7.1).Figure 5.2 gives an overview of how concepts in this chapter are related and how they are connected to other chapters of the book.Central to this chapter is the concept of a function.A function f is a quantity that relates two quantities to each other.In this book, these quantities are typically inputs x ∈ R D and targets (function values) f (x), which we assume are real-valued if not stated otherwise.Here R D is the domain of f , and the function values f (x) are the image/codomain of f .domain image/codomain  Section 2.7.3 provides much more detailed discussion in the context of linear functions.We often write(5.1b)to specify a function, where (5.1a) specifies that f is a mapping from R D to R and (5.1b) specifies the explicit assignment of an input x to a function value f (x).A function f assigns every input x exactly one function value f (x).Recall the dot product as a special case of an inner product (Section 3.2).In the previous notation, the function f (x) = x ⊤ x, x ∈ R 2 , would be specified as(5.2b)In this chapter, we will discuss how to compute gradients of functions, which is often essential to facilitate learning in machine learning models since the gradient points in the direction of steepest ascent.Therefore, Draft (2024-01-15) of "Mathematics for Machine Learning".Feedback: https://mml-book.com.The average incline of a function f between x 0 and x 0 + δx is the incline of the secant (blue) through f (x 0 ) and f (x 0 + δx) and given by δy/δx.vector calculus is one of the fundamental mathematical tools we need in machine learning.Throughout this book, we assume that functions are differentiable.With some additional technical definitions, which we do not cover here, many of the approaches presented can be extended to sub-differentials (functions that are continuous but not differentiable at certain points).We will look at an extension to the case of functions with constraints in Chapter 7.In the following, we briefly revisit differentiation of a univariate function, which may be familiar from high school mathematics.We start with the difference quotient of a univariate function y = f (x), x, y ∈ R, which we will subsequently use to define derivatives.Definition 5.1 (Difference Quotient).The difference quotient difference quotientcomputes the slope of the secant line through two points on the graph of f .In Figure 5.3, these are the points with x-coordinates x 0 and x 0 + δx.The difference quotient can also be considered the average slope of f between x and x + δx if we assume f to be a linear function.In the limit for δx → 0, we obtain the tangent of f at x, if f is differentiable.The tangent is then the derivative of f at x. Definition 5.2 (Derivative).More formally, for h > 0 the derivative of f derivative at x is defined as the limitand the secant in Figure 5.3 becomes a tangent.The derivative of f points in the direction of steepest ascent of f .We want to compute the derivative of f (x) = x n , n ∈ N. We may already know that the answer will be nx n−1 , but we want to derive this result using the definition of the derivative as the limit of the difference quotient.Using the definition of the derivative in (5.4), we obtain(5.5c)We see that x n = n 0 x n−0 h 0 .By starting the sum at 1, the x n -term cancels, and we obtain(5.6d)The Taylor series is a representation of a function f as an infinite sum of terms.These terms are determined using derivatives of f evaluated at x 0 .Definition 5.3 (Taylor Polynomial).The Taylor polynomial of degree n of Taylor polynomial f : R → R at x 0 is defined asWe define t 0 := 1 for all t ∈ R.Twhere f (k) (x 0 ) is the kth derivative of f at x 0 (which we assume exists) andare the coefficients of the polynomial.Definition 5.4 (Taylor Series).For a smooth function f ∈ C ∞ , f : R → R, the Taylor series of f at x 0 is defined asTaylor seriesDraft (2024-01-15) of "Mathematics for Machine Learning".Feedback: https://mml-book.com.143(5.8)For x 0 = 0, we obtain the Maclaurin series as a special instance of the f ∈ C ∞ means that f is continuously differentiable infinitely many times.Maclaurin seriesTaylor series.If f (x) = T ∞ (x), then f is called analytic.analytic Remark.In general, a Taylor polynomial of degree n is an approximation of a function, which does not need to be a polynomial.The Taylor polynomial is similar to f in a neighborhood around x 0 .However, a Taylor polynomial of degree n is an exact representation of a polynomial f of degree k ⩽ n since all derivatives f (i) , i > k vanish.♢We consider the polynomial(5.9)and seek the Taylor polynomial T 6 , evaluated at x 0 = 1.We start by computing the coefficients f (k) (1) for k = 0, . . ., 6:(5.11) f ′′ (1) = 12(5.12)(5.13) 5) (1) = 0 (5.15)Therefore, the desired Taylor polynomial isMultiplying out and re-arranging yieldsi.e., we obtain an exact representation of the original function.(5.19)We seek a Taylor series expansion of f at x 0 = 0, which is the Maclaurin series expansion of f .We obtain the following derivatives:(5.24) . . .We can see a pattern here: The coefficients in our Taylor series are only ±1 (since sin(0) = 0), each of which occurs twice before switching to the other one.Furthermore,Therefore, the full Taylor series expansion of f at x 0 = 0 is given bywhere we used the power series representations(5.27)Figure 5.4 shows the corresponding first Taylor polynomials T n for n = 0, 1, 5, 10.Remark.A Taylor series is a special case of a power serieswhere a k are coefficients and c is a constant, which has the special form in Definition 5.4.♢In the following, we briefly state basic differentiation rules, where we denote the derivative of f by f ′ .Product rule:(5.30) Sum rule:Here, g • f denotes function composition x → f (x) → g(f (x)).Let us compute the derivative of the function h(x) = (2x + 1) 4 using the chain rule.Withwe obtain the derivatives of f and g as (5.36) such that the derivative of h is given as(5.34)= 4(2x + 1) 3 • 2 = 8(2x + 1) 3 , (5.38)where we used the chain rule (5.32) and substituted the definition of f in (5.34) in g ′ (f ).Differentiation as discussed in Section 5.1 applies to functions f of a scalar variable x ∈ R. In the following, we consider the general case where the function f depends on one or more variables x ∈ R n , e.g., f (x) = f (x 1 , x 2 ).The generalization of the derivative to functions of several variables is the gradient.We find the gradient of the function f with respect to x by varying one variable at a time and keeping the others constant.The gradient is then the collection of these partial derivatives.Definition 5.5 (Partial Derivative).For a function f : R n → R, x → f (x), x ∈ R n of n variables x 1 , . . ., x n we define the partial derivatives as partial derivative(5.39) and collect them in the row vector (5.40)where n is the number of variables and 1 is the dimension of the image/ range/codomain of f .Here, we defined the column vector x = [x 1 , . . ., x n ] ⊤ ∈ R n .The row vector in (5.40) is called the gradient of f or the Jacobian gradient Jacobian and is the generalization of the derivative from Section 5.1.Remark.This definition of the Jacobian is a special case of the general definition of the Jacobian for vector-valued functions as the collection of partial derivatives.We will get back to this in Section 5.3.♢We can use results from scalar differentiation: Each partial derivative is a derivative with respect to a scalar.(5.42)where we used the chain rule (5.32) to compute the partial derivatives.Remark (Gradient as a Row Vector).It is not uncommon in the literature to define the gradient vector as a column vector, following the convention that vectors are generally column vectors.The reason why we define the gradient vector as a row vector is twofold: First, we can consistently generalize the gradient to vector-valued functions f : R n → R m (then the gradient becomes a matrix).Second, we can immediately apply the multi-variate chain rule without paying attention to the dimension of the gradient.We will discuss both points in Section 5.3.♢ Example 5.7 (Gradient) For f (x 1 , x 2 ) = x 2 1 x 2 + x 1 x 3 2 ∈ R, the partial derivatives (i.e., the derivatives of f with respect to x 1 and x 2 ) are(5.44) and the gradient is then(5.45)Product rule:In the multivariate case, where x ∈ R n , the basic differentiation rules that we know from school (e.g., sum rule, product rule, chain rule; see also Section 5.1.2) still apply.However, when we compute derivatives with respect to vectors x ∈ R n we need to pay attention: Our gradients now involve vectors and matrices, and matrix multiplication is not commutative (Section 2.2.1), i.e., the order matters.Here are the general product rule, sum rule, and chain rule: bles to some degree the rules for matrix multiplication where we said that neighboring dimensions have to match for matrix multiplication to be defined; see Section 2.2.1.If we go from left to right, the chain rule exhibits similar properties: ∂f shows up in the "denominator" of the first factor and in the "numerator" of the second factor.If we multiply the factors together, multiplication is defined, i.e., the dimensions of ∂f match, and ∂f "cancels", such that ∂g/∂x remains.Consider a function f : R 2 → R of two variables x 1 , x 2 .Furthermore, (5.53)This compact way of writing the chain rule as a matrix multiplication only The chain rule can be written as a matrix multiplication.makes sense if the gradient is defined as a row vector.Otherwise, we will need to start transposing gradients for the matrix dimensions to match.This may still be straightforward as long as the gradient is a vector or a matrix; however, when the gradient becomes a tensor (we will discuss this in the following), the transpose is no longer a triviality.Remark (Verifying the Correctness of a Gradient Implementation).The definition of the partial derivatives as the limit of the corresponding difference quotient (see (5.39)) can be exploited when numerically checking the correctness of gradients in computer programs: When we compute Gradient checking gradients and implement them, we can use finite differences to numerically test our computation and implementation: We choose the value h to be small (e.g., h = 10 −4 ) and compare the finite-difference approximation from (5.39) with our (analytic) implementation of the gradient.If the error is small, our gradient implementation is probably correct."Small" could mean that i (dhi−dfi) 2 i (dhi+dfi) 2 < 10 −6 , where dh i is the finite-difference approximation and df i is the analytic gradient of f with respect to the ith variable x i .♢Thus far, we discussed partial derivatives and gradients of functions f : R n → R mapping to the real numbers.In the following, we will generalize the concept of the gradient to vector-valued functions (vector fields) f : R n → R m , where n ⩾ 1 and m > 1.For a function f : R n → R m and a vector x = [x 1 , . . ., x n ] ⊤ ∈ R n , the corresponding vector of function values is given as. . .(5.54)Writing the vector-valued function in this way allows us to view a vectorvalued function f : R n → R m as a vector of functions [f 1 , . . ., f m ] ⊤ , f i : R n → R that map onto R. The differentiation rules for every f i are exactly the ones we discussed in Section 5.2.Therefore, the partial derivative of a vector-valued function f : R n → R m with respect to x i ∈ R, i = 1, . . .n, is given as the vector (5.55)From (5.40), we know that the gradient of f with respect to a vector is the row vector of the partial derivatives.In (5.55), every partial derivative ∂f /∂x i is itself a column vector.Therefore, we obtain the gradient of f : R n → R m with respect to x ∈ R n by collecting these partial derivatives:∂x n (5.56a)(5.56b) Definition 5.6 (Jacobian).The collection of all first-order partial derivatives of a vector-valued function f : R n → R m is called the Jacobian.The Jacobian Jacobian J is an m × n matrix, which we define and arrange as follows:The gradient of a function f : R n → R m is a matrix of size m × n.(5.59)As a special case of (5.58), a function f : R n → R 1 , which maps a vector x ∈ R n onto a scalar (e.g., f (x) = n i=1 x i ), possesses a Jacobian that is a row vector (matrix of dimension 1 × n); see (5.40).Remark.In this book, we use the numerator layout of the derivative, i.e., numerator layout the derivative df /dx of f ∈ R m with respect to x ∈ R n is an m × n matrix, where the elements of f define the rows and the elements of x define the columns of the corresponding Jacobian; see (5.58).Thereexists also the denominator layout, which is the transpose of the numerator denominator layout layout.In this book, we will use the numerator layout.♢We will see how the Jacobian is used in the change-of-variable method for probability distributions in Section 6.7.The amount of scaling due to the transformation of a variable is provided by the determinant.In Section 4.1, we saw that the determinant can be used to compute the area of a parallelogram.If we are given two vectors b 1 = [1, 0] ⊤ , b 2 = [0, 1] ⊤ as the sides of the unit square (blue; see Figure 5.5), the area of this square is(5.60)If we take a parallelogram with the sides(orange in Figure 5.5), its area is given as the absolute value of the determinant (see Section 4.1) (5.61) i.e., the area of this is exactly three times the area of the unit square.We can find this scaling factor by finding a mapping that transforms the unit square into the other square.In linear algebra terms, we effectively perform a variable transformation from (b 1 , b 2 ) to (c 1 , c 2 ).In our case, the mapping is linear and the absolute value of the determinant of this mapping gives us exactly the scaling factor we are looking for.We will describe two approaches to identify this mapping.First, we exploit that the mapping is linear so that we can use the tools from Chapter 2 to identify this mapping.Second, we will find the mapping using partial derivatives using the tools we have been discussing in this chapter.To get started with the linear algebra approach, we identify both {b 1 , b 2 } and {c 1 , c 2 } as bases of R 2 (see Section 2.6.1 for a recap).What we effectively perform is a change of basis from (b 1 , b 2 ) to (c 1 , c 2 ), and we are looking for the transformation matrix that implements the basis change.Using results from Section 2.7.2, we identify the desired basis change matrix as nant of J , which yields the scaling factor we are looking for, is given as |det(J )| = 3, i.e., the area of the square spanned by (c 1 , c 2 ) is three times greater than the area spanned by (b 1 , b 2 ).The linear algebra approach works for linear transformations; for nonlinear transformations (which become relevant in Section 6.7), we follow a more general approach using partial derivatives.For this approach, we consider a function f : R 2 → R 2 that performs a variable transformation.In our example, f maps the coordinate representation of any vector x ∈ R 2 with respect to (b 1 , b 2 ) onto the coordinate representation y ∈ R 2 with respect to (c 1 , c 2 ).We want to identify the mapping so that we can compute how an area (or volume) changes when it is being transformed by f .For this, we need to find out how f (x) changes if we modify x a bit.This question is exactly answered by the Jacobian matrix df dx ∈ R 2×2 .Since we can write(5.64)we obtain the functional relationship between x and y, which allows us to get the partial derivativesand compose the Jacobian as(5.66)The Jacobian represents the coordinate transformation we are looking Geometrically, the Jacobian determinant gives the magnification/ scaling factor when we transform an area or volume.for.It is exact if the coordinate transformation is linear (as in our case), and (5.66) recovers exactly the basis change matrix in (5.62).If the coordinate transformation is nonlinear, the Jacobian approximates this nonlinear transformation locally with a linear one.The absolute value of the Jacobian determinant |det(J )| is the factor by which areas or volumes are Jacobian determinant scaled when coordinates are transformed.Our case yields |det(J )| = 3.The Jacobian determinant and variable transformations will become relevant in Section 6.7 when we transform random variables and probability distributions.These transformations are extremely relevant in ma- In this chapter, we encountered derivatives of functions.Figure 5.6 summarizes the dimensions of those derivatives.If f : R → R the gradient is simply a scalar (top-left entry).For f : R D → R the gradient is a 1 × D row vector (top-right entry).For f : R → R E , the gradient is an E × 1 column vector, and for f : R D → R E the gradient is an E × D matrix.Example 5.9 (Gradient of a Vector-Valued Function) We are givenTo compute the gradient df /dx we first determine the dimension of df /dx: Since f : R N → R M , it follows that df /dx ∈ R M ×N .Second, to compute the gradient we determine the partial derivatives of f with respect to every x j :We collect the partial derivatives in the Jacobian and obtain the gradient (5.70) f (x) = exp(x 1 x 2 2 ) , (5.71)and compute the gradient of h with respect to t.Since f : R 2 → R and g : R → R 2 we note that(5.73)The desired gradient is computed by applying the chain rule:  (5.75)where θ ∈ R D is a parameter vector, Φ ∈ R N ×D are input features and y ∈ R N are the corresponding observations.We define the functions L(e) := ∥e∥ 2 , (5.76) e(θ) := y − Φθ .(5.77)We seek ∂L ∂θ , and we will use the chain rule for this purpose.L is called a least-squares loss function.least-squares loss Before we start our calculation, we determine the dimensionality of the gradient as(5.78)The chain rule allows us to compute the gradient aswhere the dth element is given by(5.80)We know that ∥e∥ 2 = e ⊤ e (see Section 3.2) and determine(5.81) Furthermore, we obtain (5.82) such that our desired derivative is(5.83)Remark.We would have obtained the same result without using the chain rule by immediately looking at the function L 2 (θ) := ∥y − Φθ∥ 2 = (y − Φθ) ⊤ (y − Φθ) .(5.84)This approach is still practical for simple functions like L 2 but becomes impractical for deep function compositions.♢Draft (2024-01-15) of "Mathematics for Machine Learning".Feedback: https://mml-book.com., each of which is a 4 × 2 matrix, and collate them in a 4 × 2 × 3 tensor.(b) Approach 2: We re-shape (flatten) A ∈ R 4×2 into a vector Ã ∈ R 8 .Then, we compute the gradient d Ã dx ∈ R 8×3 .We obtain the gradient tensor by re-shaping this gradient as illustrated above.We can think of a tensor as a multidimensional array.We will encounter situations where we need to take gradients of matrices with respect to vectors (or other matrices), which results in a multidimensional tensor.We can think of this tensor as a multidimensional array that collects partial derivatives.For example, if we compute the gradient of an m × n matrix A with respect to a p × q matrix B, the resulting Jacobian would be (m×n)×(p×q), i.e., a four-dimensional tensor J , whose entries are given as J ijkl = ∂A ij /∂B kl .Since matrices represent linear mappings, we can exploit the fact that there is a vector-space isomorphism (linear, invertible mapping) between the space R m×n of m × n matrices and the space R mn of mn vectors.Therefore, we can re-shape our matrices into vectors of lengths mn and pq, respectively.The gradient using these mn vectors results in a Jacobian of size mn × pq.plications, it is often desirable to re-shape the matrix into a vector and continue working with this Jacobian matrix: The chain rule (5.48) boils down to simple matrix multiplication, whereas in the case of a Jacobian tensor, we will need to pay more attention to what dimensions we need to sum out.Example 5.12 (Gradient of Vectors with Respect to Matrices) Let us consider the following example, whereand where we seek the gradient df /dA.Let us start again by determining the dimension of the gradient as(5.86)By definition, the gradient is the collection of the partial derivatives:(5.87)To compute the partial derivatives, it will be helpful to explicitly write out the matrix vector multiplication: (5.88) and the partial derivatives are then given as(5.89)This allows us to compute the partial derivatives of f i with respect to a row of A, which is given as (5.90)Draft (2024-01-15) of "Mathematics for Machine Learning".Feedback: https://mml-book.com.∂f i ∂A k̸ =i,:= 0 ⊤ ∈ R 1×1×N(5.91)where we have to pay attention to the correct dimensionality.Since f i maps onto R and each row of A is of size 1 × N , we obtain a 1 × 1 × Nsized tensor as the partial derivative of f i with respect to a row of A.We stack the partial derivatives (5.91) and get the desired gradient in (5.87) via(5.92)Example 5.13 (Gradient of Matrices with Respect to Matrices)where we seek the gradient dK/dR.To solve this hard problem, let us first write down what we already know: The gradient has the dimensionswhich is a tensor.Moreover,for p, q = 1, . . ., N , where K pq is the (p, q)th entry of K = f (R).Denoting the ith column of R by r i , every entry of K is given by the dot product of two columns of R, i.e.,(5.96)When we now compute the partial derivative ∂Kpq ∂Rij we obtain 158Vector Calculus(5.98)From (5.94), we know that the desired gradient has the dimension (N × N ) × (M × N ), and every single entry of this tensor is given by ∂ pqij in (5.98),where p, q, j = 1, . . ., N and i = 1, . . ., M .In the following, we list some useful gradients that are frequently required in a machine learning context (Petersen and Pedersen, 2012).Here, we use tr(•) as the trace (see Definition 4.4), det(•) as the determinant (see Section 4.1) and f (X) −1 as the inverse of f (X), assuming it exists.Remark.In this book, we only cover traces and transposes of matrices.However, we have seen that derivatives can be higher-dimensional tensors, in which case the usual trace and transpose are not defined.In these cases, the trace of a D ×D ×E ×F tensor would be an E ×F -dimensional matrix.This is a special case of a tensor contraction.Similarly, when we Draft (2024-01-15) of "Mathematics for Machine Learning".Feedback: https://mml-book.com."transpose" a tensor, we mean swapping the first two dimensions.Specifically, in (5.99) through (5.102), we require tensor-related computations when we work with multivariate functions f (•) and compute derivatives with respect to matrices (and choose not to vectorize them as discussed in Section 5.4).♢A good discussion about backpropagation and the chain rule is available at a blog by Tim Vieira at https://tinyurl.com/ycfm2yrw.In many machine learning applications, we find good model parameters by performing gradient descent (Section 7.1), which relies on the fact that we can compute the gradient of a learning objective with respect to the parameters of the model.For a given objective function, we can obtain the gradient with respect to the model parameters using calculus and applying the chain rule; see Section 5.2.2.We already had a taste in Section 5.3 when we looked at the gradient of a squared loss with respect to the parameters of a linear regression model.Consider the function(5.109)By application of the chain rule, and noting that differentiation is linear, we compute the gradient− sin x 2 + exp(x 2 ) 1 + exp(x 2 ) .(5.110)Writing out the gradient in this explicit way is often impractical since it often results in a very lengthy expression for a derivative.In practice, it means that, if we are not careful, the implementation of the gradient could be significantly more expensive than computing the function, which imposes unnecessary overhead.For training deep neural network models, the backpropagation algorithm (Kelley, 1960;Bryson, 1961;Dreyfus, backpropagation 1962;Rumelhart et al., 1986) is an efficient way to compute the gradient of an error function with respect to the parameters of the model.An area where the chain rule is used to an extreme is deep learning, where the function value y is computed as a many-level function composition (5.111)where x are the inputs (e.g., images), y are the observations (e.g., class labels), and every function f i , i = 1, . . ., K, possesses its own parameters.In neural networks with multiple layers, we have functions f i (x i−1 ) =We discuss the case, where the activation functions are identical in each layer to unclutter notation.σ(A i−1 x i−1 + b i−1 ) in the ith layer.Here x i−1 is the output of layer i − 1 and σ an activation function, such as the logistic sigmoid 1 1+e −x , tanh or a rectified linear unit (ReLU).In order to train these models, we require the gradient of a loss function L with respect to all model parameters A j , b j for j = 1, . . ., K.This also requires us to compute the gradient of L with respect to the inputs of each layer.For example, if we have inputs x and observations y and a network structure defined by(5.113) see also Figure 5.8 for a visualization, we may be interested in finding A j , b j for j = 0, . . ., K − 1, such that the squared lossis minimized, where θ = {A 0 , b 0 , . . .,To obtain the gradients with respect to the parameter set θ, we require the partial derivatives of L with respect to the parameters θ j = {A j , b j } of each layer j = 0, . . ., K − 1.The chain rule allows us to determine the partial derivatives as A more in-depth discussion about gradients of neural networks can be found in Justin Domke's lecture notes https://tinyurl.com/yalcxgtv.The orange terms are partial derivatives of the output of a layer with respect to its inputs, whereas the blue terms are partial derivatives of the output of a layer with respect to its parameters.Assuming, we have already computed the partial derivatives ∂L/∂θ i+1 , then most of the computation can be reused to compute ∂L/∂θ i .The additional terms that we Draft (2024-01-15) of "Mathematics for Machine Learning".Feedback: https://mml-book.com.Figure 5.9 Backward pass in a multi-layer neural network to compute the gradients of the loss function.x a b y need to compute are indicated by the boxes.Figure 5.9 visualizes that the gradients are passed backward through the network.It turns out that backpropagation is a special case of a general technique in numerical analysis called automatic differentiation.(5.121) Equation (5.120) would be the reverse mode because gradients are propreverse mode agated backward through the graph, i.e., reverse to the data flow.Equation (5.121) would be the forward mode, where the gradients flow with forward mode the data from left to right through the graph.In the following, we will focus on reverse mode automatic differentiation, which is backpropagation.In the context of neural networks, where the input dimensionality is often much higher than the dimensionality of the labels, the reverse mode is computationally significantly cheaper than the forward mode.Let us start with an instructive example.Example 5.14 Consider the function(5.122) from (5.109).If we were to implement a function f on a computer, we would be able to save some computation by using intermediate variables: (5.128)This is the same kind of thinking process that occurs when applying the chain rule.Note that the preceding set of equations requires fewer operations than a direct implementation of the function f (x) as defined in (5.109).The corresponding computation graph in Figure 5.11 shows the flow of data and computations required to obtain the function value f .The set of equations that include intermediate variables can be thought of as a computation graph, a representation that is widely used in implementations of neural network software libraries.We can directly compute the derivatives of the intermediate variables with respect to their corresponding inputs by recalling the definition of the derivative of elementary functions.We obtain the following: (5.138)Note that we implicitly applied the chain rule to obtain ∂f /∂x.By substituting the results of the derivatives of the elementary functions, we getBy thinking of each of the derivatives above as a variable, we observe that the computation required for calculating the derivative is of similar complexity as the computation of the function itself.This is quite counterintuitive since the mathematical expression for the derivative ∂f ∂x (5.110) is significantly more complicated than the mathematical expression of the function f (x) in (5.109).Automatic differentiation is a formalization of Example 5.14.Let x 1 , . . ., x d be the input variables to the function, x d+1 , . . ., x D−1 be the intermediate variables, and x D the output variable.Then the computation graph can be expressed as follows: is the backpropagation of the gradient through the computation graph.For neural network training, we backpropagate the error of the prediction with respect to the label.The automatic differentiation approach above works whenever we have a function that can be expressed as a computation graph, where the elementary functions are differentiable.In fact, the function may not even be a mathematical function but a computer program.However, not all computer programs can be automatically differentiated, e.g., if we cannot find differential elementary functions.Programming structures, such as for loops and if statements, require more care as well.So far, we have discussed gradients, i.e., first-order derivatives.Sometimes, we are interested in derivatives of higher order, e.g., when we want to use Newton's Method for optimization, which requires second-order derivatives (Nocedal and Wright, 2006).In Section 5.1.1,we discussed the Taylor series to approximate functions using polynomials.In the multivariate case, we can do exactly the same.In the following, we will do exactly this.But let us start with some notation.Consider a function f : R 2 → R of two variables x, y.We use the following notation for higher-order partial derivatives (and for gradients):∂ 2 f ∂x 2 is the second partial derivative of f with respect to x.∂ n f ∂x n is the nth partial derivative of f with respect to x.(a) Given a vector δ ∈ R 4 , we obtain the outer product δ 2 := δ ⊗ δ = δδ ⊤ ∈ R 4×4 as a matrix.(b) An outer product δ 3 := δ ⊗ δ ⊗ δ ∈ R 4×4×4 results in a third-order tensor ("threedimensional matrix"), i.e., an array with three indexes.Definition 5.7 (Multivariate Taylor Series).We consider a functionthat is smooth at x 0 .When we define the difference vector δ := x − x 0 , the multivariate Taylor series of f at (x 0 ) is defined as multivariate Taylor series (5.151)where D k x f (x 0 ) is the k-th (total) derivative of f with respect to x, evaluated at x 0 .Definition 5.8 (Taylor Polynomial).The Taylor polynomial of degree n of Taylor polynomial f at x 0 contains the first n + 1 components of the series in (5.151) and is defined as(5.152)In (5.151) and (5.152), we used the slightly sloppy notation of δ k , which is not defined for vectors x ∈ R D , D > 1, and k > 1.Note that both D k x f and δ k are k-th order tensors, i.e., k-dimensional arrays.The A vector can be implemented as a one-dimensional array, a matrix as a two-dimensional array.kth-order tensor δ k ∈ R k times D×D×...×D is obtained as a k-fold outer product, denoted by ⊗, of the vector δ ∈ R D .For example,(5.153)Draft (2024-01-15) of "Mathematics for Machine Learning".Feedback: https://mml-book.com.(5.154) Figure 5.13 visualizes two such outer products.In general, we obtain the termsin the Taylor series, where D k x f (x 0 )δ k contains k-th order polynomials.Now that we defined the Taylor series for vector fields, let us explicitly write down the first terms D kx f (x 0 )δ k of the Taylor series expansion for k = 0, . . ., 3 and δ := x − x 0 : np.einsum ( 'i,i',Df1,d) np.einsum( 'ij,i,j',Df2,d,d) np.einsum( 'ijk,i,j,k',Df3,d,d,d)Here, H(x 0 ) is the Hessian of f evaluated at x 0 .Consider the function f (x, y) = x 2 + 2xy + y 3 .(5.161)We want to compute the Taylor series expansion of f at (x 0 , y 0 ) = (1, 2).Before we start, let us discuss what to expect: The function in (5.161) is a polynomial of degree 3. We are looking for a Taylor series expansion, which itself is a linear combination of polynomials.Therefore, we do not expect the Taylor series expansion to contain terms of fourth or higher order to express a third-order polynomial.This means that it should be sufficient to determine the first four terms of (5.151) for an exact alternative representation of (5.161).To determine the Taylor series expansion, we start with the constant term and the first-order derivatives, which are given by f (1, 2) = 13(5.162)(1, 2) = 14 .(5.164) Therefore, we obtain(5.166)Note that D 1 x,y f (1, 2)δ contains only linear terms, i.e., first-order polynomials.The second-order partial derivatives are given by(5.170)When we collect the second-order partial derivatives, we obtain the Hessian(5.172) Therefore, the next term of the Taylor-series expansion is given byHere, D 2 x,y f (1, 2)δ 2 contains only quadratic terms, i.e., second-order polynomials.Draft (2024-01-15) of "Mathematics for Machine Learning".Feedback: https://mml-book.com.The third-order derivatives are obtained as(5.175)(5.176)Since most second-order partial derivatives in the Hessian in (5.171) are constant, the only nonzero third-order partial derivative is(5.177)Higher-order derivatives and the mixed derivatives of degree 3 (e.g.,which collects all cubic terms of the Taylor series.Overall, the (exact) Taylor series expansion of f at (x 0 , yIn this case, we obtained an exact Taylor series expansion of the polynomial in (5.161), i.e., the polynomial in (5.180c) is identical to the original polynomial in (5.161).In this particular example, this result is not surprising since the original function was a third-order polynomial, which we expressed through a linear combination of constant terms, first-order, second-order, and third-order polynomials in (5.180c).Further details of matrix differentials, along with a short review of the required linear algebra, can be found in Magnus and Neudecker (2007).Automatic differentiation has had a long history, and we refer to Griewank and Walther (2003), Griewank andWalther (2008), andElliott (2009) and the references therein.In machine learning (and other disciplines), we often need to compute expectations, i.e., we need to solve integrals of the form( approximation (MacKay, 2003;Bishop, 2006;Murphy, 2012), which uses a second-order Taylor series expansion (requiring the Hessian) for a local Gaussian approximation of p(x) around its mode.5.1 Compute the derivative f ′ (x) for f (x) = log(x 4 ) sin(x 3 ) ..where µ, σ ∈ R are constants.5.4 Compute the Taylor polynomials Tn, n = 0, . . ., 5 of f (x) = sin(x) + cos(x) at x 0 = 0. 5.5 Consider the following functions:Draft (2024-01-15) of "Mathematics for Machine Learning".Feedback: https://mml-book.com.a. What are the dimensions of ∂fi ∂x ?b.Compute the Jacobians.5.6 Differentiate f with respect to t and g with respect to X, wherewhere tr(•) denotes the trace.5.7 Compute the derivatives df /dx of the following functions by using the chain rule.Provide the dimensions of every single partial derivative.Describe your steps in detail.a.where sin(•) is applied to every element of z.5.8 Compute the derivatives df /dx of the following functions.Describe your steps in detail.a. Use the chain rule.Provide the dimensions of every single partial derivative.Here tr(A) is the trace of A, i.e., the sum of the diagonal elements A ii .Hint: Explicitly write out the outer product.c.Use the chain rule.Provide the dimensions of every single partial derivative.You do not need to compute the product of the partial derivatives explicitly.Probability, loosely speaking, concerns the study of uncertainty.Probability can be thought of as the fraction of times an event occurs, or as a degree of belief about an event.We then would like to use this probability to measure the chance of something occurring in an experiment.As mentioned in Chapter 1, we often quantify uncertainty in the data, uncertainty in the machine learning model, and uncertainty in the predictions produced by the model.Quantifying uncertainty requires the idea of a random variable, random variable which is a function that maps outcomes of random experiments to a set of properties that we are interested in.Associated with the random variable is a function that measures the probability that a particular outcome (or set of outcomes) will occur; this is called the probability distribution.probability distribution Probability distributions are used as a building block for other concepts, such as probabilistic modeling (Section 8.4), graphical models (Section 8.5), and model selection (Section 8.6).In the next section, we present the three concepts that define a probability space (the sample space, the events, and the probability of an event) and how they are related to a fourth concept called the random variable.The presentation is deliberately slightly hand wavy since a rigorous presentation may occlude the intuition behind the concepts.An outline of the concepts presented in this chapter are shown in Figure 6.1.The theory of probability aims at defining a mathematical structure to describe random outcomes of experiments.For example, when tossing a single coin, we cannot determine the outcome, but by doing a large number of coin tosses, we can observe a regularity in the average outcome.Using this mathematical structure of probability, the goal is to perform automated reasoning, and in this sense, probability generalizes logical reasoning (Jaynes, 2003).When constructing automated reasoning systems, classical Boolean logic does not allow us to express certain forms of plausible reasoning.Consider the following scenario: We observe that A is false.We find B becomes less plausible, although no conclusion can be drawn from classical logic.We observe that B is true.It seems A becomes more plausible.We use this form of reasoning daily.We are waiting for a friend, and consider three possibilities: H1, she is on time; H2, she has been delayed by traffic; and H3, she has been abducted by aliens.When we observe our friend is late, we must logically rule out H1.We also tend to consider H2 to be more likely, though we are not logically required to do so.Finally, we may consider H3 to be possible, but we continue to consider it quite unlikely.How do we conclude H2 is the most plausible answer?Seen in this way, "For plausible reasoning it is necessary to extend the discrete true and false values of truth to continuous plausibilities" (Jaynes, 2003).probability theory can be considered a generalization of Boolean logic.In the context of machine learning, it is often applied in this way to formalize the design of automated reasoning systems.Further arguments about how probability theory is the foundation of reasoning systems can be found in Pearl (1988).The philosophical basis of probability and how it should be somehow related to what we think should be true (in the logical sense) was studied by Cox (Jaynes, 2003).Another way to think about it is that if we are precise about our common sense we end up constructing probabilities.E. T. Jaynes  identified three mathematical criteria, which must apply to all plausibilities: (Grinstead and Snell, 1997;Jaynes, 2003) that introduce the three concepts of sample space, event space, and probability measure.The probability space models a real-world process (referred to as an experiment) with random outcomes.The sample space is the set of all possible outcomes of the experiment, sample space usually denoted by Ω.For example, two successive coin tosses have a sample space of {hh, tt, ht, th}, where "h" denotes "heads" and "t" denotes "tails".The event space is the space of potential results of the experiment.A event space subset A of the sample space Ω is in the event space A if at the end of the experiment we can observe whether a particular outcome ω ∈ Ω is in A. The event space A is obtained by considering the collection of subsets of Ω, and for discrete probability distributions (Section 6.2.1)A is often the power set of Ω.With each event A ∈ A, we associate a number P (A) that measures the probability or degree of belief that the event will occur.P (A) is called the probability of A.probabilityThe probability of a single event must lie in the interval [0, 1], and the total probability over all outcomes in the sample space Ω must be 1, i.e., P (Ω) = 1.Given a probability space (Ω, A, P ), we want to use it to model some real-world phenomenon.In machine learning, we often avoid explicitly referring to the probability space, but instead refer to probabilities on quantities of interest, which we denote by T .In this book, we refer to T as the target space and refer to elements of T as states.We introduce a target space function X : Ω → T that takes an element of Ω (an outcome) and returns a particular quantity of interest x, a value in T .This association/mapping from Ω to T is called a random variable.For example, in the case of tossing random variable two coins and counting the number of heads, a random variable X maps to the three possible outcomes: X(hh) = 2, X(ht) = 1, X(th) = 1, and X(tt) = 0.In this particular case, T = {0, 1, 2}, and it is the probabilities on elements of T that we are interested in.For a finite sample space Ω and The name "random variable" is a great source of misunderstanding as it is neither random nor is it a variable.It is a function.finite T , the function corresponding to a random variable is essentially a lookup table.For any subset S ⊆ T , we associate P X (S) ∈ [0, 1] (the probability) to a particular event occurring corresponding to the random variable X. Example 6.1 provides a concrete illustration of the terminology.Remark.The aforementioned sample space Ω unfortunately is referred to by different names in different books.Another common name for Ω is "state space" (Jacod and Protter, 2004), but state space is sometimes reserved for referring to states in a dynamical system (Hasselblatt and Katok, 2003).Other names sometimes used to describe Ω are: "sample description space", "possibility space," and "event space".♢ Example 6.1We assume that the reader is already familiar with computing probabilitiesThis toy example is essentially a biased coin flip example.of intersections and unions of sets of events.A gentler introduction to probability with many examples can be found in chapter 2 of Walpole et al. (2011).Consider a statistical experiment where we model a funfair game consisting of drawing two coins from a bag (with replacement).There are coins from USA (denoted as $) and UK (denoted as £) in the bag, and since we draw two coins from the bag, there are four outcomes in total.The state space or sample space Ω of this experiment is then ($, $), ($, £), (£, $), (£, £).Let us assume that the composition of the bag of coins is such that a draw returns at random a $ with probability 0.3.The event we are interested in is the total number of times the repeated draw returns $.Let us define a random variable X that maps the sample space Ω to T , which denotes the number of times we draw $ out of the bag.We can see from the preceding sample space we can get zero $, one $, or two $s, and therefore T = {0, 1, 2}.The random variable X (a function or lookup table) can be represented as a table like the following:Since we return the first coin we draw before drawing the second, this implies that the two draws are independent of each other, which we will discuss in Section 6.4.5.Note that there are two experimental outcomes, which map to the same event, where only one of the draws returns $.Therefore, the probability mass function (Section 6.2.1) of X is given byIn the calculation, we equated two different concepts, the probability of the output of X and the probability of the samples in Ω.For example, in (6.7) we say P (X = 0) = P ((£, £)).Consider the random variable X : Ω → T and a subset S ⊆ T (for example, a single element of T , such as the outcome that one head is obtained when tossing two coins).Let X −1 (S) be the pre-image of S by X, i.e., the set of elements of Ω that map to S under X; {ω ∈ Ω : X(ω) ∈ S}.One way to understand the transformation of probability from events in Ω via the random variable X is to associate it with the probability of the pre-image of S (Jacod and Protter, 2004).For S ⊆ T , we have the notationThe left-hand side of (6.8) is the probability of the set of possible outcomes (e.g., number of $ = 1) that we are interested in.Via the random variable X, which maps states to outcomes, we see in the right-hand side of (6.8) that this is the probability of the set of states (in Ω) that have the property (e.g., $£, £$).We say that a random variable X is distributed according to a particular probability distribution P X , which defines the probability mapping between the event and the probability of the outcome of the random variable.In other words, the function P X or equivalently P • X −1 is the law or distribution of random variable X.Remark.The target space, that is, the range T of the random variable X, is used to indicate the kind of probability space, i.e., a T random variable.When T is finite or countably infinite, this is called a discrete random variable (Section 6.2.1).For continuous random variables (Section 6.2.2), we only consider T = R or T = R D .♢Probability theory and statistics are often presented together, but they concern different aspects of uncertainty.One way of contrasting them is by the kinds of problems that are considered.Using probability, we can consider a model of some process, where the underlying uncertainty is captured by random variables, and we use the rules of probability to derive what happens.In statistics, we observe that something has happened and try to figure out the underlying process that explains the observations.In this sense, machine learning is close to statistics in its goals to construct a model that adequately represents the process that generated the data.We can use the rules of probability to obtain a "best-fitting" model for some data.Another aspect of machine learning systems is that we are interested in generalization error (see Chapter 8).This means that we are actually interested in the performance of our system on instances that we will observe in future, which are not identical to the instances that we have seen so far.This analysis of future performance relies on probability and statistics, most of which is beyond what will be presented in this chapter.The interested reader is encouraged to look at the books by Boucheron et al. (2013) and Shalev-Shwartz and Ben-David (2014).We will see more about statistics in Chapter 8.Let us focus our attention on ways to describe the probability of an event as introduced in Section 6.1.Depending on whether the target space is discrete or continuous, the natural way to refer to distributions is different.When the target space T is discrete, we can specify the probability that a random variable X takes a particular value x ∈ T , denoted as P (X = x).The expression P (X = x) for a discrete random variable X is known as the probability mass function.When the target space T is continuous, e.g., probability mass function the real line R, it is more natural to specify the probability that a random variable X is in an interval, denoted by P (a ⩽ X ⩽ b) for a < b.By convention, we specify the probability that a random variable X is less than a particular value x, denoted by P (X ⩽ x).The expression P (X ⩽ x) for a continuous random variable X is known as the cumulative distribution cumulative distribution function function.We will discuss continuous random variables in Section 6.2.2.We will revisit the nomenclature and contrast discrete and continuous random variables in Section 6.2.3.Remark.We will use the phrase univariate distribution to refer to distribuunivariate tions of a single random variable (whose states are denoted by non-bold x).We will refer to distributions of more than one random variable as multivariate distributions, and will usually consider a vector of random multivariate variables (whose states are denoted by bold x).♢When the target space is discrete, we can imagine the probability distribution of multiple random variables as filling out a (multidimensional) array of numbers.Figure 6.2 shows an example.The target space of the joint probability is the Cartesian product of the target spaces of each of the random variables.We define the joint probability as the entry of both joint probability values jointlywhere n ij is the number of events with state x i and y j and N the total number of events.The joint probability is the probability of the intersection of both events, that is, P (X = x i , Y = y j ) = P (X = x i ∩ Y = y j ).  that X = x and Y = y is (lazily) written as p(x, y) and is called the joint probability.One can think of a probability as a function that takes state x and y and returns a real number, which is the reason we write p(x, y).The marginal probability that X takes the value x irrespective of the value marginal probability of random variable Y is (lazily) written as p(x).We write X ∼ p(x) to denote that the random variable X is distributed according to p(x).If we consider only the instances where X = x, then the fraction of instances (the conditional probability) for which Y = y is written (lazily) as p(y | x).conditional probability Example 6.2 Consider two random variables X and Y , where X has five possible states and Y has three possible states, as shown in Figure 6.2.We denote by n ij the number of events with state X = x i and Y = y j , and denote by N the total number of events.The value c i is the sum of the individual frequencies for the ith column, that is, c i = 3 j=1 n ij .Similarly, the value r j is the row sum, that is, r j = 5 i=1 n ij .Using these definitions, we can compactly express the distribution of X and Y .The probability distribution of each random variable, the marginal probability, can be seen as the sum over a row or column where c i and r j are the ith column and jth row of the probability table, respectively.By convention, for discrete random variables with a finite number of events, we assume that probabilties sum up to one, that is, In machine learning, we use discrete probability distributions to model categorical variables, i.e., variables that take a finite set of unordered valcategorical variable ues.They could be categorical features, such as the degree taken at university when used for predicting the salary of a person, or categorical labels, such as letters of the alphabet when doing handwriting recognition.Discrete distributions are also often used to construct probabilistic models that combine a finite number of continuous distributions (Chapter 11).We consider real-valued random variables in this section, i.e., we consider target spaces that are intervals of the real line R.In this book, we pretend that we can perform operations on real random variables as if we have discrete probability spaces with finite states.However, this simplification is not precise for two situations: when we repeat something infinitely often, and when we want to draw a point from an interval.The first situation arises when we discuss generalization errors in machine learning (Chapter 8).The second situation arises when we want to discuss continuous distributions, such as the Gaussian (Section 6.5).For our purposes, the lack of precision allows for a briefer introduction to probability.Remark.In continuous spaces, there are two additional technicalities, which are counterintuitive.First, the set of all subsets (used to define the event space A in Section 6.1) is not well behaved enough.A needs to be restricted to behave well under set complements, set intersections, and set unions.Second, the size of a set (which in discrete spaces can be obtained by counting the elements) turns out to be tricky.The size of a set is called its measure.For example, the cardinality of discrete sets, the measure length of an interval in R, and the volume of a region in R d are all measures.Sets that behave well under set operations and additionally have a topology are called a Borel σ-algebra.Betancourt details a careful con-Borel σ-algebra struction of probability spaces from set theory without being bogged down in technicalities; see https://tinyurl.com/yb3t6mfd.For a more precise construction, we refer to Billingsley (1995) and Jacod and Protter (2004).In this book, we consider real-valued random variables with their cor-Draft (2024-01-15) of "Mathematics for Machine Learning".Feedback: https://mml-book.com.For probability mass functions (pmf) of discrete random variables, the integral in (6.15) is replaced with a sum (6.12).Observe that the probability density function is any function f that is non-negative and integrates to one.We associate a random variable X with this function f by (6.16) where a, b ∈ R and x ∈ R are outcomes of the continuous random variable X. States x ∈ R D are defined analogously by considering a vector of x ∈ R.This association (6.16) is called the law or distribution of the law random variable X.naturally from fulfilling the desiderata (Jaynes, 2003, chapter 2).Probabilistic modeling (Section 8.4) provides a principled foundation for designing machine learning methods.Once we have defined probability distributions (Section 6.2) corresponding to the uncertainties of the data and our problem, it turns out that there are only two fundamental rules, the sum rule and the product rule.Recall from (6.9) that p(x, y) is the joint distribution of the two random variables x, y.The distributions p(x) and p(y) are the corresponding marginal distributions, and p(y | x) is the conditional distribution of y given x.Given the definitions of the marginal and conditional probability for discrete and continuous random variables in Section 6.2, we can now present the two fundamental rules in probability theory.where Y are the states of the target space of random variable Y .This means that we sum out (or integrate out) the set of states y of the random variable Y .The sum rule is also known as the marginalization property.The sum rule relates the joint distribution to a marginal distribution.In general, when the joint distribution contains more than two random variables, the sum rule can be applied to any subset of the random variables, resulting in a marginal distribution of potentially more than one random variable.More concretely, if x = [x 1 , . . ., x D ] ⊤ , we obtain the marginal p(x i ) = p(x 1 , . . ., x D )dx \i (6.21) by repeated application of the sum rule where we integrate/sum out all random variables except x i , which is indicated by \i, which reads "all except i."Remark.Many of the computational challenges of probabilistic modeling are due to the application of the sum rule.When there are many variables or discrete variables with many states, the sum rule boils down to performing a high-dimensional sum or integral.Performing high-dimensional sums or integrals is generally computationally hard, in the sense that there is no known polynomial-time algorithm to calculate them exactly.♢The second rule, known as the product rule, relates the joint distribution The product rule can be interpreted as the fact that every joint distribution of two random variables can be factorized (written as a product)Draft (2024-01-15) of "Mathematics for Machine Learning".Feedback: https://mml-book.com.of two other distributions.The two factors are the marginal distribution of the first random variable p(x), and the conditional distribution of the second random variable given the first p(y | x).Since the ordering of random variables is arbitrary in p(x, y), the product rule also implies p(x, y) = p(x | y)p(y).To be precise, (6.22) is expressed in terms of the probability mass functions for discrete random variables.For continuous random variables, the product rule is expressed in terms of the probability density functions (Section 6.2.3).In machine learning and Bayesian statistics, we are often interested in making inferences of unobserved (latent) random variables given that we have observed other random variables.Let us assume we have some prior knowledge p(x) about an unobserved random variable x and some relationship p(y | x) between x and a second random variable y, which we can observe.If we observe y, we can use Bayes' theorem to draw some conclusions about x given the observed values of y.Bayes' theorem (also Bayes' theorem Bayes' rule or Bayes' law) In (6.23), p(x) is the prior, which encapsulates our subjective prior prior knowledge of the unobserved (latent) variable x before observing any data.We can choose any prior that makes sense to us, but it is critical to ensure that the prior has a nonzero pdf (or pmf) on all plausible x, even if they are very rare.The likelihood p(y | x) describes how x and y are related, and in the likelihoodThe likelihood is sometimes also called the "measurement model".case of discrete probability distributions, it is the probability of the data y if we were to know the latent variable x.Note that the likelihood is not a distribution in x, but only in y.We call p(y | x) either the "likelihood of x (given y)" or the "probability of y given x" but never the likelihood of y (MacKay, 2003).The posterior p(x | y) is the quantity of interest in Bayesian statistics posterior because it expresses exactly what we are interested in, i.e., what we know about x after having observed y.x ∈ R D is an average and is defined as (6.31)where (6.32) for d = 1, . . ., D, where the subscript d indicates the corresponding dimension of x.The integral and sum are over the states X of the target space of the random variable X.In one dimension, there are two other intuitive notions of "average", which are the median and the mode.The median is the "middle" value if median we sort the values, i.e., 50% of the values are greater than the median and 50% are smaller than the median.This idea can be generalized to continuous values by considering the value where the cdf (Definition 6.2) is 0.5.For distributions, which are asymmetric or have long tails, the median provides an estimate of a typical value that is closer to human intuition than the mean value.Furthermore, the median is more robust to outliers than the mean.The generalization of the median to higher dimensions is non-trivial as there is no obvious way to "sort" in more than one dimension (Hallin et al., 2010;Kong and Mizera, 2012).The mode is the most mode frequently occurring value.For a discrete random variable, the mode is defined as the value of x having the highest frequency of occurrence.For a continuous random variable, the mode is defined as a peak in the density p(x).A particular density p(x) may have more than one mode, and furthermore there may be a very large number of modes in high-dimensional distributions.Therefore, finding all the modes of a distribution can be computationally challenging.(6.33)We will define the Gaussian distribution N µ, σ 2 in Section 6.5.Also shown is its corresponding marginal distribution in each dimension.Observe that the distribution is bimodal (has two modes), but one of the Draft (2024-01-15) of "Mathematics for Machine Learning".Feedback: https://mml-book.com.189 marginal distributions is unimodal (has one mode).The horizontal bimodal univariate distribution illustrates that the mean and median can be different from each other.While it is tempting to define the twodimensional median to be the concatenation of the medians in each dimension, the fact that we cannot define an ordering of two-dimensional points makes it difficult.When we say "cannot define an ordering", we mean that there is more than one way to define the relation < so that 3 0 < 2 3 .Remark.When the random variable associated with the expectation or covariance is clear by its arguments, the subscript is often suppressed (for example, E X [x] is often written as E[x]).♢ By using the linearity of expectations, the expression in Definition 6.5 can be rewritten as the expected value of the product minus the product of the expected values, i.e., (6.37) Definition 6.6 can be applied with the same multivariate random variable in both arguments, which results in a useful concept that intuitively captures the "spread" of a random variable.For a multivariate random variable, the variance describes the relation between individual dimensions of the random variable.Definition 6.7 (Variance).The variance of a random variable X with variance states x ∈ R D and a mean vector µ ∈ R D is defined as population statistics to the realization of empirical statistics.First, we use the fact that we have a finite dataset (of size N ) to construct an empirical statistic that is a function of a finite number of identical random variables, X 1 , . . ., X N .Second, we observe the data, that is, we look at the realization x 1 , . . ., x N of each of the random variables and apply the empirical statistic.Specifically, for the mean (Definition 6.4), given a particular dataset we can obtain an estimate of the mean, which is called the empirical mean or empirical mean sample mean.The same holds for the empirical covariance.To compute the statistics for a particular dataset, we would use the realizations (observations) x 1 , . . ., x N and use (6.41) and (6.42).Empirical covariance matrices are symmetric, positive semidefinite (see Section 3.2.3).We now focus on a single random variable X and use the preceding empirical formulas to derive three possible expressions for the variance.The The derivations are exercises at the end of this chapter.following derivation is the same for the population variance, except that we need to take care of integrals.The standard definition of variance, corresponding to the definition of covariance (Definition 6.5), is the expectation of the squared deviation of a random variable X from its expected value µ, i.e.,(6.43)The expectation in (6.43) and the mean µ = E X (x) are computed using (6.32), depending on whether X is a discrete or continuous random variable.The variance as expressed in (6.43) is the mean of a new random variable Z := (X − µ) 2 .When estimating the variance in (6.43) empirically, we need to resort to a two-pass algorithm: one pass through the data to calculate the mean µ using (6.41), and then a second pass using this estimate μ calculate the variance.It turns out that we can avoid two passes by rearranging the terms.The formula in (6.43) can be converted to the so-called raw-score raw-score formula for variance formula for variance:2 .(6.44)The expression in (6.44) can be remembered as "the mean of the square minus the square of the mean".It can be calculated empirically in one pass through data since we can accumulate x i (to calculate the mean) and x 2 i simultaneously, where x i is the ith observation.Unfortunately, if imple-If the two terms in (6.44) are huge and approximately equal, we may suffer from an unnecessary loss of numerical precision in floating-point arithmetic.mented in this way, it can be numerically unstable.The raw-score version of the variance can be useful in machine learning, e.g., when deriving the bias-variance decomposition (Bishop, 2006).A third way to understand the variance is that it is a sum of pairwise differences between all pairs of observations.Consider a sample x 1 , . . ., x N of realizations of random variable X, and we compute the squared difference between pairs of x i and x j .By expanding the square, we can show that the sum of N 2 pairwise differences is the empirical variance of the observations: .(6.45)We see that (6.45) is twice the raw-score expression (6.44).This means that we can express the sum of pairwise distances (of which there are N 2 of them) as a sum of deviations from the mean (of which there are N ).Geometrically, this means that there is an equivalence between the pairwise distances and the distances from the center of the set of points.From a computational perspective, this means that by computing the mean (N terms in the summation), and then computing the variance (again N terms in the summation), we can obtain an expression (left-hand side of (6.45)) that has N 2 terms.We may want to model a phenomenon that cannot be well explained by textbook distributions (we introduce some in Sections 6.5 and 6.6), and hence may perform simple manipulations of random variables (such as adding two random variables).Consider two random variables X, Y with states x, y ∈ R D .Then: 194Mean and (co)variance exhibit some useful properties when it comes to affine transformation of random variables.Consider a random variable X with mean µ and covariance matrix Σ and a (deterministic) affine transformation y = Ax + b of x.Then y is itself a random variable whose mean vector and covariance matrix are given by The last point may not hold in converse, i.e., two random variables can have covariance zero but are not statistically independent.To understand why, recall that covariance measures only linear dependence.Therefore, random variables that are nonlinearly dependent could have covariance zero.Example 6.5 Consider a random variable X with zero mean (E X [x] = 0) and also E X [x 3 ] = 0. Let y = x 2 (hence, Y is dependent on X) and consider the covariance (6.36) between X and Y .But this givesIn machine learning, we often consider problems that can be modeled as independent and identically distributed (i.i.d.) random variables, independent and identically distributed i.i.d.X 1 , . . ., X N .For more than two random variables, the word "independent" (Definition 6.10) usually refers to mutually independent random variables, where all subsets are independent (see Pollard (2002, chapter 4) and Jacod and Protter (2004, chapter 3)).The phrase "identically distributed" means that all the random variables are from the same distribution.Another concept that is important in machine learning is conditional independence.where Z is the set of states of random variable Z.We write X ⊥ ⊥ Y | Z to denote that X is conditionally independent of Y given Z.Definition 6.11 requires that the relation in (6.55) must hold true for every value of z.The interpretation of (6.55) can be understood as "given knowledge about z, the distribution of x and y factorizes".Independence can be cast as a special case of conditional independence if we write X ⊥ ⊥ Y | ∅.By using the product rule of probability (6.22), we can expand the left-hand side of (6.55) to obtain p(x, y | z) = p(x | y, z)p(y | z) .(6.56)By comparing the right-hand side of (6.55) with (6.56), we see that p(y | z) appears in both of them so that p(x | y, z) = p(x | z) .(6.57) Equation ( 6.57) provides an alternative definition of conditional independence, i.e., X ⊥ ⊥ Y | Z.This alternative presentation provides the interpretation "given that we know z, knowledge about y does not change our knowledge of x".Recall the definition of inner products from Section 3.2.We can define an Inner products between multivariate random variables can be treated in a similar fashion inner product between random variables, which we briefly describe in this section.If we have two uncorrelated random variables X, Y , then (6.58)Since variances are measured in squared units, this looks very much like the Pythagorean theorem for right triangles c 2 = a 2 + b 2 .In the following, we see whether we can find a geometric interpretation of the variance relation of uncorrelated random variables in (6.58).For a univariate random variable, the Gaussian distribution has a density that is given byThe multivariate Gaussian distribution is fully characterized by a mean ure 6.7 shows a bivariate Gaussian (mesh), with the corresponding contour plot.Figure 6.8 shows a univariate Gaussian and a bivariate Gaussian with corresponding samples.The special case of the Gaussian with zero mean and identity covariance, that is, µ = 0 and Σ = I, is referred to as the standard normal distribution.Gaussians are widely used in statistical estimation and machine learning as they have closed-form expressions for marginal and conditional distributions.In Chapter 9, we use these closed-form expressions extensively for linear regression.A major advantage of modeling with Gaussian random variables is that variable transformations (Section 6.7) are often not needed.Since the Gaussian distribution is fully specified by its mean and covariance, we often can obtain the transformed distribution by applying the transformation to the mean and covariance of the random variable.In the following, we present marginalization and conditioning in the general case of multivariate random variables.If this is confusing at first reading, the reader is advised to consider two univariate random variables instead.Let X and Y be two multivariate random variables, that may haveRemark.We will be using the expression "probability distribution" not only for discrete probability mass functions but also for continuous probability density functions, although this is technically incorrect.In line with most machine learning literature, we also rely on context to distinguish the different uses of the phrase probability distribution.♢We think of probability theory as an extension to logical reasoning.As we discussed in Section 6.1.1,the rules of probability presented here followWe think of probability theory as an extension to logical reasoning.As we discussed in Section 6.1.1,the rules of probability presented here followIf f (x, y) is a twice (continuously) differentiable function, theni.e., the order of differentiation does not matter, and the corresponding Hessian matrixHessian matrix(5.147) is symmetric.The Hessian is denoted as ∇ 2x,y f (x, y).Generally, for x ∈ R n and f : R n → R, the Hessian is an n × n matrix.The Hessian measures the curvature of the function locally around (x, y).Remark (Hessian of a Vector Field).If f : R n → R m is a vector field, the Hessian is an (m × n × n)-tensor.♢The gradient ∇f of a function f is often used for a locally linear approximation of f around x 0 :(5.148)Here (∇ x f )(x 0 ) is the gradient of f with respect to x, evaluated at x 0 .Figure 5.12 illustrates the linear approximation of a function f at an input x 0 .The original function is approximated by a straight line.This approximation is locally accurate, but the farther we move away from x 0 the worse the approximation gets.Equation (5.148) is a special case of a multivariate Taylor series expansion of f at x 0 , where we consider only the first two terms.We discuss the more general case in the following, which will allow for better approximations.Probability and Distributions 3. The resulting reasoning must be consistent, with the three following meanings of the word "consistent":(a) Consistency or non-contradiction: When the same result can be reached through different means, the same plausibility value must be found in all cases.(b) Honesty: All available data must be taken into account.(c) Reproducibility: If our state of knowledge about two problems are the same, then we must assign the same degree of plausibility to both of them.The Cox-Jaynes theorem proves these plausibilities to be sufficient to define the universal mathematical rules that apply to plausibility p, up to transformation by an arbitrary monotonic function.Crucially, these rules are the rules of probability.Remark.In machine learning and statistics, there are two major interpretations of probability: the Bayesian and frequentist interpretations (Bishop, 2006;Efron and Hastie, 2016).The Bayesian interpretation uses probability to specify the degree of uncertainty that the user has about an event.It is sometimes referred to as "subjective probability" or "degree of belief".The frequentist interpretation considers the relative frequencies of events of interest to the total number of events that occurred.The probability of an event is defined as the relative frequency of the event in the limit when one has infinite data.♢ Some machine learning texts on probabilistic models use lazy notation and jargon, which is confusing.This text is no exception.Multiple distinct concepts are all referred to as "probability distribution", and the reader has to often disentangle the meaning from the context.One trick to help make sense of probability distributions is to check whether we are trying to model something categorical (a discrete random variable) or something continuous (a continuous random variable).The kinds of questions we tackle in machine learning are closely related to whether we are considering categorical or continuous models.There are three distinct ideas that are often confused when discussing probabilities.First is the idea of a probability space, which allows us to quantify the idea of a probability.However, we mostly do not work directly with this basic probability space.Instead, we work with random variables (the second idea), which transfers the probability to a more convenient (often numerical) space.The third idea is the idea of a distribution or law associated with a random variable.We will introduce the first two ideas in this section and expand on the third idea in Section 6.2.Modern probability is based on a set of axioms proposed by Kolmogorov Draft (2024-01-15) of "Mathematics for Machine Learning".Feedback: https://mml-book.com.6.1 Construction of a Probability Space tinuous random variable X taking a particular value P (X = x) is zero.This is like trying to specify an interval in (6.16)where a = b.♢ Definition 6.2 (Cumulative Distribution Function).A cumulative distribucumulative distribution function tion function (cdf) of a multivariate real-valued random variable X with states x ∈ R D is given bywhere X = [X 1 , . . ., X D ] ⊤ , x = [x 1 , . . ., x D ] ⊤ , and the right-hand side represents the probability that random variable X i takes the value smaller than or equal to x i .There are cdfs, which do not have corresponding pdfs.The cdf can be expressed also as the integral of the probability density function f (x) so that(6.18)Remark.We reiterate that there are in fact two distinct concepts when talking about distributions.First is the idea of a pdf (denoted by f (x)), which is a nonnegative function that sums to one.Second is the law of a random variable X, that is, the association of a random variable X with the pdf f (x).♢ For most of this book, we will not use the notation f (x) and F X (x) as we mostly do not need to distinguish between the pdf and cdf.However, we will need to be careful about pdfs and cdfs in Section 6.7.Recall from Section 6.1.2that probabilities are positive and the total probability sums up to one.For discrete random variables (see (6.12)), this implies that the probability of each state must lie in the interval [0, 1].However, for continuous random variables the normalization (see (6.15)) does not imply that the value of the density is less than or equal to 1 for all values.We illustrate this in Figure 6.3 using the uniform distribution uniform distribution for both discrete and continuous random variables.We consider two examples of the uniform distribution, where each state is equally likely to occur.This example illustrates some differences between discrete and continuous probability distributions.Let Z be a discrete uniform random variable with three states {z = −1.1,z = 0.3, z = 1.5}.The probability mass function can be representedThe actual values of these states are not meaningful here, and we deliberately chose numbers to drive home the point that we do not want to use (and should ignore) the ordering of the states.as a table of probability values:Alternatively, we can think of this as a graph (Figure 6.3(a)), where we use the fact that the states can be located on the x-axis, and the y-axis represents the probability of a particular state.The y-axis in Figure 6.3(a) is deliberately extended so that is it the same as in Figure 6.3(b).Let X be a continuous random variable taking values in the range 0.9 ⩽ X ⩽ 1.6, as represented by Figure 6.3(b).Observe that the height of the Draft (2024-01-15) of "Mathematics for Machine Learning".Feedback: https://mml-book.com.density can be greater than 1.However, it needs to hold that 1.6 0.9 p(x)dx = 1 .(Remark.There is an additional subtlety with regards to discrete probability distributions.The states z 1 , . . ., z d do not in principle have any structure, i.e., there is usually no way to compare them, for example z 1 = red, z 2 = green, z 3 = blue.However, in many machine learning applications discrete states take numerical values, e.g.,, where we could say z 1 < z 2 < z 3 .Discrete states that assume numerical values are particularly useful because we often consider expected values (Section 6.4.1) of random variables.♢Unfortunately, machine learning literature uses notation and nomenclature that hides the distinction between the sample space Ω, the target space T , and the random variable X.For a value x of the set of possible outcomes of the random variable X, i.e., x ∈ T , p(x) denotes the prob-We think of the outcome x as the argument that results in the probability p(x).ability that random variable X has the outcome x.For discrete random variables, this is written as P (X = x), which is known as the probability mass function.The pmf is often referred to as the "distribution".For continuous variables, p(x) is called the probability density function (often referred to as a density).To muddy things even further, the cumulative distribution function P (X ⩽ x) is often also referred to as the "distribution".In this chapter, we will use the notation X to refer to both univariate and multivariate random variables, and denote the states by x and x respectively.We summarize the nomenclature in Table 6.1.The quantityis the marginal likelihood/evidence.The right-hand side of (6.27) uses the marginal likelihood evidence expectation operator which we define in Section 6.4.1.By definition, the marginal likelihood integrates the numerator of (6.23) with respect to the latent variable x.Therefore, the marginal likelihood is independent of x, and it ensures that the posterior p(x | y) is normalized.The marginal likelihood can also be interpreted as the expected likelihood where we take the expectation with respect to the prior p(x).Beyond normalization of the posterior, the marginal likelihood also plays an important role in Bayesian model selection, as we will discuss in Section 8.6.Due to the integration in (8.44), the evidence is often hard to compute.Bayes' theorem is also called the "probabilistic inverse."Bayes' theorem (6.23) allows us to invert the relationship between x and y given by the likelihood.Therefore, Bayes' theorem is sometimes called the probabilistic inverse.We will discuss Bayes' theorem further in probabilistic inverse Section 8.4.Remark.In Bayesian statistics, the posterior distribution is the quantity of interest as it encapsulates all available information from the prior and the data.Instead of carrying the posterior around, it is possible to focus on some statistic of the posterior, such as the maximum of the posterior, which we will discuss in Section 8.3.However, focusing on some statistic of the posterior leads to loss of information.If we think in a bigger context, then the posterior can be used within a decision-making system, and having the full posterior can be extremely useful and lead to decisions that are robust to disturbances.For example, in the context of model-based reinforcement learning, Deisenroth et al. (2015) show that using the full posterior distribution of plausible transition functions leads to very fast (data/sample efficient) learning, whereas focusing on the maximum of the posterior leads to consistent failures.Therefore, having the full posterior can be very useful for a downstream task.In Chapter 9, we will continue this discussion in the context of linear regression.♢We are often interested in summarizing sets of random variables and comparing pairs of random variables.A statistic of a random variable is a deterministic function of that random variable.The summary statistics of a distribution provide one useful view of how a random variable behaves, and as the name suggests, provide numbers that summarize and characterize the distribution.We describe the mean and the variance, two wellknown summary statistics.Then we discuss two ways to compare a pair of random variables: first, how to say that two random variables are independent; and second, how to compute an inner product between them.Draft (2024-01-15) of "Mathematics for Machine Learning".Feedback: https://mml-book.com.187Mean and (co)variance are often useful to describe properties of probability distributions (expected values and spread).We will see in Section 6.6 that there is a useful family of distributions (called the exponential family), where the statistics of the random variable capture all possible information.The concept of the expected value is central to machine learning, and the foundational concepts of probability itself can be derived from the expected value (Whittle, 2000).Definition 6.3 (Expected Value).The expected value of a function g : R → expected value R of a univariate continuous random variable X ∼ p(x) is given by(6.28)Correspondingly, the expected value of a function g of a discrete random variable X ∼ p(x) is given bywhere X is the set of possible outcomes (the target space) of the random variable X.In this section, we consider discrete random variables to have numerical outcomes.This can be seen by observing that the function g takes real numbers as inputs.The expected value of a function of a random variable is sometimes referred to as the law of the unconscious statistician (Casella and Berger, 2002, Section 2.2).Remark.We consider multivariate random variables X as a finite vector of univariate random variables [X 1 , . . ., X D ] ⊤ .For multivariate random variables, we define the expected value element wise (6.30)where the subscript E X d indicates that we are taking the expected value with respect to the dth element of the vector x.♢ Definition 6.3 defines the meaning of the notation E X as the operator indicating that we should take the integral with respect to the probability density (for continuous distributions) or the sum over all states (for discrete distributions).The definition of the mean (Definition 6.4), is a special case of the expected value, obtained by choosing g to be the identity function.p(x i ) = p(x 1 , . . ., x D )dx \i , (6.39)where "\i" denotes "all variables but i".The off-diagonal entries are the cross-covariance terms Cov[x i , x j ] for i, j = 1, . . ., D, i ̸ = j.Remark.In this book, we generally assume that covariance matrices are positive definite to enable better intuition.We therefore do not discuss corner cases that result in positive semidefinite (low-rank) covariance matrices.♢When we want to compare the covariances between different pairs of random variables, it turns out that the variance of each random variable affects the value of the covariance.The normalized version of covariance is called the correlation.Definition 6.8 (Correlation).The correlation between two random varicorrelation ables X, Y is given by corrThe correlation matrix is the covariance matrix of standardized random variables, x/σ(x).In other words, each random variable is divided by its standard deviation (the square root of the variance) in the correlation matrix.The covariance (and correlation) indicate how two random variables are related; see Figure 6.5.Positive correlation corr[x, y] means that when x grows, then y is also expected to grow.Negative correlation means that as x increases, then y decreases.The definitions in Section 6.4.1 are often also called the population mean population mean and covariance and covariance, as it refers to the true statistics for the population.In machine learning, we need to learn from empirical observations of data.Consider a random variable X.There are two conceptual steps to go from for zero mean random variables X and Y , we obtain an inner product.We see that the covariance is symmetric, positive definite, and linear in eitherargument.The length of a random variable is (6.60) i.e., its standard deviation.The "longer" the random variable, the more uncertain it is; and a random variable with length 0 is deterministic.If we look at the angle θ between two random variables X, Y , we getwhich is the correlation (Definition 6.8) between the two random variables.This means that we can think of correlation as the cosine of the angle between two random variables when we consider them geometrically.We know from Definition 3.7 that X ⊥ Y ⇐⇒ ⟨X, Y ⟩ = 0.In our case, this means that X and Y are orthogonal if and only if Cov[x, y] = 0, i.e., they are uncorrelated.Figure 6.6 illustrates this relationship.Remark.While it is tempting to use the Euclidean distance (constructed Draft (2024-01-15) of "Mathematics for Machine Learning".Feedback: https://mml-book.com.from the preceding definition of inner products) to compare probability distributions, it is unfortunately not the best way to obtain distances between distributions.Recall that the probability mass (or density) is positive and needs to add up to 1.These constraints mean that distributions live on something called a statistical manifold.The study of this space of probability distributions is called information geometry.Computing distances between distributions are often done using Kullback-Leibler divergence, which is a generalization of distances that account for properties of the statistical manifold.Just like the Euclidean distance is a special case of a metric (Section 3.3), the Kullback-Leibler divergence is a special case of two more general classes of divergences called Bregman divergences and f -divergences.The study of divergences is beyond the scope of this book, and we refer for more details to the recent book by Amari (2016), one of the founders of the field of information geometry.♢The Gaussian distribution is the most well-studied probability distribution for continuous-valued random variables.It is also referred to as the normal normal distribution distribution.Its importance originates from the fact that it has many com-The Gaussian distribution arises naturally when we consider sums of independent and identically distributed random variables.This is known as the central limit theorem (Grinstead and Snell, 1997).putationally convenient properties, which we will be discussing in the following.In particular, we will use it to define the likelihood and prior for linear regression (Chapter 9), and consider a mixture of Gaussians for density estimation (Chapter 11).There are many other areas of machine learning that also benefit from using a Gaussian distribution, for example Gaussian processes, variational inference, and reinforcement learning.It is also widely used in other application areas such as signal processing (e.g., Kalman filter), control (e.g., linear quadratic regulator), and statistics (e.g., hypothesis testing).199 different dimensions.To consider the effect of applying the sum rule of probability and the effect of conditioning, we explicitly write the Gaussian distribution in terms of the concatenated states  (6.67)Note that in the computation of the mean in (6.66), the y-value is an observation and no longer random.Remark.The conditional Gaussian distribution shows up in many places, where we are interested in posterior distributions:The Kalman filter (Kalman, 1960), one of the most central algorithms for state estimation in signal processing, does nothing but computing Gaussian conditionals of joint distributions (Deisenroth and Ohlsson, 2011;Särkkä, 2013).Gaussian processes (Rasmussen and Williams, 2006), which are a practical implementation of a distribution over functions.In a Gaussian process, we make assumptions of joint Gaussianity of random variables.By (Gaussian) conditioning on observed data, we can determine a posterior distribution over functions.Latent linear Gaussian models (Roweis and Ghahramani, 1999;Murphy, 2012), which include probabilistic principal component analysis (PPCA) (Tipping and Bishop, 1999).We will look at PPCA in more detail in Section 10.7.The marginal distribution p(x) of a joint Gaussian distribution p(x, y) (see (6.64)) is itself Gaussian and computed by applying the sum rule (6.20) and given by p(x) = p(x, y)dy = N x | µ x , Σ xx .(6.68)The corresponding result holds for p(y), which is obtained by marginalizing with respect to x. Intuitively, looking at the joint distribution in (6.64), we ignore (i.e., integrate out) everything we are not interested in.This is illustrated in Figure 6.9(b).Example 6.6 Consider the bivariate Gaussian distribution (illustrated in Figure 6.9):We can compute the parameters of the univariate Gaussian, conditioned on x 2 = −1, by applying (6.66) and (6.67) to obtain the mean and variance respectively.Numerically, this isTherefore, the conditional Gaussian is given byThe marginal distribution p(x 1 ), in contrast, can be obtained by applying (6.68), which is essentially using the mean and variance of the random variable x 1 , giving us p(x 1 ) = N 0, 0.3 .(6.73)Draft (2024-01-15) of "Mathematics for Machine Learning".Feedback: https://mml-book.com.6.5 Gaussian Distribution 201For linear regression (Chapter 9), we need to compute a Gaussian likelihood.Furthermore, we may wish to assume a Gaussian prior (Section 9.3).We apply Bayes' Theorem to compute the posterior, which results in a multiplication of the likelihood and the prior, that is, the multiplication of two Gaussian densities.The product of two Gaussians NThe derivation is an exercise at the end of this chapter.is a Gaussian distribution scaled by a c ∈ R, given by c N x | c, C with76) The scaling constant c itself can be written in the form of a Gaussian density either in a or in b with an "inflated" covariance matrixRemark.For notation convenience, we will sometimes use N x | m, S to describe the functional form of a Gaussian density even if x is not a random variable.We have just done this in the preceding demonstration when we wroteHere, neither a nor b are random variables.However, writing c in this way is more compact than (6.76).♢If X, Y are independent Gaussian random variables (i.e., the joint distribution is given as p(x, y) = p(x)p(y)) with p(x) = N x | µ x , Σ x and p(y) = N y | µ y , Σ y , then x + y is also Gaussian distributed and given by p(x + y) = N µ x + µ y , Σ x + Σ y .(6.78) Knowing that p(x + y) is Gaussian, the mean and covariance matrix can be determined immediately using the results from (6.46) through (6.49).This property will be important when we consider i.i.d.Gaussian noise acting on random variables, as is the case for linear regression (Chapter 9).Since expectations are linear operations, we can obtain the weighted sum of independent Gaussian random variables p(ax + by) = N aµ x + bµ y , a 2 Σ x + b 2 Σ y .(6.79)Remark.A case that will be useful in Chapter 11 is the weighted sum of Gaussian densities.This is different from the weighted sum of Gaussian random variables.♢In Theorem 6.12, the random variable x is from a density that is a mixture of two densities p 1 (x) and p 2 (x), weighted by α.The theorem can be generalized to the multivariate random variable case, since linearity of expectations holds also for multivariate random variables.However, the idea of a squared random variable needs to be replaced by xx ⊤ .Theorem 6.12.Consider a mixture of two univariate Gaussian densities p(x) = αp 1 (x) + (1 − α)p 2 (x) , (6.80)where the scalar 0 < α < 1 is the mixture weight, and p 1 (x) and p 2 (x) are univariate Gaussian densities (Equation (6.62)) with different parameters, i.e., (µ 1 , σ 2 1 ) ̸ = (µ 2 , σ 2 2 ).Then the mean of the mixture density p(x) is given by the weighted sum of the means of each random variable:(6.81)The variance of the mixture density p(x) is given by(6.82)Proof The mean of the mixture density p(x) is given by the weighted sum of the means of each random variable.We apply the definition of the mean (Definition 6.4), and plug in our mixture (6.80), which yieldsIf f (x, y) is a twice (continuously) differentiable function, theni.e., the order of differentiation does not matter, and the corresponding Hessian matrixHessian matrix(5.147) is symmetric.The Hessian is denoted as ∇ 2x,y f (x, y).Generally, for x ∈ R n and f : R n → R, the Hessian is an n × n matrix.The Hessian measures the curvature of the function locally around (x, y).Remark (Hessian of a Vector Field).If f : R n → R m is a vector field, the Hessian is an (m × n × n)-tensor.♢The gradient ∇f of a function f is often used for a locally linear approximation of f around x 0 :(5.148)Here (∇ x f )(x 0 ) is the gradient of f with respect to x, evaluated at x 0 .Figure 5.12 illustrates the linear approximation of a function f at an input x 0 .The original function is approximated by a straight line.This approximation is locally accurate, but the farther we move away from x 0 the worse the approximation gets.Equation (5.148) is a special case of a multivariate Taylor series expansion of f at x 0 , where we consider only the first two terms.We discuss the more general case in the following, which will allow for better approximations.Probability and Distributions 3. The resulting reasoning must be consistent, with the three following meanings of the word "consistent":(a) Consistency or non-contradiction: When the same result can be reached through different means, the same plausibility value must be found in all cases.(b) Honesty: All available data must be taken into account.(c) Reproducibility: If our state of knowledge about two problems are the same, then we must assign the same degree of plausibility to both of them.The Cox-Jaynes theorem proves these plausibilities to be sufficient to define the universal mathematical rules that apply to plausibility p, up to transformation by an arbitrary monotonic function.Crucially, these rules are the rules of probability.Remark.In machine learning and statistics, there are two major interpretations of probability: the Bayesian and frequentist interpretations (Bishop, 2006;Efron and Hastie, 2016).The Bayesian interpretation uses probability to specify the degree of uncertainty that the user has about an event.It is sometimes referred to as "subjective probability" or "degree of belief".The frequentist interpretation considers the relative frequencies of events of interest to the total number of events that occurred.The probability of an event is defined as the relative frequency of the event in the limit when one has infinite data.♢ Some machine learning texts on probabilistic models use lazy notation and jargon, which is confusing.This text is no exception.Multiple distinct concepts are all referred to as "probability distribution", and the reader has to often disentangle the meaning from the context.One trick to help make sense of probability distributions is to check whether we are trying to model something categorical (a discrete random variable) or something continuous (a continuous random variable).The kinds of questions we tackle in machine learning are closely related to whether we are considering categorical or continuous models.There are three distinct ideas that are often confused when discussing probabilities.First is the idea of a probability space, which allows us to quantify the idea of a probability.However, we mostly do not work directly with this basic probability space.Instead, we work with random variables (the second idea), which transfers the probability to a more convenient (often numerical) space.The third idea is the idea of a distribution or law associated with a random variable.We will introduce the first two ideas in this section and expand on the third idea in Section 6.2.Modern probability is based on a set of axioms proposed by Kolmogorov Draft (2024-01-15) of "Mathematics for Machine Learning".Feedback: https://mml-book.com.6.1 Construction of a Probability Space tinuous random variable X taking a particular value P (X = x) is zero.This is like trying to specify an interval in (6.16)where a = b.♢ Definition 6.2 (Cumulative Distribution Function).A cumulative distribucumulative distribution function tion function (cdf) of a multivariate real-valued random variable X with states x ∈ R D is given bywhere X = [X 1 , . . ., X D ] ⊤ , x = [x 1 , . . ., x D ] ⊤ , and the right-hand side represents the probability that random variable X i takes the value smaller than or equal to x i .There are cdfs, which do not have corresponding pdfs.The cdf can be expressed also as the integral of the probability density function f (x) so that(6.18)Remark.We reiterate that there are in fact two distinct concepts when talking about distributions.First is the idea of a pdf (denoted by f (x)), which is a nonnegative function that sums to one.Second is the law of a random variable X, that is, the association of a random variable X with the pdf f (x).♢ For most of this book, we will not use the notation f (x) and F X (x) as we mostly do not need to distinguish between the pdf and cdf.However, we will need to be careful about pdfs and cdfs in Section 6.7.Recall from Section 6.1.2that probabilities are positive and the total probability sums up to one.For discrete random variables (see (6.12)), this implies that the probability of each state must lie in the interval [0, 1].However, for continuous random variables the normalization (see (6.15)) does not imply that the value of the density is less than or equal to 1 for all values.We illustrate this in Figure 6.3 using the uniform distribution uniform distribution for both discrete and continuous random variables.We consider two examples of the uniform distribution, where each state is equally likely to occur.This example illustrates some differences between discrete and continuous probability distributions.Let Z be a discrete uniform random variable with three states {z = −1.1,z = 0.3, z = 1.5}.The probability mass function can be representedThe actual values of these states are not meaningful here, and we deliberately chose numbers to drive home the point that we do not want to use (and should ignore) the ordering of the states.as a table of probability values:Alternatively, we can think of this as a graph (Figure 6.3(a)), where we use the fact that the states can be located on the x-axis, and the y-axis represents the probability of a particular state.The y-axis in Figure 6.3(a) is deliberately extended so that is it the same as in Figure 6.3(b).Let X be a continuous random variable taking values in the range 0.9 ⩽ X ⩽ 1.6, as represented by Figure 6.3(b).Observe that the height of the Draft (2024-01-15) of "Mathematics for Machine Learning".Feedback: https://mml-book.com.density can be greater than 1.However, it needs to hold that 1.6 0.9 p(x)dx = 1 .(Remark.There is an additional subtlety with regards to discrete probability distributions.The states z 1 , . . ., z d do not in principle have any structure, i.e., there is usually no way to compare them, for example z 1 = red, z 2 = green, z 3 = blue.However, in many machine learning applications discrete states take numerical values, e.g.,, where we could say z 1 < z 2 < z 3 .Discrete states that assume numerical values are particularly useful because we often consider expected values (Section 6.4.1) of random variables.♢Unfortunately, machine learning literature uses notation and nomenclature that hides the distinction between the sample space Ω, the target space T , and the random variable X.For a value x of the set of possible outcomes of the random variable X, i.e., x ∈ T , p(x) denotes the prob-We think of the outcome x as the argument that results in the probability p(x).ability that random variable X has the outcome x.For discrete random variables, this is written as P (X = x), which is known as the probability mass function.The pmf is often referred to as the "distribution".For continuous variables, p(x) is called the probability density function (often referred to as a density).To muddy things even further, the cumulative distribution function P (X ⩽ x) is often also referred to as the "distribution".In this chapter, we will use the notation X to refer to both univariate and multivariate random variables, and denote the states by x and x respectively.We summarize the nomenclature in Table 6.1.The quantityis the marginal likelihood/evidence.The right-hand side of (6.27) uses the marginal likelihood evidence expectation operator which we define in Section 6.4.1.By definition, the marginal likelihood integrates the numerator of (6.23) with respect to the latent variable x.Therefore, the marginal likelihood is independent of x, and it ensures that the posterior p(x | y) is normalized.The marginal likelihood can also be interpreted as the expected likelihood where we take the expectation with respect to the prior p(x).Beyond normalization of the posterior, the marginal likelihood also plays an important role in Bayesian model selection, as we will discuss in Section 8.6.Due to the integration in (8.44), the evidence is often hard to compute.Bayes' theorem is also called the "probabilistic inverse."Bayes' theorem (6.23) allows us to invert the relationship between x and y given by the likelihood.Therefore, Bayes' theorem is sometimes called the probabilistic inverse.We will discuss Bayes' theorem further in probabilistic inverse Section 8.4.Remark.In Bayesian statistics, the posterior distribution is the quantity of interest as it encapsulates all available information from the prior and the data.Instead of carrying the posterior around, it is possible to focus on some statistic of the posterior, such as the maximum of the posterior, which we will discuss in Section 8.3.However, focusing on some statistic of the posterior leads to loss of information.If we think in a bigger context, then the posterior can be used within a decision-making system, and having the full posterior can be extremely useful and lead to decisions that are robust to disturbances.For example, in the context of model-based reinforcement learning, Deisenroth et al. (2015) show that using the full posterior distribution of plausible transition functions leads to very fast (data/sample efficient) learning, whereas focusing on the maximum of the posterior leads to consistent failures.Therefore, having the full posterior can be very useful for a downstream task.In Chapter 9, we will continue this discussion in the context of linear regression.♢We are often interested in summarizing sets of random variables and comparing pairs of random variables.A statistic of a random variable is a deterministic function of that random variable.The summary statistics of a distribution provide one useful view of how a random variable behaves, and as the name suggests, provide numbers that summarize and characterize the distribution.We describe the mean and the variance, two wellknown summary statistics.Then we discuss two ways to compare a pair of random variables: first, how to say that two random variables are independent; and second, how to compute an inner product between them.Draft (2024-01-15) of "Mathematics for Machine Learning".Feedback: https://mml-book.com.187Mean and (co)variance are often useful to describe properties of probability distributions (expected values and spread).We will see in Section 6.6 that there is a useful family of distributions (called the exponential family), where the statistics of the random variable capture all possible information.The concept of the expected value is central to machine learning, and the foundational concepts of probability itself can be derived from the expected value (Whittle, 2000).Definition 6.3 (Expected Value).The expected value of a function g : R → expected value R of a univariate continuous random variable X ∼ p(x) is given by(6.28)Correspondingly, the expected value of a function g of a discrete random variable X ∼ p(x) is given bywhere X is the set of possible outcomes (the target space) of the random variable X.In this section, we consider discrete random variables to have numerical outcomes.This can be seen by observing that the function g takes real numbers as inputs.The expected value of a function of a random variable is sometimes referred to as the law of the unconscious statistician (Casella and Berger, 2002, Section 2.2).Remark.We consider multivariate random variables X as a finite vector of univariate random variables [X 1 , . . ., X D ] ⊤ .For multivariate random variables, we define the expected value element wise (6.30)where the subscript E X d indicates that we are taking the expected value with respect to the dth element of the vector x.♢ Definition 6.3 defines the meaning of the notation E X as the operator indicating that we should take the integral with respect to the probability density (for continuous distributions) or the sum over all states (for discrete distributions).The definition of the mean (Definition 6.4), is a special case of the expected value, obtained by choosing g to be the identity function.p(x i ) = p(x 1 , . . ., x D )dx \i , (6.39)where "\i" denotes "all variables but i".The off-diagonal entries are the cross-covariance terms Cov[x i , x j ] for i, j = 1, . . ., D, i ̸ = j.Remark.In this book, we generally assume that covariance matrices are positive definite to enable better intuition.We therefore do not discuss corner cases that result in positive semidefinite (low-rank) covariance matrices.♢When we want to compare the covariances between different pairs of random variables, it turns out that the variance of each random variable affects the value of the covariance.The normalized version of covariance is called the correlation.Definition 6.8 (Correlation).The correlation between two random varicorrelation ables X, Y is given by corrThe correlation matrix is the covariance matrix of standardized random variables, x/σ(x).In other words, each random variable is divided by its standard deviation (the square root of the variance) in the correlation matrix.The covariance (and correlation) indicate how two random variables are related; see Figure 6.5.Positive correlation corr[x, y] means that when x grows, then y is also expected to grow.Negative correlation means that as x increases, then y decreases.The definitions in Section 6.4.1 are often also called the population mean population mean and covariance and covariance, as it refers to the true statistics for the population.In machine learning, we need to learn from empirical observations of data.Consider a random variable X.There are two conceptual steps to go from for zero mean random variables X and Y , we obtain an inner product.We see that the covariance is symmetric, positive definite, and linear in eitherargument.The length of a random variable is (6.60) i.e., its standard deviation.The "longer" the random variable, the more uncertain it is; and a random variable with length 0 is deterministic.If we look at the angle θ between two random variables X, Y , we getwhich is the correlation (Definition 6.8) between the two random variables.This means that we can think of correlation as the cosine of the angle between two random variables when we consider them geometrically.We know from Definition 3.7 that X ⊥ Y ⇐⇒ ⟨X, Y ⟩ = 0.In our case, this means that X and Y are orthogonal if and only if Cov[x, y] = 0, i.e., they are uncorrelated.Figure 6.6 illustrates this relationship.Remark.While it is tempting to use the Euclidean distance (constructed Draft (2024-01-15) of "Mathematics for Machine Learning".Feedback: https://mml-book.com.from the preceding definition of inner products) to compare probability distributions, it is unfortunately not the best way to obtain distances between distributions.Recall that the probability mass (or density) is positive and needs to add up to 1.These constraints mean that distributions live on something called a statistical manifold.The study of this space of probability distributions is called information geometry.Computing distances between distributions are often done using Kullback-Leibler divergence, which is a generalization of distances that account for properties of the statistical manifold.Just like the Euclidean distance is a special case of a metric (Section 3.3), the Kullback-Leibler divergence is a special case of two more general classes of divergences called Bregman divergences and f -divergences.The study of divergences is beyond the scope of this book, and we refer for more details to the recent book by Amari (2016), one of the founders of the field of information geometry.♢The Gaussian distribution is the most well-studied probability distribution for continuous-valued random variables.It is also referred to as the normal normal distribution distribution.Its importance originates from the fact that it has many com-The Gaussian distribution arises naturally when we consider sums of independent and identically distributed random variables.This is known as the central limit theorem (Grinstead and Snell, 1997).putationally convenient properties, which we will be discussing in the following.In particular, we will use it to define the likelihood and prior for linear regression (Chapter 9), and consider a mixture of Gaussians for density estimation (Chapter 11).There are many other areas of machine learning that also benefit from using a Gaussian distribution, for example Gaussian processes, variational inference, and reinforcement learning.It is also widely used in other application areas such as signal processing (e.g., Kalman filter), control (e.g., linear quadratic regulator), and statistics (e.g., hypothesis testing).199 different dimensions.To consider the effect of applying the sum rule of probability and the effect of conditioning, we explicitly write the Gaussian distribution in terms of the concatenated states  (6.67)Note that in the computation of the mean in (6.66), the y-value is an observation and no longer random.Remark.The conditional Gaussian distribution shows up in many places, where we are interested in posterior distributions:The Kalman filter (Kalman, 1960), one of the most central algorithms for state estimation in signal processing, does nothing but computing Gaussian conditionals of joint distributions (Deisenroth and Ohlsson, 2011;Särkkä, 2013).Gaussian processes (Rasmussen and Williams, 2006), which are a practical implementation of a distribution over functions.In a Gaussian process, we make assumptions of joint Gaussianity of random variables.By (Gaussian) conditioning on observed data, we can determine a posterior distribution over functions.Latent linear Gaussian models (Roweis and Ghahramani, 1999;Murphy, 2012), which include probabilistic principal component analysis (PPCA) (Tipping and Bishop, 1999).We will look at PPCA in more detail in Section 10.7.The marginal distribution p(x) of a joint Gaussian distribution p(x, y) (see (6.64)) is itself Gaussian and computed by applying the sum rule (6.20) and given by p(x) = p(x, y)dy = N x | µ x , Σ xx .(6.68)The corresponding result holds for p(y), which is obtained by marginalizing with respect to x. Intuitively, looking at the joint distribution in (6.64), we ignore (i.e., integrate out) everything we are not interested in.This is illustrated in Figure 6.9(b).Example 6.6 Consider the bivariate Gaussian distribution (illustrated in Figure 6.9):We can compute the parameters of the univariate Gaussian, conditioned on x 2 = −1, by applying (6.66) and (6.67) to obtain the mean and variance respectively.Numerically, this isTherefore, the conditional Gaussian is given byThe marginal distribution p(x 1 ), in contrast, can be obtained by applying (6.68), which is essentially using the mean and variance of the random variable x 1 , giving us p(x 1 ) = N 0, 0.3 .(6.73)Draft (2024-01-15) of "Mathematics for Machine Learning".Feedback: https://mml-book.com.6.5 Gaussian Distribution 201For linear regression (Chapter 9), we need to compute a Gaussian likelihood.Furthermore, we may wish to assume a Gaussian prior (Section 9.3).We apply Bayes' Theorem to compute the posterior, which results in a multiplication of the likelihood and the prior, that is, the multiplication of two Gaussian densities.The product of two Gaussians NThe derivation is an exercise at the end of this chapter.is a Gaussian distribution scaled by a c ∈ R, given by c N x | c, C with76) The scaling constant c itself can be written in the form of a Gaussian density either in a or in b with an "inflated" covariance matrixRemark.For notation convenience, we will sometimes use N x | m, S to describe the functional form of a Gaussian density even if x is not a random variable.We have just done this in the preceding demonstration when we wroteHere, neither a nor b are random variables.However, writing c in this way is more compact than (6.76).♢If X, Y are independent Gaussian random variables (i.e., the joint distribution is given as p(x, y) = p(x)p(y)) with p(x) = N x | µ x , Σ x and p(y) = N y | µ y , Σ y , then x + y is also Gaussian distributed and given by p(x + y) = N µ x + µ y , Σ x + Σ y .(6.78) Knowing that p(x + y) is Gaussian, the mean and covariance matrix can be determined immediately using the results from (6.46) through (6.49).This property will be important when we consider i.i.d.Gaussian noise acting on random variables, as is the case for linear regression (Chapter 9).Since expectations are linear operations, we can obtain the weighted sum of independent Gaussian random variables p(ax + by) = N aµ x + bµ y , a 2 Σ x + b 2 Σ y .(6.79)Remark.A case that will be useful in Chapter 11 is the weighted sum of Gaussian densities.This is different from the weighted sum of Gaussian random variables.♢In Theorem 6.12, the random variable x is from a density that is a mixture of two densities p 1 (x) and p 2 (x), weighted by α.The theorem can be generalized to the multivariate random variable case, since linearity of expectations holds also for multivariate random variables.However, the idea of a squared random variable needs to be replaced by xx ⊤ .Theorem 6.12.Consider a mixture of two univariate Gaussian densities p(x) = αp 1 (x) + (1 − α)p 2 (x) , (6.80)where the scalar 0 < α < 1 is the mixture weight, and p 1 (x) and p 2 (x) are univariate Gaussian densities (Equation (6.62)) with different parameters, i.e., (µ 1 , σ 2 1 ) ̸ = (µ 2 , σ 2 2 ).Then the mean of the mixture density p(x) is given by the weighted sum of the means of each random variable:(6.81)The variance of the mixture density p(x) is given by(6.82)Proof The mean of the mixture density p(x) is given by the weighted sum of the means of each random variable.We apply the definition of the mean (Definition 6.4), and plug in our mixture (6.80), which yields