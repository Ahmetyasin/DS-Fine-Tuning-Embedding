Having established that we can obtain valid samples by simulating forward from times in the past, starting in all possible states at those times, the third trick of Propp and Wilson, which makes the exact sampling method useful in practice, is the idea that, for some Markov chains, it may be possible to detect coalescence of all trajectories without simulating all those trajectories.This property holds, for example, in the chain of figure 32.1b, which has the property that two trajectories never cross.So if we simply track the two trajectories starting from the leftmost and rightmost states, we will know thatIn the toy problem we studied, the states could be put in a one-dimensional order such that no two trajectories crossed.The states of many interesting state spaces can also be put into a partial order and coupled Markov chains can be found that respect this partial order.[An example of a partial order on the four possible states of two spins is this: (+, +) > (+, −) > (−, −); and (+, +) > (−, +) > (−, −); and the states (+, −) and (−, +) are not ordered.]For such systems, we can show that coalescence has occurred merely by verifying that coalescence has occurred for all the histories whose initial states were 'maximal' and 'minimal' states of the state space.Compute a i := j J ij x j Draw u from Uniform(0, 1) If u < 1/(1 + e −2a i )x i := +1 Elsex i := −1 Algorithm 32.4.Gibbs sampling coupling method.The Markov chains are coupled together by having all chains update the same spin i at each time step and having all chains share a common sequence of random numbers u.As an example, consider the Gibbs sampling method applied to a ferromagnetic Ising spin system, with the partial ordering of states being defined thus: state x is 'greater than or equal to' state y if x i ≥ y i for all spins i.The maximal and minimal states are the the all-up and all-down states.The Markov chains are coupled together as shown in algorithm 32.4.Propp and Wilson (1996) show that exact samples can be generated for this system, although the time to find exact samples is large if the Ising model is below its critical temperature, since the Gibbs sampling method itself is slowly-mixing under these conditions.Propp and Wilson have improved on this method for the Ising model by using a Markov chain called the single-bond heat bath algorithm to sample from a related model called the random cluster model; they show that exact samples from the random cluster model can be obtained rapidly and can be converted into exact samples from the Ising model.Their ground-breaking paper includes an exact sample from a 16-million-spin Ising model at its critical temperature.A sample for a smaller Ising model is shown in figure 32.5.A generalization of the exact sampling method for 'non-attractive' distributionsThe method of Propp and Wilson for the Ising model, sketched above, can be applied only to probability distributions that are, as they call them, 'attractive'.Rather than define this term, let's say what it means, for practical purposes: the method can be applied to spin systems in which all the couplings are positive (e.g., the ferromagnet), and to a few special spin systems with negative couplings (e.g., as we already observed in Chapter 31, the rectangular ferromagnet and antiferromagnet are equivalent); but it cannot be applied to general spin systems in which some couplings are negative, because in such systems the trajectories followed by the all-up and all-down states are not guaranteed to be upper and lower bounds for the set of all trajectories.Fortunately, however, we do not need to be so strict.It is possible to re-express the Propp and Wilson algorithm in a way that generalizes to the case of spin systems with negative couplings.The idea of the summary state version of exact sampling is still that we keep track of bounds on the set of all trajectories, and detect when these bounds are equal, so as to find exact samples.But the bounds will not themselves be actual trajectories, and they will not necessarily be tight bounds.Instead of simulating two trajectories, each of which moves in a state space {−1, +1} N , we simulate one trajectory envelope in an augmented state space {−1, +1, ?}N , where the symbol ?denotes 'either −1 or +1'.We call the state of this augmented system the 'summary state'.An example summary state of a six-spin system is ++-?+?.This summary state is shorthand for the set of states ++-+++, ++-++-, ++--++, ++--+-.The update rule at each step of the Markov chain takes a single spin, enumerates all possible states of the neighbouring spins that are compatible with the current summary state, and, for each of these local scenarios, computes the new value (+ or -) of the spin using Gibbs sampling (coupled to a random number u as in algorithm 32.4).If all these new values agree, then the new value of the updated spin in the summary state is set to the unanimous value (+ or -).Otherwise, the new value of the spin in the summary state is '?'.The initial condition, at time T 0 , is given by setting all the spins in the summary state to '?', which corresponds to considering all possible start configurations.In the case of a spin system with positive couplings, this summary state simulation will be identical to the simulation of the uppermost state and lowermost states, in the style of Propp and Wilson, with coalescence occuring when all the '?' symbols have disappeared.The summary state method can be applied to general spin systems with any couplings.The only shortcoming of this method is that the envelope may describe an unnecessarily large set of states, so there is no guarantee that the summary state algorithm will converge; the time for coalescence to be detected may be considerably larger than the actual time taken for the underlying Markov chain to coalesce.The summary state scheme has been applied to exact sampling in belief networks by Harvey and Neal (2000), and to the triangular antiferromagnetic Ising model by Childs et al. (2001).Summary state methods were first introduced by Huber (1998); they also go by the names sandwiching methods and bounding chains.For further reading, impressive pictures of exact samples from other distributions, and generalizations of the exact sampling method, browse the perfectlyrandom sampling website. 1 For beautiful exact-sampling demonstrations running live in your webbrowser, see Jim Propp's website. 2The idea of coupling together Markov chains by having them share a random number generator has other applications beyond exact sampling.Pinto and Neal (2001) have shown that the accuracy of estimates obtained from a Markov chain Monte Carlo simulation (the second problem discussed in section 29.1, p.357), using the estimator ΦP ≡ 1 T t φ(x (t) ), (32.1) can be improved by coupling the chain of interest, which converges to P , to a second chain, which generates samples from a second, simpler distribution, Q.The coupling must be set up in such a way that the states of the two chains are strongly correlated.The idea is that we first estimate the expectations of a function of interest, φ, under P and under Q in the normal way (32.1) and compare the estimate under Q, ΦQ , with the true value of the expectation under Q, Φ Q which we assume can be evaluated exactly.If ΦQ is an overestimate then it is likely that ΦP will be an overestimate too.The difference ( ΦQ − Φ Q ) can thus be used to correct ΦP .p.421] Is there any relationship between the probability distribution of the time taken for all trajectories to coalesce, and the equilibration time of a Markov chain?Prove that there is a relationship, or find a single chain that can be realized in two different ways that have different coalescence times.Exercise 32.2. [2 ]Imagine that Fred ignores the requirement that the random bits used at some time t, in every run from increasingly distant times T 0 , must be identical, and makes a coupled-Markov-chain simulator that uses fresh random numbers every time T 0 is changed.Describe what happens if Fred applies his method to the Markov chain that is intended to sample from the uniform distribution over the states 0, 1, and 2, using the Metropolis method, driven by a random bit source as in figure 32.1b.Exercise 32.3. [5 ]Investigate the application of perfect sampling to linear regression in Holmes and Mallick (1998) or Holmes and Denison (2002) and try to generalize it.Exercise 32.4. [3 ]The concept of coalescence has many applications.Some surnames are more frequent than others, and some die out altogether.Make a model of this process; how long will it take until everyone has the same surname?Similarly, variability in any particular portion of the human genome (which forms the basis of forensic DNA fingerprinting) is inherited like a surname.A DNA fingerprint is like a string of surnames.Should the fact that these surnames are subject to coalescences, so that some surnames are by chance more prevalent than others, affect the way in which DNA fingerprint evidence is used in court?Exercise 32.5. [2 ]How can you use a coin to create a random ranking of 3 people?Construct a solution that uses exact sampling.For example, you could apply exact sampling to a Markov chain in which the coin is repeatedly used alternately to decide whether to switch first and second, then whether to switch second and third.Exercise 32.6. [5 ]Finding the partition function Z of a probability distribution is a difficult problem.Many Markov chain Monte Carlo methods produce valid samples from a distribution without ever finding out what Z is.Is there any probability distribution and Markov chain such that either the time taken to produce a perfect sample or the number of random bits used to create a perfect sample are related to the value of Z? Are there some situations in which the time to coalescence conveys information about Z?Solution to exercise 32.1 (p.420).It is perhaps surprising that there is no direct relationship between the equilibration time and the time to coalescence.We can prove this using the example of the uniform distribution over the integers A = {0, 1, 2, . . ., 20}.A Markov chain that converges to this distribution in exactly one iteration is the chain for which the probability of state s t+1 given s t is the uniform distribution, for all s t .Such a chain can be coupled to a random number generator in two ways: (a) we could draw a random integer u ∈ A, and set s t+1 equal to u regardless of s t ; or (b) we could draw a random integer u ∈ A, and set s t+1 equal to (s t + u) mod 21.Method (b) would produce a cohort of trajectories locked together, similar to the trajectories in figure 32.1, except that no coalescence ever occurs.Thus, while the equilibration times of methods (a) and (b) are both one, the coalescence times are respectively one and infinity.It seems plausible on the other hand that coalescence time provides some sort of upper bound on equilibration time.Variational methods are an important technique for the approximation of complicated probability distributions, having applications in statistical physics, data modelling and neural networks.One method for approximating a complex distribution in a physical system is mean field theory.Mean field theory is a special case of a general variational free energy approach of Feynman and Bogoliubov which we will now study.The key piece of mathematics needed to understand this method is Gibbs' inequality, which we repeat here.Gibbs' inequality first appeared in equation (1.24); see also exercise 2. 26 (p.37).The relative entropy between two probability distributions Q(x) and P (x) that are defined over the same alphabet A X is D KL (Q||P ) =x Q(x) log Q(x) P (x) .(33.1)The relative entropy satisfies D KL (Q||P ) ≥ 0 (Gibbs' inequality) with equality only if Q = P .In general D KL (Q||P ) = D KL (P ||Q).In this chapter we will replace the log by ln, and measure the divergence in nats.In statistical physics one often encounters probability distributions of the formwhere for example the state vector is x ∈ {−1, +1} N , and E(x; J) is some energy function such as3)The partition function (normalizing constant) is Z(β, J) ≡x exp[−βE(x; J)] .(33.4)The probability distribution of equation (33.2) is complex.Not unbearably complex -we can, after all, evaluate E(x; J) for any particular x in a time polynomial in the number of spins.But evaluating the normalizing constant Z(β, J) is difficult, as we saw in Chapter 29, and describing the properties of the probability distribution is also hard.Knowing the value of E(x; J) at a few arbitrary points x, for example, gives no useful information about what the average properties of the system are.An evaluation of Z(β, J) would be particularly desirable because from Z we can derive all the thermodynamic properties of the system.Variational free energy minimization is a method for approximating the complex distribution P (x) by a simpler ensemble Q(x; θ) that is parameterized by adjustable parameters θ.We adjust these parameters so as to get Q to best approximate P , in some sense.A by-product of this approximation is a lower bound on Z(β, J).The objective function chosen to measure the quality of the approximation is the variational free energy .5)This expression can be manipulated into a couple of interesting forms: first,where E(x; J) Q is the average of the energy function under the distribution Q(x; θ), and S Q is the entropy of the distribution Q(x; θ) (we set k B to one in the definition of S so that it is identical to the definition of the entropy H in Part I).Second, we can use the definition of P (x | β, J) to write: (33.9)where F is the true free energy, defined by βF ≡ − ln Z(β, J), (33.10) and D KL (Q||P ) is the relative entropy between the approximating distribution Q(x; θ) and the true distribution P (x | β, J).Thus by Gibbs' inequality, the variational free energy F (θ) is bounded below by F and attains this value only for Q(x; θ) = P (x | β, J).Our strategy is thus to vary θ in such a way that β F (θ) is minimized.The approximating distribution then gives a simplified approximation to the true distribution that may be useful, and the value of β F (θ) will be an upper bound for βF .Equivalently, Z ≡ e −β F ( ) is a lower bound for Z.Can the objective function β F be evaluated?An example of a tractable variational free energy is given by the spin system whose energy function was given in equation (33.3), which we can approximate with a separable approximating distribution,The variational parameters θ of the variational free energy (33.5) are the components of the vector a.To evaluate the variational free energy we need the entropy of this distribution,, (33.15) and the mean of the energy, E(x; J) Q =x Q(x; a)E(x; J).(33.16)The entropy of the separable approximating distribution is simply the sum of the entropies of the individual spins (exercise 4.2, p.68),2 (q n ), (33.17)where q n is the probability that spin n is +1,e an e an + e −an = 1 1 + exp(−2a n ) , (33.18) and H (e) 2 (q) = q ln 1 q + (1 − q) ln 1 (1 − q) .(33.19)The mean energy under Q is easy to obtain because m,n J mn x m x n is a sum of terms each involving the product of two independent random variables.(There are no self-couplings, so J mn = 0 when m = n.)If we define the mean value of x n to be xn , which is given by xn = e an − e −an e an + e −an = tanh(a n ) = 2q n − 1, (33.20) we obtain (33.22)So the variational free energy is given by2 (q n ).(33.23)The variational free energy of the two-spin system whose energy is E(x) = −x 1 x 2 , as a function of the two variational parameters q 1 and q 2 .The inverse-temperature is β = 1.44.The function plotted is2 (q 1 )−H (e) 2 (q 2 ),where xn = 2q n − 1.Notice that for fixed q 2 the function is convex with respect to q 1 , and for fixed q 1 it is convex with respect to q 2 .We now consider minimizing this function with respect to the variational parameters a.If q = 1/(1 + e −2a ), the derivative of the entropy is ∂ ∂q H e 2 (q) = ln 1 − q q = −2a.(33.24)So we obtain The variational free energy F (a) may be a multimodal function, in which case each stationary point (maximum, minimum or saddle) will satisfy equations (33.26) and (33.27).One way of using these equations, in the case of a system with an arbitrary coupling matrix J, is to update each parameter a m and the corresponding value of xm using equation (33.26), one at a time.This asynchronous updating of the parameters is guaranteed to decrease β F (a). Equations (33.26) and (33.27) may be recognized as the mean field equations for a spin system.The variational parameter a n may be thought of as the strength of a fictitious field applied to an isolated spin n.Equation (33.27) describes the mean response of spin n, and equation (33.26) describes how the field a m is set in response to the mean state of all the other spins.The variational free energy derivation is a helpful viewpoint for mean field theory for two reasons.1.This approach associates an objective function β F with the mean field equations; such an objective function is useful because it can help identify alternative dynamical systems that minimize the same function.2. The theory is readily generalized to other approximating distributions.We can imagine introducing a more complex approximation Q(x; θ) that might for example capture correlations among the spins instead of modelling the spins as independent.One could then evaluate the variational free energy and optimize the parameters θ of this more complex approximation.The more degrees of freedom the approximating distribution has, the tighter the bound on the free energy becomes.However, if the complexity of an approximation is increased, the evaluation of either the mean energy or the entropy typically becomes more challenging.In the simple Ising model studied in Chapter 31, every coupling J mn is equal to J if m and n are neighbours and zero otherwise.There is an applied field h n = h that is the same for all spins.A very simple approximating distribution is one with just a single variational parameter a, which defines a separable distribution 33.28) in which all spins are independent and have the same probabilityof being up.The mean magnetization is x = tanh(a) (33.30) and the equation (33.26) which defines the minimum of the variational free energy becomes a = β (CJ x + h) , (33.31) where C is the number of couplings that a spin is involved in -C = 4 in the case of a rectangular two-dimensional Ising model.We can solve equations (33.30) and (33.31) for x numerically -in fact, it is easiest to vary x and solve for β -and obtain graphs of the free energy minima and maxima as a function of temperature as shown in figure 33.2.The solid line shows x versus T = 1/β for the case C = 4, J = 1.When h = 0, there is a pitchfork bifurcation at a critical temperature T mft c .[A pitchfork bifurcation is a transition like the one shown by the solid lines in figure 33.2, from a system with one minimum as a function of a (on the right) to a system (on the left) with two minima and one maximum; the maximum is the middle one of the three lines.The solid lines look like a pitchfork.]Above this temperature, there is only one minimum in the variational free energy, at a = 0 and x = 0; this minimum corresponds to an approximating distribution that is uniform over all states.Below the critical temperature, there are two minima corresponding to approximating distributions that are symmetry-broken, with all spins more likely to be up, or all spins more likely to be down.The state x = 0 persists as a stationary point of the variational free energy, but now it is a local maximum of the variational free energy.When h > 0, there is a global variational free energy minimum at any temperature for a positive value of x, shown by the upper dotted curves in figure 33.2.As long as h < JC, there is also a second local minimum in the free energy, if the temperature is sufficiently small.This second minimum corresponds to a self-preserving state of magnetization in the opposite direction to the applied field.The temperature at which the second minimum appears is smaller than T mft c , and when it appears, it is accompanied by a saddle point located between the two minima.A name given to this type of bifurcation is a saddle-node bifurcation.The variational free energy per spin is given byx + 1 2 .(33.32)Exercise 33.1. [2 ]Sketch the variational free energy as a function of its one parameter x for a variety of values of the temperature T and the applied field h.Figure 33.2 reproduces the key properties of the real Ising system -that, for h = 0, there is a critical temperature below which the system has longrange order, and that it can adopt one of two macroscopic states.However, by probing a little more we can reveal some inadequacies of the variational approximation.To start with, the critical temperature T mft c is 4, which is nearly a factor of 2 greater than the true critical temperature T c = 2.27.Also, the variational model has equivalent properties in any number of dimensions, including d = 1, where the true system does not have a phase transition.So the bifurcation at T mft c should not be described as a phase transition.For the case h = 0 we can follow the trajectory of the global minimum as a function of β and find the entropy, heat capacity and fluctuations of the approximating distribution and compare them with those of a real 8×8 fragment using the matrix method of Chapter 31.As shown in figure 33.3, one of the biggest differences is in the fluctuations in energy.The real system has large fluctuations near the critical temperature, whereas the approximating distribution has no correlations among its spins and thus has an energy-variance which scales simply linearly with the number of spins.In statistical data modelling we are interested in the posterior probability distribution of a parameter vector w given data D and model assumptions H, P (w | D, H).In traditional approaches to model fitting, a single parameter vector w is optimized to find the mode of this distribution.What is really of interest isFree Energy Energy the whole distribution.We may also be interested in its normalizing constant P (D | H) if we wish to do model comparison.The probability distribution P (w | D, H) is often a complex distribution.In a variational approach to inference, we introduce an approximating probability distribution over the parameters, Q(w; θ), and optimize this distribution (by varying its own parameters θ) so that it approximates the posterior distribution of the parameters P (w | D, H) well.One objective function we may choose to measure the quality of the approximation is the variational free energy For certain models and certain approximating distributions, this free energy, and its derivatives with respect to the approximating distribution's parameters, can be evaluated.The approximation of posterior probability distributions using variational free energy minimization provides a useful approach to approximating Bayesian inference in a number of fields ranging from neural networks to the decoding of error-correcting codes (Hinton and van Camp, 1993;Hinton and Zemel, 1994;Dayan et al., 1995;Neal and Hinton, 1998;MacKay, 1995a).The method is sometimes called ensemble learning to contrast it with traditional learning processes in which a single parameter vector is optimized.Another name for it is variational Bayes.Let us examine how ensemble learning works in the simple case of a Gaussian distribution.33.5 The case of an unknown Gaussian: approximating the posterior distribution of µ and σWe will fit an approximating ensemble Q(µ, σ) to the posterior distribution that we studied in Chapter 24,1 σµ.(33.36)We make the single assumption that the approximating ensemble is separable in the form Q(µ, σ) = Q µ (µ)Q σ (σ).No restrictions on the functional form of Q µ (µ) and Q σ (σ) are made.We write down a variational free energy,.(33.37)We can find the optimal separable distribution Q by considering separately the optimization of F over Q µ (µ) for fixed Q σ (σ), and then the optimization of Q σ (σ) for fixed Q µ (µ).Optimization of Q µ (µ)As a functional of Q µ (µ), F is: (33.39)where β ≡ 1/σ 2 and κ denote constants that do not depend on Q µ (µ).The dependence on Q σ thus collapses down to a simple dependence on the mean β ≡ dσ Q σ (σ)1/σ 2 .(33.40)Now we can recognize the function −N β 1 2 (µ − x) 2 as the logarithm of a Gaussian identical to the posterior distribution for a particular value of β = β.Since a relative entropy Q ln(Q/P ) is minimized by setting Q = P , we can immediately write down the distribution Q opt µ (µ) that minimizes F for fixed (33.41)where σ 2 µ|D = 1/(N β).Optimization of Q σ (σ)We represent Q σ (σ) using the density over β, Q σ (β) ≡ Q σ (σ) |dσ/dβ|.As aThe prior P (σ) ∝ 1/σ transforms to P (β) ∝ 1/β.where the integral over µ is performed assuming Q µ (µ) = Q opt µ (µ).Here, the βdependent expression in square brackets can be recognized as the logarithm of a gamma distribution over β -see equation (23.15) -giving as the distribution that minimizes F for fixed Q µ : In figure 33.4,these two update rules (33.41, 33.44) are applied alternately, starting from an arbitrary initial condition.The algorithm converges to the optimal approximating ensemble in a few iterations.Direct solution for the joint optimum Q µ (µ)Q σ (σ)In this problem, we do not need to resort to iterative computation to find the optimal approximating ensemble.Equations (33.41) and (33.44) define the optimum implicitly.We must simultaneously have σ 2 µ|D = 1/(N β), and β = b c .The solution is:1/ β = S/(N − 1).(33.46)This is similar to the true posterior distribution of σ, which is a gamma distribution with c = N −1 2 and 1/b = S/2 (see equation 24.13).This true posterior also has a mean value of β satisfying 1/ β = S/(N − 1); the only difference is that the approximating distribution's parameter c is too large by 1/2.The approximations given by variational free energy minimization always tend to be more compact than the true distribution.In conclusion, ensemble learning gives an approximation to the posterior that agrees nicely with the conventional estimators.The approximate posterior distribution over β is a gamma distribution with mean β corresponding to a variance of σ 2 = S/(N − 1) = σ 2 N−1 .And the approximate posterior distribution over µ is a Gaussian with mean x and standard deviation σ N−1 / √ N .The variational free energy minimization approach has the nice property that it is parameterization-independent; it avoids the problem of basisdependence from which MAP methods and Laplace's method suffer.A convenient software package for automatic implementation of variational inference in graphical models is VIBES (Bishop et al., 2002).It plays the same role for variational inference as BUGS plays for Monte Carlo inference.One of my students asked: How do you ever come up with a useful approximating distribution, given that the true distribution is so complex you can't compute it directly?Let's answer this question in the context of Bayesian data modelling.Let the 'true' distribution of interest be the posterior probability distribution over a set of parameters x, P (x | D).A standard data modelling practice is to find a single, 'best-fit' setting of the parameters, x * , for example, by finding the maximum of the likelihood function P (D | x), or of the posterior distribution.One interpretation of this standard practice is that the full description of our knowledge about x, P (x | D), is being approximated by a delta-function, a probability distribution concentrated on x * .From this perspective, any approximating distribution Q(x; θ), no matter how crummy it is, has to be an improvement on the spike produced by the standard method!So even if we use only a simple Gaussian approximation, we are doing well.We now study an application of the variational approach to a realistic example -data clustering.In Chapter 20, we introduced the soft K-means clustering algorithm, version 1.In Chapter 22, we introduced versions 2 and 3 of this algorithm, and motivated the algorithm as a maximum likelihood algorithm.K-means clustering is an example of an 'expectation-maximization' (EM) algorithm, with the two steps, which we called 'assignment' and 'update', being known as the 'E-step' and the 'M-step' respectively.We now give a more general view of K-means clustering, due to Neal and Hinton (1998), in which the algorithm is shown to optimize a variational objective function.Neal and Hinton's derivation applies to any EM algorithm.Let the parameters of the mixture model -the means, standard deviations, and weights -be denoted by θ.For each data point, there is a missing variable (also known as a latent variable), the class label k n for that point.The probability of everything, given our assumed model H, isThe posterior probability of everything, given the data, is proportional to the probability of everything: .48)We now approximate this posterior distribution by a separable distribution (33.49) and define a variational free energy in the usual way:.(33.50) F is bounded below by minus the evidence, ln P ({x (n) } N n=1 | H).We can now make an iterative algorithm with an 'assignment' step and an 'update' step.In the assignment step, Q k ({k n } N n=1 ) is adjusted to reduce F , for fixed Q ; in the update step, Q is adjusted to reduce F , for fixed Q k .If we wish to obtain exactly the soft K-means algorithm, we impose a further constraint on our approximating distribution: Q is constrained to be a delta function centred on a point estimate of θ, θ = θ * :(33.51)33.8: Variational methods other than free energy minimization 433 0 1 2 -5 0 5Upper boundLower boundwhere λ(ν) = [g(ν) − 1/2] /2ν.These upper and lower bounds are exponential or Gaussian functions of a, and so easier to integrate over.The graph shows the sigmoid function and upper and lower bounds with µ = 0.505 and ν = −2.015.Unfortunately, this distribution contributes to the variational free energy an infinitely large integral d D θ Q (θ) ln Q (θ), so we'd better leave that term out of F , treating it as an additive constant.[Using a delta function Q is not a good idea if our aim is to minimize F !] Moving on, our aim is to derive the soft K-means algorithm.Exercise 33.2. [2 ]Show that, given Q (θ) = δ(θ − θ * ), the optimal Q k , in the sense of minimizing F , is a separable distribution in which the probability that k n = k is given by the responsibility r(n)k .Exercise 33.3. [3 ]Show that, given a separable Q k as described above, the optimal θ * , in the sense of minimizing F , is obtained by the update step of the soft K-means algorithm.(Assume a uniform prior on θ.)Exercise 33.4. [4 ]We can instantly improve on the infinitely large value of F achieved by soft K-means clustering by allowing Q to be a more general distribution than a delta-function.Derive an update step in which Q is allowed to be a separable distribution, a product of Q µ ({µ}), Q σ ({σ}), and Q π (π).Discuss whether this generalized algorithm still suffers from soft K-means's 'kaboom' problem, where the algorithm glues an evershrinking Gaussian to one data point.Sadly, while it sounds like a promising generalization of the algorithm to allow Q to be a non-delta-function, and the 'kaboom' problem goes away, other artefacts can arise in this approximate inference method, involving local minima of F .For further reading, see (MacKay, 1997a;MacKay, 2001).There are other strategies for approximating a complicated distribution P (x), in addition to those based on minimizing the relative entropy between an approximating distribution, Q, and P .One approach pioneered by Jaakkola and Jordan is to create adjustable upper and lower bounds Q U and Q L to P , as illustrated in figure 33.5.These bounds (which are unnormalized densities) are parameterized by variational parameters which are adjusted in order to obtain the tightest possible fit.The lower bound can be adjusted to maximize (33.52) and the upper bound can be adjusted to minimizeUsing the normalized versions of the optimized bounds we then compute approximations to the predictive distributions.Further reading on such methods can be found in the references (Jaakkola and Jordan, 2000a;Jaakkola and Jordan, 2000b;Jaakkola and Jordan, 1996;Gibbs and MacKay, 2000).The Bethe and Kikuchi free energiesIn Chapter 26 we discussed the sum-product algorithm for functions of the factor-graph form (26.1).If the factor graph is tree-like, the sum-product algorithm converges and correctly computes the marginal function of any variablex n and can also yield the joint marginal function of subsets of variables that appear in a common factor, such as x m .The sum-product algorithm may also be applied to factor graphs that are not tree-like.If the algorithm converges to a fixed point, it has been shown that that fixed point is a stationary point (usually a minimum) of a function of the messages called the Kikuchi free energy.In the special case where all factors in factor graph are functions of one or two variables, the Kikuchi free energy is called the Bethe free energy.For articles on this idea, and new approximate inference algorithms motivated by it, see Yedidia (2000); Yedidia et al. (2000); Welling and Teh (2001); Yuille (2001); Yedidia et al. (2001b); Yedidia et al. (2001a).p.435]This exercise explores the assertion, made above, that the approximations given by variational free energy minimization always tend to be more compact than the true distribution.Consider a two dimensional Gaussian distribution P (x) with axes aligned with the directions e (1) = (1, 1) and e (2) = (1, −1).Let the variances in these two directions be σ 2 1 and σ 2 2 .What is the optimal variance if this distribution is approximated by a spherical Gaussian with variance σ 2 Q , optimized by variational free energy minimization?If we instead optimized the objective functionwhat would be the optimal value of σ 2 ?Sketch a contour of the true distribution P (x) and the two approximating distributions in the case σ 1 /σ 2 = 10.[Note that in general it is not possible to evaluate the objective function G, because integrals under the true distribution P (x) are usually intractable.]p.436] What do you think of the idea of using a variational method to optimize an approximating distribution Q which we then use as a proposal density for importance sampling?Exercise 33.7. [2 ]Define the relative entropy or Kullback-Leibler divergence between two probability distributions P and Q, and state Gibbs' inequality.Consider the problem of approximating a joint distribution P (x, y) by a separable distributionthat the minimal value of G is achieved when Q X and Q Y are equal to the marginal distributions over x and y.the probability distribution P (x, y) shown in the margin is to be ap- Show that F (Q X , Q Y ) has three distinct minima, identify those minima, and evaluate F at each of them.Solution to exercise 33.5 (p.434).We need to know the relative entropy between two one-dimensional Gaussian distributions:So, if we approximate P , whose variances are σ 2 1 and σ 2 2 , by Q, whose variances are both σ 2 Q , we find(33.59)Thus we set the approximating distribution's inverse variance to the mean inverse variance of the target distribution P .In the case σ 1 = 10 and σ 2 = 1, we obtain σ Q √ 2, which is just a factor of √ 2 larger than σ 2 , pretty much independent of the value of the larger standard deviation σ 1 .Variational free energy minimization typically leads to approximating distributions whose length scales match the shortest length scale of the target distribution.The approximating distribution might be viewed as too compact.In contrast, if we use the objective function G then we find: (33.60)where the constant depends on σ 1 and σ 2 only.Differentiating,which is zero whenThus we set the approximating distribution's variance to the mean variance of the target distribution P .In the case σ 1 = 10 and σ 2 = 1, we obtain σ Q 10/ √ 2, which is just a factor of √ 2 smaller than σ 1 , independent of the value of σ 2 .The two approximations are shown to scale in figure 33.6.Solution to exercise 33.6 (p.434).The best possible variational approximation is of course the target distribution P .Assuming that this is not possible, a good variational approximation is more compact than the true distribution.In contrast, a good sampler is more heavy tailed than the true distribution.An over-compact distribution would be a lousy sampler with a large variance.Independent Component Analysis and Latent Variable ModellingMany statistical models are generative models (that is, models that specify a full probability density over all variables in the situation) that make use of latent variables to describe a probability distribution over observables.Examples of latent variable models include Chapter 22's mixture models, which model the observables as coming from a superposed mixture of simple probability distributions (the latent variables are the unknown class labels of the examples); hidden Markov models (Rabiner and Juang, 1986;Durbin et al., 1998); and factor analysis.The decoding problem for error-correcting codes can also be viewed in terms of a latent variable model -figure 34.1.In that case, the encoding matrix G is normally known in advance.In latent variable modelling, the parameters equivalent to G are usually not known, and must be inferred from the data along with the latent variables s.Error-correcting codes as latent variable models.The K latent variables are the independent source bits s 1 , . . ., s K ; these give rise to the observables via the generator matrix G.Usually, the latent variables have a simple distribution, often a separable distribution.Thus when we fit a latent variable model, we are finding a description of the data in terms of 'independent components'.The 'independent component analysis' algorithm corresponds to perhaps the simplest possible latent variable model with continuous latent variables.n=1 are assumed to be generated as follows.Each J-dimensional vector x is a linear mixture of I underlying source signals, s:x = Gs, (34.1)where the matrix of mixing coefficients G is not known.The simplest algorithm results if we assume that the number of sources is equal to the number of observations, i.e., I = J.Our aim is to recover the source variables s (within some multiplicative factors, and possibly permuted).To put it another way, we aim to create the inverse of G (within a post-multiplicative factor) given only a set of examples {x}.We assume that the latent variables are independently distributed, with marginal distributions P (s i | H) ≡ p i (s i ).Here H denotes the assumed form of this model and the assumed probability distributions p i of the latent variables.The probability of the observables and the hidden variables, given G and 438 34 -Independent Component Analysis and Latent Variable Modelling H, is:We assume that the vector x is generated without noise.This assumption is not usually made in latent variable modelling, since noise-free data are rare; but it makes the inference problem far simpler to solve.For learning about G from the data D, the relevant quantity is the likelihood functionwhich is a product of factors each of which is obtained by marginalizing over the latent variables.When we marginalize over delta functions, remember that ds δ(x − vs)f (s) = 1 v f (x/v).We adopt summation convention at this point, such that, for example, G ji si .A single factor in the likelihood is given byTo obtain a maximum likelihood algorithm we find the gradient of the log likelihood.If we introduce W ≡ G −1 , the log likelihood contributed by a single example may be written:(34.9)We'll assume from now on that det W is positive, so that we can omit the absolute value sign.We will need the following identities: (34.13)Repeat for each datapoint x:1. Put x through a linear mapping:2. Put a through a nonlinear map:where a popular choice for φ is φ = − tanh(a i ).3. Adjust the weights in accordance withAlgorithm 34.2.Independent component analysis -online steepest ascents version.See also algorithm 34.4,which is to be preferred.and z i = φ i (a i ), which indicates in which direction a i needs to change to make the probability of the data greater.We may then obtain the gradient with respect to G ji using equations (34.10) and (34.11):Or alternatively, the derivative with respect to W ij : (34.15)If we choose to change W so as to ascend this gradient, we obtain the learning rule ∆W ∝ [W T ] −1 + zx T .(34.16)The algorithm so far is summarized in algorithm 34.2.The choice of the function φ defines the assumed prior distribution of the latent variable s.Let's first consider the linear choice φ i (a i ) = −κa i , which implicitly (via equation 34.13) assumes a Gaussian distribution on the latent variables.The Gaussian distribution on the latent variables is invariant under rotation of the latent variables, so there can be no evidence favouring any particular alignment of the latent variable space.The linear algorithm is thus uninteresting in that it will never recover the matrix G or the original sources.Our only hope is thus that the sources are non-Gaussian.Thankfully, most real sources have non-Gaussian distributions; often they have heavier tails than Gaussians.We thus move on to the popular tanh nonlinearity.Ifthen implicitly we are assumingThis is a heavier-tailed distribution for the latent variables than the Gaussian distribution.440 34 -Independent Component Analysis and Latent Variable Modelling We could also use a tanh nonlinearity with gain β, that is, φ i (a i ) = − tanh(βa i ), whose implicit probabilistic model is p i (s i ) ∝ 1/[cosh(βs i )] 1/β .In the limit of large β, the nonlinearity becomes a step function and the probability distribution p i (s i ) becomes a biexponential distribution, p i (s i ) ∝ exp(−|s|).In the limit β → 0, p i (s i ) approaches a Gaussian with mean zero and variance 1/β.Heavier-tailed distributions than these may also be used.The Student and Cauchy distributions spring to mind.Figures 34.3(a-c) illustrate typical distributions generated by the independent components model when the components have 1/ cosh and Cauchy distributions. Figure 34.3dshows some samples from the Cauchy model.The Cauchy distribution, being the more heavy-tailed, gives the clearest picture of how the predictive distribution depends on the assumed generative parameters G.We have thus derived a learning algorithm that performs steepest descents on the likelihood function.The algorithm does not work very quickly, even on toy data; the algorithm is ill-conditioned and illustrates nicely the general advice that, while finding the gradient of an objective function is a splendid idea, ascending the gradient directly may not be.The fact that the algorithm is ill-conditioned can be seen in the fact that it involves a matrix inverse, which can be arbitrarily large or even undefined.1968).A prime example of a non-covariant algorithm is the popular steepest descents rule.A dimensionless objective function L(w) is defined, its derivative with respect to some parameters w is computed, and then w is changed by the rule the Munro-Robbins theorem (Bishop, 1992, p. 41) shows that the parameters will asymptotically converge to the maximum likelihood parameters.where M is a positive-definite matrix whose i, i element has dimensions [w i w i ].From where can we obtain such a matrix?Two sources of such matrices are metrics and curvatures.If there is a natural metric that defines distances in our parameter space w, then a matrix M can be obtained from the metric.There is often a natural choice.In the special case where there is a known quadratic metric defining the length of a vector w, then the matrix can be obtained from the quadratic form.For example if the length is w 2 then the natural matrix is M = I, and steepest descents is appropriate.Another way of finding a metric is to look at the curvature of the objective function, defining A ≡ −∇∇L (where ∇ ≡ ∂/∂w).Then the matrix M = A −1 will give a covariant algorithm; what is more, this algorithm is the Newton algorithm, so we recognize that it will alleviate one of the principal difficulties with steepest descents, namely its slow convergence to a minimum when the objective function is at all ill-conditioned.The Newton algorithm converges to the minimum in a single step if L is quadratic.In some problems it may be that the curvature A consists of both datadependent terms and data-independent terms; in this case, one might choose to define the metric using the data-independent terms only (Gull, 1989).The resulting algorithm will still be covariant but it will not implement an exact Newton step.Obviously there are many covariant algorithms; there is no unique choice.But covariant algorithms are a small subset of the set of all algorithms!34 -Independent Component Analysis and Latent Variable ModellingFor the present maximum likelihood problem we have evaluated the gradient with respect to G and the gradient with respect to W = G −1 .Steepest ascents in W is not covariant.Let us construct an alternative, covariant algorithm with the help of the curvature of the log likelihood.Taking the second derivative of the log likelihood with respect to W we obtain two terms, the first of which is data-independent: (34.21) and the second of which is data-dependent:where z is the derivative of z.It is tempting to drop the data-dependent term and define the matrixHowever, this matrix is not positive definite (it has at least one non-positive eigenvalue), so it is a poor approximation to the curvature of the log likelihood, which must be positive definite in the neighbourhood of a maximum likelihood solution.We must therefore consult the data-dependent term for inspiration.The aim is to find a convenient approximation to the curvature and to obtain a covariant algorithm, not necessarily to implement an exact Newton step.What is the average value of x j x l δ ik z i ?If the true value of G is G * , then (34.23)We now make several severe approximations: we replace G * by the present value of G, and replace the correlated average s m s n z i by s m s n z i ≡ Σ mn D i .Here Σ is the variance-covariance matrix of the latent variables (which is assumed to exist), and D i is the typical value of the curvature d 2 ln p i (a)/da 2 .Given that the sources are assumed to be independent, Σ and D are both diagonal matrices.These approximations motivate the matrix M given by: (34.24) that is,For simplicity, we further assume that the sources are similar to each other so that Σ and D are both homogeneous, and that ΣD = 1.This will lead us to an algorithm that is covariant with respect to linear rescaling of the data x, but not with respect to linear rescaling of the latent variables.We thus use: (34.26)Multiplying this matrix by the gradient in equation (34.15) we obtain the following covariant learning algorithm: (34.27)Notice that this expression does not require any inversion of the matrix W.The only additional computation once z has been computed is a single backward pass through the weights to compute the quantityRepeat for each datapoint x:1. Put x through a linear mapping:2. Put a through a nonlinear map:where a popular choice for φ is φ = − tanh(a i ).3. Put a back through W:x = W T a.∆W ∝ W + zx T .Algorithm 34.4.Independent component analysis -covariant version.in terms of which the covariant algorithm reads:The quantity W ij + x j z i on the right-hand side is sometimes called the natural gradient.The covariant independent component analysis algorithm is summarized in algorithm 34.4.ICA was originally derived using an information maximization approach (Bell and Sejnowski, 1995).Another view of ICA, in terms of energy functions, which motivates more general models, is given by .Another generalization of ICA can be found in Parra (1996, 1997).There is now an enormous literature on applications of ICA.A variational free energy minimization approach to ICA-like models is given in (Miskin, 2001;Miskin and MacKay, 2000;Miskin and MacKay, 2001).Further reading on blind separation, including non-ICA algorithms, can be found in (Jutten and Herault, 1991;Comon et al., 1991;Hendin et al., 1994;Amari et al., 1996;Hojen-Sorensen et al., 2002).While latent variable models with a finite number of latent variables are widely used, it is often the case that our beliefs about the situation would be most accurately captured by a very large number of latent variables.Consider clustering, for example.If we attack speech recognition by modelling words using a cluster model, how many clusters should we use?The number of possible words is unbounded (section 18.2), so we would really like to use a model in which it's always possible for new clusters to arise.Furthermore, if we do a careful job of modelling the cluster corresponding to just one English word, we will probably find that the cluster for one word should itself be modelled as composed of clusters -indeed, a hierarchy of clusters within clusters.The first levels of the hierarchy would divide male speakers from female, and would separate speakers from different regions -India, Britain, Europe, and so forth.Within each of those clusters would be subclusters for the different accents within each region.The subclusters could have subsubclusters right down to the level of villages, streets, or families.Thus we would often like to have infinite numbers of clusters; in some cases the clusters would have a hierarchical structure, and in other cases the hierarchy would be flat.So, how should such infinite models be implemented in finite computers?And how should we set up our Bayesian models so as to avoid getting silly answers?Infinite mixture models for categorical data are presented in Neal (1991), along with a Monte Carlo method for simulating inferences and predictions.Infinite Gaussian mixture models with a flat hierarchical structure are presented in Rasmussen (2000).Neal (2001) shows how to use Dirichlet diffusion trees to define models of hierarchical clusters.Most of these ideas build on the Dirichlet process (section 18.2).This remains an active research area (Rasmussen and Ghahramani, 2002;Beal et al., 2002).Exercise 34.1. [3 ]Repeat the derivation of the algorithm, but assume a small amount of noise in x: x = Gs + n; so the term δ xin the joint probability (34.3) is replaced by a probability distribution over x(n)i .Show that, if this noise distribution has sufficiently small standard deviation, the identical algorithm results.Exercise 34.2. [3 ]Implement the covariant ICA algorithm and apply it to toy data.Exercise 34.3. [4][5]Create algorithms appropriate for the situations: (a) x includes substantial Gaussian noise; (b) more measurements than latent variables (J > I); (c) fewer measurements than latent variables (J < I).Factor analysis assumes that the observations x can be described in terms of independent latent variables {s k } and independent additive noise.Thus the observable x is given by (34.30)where n is a noise vector whose components have a separable probability distribution.In factor analysis it is often assumed that the probability distributions of {s k } and {n i } are zero-mean Gaussians; the noise terms may have different variances σ 2 i .Exercise 34.4. [4 ]Make a maximum likelihood algorithm for inferring G from data, assuming the generative model x = Gs + n is correct and that s and n have independent Gaussian distributions.Include parameters σ 2 j to describe the variance of each n j , and maximize the likelihood with respect to them too.Let the variance of each s i be 1.Random Inference Topics And what is the probability that it starts with a '9', like the Faraday constant, F = 9.648 . . .× 10 4 C mol −1 ?What about the second digit?What is the probability that the mantissa of x starts '1.1...', and what is the probability that x starts '9.9...' ?Solution.An expert on neutrons, fireflies, Antarctica, or Jove might be able to predict the value of x, and thus predict the first digit with some confidence, but what about someone with no knowledge of the topic?What is the probability distribution corresponding to 'knowing nothing' ?One way to attack this question is to notice that the units of x have not been specified.If the half-life of the neutron were measured in fortnights instead of seconds, the number x would be divided by 1 209 600; if it were measured in years, it would be divided by 3 × 10 7 .Now, is our knowledge about x, and, in particular, our knowledge of its first digit, affected by the change in units?For the expert, the answer is yes; but let us take someone truly ignorant, for whom the answer is no; their predictions about the first digit of x are independent of the units.The arbitrariness of the units corresponds to invariance of the probability distribution when x is multiplied by any number.If you don't know the units that a quantity is measured in, the probability of the first digit must be proportional to the length of the corresponding piece of logarithmic scale.The probability that the first digit of a number isNow, 2 10 = 1024 10 3 = 1000, so without needing a calculator, we have  (35.3)This observation about initial digits is known as Benford's law.Ignorance does not correspond to a uniform probability distribution over d. 2Exercise 35.2. [2 ]A pin is thrown tumbling in the air.What is the probability distribution of the angle θ 1 between the pin and the vertical at a moment while it is in the air?The tumbling pin is photographed.What is the probability distribution of the angle θ 3 between the pin and the vertical as imaged in the photograph?Exercise 35.3. [2 ]Record breaking.Consider keeping track of the world record for some quantity x, say earthquake magnitude, or longjump distances jumped at world championships.If we assume that attempts to break the record take place at a steady rate, and if we assume that the underlying probability distribution of the outcome x, P (x), is not changingan assumption that I think is unlikely to be true in the case of sports endeavours, but an interesting assumption to consider nonetheless -and assuming no knowledge at all about P (x), what can be predicted about successive intervals between the dates when records are broken?3C, p.449]In their landmark paper demonstrating that bacteria could mutate from virus sensitivity to virus resistance, Luria and Delbrück (1943) wanted to estimate the mutation rate in an exponentially-growing population from the total number of mutants found at the end of the experiment.This problem is difficult because the quantity measured (the number of mutated bacteria) has a heavy-tailed probability distribution: a mutation occuring early in the experiment can give rise to a huge number of mutants.Unfortunately, Luria and Delbrück didn't know Bayes' theorem, and their way of coping with the heavy-tailed distribution involves arbitrary hacks leading to two different estimators of the mutation rate.One of these estimators (based on the mean number of mutated bacteria, averaging over several experiments) has appallingly large variance, yet sampling theorists continue to use it and base confidence intervals around it (Kepler and Oprea, 2001).In this exercise you'll do the inference right.In each culture, a single bacterium that is not resistant gives rise, after g generations, to N = 2 g descendants, all clones except for differences arising from mutations.The final culture is then exposed to a virus, and the number of resistant bacteria n is measured.According to the now accepted mutation hypothesis, these resistant bacteria got their resistance from random mutations that took place during the growth of the colony.The mutation rate (per cell per generation), a, is about one in a hundred million.The total number of opportunities to mutate is N , since g−1 i=0 2 i 2 g = N .If a bacterium mutates at the ith generation, its descendants all inherit the mutation, and the final number of resistant bacteria contributed by that one ancestor is 2 g−i .Given M separate experiments, in each of which a colony of size N is created, and where the measured numbers of resistant bacteria are {n m } M m=1 , what can we infer about the mutation rate, a? Make the inference given the following dataset from Luria and Delbrück, for N = 2.4 × 10 8 : {n m } = {1, 0, 3, 0, 0, 5, 0, 5, 0, 6, 107, 0, 0, 0, 1, 0, 0, 64, 0, 35}.[A small amount of computation is required to solve this problem.]p.450]In the Bayesian graphical model community, the task of inferring which way the arrows point -that is, which nodes are parents, and which children -is one on which much has been written.Inferring causation is tricky because of 'likelihood equivalence'.Two graphical models are likelihood-equivalent if for any setting of the parameters of either, there exists a setting of the parameters of the other such that the two joint probability distributions of all observables are identical.An example of a pair of likelihood-equivalent models are A → B and B → A. The model A → B asserts that A is the parent of B, or, in very sloppy terminology, 'A causes B'.An example of a situation where 'B → A' is true is the case where B is the variable 'burglar in house' and A is the variable 'alarm is ringing'.Here it is literally true that B causes A. But this choice of words is confusing if applied to another example, R → D, where R denotes 'it rained this morning' and D denotes 'the pavement is dry'.'R causes D' is confusing.I'll therefore use the words 'B is a parent of A' to denote causation.Some statistical methods that use the likelihood alone are unable to use data to distinguish between likelihood-equivalent models.In a Bayesian approach, on the other hand, two likelihood-equivalent models may nevertheless be somewhat distinguished, in the light of data, since likelihood-equivalence does not force a Bayesian to use priors that assign equivalent densities over the two parameter spaces of the models.However, many Bayesian graphical modelling folks, perhaps out of sympathy for their non-Bayesian colleagues, or from a latent urge not to appear different from them, deliberately discard this potential advantage of Bayesian methods -the ability to infer causation from data -by skewing their models so that the ability goes away; a widespread orthodoxy holds that one should identify the choices of prior for which 'prior equivalence' holds, i.e., the priors such that models that are likelihood-equivalent also have identical posterior probabilities; and then one should use one of those priors in inference and prediction.This argument motivates the use, as the prior over all probability vectors, of specially-constructed Dirichlet distributions.In my view it is a philosophical error to use only those priors such that causation cannot be inferred.Priors should be set to describe one's assumptions; when this is done, it's likely that interesting inferences about causation can be made from data.In this exercise, you'll make an example of such an inference.Consider the toy problem where A and B are binary variables.The two models are H A→B and H B→A .H A→B asserts that the marginal probability of A comes from a beta distribution with parameters (1, 1), i.e., the uniform distribution; and that the two conditional distributions P (b | a = 0) and P (b | a = 1) also come independently from beta distributions with parameters (1, 1).The other model assigns similar priors to the marginal probability of B and the conditional distributions of A given B. Data are gathered, and the Hint: it's a good idea to work this exercise out symbolically in order to spot all the simplifications that emerge.The topic of inferring causation is a complex one.The fact that Bayesian inference can sensibly be used to infer the directions of arrows in graphs seems to be a neglected view, but it is certainly not the whole story.See Pearl (2000) for discussion of many other aspects of causality.Exercise 35.6. [3 ]Photons arriving at a photon detector are believed to be emitted as a Poisson process with a time-varying rate,where the parameters a, b, ω, and φ are known.Data are collected during the time t = 0 . . .T .Given that N photons arrived at times {t n } N n=1 , discuss the inference of a, b, ω, and φ. [Further reading: Gregory and Loredo (1992).]Exercise 35.7. [2 ]A data file consisting of two columns of numbers has been printed in such a way that the boundaries between the columns are unclear.Here are the resulting strings.[A parsing of a string is a grammatical interpretation of the string.For example, 'Punch bores' could be parsed as 'Punch (noun) bores (verb)', or 'Punch (imperative verb) bores (plural noun)'.]Exercise 35.8. [2 ]In an experiment, the measured quantities {x n } come independently from a biexponential distribution with mean µ,where Z is the normalizing constant, Z = 2.The mean µ is not known.An example of this distribution, with µ = 1, is shown in figure 35.2.Assuming the four datapoints are {x n } = {0, 0.9, 2, 6},what do these data tell us about µ? Include detailed sketches in your answer.Give a range of plausible values of µ.Solution to exercise 35.4 (p.446).A population of size N has N opportunities to mutate.The probability of the number of mutations that occurred, r, is roughly Poisson(This is slightly inaccurate because the descendants of a mutant cannot themselves undergo the same mutation.)Each mutation gives rise to a number of final mutant cells n i that depends on the generation time of the mutation.If multiplication went like clockwork then the probability of n i being 1 would be 1/2, the probability of 2 would be 1/4, the probability of 4 would be 1/8, and P (n i ) = 1/(2n) for all n i that are powers of two.But we don't expect the mutant progeny to divide in exact synchrony, and we don't know the precise timing of the end of the experiment compared to the division times.A smoothed version of this distribution that permits all integers to occur iswhere Z = π 2 /6 = 1.645.[This distribution's moments are all wrong, since n i can never exceed N , but who cares about moments?-only sampling theory statisticians who are barking up the wrong tree, constructing 'unbiased estimators' such as â = (n/N )/ log N .The error that we introduce in the likelihood function by using the approximation to P (n i ) is negligible.]The observed number of mutants n is the sumThe probability distribution of n given r is the convolution of r identical distributions of the form (35.8).For example,(35.10)The probability distribution of n given a, which is what we need for the Bayesian inference, is given by summing over r. (35.11)This quantity can't be evaluated analytically, but for small a, it's easy to evaluate to any desired numerical precision by explicitly summing over r from r = 0 to some r max , with P (n | r) also being found for each r by r max explicit convolutions for all required values of n; if r max = n max , the largest value of n encountered in the data, then P (n | a) is computed exactly; but for this question's data, r max = 9 is plenty for an accurate result; I used r max = 74 to make the graphs in figure 35 Incidentally, for data sets like the one in this exercise, which have a substantial number of zero counts, very little is lost by making Luria and Delbruck's second approximation, which is to retain only the count of how many n were equal to zero, and how many were non-zero.The likelihood function found using this weakened data set, L(a) = (e −aN ) 11 (1 − e −aN ) 9 , (35.12) is scarcely distinguishable from the likelihood computed using full information.Solution to exercise 35.5 (p.447).From the six terms of the formDecision theory is trivial, apart from computational details (just like playing chess!).You have a choice of various actions, a.The world may be in one of many states x; which one occurs may be influenced by your action.The world's state has a probability distribution P (x | a).Finally, there is a utility function U (x, a) which specifies the payoff you receive when the world is in state x and you chose action a.The task of decision theory is to select the action that maximizes the expected utility,(36.1)That's all.The computational problem is to maximize E[U | a] over a. [Pessimists may prefer to define a loss function L instead of a utility function U and minimize the expected loss.]Is there anything more to be said about decision theory?Well, in a real problem, the choice of an appropriate utility function may be quite difficult.Furthermore, when a sequence of actions is to be taken, with each action providing information about x, we have to take into account the effect that this anticipated information may have on our subsequent actions.The resulting mixture of forward probability and inverse probability computations in a decision problem is distinctive.In a realistic problem such as playing a board game, the tree of possible cogitations and actions that must be considered becomes enormous, and 'doing the right thing' is not simple, because the expected utility of an action cannot be computed exactly (Russell and Wefald, 1991;Baum and Smith, 1993;Baum and Smith, 1997).Let's explore an example.Suppose you have the task of choosing the site for a Tanzanite mine.Your final action will be to select the site from a list of N sites.The nth site has a net value called the return x n which is initially unknown, and will be found out exactly only after site n has been chosen.[x n equals the revenue earned from selling the Tanzanite from that site, minus the costs of buying the site, paying the staff, and so forth.]At the outset, the return x n has a probability distribution P (x n ), based on the information already available.Before you take your final action you have the opportunity to do some prospecting.Prospecting at the nth site has a cost c n and yields data d n which reduce the uncertainty about x n .[We'll assume that the returns of the N sites are unrelated to each other, and that prospecting at one site only yields information about that site and doesn't affect the return from that site.]Your decision problem is:given the initial probability distributions P (x 1 ), P (x 2 ), . . ., P (x N ), first, decide whether to prospect, and at which sites; then, in the light of your prospecting results, choose which site to mine.For simplicity, let's make everything in the problem Gaussian and focusThe notation P (y) = Normal(y; µ, σ 2 ) indicates that y has Gaussian distribution with mean µ and variance σ 2 .on the question of whether to prospect once or not.We'll assume our utility function is linear in x n ; we wish to maximize our expected return.The utility function isif no prospecting is done, where n a is the chosen 'action' site; and, if prospecting is done, the utility is (36.3)where n p is the site at which prospecting took place.The prior distribution of the return of site n isIf you prospect at site n, the datum d n is a noisy version of x n :Exercise 36.1. [2 ]Given these assumptions, show that the prior probability distribution of d n is(mnemonic: when independent variables add, variances add), and that the posterior distribution of x n given d n iswhere(mnemonic: when Gaussians multiply, precisions add).To start with, let's evaluate the expected utility if we do no prospecting (i.e., choose the site immediately); then we'll evaluate the expected utility if we first prospect at one site and then make our choice.From these two results we will be able to decide whether to prospect once or zero times, and, if we prospect once, at which site.So, first we consider the expected utility without any prospecting.Exercise 36.2. [2 ]Show that the optimal action, assuming no prospecting, is to select the site with biggest mean (36.9) and the expected utility of this action is (36.10)[If your intuition says 'surely the optimal decision should take into account the different uncertainties σ n too?', the answer to this question is 'reasonable -if so, then the utility function should be nonlinear in x'.]Now the exciting bit.Should we prospect?Once we have prospected at site n p , we will choose the site using the decision rule (36.9) with the value of mean µ np replaced by the updated value µ n given by (36.8).What makes the problem exciting is that we don't yet know the value of d n , so we don't know what our action n a will be; indeed the whole value of doing the prospecting comes from the fact that the outcome d n may alter the action from the one that we would have taken in the absence of the experimental information.From the expression for the new mean in terms of d n (36.8), and the known variance of d n (36.6), we can compute the probability distribution of the key quantity, µ n , and can work out the expected utility by integrating over all possible outcomes and their associated actions.Exercise 36.3. [2 ]Show that the probability distribution of the new mean µ n (36.8) is Gaussian with mean µ n and variance .11)Consider prospecting at site n.Let the biggest mean of the other sites be µ 1 .When we obtain the new value of the mean, µ n , we will choose site n and get an expected return of µ n if µ n > µ 1 , and we will choose site 1 and get an expected return of µ 1 if µ n < µ 1 .So the expected utility of prospecting at site n, then picking the best site, is(36.12)The difference in utility between prospecting and not prospecting is the quantity of interest, and it depends on what we would have done without prospecting; and that depends on whether µ 1 is bigger than µ n .(36.14)We can plot the change in expected utility due to prospecting (omitting c n ) as a function of the difference (µ n − µ 1 ) (horizontal axis) and the initial standard deviation σ n (vertical axis).In the figure the noise variance is σ 2 = 1.If the world in which we act is a little more complicated than the prospecting problem -for example, if multiple iterations of prospecting are possible, and the cost of prospecting is uncertain -then finding the optimal balance between exploration and exploitation becomes a much harder computational problem.Reinforcement learning addresses approximate methods for this problem (Sutton and Barto, 1998).Exercise 36.4. [2 ]The four doors problem.A new game show uses rules similar to those of the three doors (exercise 3.8 (p.57)), but there are four doors, and the host explains: 'First you will point to one of the doors, and then I will open one of the other doors, guaranteeing to choose a non-winner.Then you decide whether to stick with your original pick or switch to one of the remaining doors.Then I will open another non-winner (but never the current pick).You will then make your final decision by sticking with the door picked on the previous decision or by switching to the only other remaining door.'What is the optimal strategy?Should you switch on the first opportunity?Should you switch on the second opportunity?Exercise 36.5. [3 ]One of the challenges of decision theory is figuring out exactly what the utility function is.The utility of money, for example, is notoriously nonlinear for most people.In fact, the behaviour of many people cannot be captured by a coherent utility function, as illustrated by the Allais paradox, which runs as follows.Which Exercise 36.6. [4 ]Optimal stopping.A large queue of N potential partners is waiting at your door, all asking to marry you.-what is the optimal value of M ?Exercise 36.7. [3 ]Regret as an objective function?The better off.The action that minimizes the maximum possible regret is thus to buy the ticket.Discuss whether this use of regret to choose actions can be philosophically justified.The above problem can be turned into an investment portfolio decision problem by imagining that you have been given one pound to invest in two possible funds for one day: Fred's lottery fund, and the cash fund.If you put £f 1 into Fred's lottery fund, Fred promises to return £9f 1 to you if the lottery ticket is a winner, and otherwise nothing.The remaining £f 0 (with f 0 = 1 − f 1 ) is kept as cash.What is the best investment?Show that the minimax regret community will invest f 1 = 9/10 of their money in the high risk, high return lottery fund, and only f 0 = 1/10 in cash.Can this investment method be justified?Exercise 36.8. [3 ]Gambling oddities (from Cover and Thomas (1991) (36.15) and assuming that the probability that horse i wins is p i , work out the optimal betting strategy if your aim is Cover's aim, namely, to maximize the expected value of log m(T ).Show that the optimal strategy sets b equal to p, independent of the bookies' odds o.Show that when this strategy is used, the money is expected to grow exponentially as:whereIf you only bet once, is the optimal strategy any different?Do you think this optimal strategy makes sense?Do you think that it's 'optimal', in common language, to ignore the bookies' odds?What can you conclude about 'Cover's aim' ?Exercise 36.9. [3 ]Two ordinary dice are thrown repeatedly; the outcome of each throw is the sum of the two numbers.Joe Shark, who says that 6 and 8 are his lucky numbers, bets even money that a 6 will be thrown before the first 7 is thrown.If you were a gambler, would you take the bet?What is your probability of winning?Joe then bets even money that an 8 will be thrown before the first 7 is thrown.Would you take the bet?Having gained your confidence, Joe suggests combining the two bets into a single bet: he bets a larger sum, still at even odds, that an 8 and a 6 will be thrown before two 7s have been thrown.Would you take the bet?What is your probability of winning?There are two schools of statistics.Sampling theorists concentrate on having methods guaranteed to work most of the time, given minimal assumptions.Bayesians try to make inferences that take into account all available information and answer the question of interest given the particular data set.As you have probably gathered, I strongly recommend the use of Bayesian methods.Sampling theory is the widely used approach to statistics, and most papers in most journals report their experiments using quantities like confidence intervals, significance levels, and p-values.A p-value (e.g.p = 0.05) is the probability, given a null hypothesis for the probability distribution of the data, that the outcome would be as extreme as, or more extreme than, the observed outcome.Untrained readers -and perhaps, more worryingly, the authors of many papers -usually interpret such a p-value as if it is a Bayesian probability (for example, the posterior probability of the null hypothesis), an interpretation that both sampling theorists and Bayesians would agree is incorrect.In this chapter we study a couple of simple inference problems in order to compare these two approaches to statistics.While in some cases, the answers from a Bayesian approach and from sampling theory are very similar, we can also find cases where there are significant differences.We have already seen such an example in exercise 3.15 (p.59),where a sampling theorist got a p-value smaller than 7%, and viewed this as strong evidence against the null hypothesis, whereas the data actually favoured the null hypothesis over the simplest alternative.On p.64, another example was given where the p-value was smaller than the mystical value of 5%, yet the data again favoured the null hypothesis.Thus in some cases, sampling theory can be trigger-happy, declaring results to be 'sufficiently improbable that the null hypothesis should be rejected', when those results actually weakly support the null hypothesis.As we will now see, there are also inference problems where sampling theory fails to detect 'significant' evidence where a Bayesian approach and everyday intuition agree that the evidence is strong.Most telling of all are the inference problems where the 'significance' assigned by sampling theory changes depending on irrelevant factors concerned with the design of the experiment.This chapter is only provided for those readers who are curious about the sampling theory / Bayesian methods debate.If you find any of this chapter tough to understand, please skip it.There is no point trying to understand the debate.Just use Bayesian methods -they are much easier to understand than the debate itself!We are trying to reduce the incidence of an unpleasant disease called microsoftus.Two vaccinations, A and B, are tested on a group of volunteers.Vaccination B is a control treatment, a placebo treatment with no active ingredients.Of the 40 subjects, 30 are randomly assigned to have treatment A and the other 10 are given the control treatment B. We observe the subjects for one year after their vaccinations.Of the 30 in group A, one contracts microsoftus.Of the 10 in group B, three contract microsoftus.Is treatment A better than treatment B?The standard sampling theory approach to the question 'is A better than B?' is to construct a statistical test.The test usually compares a hypothesis such as H 1 : 'A and B have different effectivenesses' with a null hypothesis such as H 0 : 'A and B have exactly the same effectivenesses as each other'.A novice might object 'no, no, I want to compare the hypothesis "A is better than B" with the alternative "B is better than A"!' but such objections are not welcome in sampling theory.Once the two hypotheses have been defined, the first hypothesis is scarcely mentioned again -attention focuses solely on the null hypothesis.It makes me laugh to write this, but it's true!The null hypothesis is accepted or rejected purely on the basis of how unexpected the data were to H 0 , not on how much better H 1 predicted the data.One chooses a statistic which measures how much a data set deviates from the null hypothesis.In the example here, the standard statistic to use would be one called χ 2 (chi-squared).To compute χ 2 , we take the difference between each data measurement and its expected value assuming the null hypothesis to be true, and divide the square of that difference by the variance of the measurement, assuming the null hypothesis to be true.In the present problem, the four data measurements are the integers F A+ , F A− , F B+ , and F B− , that is, the number of subjects given treatment A who contracted microsoftus (F A+ ), the number of subjects given treatment A who didn't (F A− ), and so forth.The definition of χ 2 is:Actually, in my elementary statistics book (Spiegel, 1988)In this case, given the null hypothesis that treatments A and B are equally effective, and have rates f + and f − for the two outcomes, the expected counts are:(37.3)The test accepts or rejects the null hypothesis on the basis of how big χ 2 is.To make this test precise, and give it a 'significance level', we have to work out what the sampling distribution of χ 2 is, taking into account the fact thatThe sampling distribution of a statistic is the probability distribution of its value under repetitions of the experiment, assuming that the null hypothesis is true.the four data points are not independent (they satisfy the two constraints) and the fact that the parameters f ± are not known.These three constraints reduce the number of degrees of freedom in the data from four to one.[If you want to learn more about computing the 'number of degrees of freedom', read a sampling theory book; in Bayesian methods we don't need to know all that, and quantities equivalent to the number of degrees of freedom pop straight out of a Bayesian analysis when they are appropriate.]These sampling distributions are tabulated by sampling theory gnomes and come accompanied by warnings about the conditions under which they are accurate.For example, standard tabulated distributions for χ 2 are only accurate if the expected numbers F i are about 5 or more.Once the data arrive, sampling theorists estimate the unknown parameters f ± of the null hypothesis from the data: (37.4) and evaluate χ 2 .At this point, the sampling theory school divides itself into two camps.One camp uses the following protocol: first, before looking at the data, pick the significance level of the test (e.g.5%), and determine the critical value of χ 2 above which the null hypothesis will be rejected.(The significance level is the fraction of times that the statistic χ 2 would exceed the critical value, if the null hypothesis were true.)Then evaluate χ 2 , compare with the critical value, and declare the outcome of the test, and its significance level (which was fixed beforehand).The second camp looks at the data, finds χ 2 , then looks in the table of χ 2 -distributions for the significance level, p, for which the observed value of χ 2 would be the critical value.The result of the test is then reported by giving this value of p, which is the fraction of times that a result as extreme as the one observed, or more extreme, would be expected to arise if the null hypothesis were true.Let's apply these two methods.First camp: let's pick 5% as our significance level.The critical value for χ 2 with one degree of freedom is χ 2 0.05 = 3.84.The estimated values of f ± are f + = 1/10, f − = 9/10.(37.5)The expected values of the four measurements areF B− = 9 (37.9) and χ 2 (as defined in equation (37.1)) is (37.10)Since this value exceeds 3.84, we reject the null hypothesis that the two treatments are equivalent at the 0.05 significance level.However, if we use Yates's correction, we find χ 2 = 3.33, and therefore accept the null hypothesis.Camp two runs a finger across the χ 2 table found at the back of any good sampling theory book and finds χ 2.10 = 2.71.Interpolating between χ 2 .10 and χ 2.05 , camp two reports 'the p-value is p = 0.07'.Notice that this answer does not say how much more effective A is than B, it simply says that A is 'significantly' different from B. And here, 'significant' means only 'statistically significant', not practically significant.The man in the street, reading the statement that 'the treatment was significantly different from the control (p = 0.07)', might come to the conclusion that 'there is a 93% chance that the treatments differ in effectiveness'.But what 'p = 0.07' actually means is 'if you did this experiment many times, and the two treatments had equal effectiveness, then 7% of the time you would find a value of χ 2 more extreme than the one that happened here'.This has almost nothing to do with what we want to know, which is how likely it is that treatment A is better than B.OK, now let's infer what we really want to know.We scrap the hypothesis that the two treatments have exactly equal effectivenesses, since we do not believe it.There are two unknown parameters, p A+ and p B+ , which are the probabilities that people given treatments A and B, respectively, contract the disease.Given the data, we can infer these two probabilities, and we can answer questions of interest by examining the posterior distribution.The posterior distribution is(37.11)The likelihood function is We can now plot the posterior distribution.Given the assumption of a separable prior on p A+ and p B+ , the posterior distribution is also separable: shown in figure 37.2 over the region in which p A+ < p B+ , i.e., the shaded triangle in figure 37.3.The value of this integral (obtained by a straightforward numerical integration of the likelihood function (37.13) over the relevant region) is P (p A+ < p B+ | Data) = 0.990.Thus there is a 99% chance, given the data and our prior assumptions, that treatment A is superior to treatment B. In conclusion, according to our Bayesian model, the data (1 out of 30 contracted the disease after vaccination A, and 3 out of 10 contracted the disease after vaccination B) give very strong evidence -about 99 to one -that treatment A is superior to treatment B.In the Bayesian approach, it is also easy to answer other relevant questions.For example, if we want to know 'how likely is it that treatment A is ten times more effective than treatment B?', we can integrate the joint posterior probability P (p A+ , p B+ | Data) over the region in which p A+ < 10 p B+ (figure 37.4).If there were a situation in which we really did want to compare the two hypotheses H 0 : p A+ = p B+ and H 1 : p A+ = p B+ , we can of course do this directly with Bayesian methods also.As an example, consider the data set: D: One subject, given treatment A, subsequently contracted microsoftus.One subject, given treatment B, did not.Got disease 1 0 Did not 0 1Total treated 1 1 How strongly does this data set favour H 1 over H 0 ?We answer this question by computing the evidence for each hypothesis.Let's assume uniform priors over the unknown parameters of the models.The first hypothesis H 0 : p A+ = p B+ has just one unknown parameter, let's call it p.P (p | H 0 ) = 1 p ∈ (0, 1).(37.17)We'll use the uniform prior over the two parameters of model H 1 that we used before: [The sampling theory answer to this question would involve the identical significance test that was used in the preceding problem; that test would yield a 'not significant' result.I think it is greatly preferable to acknowledge what is obvious to the intuition, namely that the data D do give weak evidence in favour of H 1 .Bayesian methods quantify how weak the evidence is.]In an expensive laboratory, Dr. Bloggs tosses a coin labelled a and b twelve times and the outcome is the string aaabaaaabaab, which contains three bs and nine as.What evidence do these data give that the coin is biased in favour of a?Dr. Bloggs consults his sampling theory friend who says 'let r be the number of bs and n = 12 be the total number of tosses; I view r as the random variable and find the probability of r taking on the value r = 3 or a more extreme value, assuming the null hypothesis p a = 0.5 to be true'.He thus computes (37.27) and reports 'at the significance level of 5%, there is not significant evidence of bias in favour of a'.Or, if the friend prefers to report p-values rather than simply compare p with 5%, he would report 'the p-value is 7%, which is not conventionally viewed as significantly small'.If a two-tailed test seemed more appropriate, he might compute the two-tailed area, which is twice the above probability, and report 'the p-value is 15%, which is not significantly small'.We won't focus on the issue of the choice between the one-tailed and two-tailed tests, as we have bigger fish to catch.Dr. Bloggs pays careful attention to the calculation (37.27), and responds 'no, no, the random variable in the experiment was not r: I decided before running the experiment that I would keep tossing the coin until I saw three bs; the random variable is thus n'.Such experimental designs are not unusual.In my experiments on errorcorrecting codes I often simulate the decoding of a code until a chosen number r of block errors (bs) has occurred, since the error on the inferred value of log p b goes roughly as √ r, independent of n.Exercise 37.1. [2 ]Find the Bayesian inference about the bias p a of the coin given the data, and determine whether a Bayesian's inferences depend on what stopping rule was in force.According to sampling theory, a different calculation is required in order to assess the 'significance' of the result n = 12.The probability distribution of n given H 0 is the probability that the first n−1 tosses contain exactly r−1 bs and then the nth toss is a b.(37.28)The sampling theorist thus computes (37.29)He reports back to Dr. Bloggs, 'the p-value is 3% -there is significant evidence of bias after all!' What do you think Dr. Bloggs should do?Should he publish the result, with this marvellous p-value, in one of the journals that insists that all experimental results have their 'significance' assessed using sampling theory?Or should he boot the sampling theorist out of the door and seek a coherent method of assessing significance, one that does not depend on the stopping rule?At this point the audience divides in two.Half the audience intuitively feel that the stopping rule is irrelevant, and don't need any convincing that the answer to exercise 37.1 (p.463) is 'the inferences about p a do not depend on the stopping rule'.The other half, perhaps on account of a thorough training in sampling theory, intuitively feel that Dr. Bloggs's stopping rule, which stopped tossing the moment the third b appeared, may have biased the experiment somehow.If you are in the second group, I encourage you to reflect on the situation, and hope you'll eventually come round to the view that is consistent with the likelihood principle, which is that the stopping rule is not relevant to what we have learned about p a .As a thought experiment, consider some onlookers who (in order to save money) are spying on Dr. Bloggs's experiments: each time he tosses the coin, the spies update the values of r and n.The spies are eager to make inferences from the data as soon as each new result occurs.Should the spies' beliefs about the bias of the coin depend on Dr. Bloggs's intentions regarding the continuation of the experiment?The fact that the p-values of sampling theory do depend on the stopping rule (indeed, whole volumes of the sampling theory literature are concerned with the task of assessing 'significance' when a complicated stopping rule is required -'sequential probability ratio tests', for example) seems to me a compelling argument for having nothing to do with p-values at all.A Bayesian solution to this inference problem was given in sections 3.2 and 3.3 and exercise 3.15 (p.59).Would it help clarify this issue if I added one more scene to the story?The janitor, who's been eavesdropping on Dr. Bloggs's conversation, comes in and says 'I happened to notice that just after you stopped doing the experiments on the coin, the Officer for Whimsical Departmental Rules ordered the immediate destruction of all such coins.Your coin was therefore destroyed by the departmental safety officer.There is no way you could have continued the experiment much beyond n = 12 tosses.Seems to me, you need to recompute your p-value?'In an experiment in which data D are obtained from a system with an unknown parameter θ, a standard concept in sampling theory is the idea of a confidence interval for θ.Such an interval (θ min (D), θ max (D)) has associated with it a confidence level such as 95% which is informally interpreted as 'the probability that θ lies in the confidence interval'.Let's make precise what the confidence level really means, then give an example.A confidence interval is a function (θ min (D), θ max (D)) of the data set D. The confidence level of the confidence interval is a property that we can compute before the data arrive.We imagine generating many data sets from a particular true value of θ, and calculating the interval (θ min (D), θ max (D)), and then checking whether the true value of θ lies in that interval.If, averaging over all these imagined repetitions of the experiment, the true value of θ lies in the confidence interval a fraction f of the time, and this property holds for all true values of θ, then the confidence level of the confidence interval is f .For example, if θ is the mean of a Gaussian distribution which is known to have standard deviation 1, and D is a sample from that Gaussian, then (θ min (D), θ max (D)) = (D−2, D+2) is a 95% confidence interval for θ.Let us now look at a simple example where the meaning of the confidence level becomes clearer.Let the parameter θ be an integer, and let the data be a pair of points x 1 , x 2 , drawn independently from the following distribution:(37.30)For example, if θ were 39, then we could expect the following data sets: D = (x 1 , x 2 ) = (39, 39) with probability 1/ 4;(x 1 , x 2 ) = (39, 40) with probability 1/ 4; (x 1 , x 2 ) = (40, 39) with probability 1/ 4; (x 1 , x 2 ) = (40, 40) with probability 1/ 4.(37.31)We now consider the following confidence interval:For example, if (x 1 , x 2 ) = (40, 39), then the confidence interval for θ would beLet's think about this confidence interval.What is its confidence level?By considering the four possibilities shown in (37.31), we can see that there is a 75% chance that the confidence interval will contain the true value.The confidence interval therefore has a confidence level of 75%, by definition.Now, what if the data we acquire are (x 1 , x 2 ) = (29, 29)?Well, we can compute the confidence interval, and it is [29,29].So shall we report this interval, and its associated confidence level, 75%?This would be correct by the rules of sampling theory.But does this make sense?What do we actually know in this case?Intuitively, or by Bayes' theorem, it is clear that θ could either be 29 or 28, and both possibilities are equally likely (if the prior probabilities of 28 and 29 were equal).The posterior probability of θ is 50% on 29 and 50% on 28.What if the data are (x 1 , x 2 ) = (29, 30)?In this case, the confidence interval is still [29,29], and its associated confidence level is 75%.But in this case, by Bayes' theorem, or common sense, we are 100% sure that θ is 29.In neither case is the probability that θ lies in the '75% confidence interval' equal to 75%! Thus 1. the way in which many people interpret the confidence levels of sampling theory is incorrect;2. given some data, what people usually want to know (whether they know it or not) is a Bayesian posterior probability distribution.Are all these examples contrived?Am I making a fuss about nothing?If you are sceptical about the dogmatic views I have expressed, I encourage you to look at a case study: look in depth at exercise 35.4 (p.446) and the reference (Kepler and Oprea, 2001), in which sampling theory estimates and confidence intervals for a mutation rate are constructed.Try both methods on simulated data -the Bayesian approach based on simply computing the likelihood function, and the confidence interval from sampling theory; and let me know if you don't find that the Bayesian answer is always better than the sampling theory answer; and often much, much better.This suboptimality of sampling theory, achieved with great effort, is why I am passionate about Bayesian methods.Bayesian methods are straightforward, and they optimally use all the information in the data.Let's end on a conciliatory note.Many sampling theorists are pragmaticthey are happy to choose from a selection of statistical methods, choosing whichever has the 'best' long-run properties.In contrast, I have no problem with the idea that there is only one answer to a well-posed problem; but it's not essential to convert sampling theorists to this viewpoint: instead, we can offer them Bayesian estimators and Bayesian confidence intervals, and request that the sampling theoretical properties of these methods be evaluated.We don't need to mention that the methods are derived from a Bayesian perspective.If the sampling properties are good then the pragmatic sampling theorist will choose to use the Bayesian methods.It is indeed the case that many Bayesian methods have good sampling-theoretical properties.Perhaps it's not surprising that a method that gives the optimal answer for each individual case should also be good in the long run!Another piece of common ground can be conceded: while I believe that most well-posed inference problems have a unique correct answer, which can be found by Bayesian methods, not all problems are well-posed.A common question arising in data modelling is 'am I using an appropriate model?' Model criticism, that is, hunting for defects in a current model, is a task that may be aided by sampling theory tests, in which the null hypothesis ('the current model is correct') is well defined, but the alternative model is not specified.One could use sampling theory measures such as p-values to guide one's search for the aspects of the model most in need of scrutiny.My favourite reading on this topic includes (Jaynes, 1983;Loredo, 1990;Berger, 1985;Jaynes, 2003).Treatises on Bayesian statistics from the statistics community include (Box and Tiao, 1973;O'Hagan, 1994).3C ] A traffic survey records traffic on two successive days.OnIn the field of neural networks, we study the properties of networks of idealized 'neurons'.Three motivations underlie work in this broad and interdisciplinary field.Biology.The task of understanding how the brain works is one of the outstanding unsolved problems in science.Some neural network models are intended to shed light on the way in which computation and memory are performed by brains.Engineering.Many researchers would like to create machines that can 'learn', perform 'pattern recognition' or 'discover patterns in data'.Complex systems.A third motivation for being interested in neural networks is that they are complex adaptive systems whose properties are interesting in their own right.I should emphasize several points at the outset.• This book gives only a taste of this field.There are many interesting neural network models which we will not have time to touch on.• The models that we discuss are not intended to be faithful models of biological systems.If they are at all relevant to biology, their relevance is on an abstract level.• I will describe some neural network methods that are widely used in nonlinear data modelling, but I will not be able to give a full description of the state of the art.If you wish to solve real problems with neural networks, please read the relevant papers.In the next few chapters we will meet several neural network models which come with simple learning algorithms which make them function as memories.Perhaps we should dwell for a moment on the conventional idea of memory in digital computation.A memory (a string of 5000 bits describing the name of a person and an image of their face, say) is stored in a digital computer at an address.To retrieve the memory you need to know the address.The address has nothing to do with the memory itself.Notice the properties that this scheme does not have: 2. Address-based memory is not robust or fault-tolerant.If a one-bit mistake is made in specifying the address then a completely different memory will be retrieved.If one bit of a memory is flipped then whenever that memory is retrieved the error will be present.Of course, in all modern computers, error-correcting codes are used in the memory, so that small numbers of errors can be detected and corrected.But this errortolerance is not an intrinsic property of the memory system.If minor damage occurs to certain hardware that implements memory retrieval, it is likely that all functionality will be catastrophically lost.3. Address-based memory is not distributed.In a serial computer that is accessing a particular memory, only a tiny fraction of the devices participate in the memory recall: the CPU and the circuits that are storing the required byte.All the other millions of devices in the machine are sitting idle.Are there models of truly parallel computation, in which multiple devices participate in all computations?[Present-day parallel computers scarcely differ from serial computers from this point of view.Memory retrieval works in just the same way, and control of the computation process resides in CPUs.There are simply a few more CPUs.Most of the devices sit idle most of the time.]Biological memory systems are completely different.1. Biological memory is associative.Memory recall is content-addressable.Given a person's name, we can often recall their face; and vice versa.Memories are apparently recalled spontaneously, not just at the request of some CPU.2. Biological memory recall is error-tolerant and robust.• Errors in the cues for memory recall can be corrected.An example asks you to recall 'An American politician who was very intelligent and whose politician father did not like broccoli'.Many people think of president Bush -even though one of the cues contains an error.• Hardware faults can also be tolerated.Our brains are noisy lumps of meat that are in a continual state of change, with cells being damaged by natural processes, alcohol, and boxing.While the cells in our brains and the proteins in our cells are continually changing, many of our memories persist unaffected.3. Biological memory is parallel and distributed -not completely distributed throughout the whole brain: there does appear to be some functional specialization -but in the parts of the brain where memories are stored, it seems that many neurons participate in the storage of multiple memories.These properties of biological memory systems motivate the study of 'artificial neural networks' -parallel distributed computational systems consisting of many interacting simple elements.The hope is that these model systems might give some hints as to how neural computation is achieved in real biological neural networks.Each time we describe a neural network algorithm we will typically specify three things.[If any of this terminology is hard to understand, it's probably best to dive straight into the next chapter.]Architecture.The architecture specifies what variables are involved in the network and their topological relationships -for example, the variables involved in a neural net might be the weights of the connections between the neurons, along with the activities of the neurons.Activity rule.Most neural network models have short time-scale dynamics: local rules define how the activities of the neurons change in response to each other.Typically the activity rule depends on the weights (the parameters) in the network.Learning rule.The learning rule specifies the way in which the neural network's weights change with time.This learning is usually viewed as taking place on a longer time scale than the time scale of the dynamics under the activity rule.Usually the learning rule will depend on the activities of the neurons.It may also depend on the values of target values supplied by a teacher and on the current value of the weights.Where do these rules come from?Often, activity rules and learning rules are invented by imaginative researchers.Alternatively, activity rules and learning rules may be derived from carefully chosen objective functions.Neural network algorithms can be roughly divided into two classes.Supervised neural networks are given data in the form of inputs and targets, the targets being a teacher's specification of what the neural network's response to the input should be.Unsupervised neural networks are given data in an undivided form -simply a set of examples {x}.Some learning algorithms are intended simply to memorize these data in such a way that the examples can be recalled in the future.Other algorithms are intended to 'generalize', to discover 'patterns' in the data, or extract the underlying 'features' from them.Some unsupervised algorithms are able to make predictions -for example, some algorithms can 'fill in' missing variables in an example x -and so can also be viewed as supervised networks.The Single Neuron as a ClassifierWe will study a single neuron for two reasons.First, many neural network models are built out of single neurons, so it is good to understand them in detail.And second, a single neuron is itself capable of 'learning' -indeed, various standard statistical methods can be viewed in terms of single neurons -so this model will serve as a first example of a supervised neural network.We will start by defining the architecture and the activity rule of a single neuron, and we will then derive a learning rule.There may be an additional parameter w 0 of the neuron called a bias which we may view as being the weight associated with an input x 0 that is permanently set to 1.The single neuron is a feedforward device -the connections are directed from the inputs to the output of the neuron.Activity rule.The activity rule has two steps.1. First, in response to the imposed inputs x, we compute the activation of the neuron,where the sum is over i = 0, . . ., I if there is a bias and i = 1, . . ., I otherwise.2. Second, the output y is set as a function f (a) of the activation.The output is also called the activity of the neuron, not to be confused with the activation a.There are several possible activation ii.Sigmoid (logistic function).A neural network implements a function y(x; w); the 'output' of the network, y, is a nonlinear function of the 'inputs' x; this function is parameterized by 'weights' w.We will study a single neuron which produces an output between 0 and 1 as the following function of x:(39.7)Exercise 39.1. [1 ]In what contexts have we encountered the function y(x; w) = 1/(1 + e −w•x ) already?In section 11.2 we studied 'the best detection of pulses', assuming that one of two signals x 0 and x 1 had been transmitted over a Gaussian channel with variance-covariance matrix A −1 .We found that the probability that the source signal was s = 1 rather than s = 0, given the received signal y, waswhere a(y) was a linear function of the received vector, a(y) = w T y + θ, (39.9) with w ≡ A(x 1 − x 0 ).The linear logistic function can be motivated in several other ways -see the exercises.For convenience let us study the case where the input vector x and the parameter vector w are both two-dimensional: x = (x 1 , x 2 ), w = (w 1 , w 2 ).Then we can spell out the function performed by the neuron thus: w1x1+w2x2) .(39.10) Figure 39.2 shows the output of the neuron as a function of the input vector, for w = (0, 2).The two horizontal axes of this figure are the inputs x 1 and x 2 , with the output y on the vertical axis.Notice that on any line perpendicular to w, the output is constant; and along a line in the direction of w, the output is a sigmoid function.We now introduce the idea of weight space, that is, the parameter space of the network.In this case, there are two parameters w 1 and w 2 , so the weight space is two dimensional.This weight space is shown in figure 39.3.For a selection of values of the parameter vector w, smaller inset figures show the function of x performed by the network when w is set to those values.Each of these smaller figures is equivalent to figure 39.2.Thus each point in w space corresponds to a function of x.Notice that the gain of the sigmoid function (the gradient of the ramp) increases as the magnitude of w increases.Now, the central idea of supervised neural networks is this.Given examples of a relationship between an input vector x, and a target t, we hope to make the neural network 'learn' a model of the relationship between x and t.A successfully trained network will, for any given x, give an output y that is close (in some sense) to the target value t.Training the network involves searching in the weight space of the network for a value of w that produces a function that fits the provided training data well.Typically an objective function or error function is defined, as a function of w, to measure how well the network with weights set to w solves the task.The objective function is a sum of terms, one for each input/target pair {x, t}, measuring how close the output y(x; w) is to the target t.The training process is an exercise in function minimization -i.e., adjusting w in such a way as to find a w that minimizes the objective function.Many function-minimization algorithms make use not only of the objective function, but also its gradient with respect to the parameters w.For general feedforward neural networks the backpropagation algorithm efficiently evaluates the gradient of the output y with respect to the parameters w, and thence the gradient of the objective function with respect to w. 39.3: Training the single neuron as a binary classifier 475We assume we have a data set of inputs {x (n) } N n=1 with binary labels {t (n) } N n=1 , and a neuron whose output y(x; w) is bounded between 0 and 1.We can then write down the following error function:(39.11)Each term in this objective function may be recognized as the information content of one outcome.It may also be described as the relative entropy between the empirical probability distribution (t (n) , 1 − t (n) ) and the probability distribution implied by the output of the neuron (y, 1−y).The objective function is bounded below by zero and only attains this value if y(x (n) ; w) = t (n) for all n.We now differentiate this objective function with respect to w.Exercise 39.2. [2 ]The backpropagation algorithm.Show that the derivative g = ∂G/∂w is given by:(39.12)Notice that the quantity e (n) ≡ t (n) − y (n) is the error on example n -the difference between the target and the output.The simplest thing to do with a gradient of an error function is to descend it (even though this is often dimensionally incorrect, since a gradient has dimensions [1/parameter], whereas a change in a parameter has dimensions [parameter]).Since the derivative ∂G/∂w is a sum of terms g (n) defined by(39.13)for n = 1, . . ., N , we can obtain a simple on-line algorithm by putting each input through the network one at a time, and adjusting w a little in a direction opposite to g (n) .We summarize the whole learning algorithm.Architecture.A single neuron has a number I of inputs x i and one output y.Associated with each input is a weight w i (i = 1, . . ., I).Activity rule.1.First, in response to the received inputs x (which may be arbitrary real numbers), we compute the activation of the neuron, (39.14)where the sum is over i = 0, . . ., I if there is a bias and i = 1, . . ., I otherwise.2. Second, the output y is set as a sigmoid function of the activation.(39.15)This output might be viewed as stating the probability, according to the neuron, that the given input is in class 1 rather than class 0.Learning rule.The teacher supplies a target value t ∈ {0, 1} which says what the correct answer is for the given input.We compute the error signal e = t − y (39.16) then adjust the weights w in a direction that would reduce the magnitude of this error: (39.17)where η is the 'learning rate'.Commonly η is set by trial and error to a constant value or to a decreasing function of simulation time τ such as η 0 /τ .The activity rule and learning rule are repeated for each input/target pair (x, t) that is presented.If there is a fixed data set of size N , we can cycle through the data multiple times.Here we have described the on-line learning algorithm, in which a change in the weights is made after every example is presented.An alternative paradigm is to go through a batch of examples, computing the outputs and errors and accumulating the changes specified in equation (39.17) which are then made at the end of the batch.For each input/target pair (x (n) , t (n) ) (n = 1, . . ., N ), compute y (n) = y(x (n) ; w), where n) , and compute for each weight (39.20)This batch learning algorithm is a gradient descent algorithm, whereas the on-line algorithm is a stochastic gradient descent algorithm.Source code implementing batch learning is given in algorithm 39.5.This algorithm is demonstrated in figure 39.4 for a neuron with two inputs with weights w 1 and w 2 and a bias w 0 , performing the function w0+w1x1+w2x2) .(39.21)The bias w 0 is included, in contrast to figure 39.3, where it was omitted.The neuron is trained on a data set of ten labelled examples.The function performed by the neuron (shown by three of its contours) after 30, 80, 500, 3000, 10 000 and 40 000 iterations.The contours shown are those corresponding to a = 0, ±1, namely y = 0.5, 0.27 and 0.73.Also shown is a vector proportional to (w 1 , w 2 ).The larger the weights are, the bigger this vector becomes, and the closer together are the contours.(a)x 1x 2 39.4: Beyond descent on the error function: regularization 479If the parameter η is set to an appropriate value, this algorithm works: the algorithm finds a setting of w that correctly classifies as many of the examples as possible.If the examples are in fact linearly separable then the neuron finds this linear separation and its weights diverge to ever-larger values as the simulation continues.This can be seen happening in figure 39.4(f-k).This is an example of overfitting, where a model fits the data so well that its generalization performance is likely to be adversely affected.This behaviour may be viewed as undesirable.How can it be rectified?An ad hoc solution to overfitting is to use early stopping, that is, use an algorithm originally intended to minimize the error function G(w), then prevent it from doing so by halting the algorithm at some point.A more principled solution to overfitting makes use of regularization.Regularization involves modifying the objective function in such a way as to incorporate a bias against the sorts of solution w which we dislike.In the above example, what we dislike is the development of a very sharp decision boundary in figure 39.4k; this sharp boundary is associated with large weight values, so we use a regularizer that penalizes large weight values.We modify the objective function to:where the simplest choice of regularizer is the weight decay regularizerThe regularization constant α is called the weight decay rate.This additional term favours small values of w and decreases the tendency of a model to overfit fine details of the training data.The quantity α is known as a hyperparameter.Hyperparameters play a role in the learning algorithm but play no role in the activity rule of the network.Exercise 39.3. [1 ]Compute the derivative of M (w) with respect to w i .Why is the above regularizer known as the 'weight decay' regularizer?The gradient descent source code of algorithm 39.5 implements weight decay.This gradient descent algorithm is demonstrated in figure 39.6 using weight decay rates α = 0.01, 0.1, and 1.As the weight decay rate is increased the solution becomes biased towards broader sigmoid functions with decision boundaries that are closer to the origin.Gradient descent with a step size η is in general not the most efficient way to minimize a function.A modification of gradient descent known as momentum, while improving convergence, is also not recommended.Most neural network experts use more advanced optimizers such as conjugate gradient algorithms.[Please do not confuse momentum, which is sometimes given the symbol α, with weight decay.]39 -The Single Neuron as a ClassifierExercise 39.4. [2 ]Consider the task of recognizing which of two Gaussian distributions a vector z comes from.Unlike the case studied in section 11.2, where the distributions had different means but a common variancecovariance matrix, we will assume that the two distributions have exactly the same mean but different variances.Let the probability of z given s (s ∈ {0, 1}) bewhere σ 2 si is the variance of z i when the source symbol is s.Show that P (s = 1 | z) can be written in the formwhere x i is an appropriate function of z i , x i = g(z i ).Exercise 39.5. [2 ]The noisy LED.Consider an LED display with 7 elements numbered as shown above.The state of the display is a vector x.When the controller wants the display to show character number s, e.g.s = 2, each element x j (j = 1, . . ., 7) either adopts its intended state c j (s), with probability 1−f , or is flipped, with probability f .Let's call the two states of x '+1' and '−1'.(a) Assuming that the intended character s is actually a 2 or a 3, what is the probability of s, given the state x? Show that P (s = 2 | x) can be written in the formand compute the values of the weights w in the case Exercise 39.6. [2 ]A (3, 1) error-correcting code consists of the two codewords x (1) = (1, 0, 0) and x (2) = (0, 0, 1).A source bit s ∈ {1, 2} having probability distribution {p 1 , p 2 } is used to select one of the two codewords for transmission over a binary symmetric channel with noise level f .The received vector is r.Show that the posterior probability of s given r can be written in the formand give expressions for the coefficients {w n } 3 n=1 and the bias, w 0 .Describe, with a diagram, how this optimal decoder can be expressed in terms of a 'neuron'.Problems to look at before Chapter 40Exercise 40.2. [2 ]If the top row of Pascal's triangle (which contains the single number '1') is denoted row zero, what is the sum of all the numbers in the triangle above row N ?Exercise 40.3. [2 ]3 points are selected at random on the surface of a sphere.What is the probability that all of them lie on a single hemisphere?This chapter's material is originally due to Polya (1954) and Cover (1965) and the exposition that follows is Yaser Abu-Mostafa's.Capacity of a Single NeuronMany neural network models involve the adaptation of a set of weights w in response to a set of data points, for example a set of N target valuesThe adapted weights are then used to process subsequent input data.This process can be viewed as a communication process, in which the sender examines the data D N and creates a message w that depends on those data.The receiver then uses w; for example, the receiver might use the weights to try to reconstruct what the data D N was.[In neural network parlance, this is using the neuron for 'memory' rather than for 'generalization'; 'generalizing' means extrapolating from the observed data to the value of t N +1 at some new location x N +1 .]Just as a disk drive is a communication channel, the adapted network weights w therefore play the role of a communication channel, conveying information about the training data to a future user of that neural net.The question we now address is, 'what is the capacity of this channel?' -that is, 'how much information can be stored by training a neural network?'If we had a learning algorithm that either produces a network whose response to all inputs is +1 or a network whose response to all inputs is 0, depending on the training data, then the weights allow us to distinguish between just two sorts of data set.The maximum information such a learning algorithm could convey about the data is therefore 1 bit, this information content being achieved if the two sorts of data set are equiprobable.How much more information can be conveyed if we make full use of a neural network's ability to represent other functions?We will look at the simplest case, that of a single binary threshold neuron.We will find that the capacity of such a neuron is two bits per weight.A neuron with K inputs can store 2K bits of information.To obtain this interesting result we lay down some rules to exclude less interesting answers, such as: 'the capacity of a neuron is infinite, because each of its weights is a real number and so can convey an infinite number of bits'.We exclude this answer by saying that the receiver is not able to examine the weights directly, nor is the receiver allowed to probe the weights by observing the output of the neuron for arbitrarily chosen inputs.We constrain the receiver to observe the output of the neuron at the same fixed set of N points {x n } that were in the training set.What matters now is how many different distinguishable functions our neuron can produce, given that we can observe the function only at these N points.How many different binary labellings of N points can a linear threshold function produce?And how does this number compare with the maximum possible number of binary labellings, 2 N ?If nearly all of the 2 N labellings can be realized by our neuron, then it is a communication channel that can convey all N bits (the target values {t n }) with small probability of error.We will identify the capacity of the neuron as the maximum value that N can have such that the probability of error is very small.[We are departing a little from the definition of capacity in Chapter 9.]We thus examine the following scenario.The sender is given a neuron with K inputs and a data set D N which is a labelling of N points.The sender uses an adaptive algorithm to try to find a w that can reproduce this labelling exactly.We will assume the algorithm finds such a w if it exists.The receiver then evaluates the threshold function on the N input values.What is the probability that all N bits are correctly reproduced?How large can N become, for a given K, without this probability becoming substantially less than one?One technical detail needs to be pinned down: what set of inputs {x n } are we considering?Our answer might depend on this choice.We will assume that the points are in general position.Definition 40.1 A set of points {x n } in K-dimensional space are in general position if any subset of size ≤ K is linearly independent, and no K + 1 of them lie in a (K − 1)-dimensional plane.In K = 3 dimensions, for example, a set of points are in general position if no three points are colinear and no four points are coplanar.The intuitive idea is that points in general position are like random points in the space, in terms of the linear dependences between points.You don't expect three random points in three dimensions to lie on a straight line.The neuron we will consider performs the functionwhereWe will not have a bias w 0 ; the capacity for a neuron with a bias can be obtained by replacing K by K + 1 in the final result below, i.e., considering one of the inputs to be fixed to 1. (These input points would not then be in general position; the derivation still works.)(a)x 1x 2x (1)Figure 40.2.One data point in a two-dimensional input space, and the two regions of weight space that give the two alternative labellings of that point.Let us denote by T (N, K) the number of distinct threshold functions on N points in general position in K dimensions.We will derive a formula for T (N, K).To start with, let us work out a few cases by hand.In K = 1 dimension, for any NThe N points lie on a line.By changing the sign of the one weight w 1 we can label all points on the right side of the origin 1 and the others 0, or vice versa.Thus there are two distinct threshold functions.T (N, 1) = 2.With N = 1 point, for any KIf there is just one point x (1) then we can realize both possible labellings by setting w = ±x (1) .Thus T (1, K) = 2.In K = 2 dimensionsIn two dimensions with N points, we are free to spin the separating line around the origin.Each time the line passes over a point we obtain a new function.Once we have spun the line through 360 degrees we reproduce the function we started from.Because the points are in general position, the separating plane (line) crosses only one point at a time.In one revolution, every point is passed over twice.There are therefore 2N distinct threshold functions.Comparing with the total number of binary functions, 2 N , we may note that for N ≥ 3, not all binary functions can be realized by a linear threshold function.One famous example of an unrealizable function with N = 4 and K = 2 is the exclusive-or function on the points x = (±1, ±1).[These points are not in general position, but you may confirm that the function remains unrealizable even if the points are perturbed into general position.]There is another way of visualizing this problem.Instead of visualizing a plane separating points in the two-dimensional input space, we can consider the two-dimensional weight space, colouring regions in weight space different colours if they label the given datapoints differently.We can then count the number of threshold functions by counting how many distinguishable regions there are in weight space.Consider first the set of weight vectors in weight (a) (1,1,0)(1,0,0) (0,1,1) (0,0,1)(1,0,1) (0,1,0)Figure 40.4.Three data points in a two-dimensional input space, and the six regions of weight space that give alternative labellings of those points.In this case, the labellings (0, 0, 0) and (1, 1, 1) cannot be realized.For any three points in general position there are always two labellings that cannot be realized.space that classify a particular example x (n) as a 1.For example, figure 40.2a shows a single point in our two-dimensional x-space, and figure 40.2b shows the two corresponding sets of points in w-space.One set of weight vectors occupy the half space x (n) •w > 0, (40.3) and the others occupy x (n) •w < 0. In figure 40.3a we have added a second point in the input space.There are now 4 possible labellings: (1, 1), (1, 0), (0, 1), and (0, 0). Figure 40.3bshows the two hyperplanes x (1) •w = 0 and x (2) •w = 0 which separate the sets of weight vectors that produce each of these labellings.When N = 3 (figure 40.4), weight space is divided by three hyperplanes into six regions.Not all of the eight conceivable labellings can be realized.Thus T (3, 2) = 6.We now use this weight space visualization to study the three dimensional case.Let us imagine adding one point at a time and count the number of threshold functions as we do so.When N = 2, weight space is divided by two hyperplanes x (1) •w = 0 and x (2) •w = 0 into four regions; in any one region all vectors w produce the same function on the 2 input vectors.Thus T (2, 3) = 4.Adding a third point in general position produces a third plane in w space, so that there are 8 distinguishable regions.T (3, 3) = 8.The three bisecting planes are shown in figure 40.5a.At this point matters become slightly more tricky.As figure 40.5b illustrates, the fourth plane in the three-dimensional w space cannot transect all eight of the sets created by the first three planes.Six of the existing regions are cut in two and the remaining two are unaffected.So T (4, 3) = 14.Two 40.3:Counting threshold functions 487  (a)  of the binary functions on 4 points in 3 dimensions cannot be realized by a linear threshold function.We have now filled in the values of T (N, K) shown in table 40.6.Can we obtain any insights into our derivation of T (4, 3) in order to fill in the rest of the table for T (N, K)?Why was T (4, 3) greater than T (3, 3) by six?Six is the number of regions that the new hyperplane bisected in w-space (figure 40.7a b).Equivalently, if we look in the K − 1 dimensional subspace that is the N th hyperplane, that subspace is divided into six regions by the N −1 previous hyperplanes (figure 40 Recurrence relation for any N, KGeneralizing this picture, we see that when we add an N th hyperplane in K dimensions, it will bisect T (N −1, K −1) of the T (N −1, K) regions that were created by the previous N − 1 hyperplanes.Therefore, the total number of regions obtained after adding theregions not split by the N th hyperplane, which gives the following equation for T (N, K):1 4 6 4 1 5 1 5 10 10 5 1 Combinations N K satisfy the equation[Here we are adopting the convention that N K ≡ 0 if K > N or K < 0.] So N K satisfies the required recurrence relation (40.5).This doesn't mean T (N, K) = N K , since many functions can satisfy one recurrence relation.But perhaps we can express T (N, K) as a linear superposition of combination functions of the form C α,β (N, K) ≡ N +α K+β .By comparing tables 40.8 and 40.6 we can see how to satisfy the boundary conditions: we simply need to translate Pascal's triangle to the right by 1, 2, 3, . ..; superpose; add; multiply by two, and drop the whole table by one line.Thus:Using the fact that the N th row of Pascal's triangle sums to 2 N , that is,It is natural to compare T (N, K) with the total number of binary functions on N points, 2 N .The ratio T (N, K)/2 N tells us the probability that an arbitrary labelling {t n } N n=1 can be memorized by our neuron.The two functions are equal for all N ≤ K.The line N = K is thus a special line, defining the maximum number of points on which any arbitrary labelling can be realized.This number of points is referred to as the Vapnik-Chervonenkis dimension (VC dimension) of the class of functions.The VC dimension of a binary threshold function on K dimensions is thus K.What is interesting is (for large K) the number of points N such that almost any labelling can be realized.The ratio T (N, K)/2 N is, for N < 2K, still greater than 1/2, and for large K the ratio is very close to 1.For our purposes the sum in equation (40.9) is well approximated by the error function, 40.9 shows the realizable fraction T (N, K)/2 N as a function of N and K.The take-home message is shown in figure 40.9c: although the fraction T (N, K)/2 N is less than 1 for N > K, it is only negligibly less than 1 up to N = 2K; there, there is a catastrophic drop to zero, so that for N > 2K, only a tiny fraction of the binary labellings can be realized by the threshold function.The capacity of a linear threshold neuron, for large K, is 2 bits per weight.A single neuron can almost certainly memorize up to N = 2K random binary labels perfectly, but will almost certainly fail to memorize more.Exercise 40.4. [2 ]Can a finite set of 2N distinct points in a two-dimensional space be split in half by a straight line• if the points are in general position?• if the points are not in general position?Can 2N points in a K dimensional space be split in half by a K − 1 dimensional hyperplane?p.491] Four points are selected at random on the surface of a sphere.What is the probability that all of them lie on a single hemisphere?How does this question relate to T (N, K)?Exercise 40.6. [2 ]Consider the binary threshold neuron in K = 3 dimensions, and the set of points {x} = {(1, 0, 0), (0, 1, 0), (0, 0, 1), (1, 1, 1)}.Find a parameter vector w such that the neuron memorizes the labels: (a)Find an unrealizable labelling {t}.Exercise 40.7. [3 ]In this chapter we constrained all our hyperplanes to go through the origin.In this exercise, we remove this constraint.How many regions in a plane are created by N lines in general position?Exercise 40.8. [2 ]Estimate in bits the total sensory experience that you have had in your life -visual information, auditory information, etc. Estimate how much information you have memorized.Estimate the information content of the works of Shakespeare.Compare these with the capacity of your brain assuming you have 10 11 neurons each making 1000 synaptic connections, and that the capacity result for one neuron (two bits per connection) applies.Is your brain full yet?Exercise 40.9. [3 ]What is the capacity of the axon of a spiking neuron, viewed as a communication channel, in bits per second?[See MacKay and McCulloch (1952) for an early publication on this topic.]Multiply by the number of axons in the optic nerve (about 10 6 ) or cochlear nerve (about 50 000 per ear) to estimate again the rate of acquisition sensory experience.Solution to exercise 40.5 (p.490).The probability that all four points lie on a single hemisphere is T (4, 3)/2 4 = 14/16 = 7/8.(40.11)Learning as InferenceIn Chapter 39 we trained a simple neural network as a classifier by minimizing an objective functionmade up of an error functionand a regularizerThis neural network learning process can be given the following probabilistic interpretation.We interpret the output y(x; w) of the neuron literally as defining (when its parameters w are specified) the probability that an input x belongs to class t = 1, rather than the alternative t = 0. Thus y(x; w) ≡ P (t = 1 | x, w).Then each value of w defines a different hypothesis about the probability of class 1 relative to class 0 as a function of x.We define the observed data D to be the targets {t} -the inputs {x} are assumed to be given, and not to be modelled.To infer w given the data, we require a likelihood function and a prior probability over w.The likelihood function measures how well the parameters w predict the observed data; it is the probability assigned to the observed t values by the model with parameters set to w.Now the two equationscan be rewritten as the single equationSo the error function G can be interpreted as minus the log likelihood:Similarly the regularizer can be interpreted in terms of a log prior probability distribution over the parameters:If E W is quadratic as defined above, then the corresponding prior distribution is a Gaussian with variance σ 2 W = 1/α, and 1/Z W (α) is equal to (α/2π) K/2 , where K is the number of parameters in the vector w.The objective function M (w) then corresponds to the inference of the parameters w, given the data:(41.10)So the w found by (locally) minimizing M (w) can be interpreted as the (locally) most probable parameter vector, w * .From now on we will refer to w * as w MP .Why is it natural to interpret the error functions as log probabilities?Error functions are usually additive.For example, G is a sum of information contents, and E W is a sum of squared weights.Probabilities, on the other hand, are multiplicative: for independent events X and Y , the joint probability is P (x, y) = P (x)P (y).The logarithmic mapping maintains this correspondence.The interpretation of M (w) as a log probability has numerous benefits, some of which we will discuss in a moment.In the case of a neuron with just two inputs and no bias, The product of traditional learning is a point in w-space, the estimator w * , which maximizes the posterior probability density.In contrast, in the Bayesian view, the product of learning is an ensemble of plausible parameter values (bottom right of figure 41.1).We do not choose one particular hypothesis w; rather we evaluate their posterior probabilities.The posterior distribution is obtained by multiplying the likelihood by a prior distribution over w space (shown as a broad Gaussian at the upper right of figure 41.1).The posterior ensemble (within a multiplicative constant) is shown in the third column of figure 41.1, and as a contour plot in the fourth column.As the amount of data increases (from top to bottom), the posterior ensemble becomes increasingly concentrated around the most probable value w * .Let us consider the task of making predictions with the neuron which we trained as a classifier in section 39.3.This was a neuron with two inputs and a bias.y(x; w) = 1 1 + e −(w0+w1x1+w2x2) .(41.12)We now consider the task of predicting the class t (N+1) corresponding to a new input x (N+1) .It is common practice, when making predictions, simply to use a neural network with its weights fixed to their optimized value w MP , but this is not optimal, as can be seen intuitively by considering the predictions shown in figure 41.2a.Are these reasonable predictions?Consider new data arriving at points A and B. The best-fit model assigns both of these examples probability 0.2 of being in class 1, because they have the same value of w MP •x.If we really knew that w was equal to w MP , then these predictions would be correct.But we do not know w.The parameters are uncertain.Intuitively we might be inclined to assign a less confident probability (closer to 0.5) at B than at A, as shown in figure 41.2b, since point B is far from the training data.The best-fit parameters w MP often give over-confident predictions.A non-Bayesian approach to this problem is to downweight all predictions uniformly, by an empirically determined factor (Copas, 1983).This is not ideal, since intuition suggests the strength of the predictions at B should be downweighted more than those at A. A Bayesian viewpoint helps us to understand the cause of the problem, and provides a straightforward solution.In a nutshell, we obtain Bayesian predictions by taking into account the whole posterior ensemble, shown schematically in figure 41.2c.The Bayesian prediction of a new datum t (N+1) involves marginalizing over the parameters (and over anything else about which we are uncertain).For simplicity, let us assume that the weights w are the only uncertain quantities -the weight decay rate α and the model H itself are assumed to be fixed.Then by the sum rule, the predictive probability of a new target t (N+1) at a location x (N+1) is:where K is the dimensionality of w, three in the toy problem.Thus the predictions are obtained by weighting the prediction for each possible w,with a weight given by the posterior probability of w, P (w | D, α), which we most recently wrote down in equation (41.10).This posterior probability is (41.16)where(41.17)In summary, we can get the Bayesian predictions if we can find a way of computing the integralwhich is the average of the output of the neuron at x (N+1) under the posterior distribution of w.(a) Dumb MetropolisHow shall we compute the integral (41.18)?For our toy problem, the weight space is three dimensional; for a realistic neural network the dimensionality K might be in the thousands.Bayesian inference for general data modelling problems may be implemented by exact methods (Chapter 25), by Monte Carlo sampling (Chapter 29), or by deterministic approximate methods, for example, methods that make Gaussian approximations to P (w | D, α) using Laplace's method (Chapter 27) or variational methods (Chapter 33).For neural networks there are few exact methods.The two main approaches to implementing Bayesian inference for neural networks are the Monte Carlo methods developed by Neal (1996) and the Gaussian approximation methods developed by MacKay (1991).First we will use a Monte Carlo approach in which the task of evaluating the integral (41.18) is solved by treating y(x (N+1) ; w) as a function f of w whose mean we compute usingwhere {w (r) } are samples from the posterior distribution 1 Z M exp(−M (w)) (cf.equation (29.6)).We obtain the samples using a Metropolis method (section 29.4).As an aside, a possible disadvantage of this Monte Carlo approach is that it is a poor way of estimating the probability of an improbable event, i.e., a P (t | D, H) that is very close to zero, if the improbable event is most likely to occur in conjunction with improbable parameter values.How to generate the samples {w (r) }? Radford Neal introduced the Hamiltonian Monte Carlo method to neural networks.We met this sophisticated Metropolis method, which makes use of gradient information, in Chapter 30.The method we now demonstrate is a simple version of Hamiltonian Monte Carlo called the Langevin Monte Carlo method.The Langevin method (algorithm 41.4) may be summarized as 'gradient descent with added noise', as shown pictorially in figure 41.3.A noise vector p is generated from a Gaussian with unit variance.The gradient g is computed,  (41.20)Notice that if the p term were omitted this would simply be gradient descent with learning rate η = 1 2 2 .This step in w is accepted or rejected depending on the change in the value of the objective function M (w) and on the change in gradient, with a probability of acceptance such that detailed balance holds.The Langevin method has one free parameter, , which controls the typical step size.If is set to too large a value, moves may be rejected.If it is set to a very small value, progress around the state space will be slow.The Langevin method is demonstrated in figures 41.5, 41.6 and 41.7.Here, the objective function is M (w) = G(w) + αE W (w), with α = 0.01.These figures include, for comparison, the results of the previous optimization method using gradient descent on the same objective function (figure 39.6).It can be seen that the mean evolution of w is similar to the evolution of the parameters under gradient descent.The Monte Carlo method appears to have converged to the posterior distribution after about 10 000 iterations.The average acceptance rate during this simulation was 93%; only 7% of the proposed moves were rejected.Probably, faster progress around the state space would have been made if a larger step size had been used, but the value was chosen so that the 'descent rate' η = 1 2 2 matched the step size of the earlier simulations.From iteration 10,000 to 40,000, the weights were sampled every 1000 iterations and the corresponding functions of x are plotted in figure 41.6.There is a considerable variety of plausible functions.We obtain a Monte Carlo approximation to the Bayesian predictions by averaging these thirty functions of x together.The result is shown in figure 41.7 and contrasted with the predictions given by the optimized parameters.The Bayesian predictions become satisfyingly moderate as we move away from the region of highest data density.The Bayesian classifier is better able to identify the points where the classification is uncertain.This pleasing behaviour results simply from a mechanical application of the rules of probability.A final observation concerns the behaviour of the functions G(w) and M (w) during the Monte Carlo sampling process, compared with the values of G and M at the optimum w MP (figure 41.5).The function G(w) fluctuates around the value of G(w MP ), though not in a symmetrical way.The function M (w) also fluctuates, but it does not fluctuate around M (w MP ) -obviously it cannot, because M is minimized at w MP , so M could not go any smaller -furthermore, M only rarely drops close to M (w MP ).In the language of information theory, the typical set of w has different properties from the most probable state w MP .A general message therefore emerges -applicable to all data models, not just neural networks: one should be cautious about making use of optimized parameters, as the properties of optimized parameters may be unrepresentative of the properties of typical, plausible parameters; and the predictions obtained using optimized parameters alone will often be unreasonably overconfident.As a final study of Monte Carlo methods, we now compare the Langevin Monte Carlo method with its big brother, the Hamiltonian Monte Carlo method.The change to Hamiltonian Monte Carlo is simple to implement, as shown in algorithm 41.8.Each single proposal makes use of multiple gradient evaluations along a dynamical trajectory in w, p space, where p are the extra 'momentum' variables of the Langevin and Hamiltonian Monte Carlo methods.The number of steps 'Tau' was set at random to a number between 100 and 200 for each trajectory.The step size was kept fixed so as to retain comparability with the simulations that have gone before; it is recommended that one randomize the step size in practical applications, however.Figure 41.9 compares the sampling properties of the Langevin and Hamiltonian Monte Carlo methods.The autocorrelation of the state of the Hamiltonian Monte Carlo simulation falls much more rapidly with simulation time than that of the Langevin method.For this toy problem, Hamiltonian Monte Carlo is at least ten times more efficient in its use of computer time.Physicists love to take nonlinearities and locally linearize them, and they love to approximate probability distributions by Gaussians.Such approximations offer an alternative strategy for dealing with the integralwhich we just evaluated using Monte Carlo methods.We start by making a Gaussian approximation to the posterior probability.We go to the minimum of M (w) (using a gradient-based optimizer) and Taylorexpand M there:where A is the matrix of second derivatives, also known as the Hessian, defined byWe thus define our Gaussian approximation:We can think of the matrix A as defining error bars on w.To be precise, Q is a normal distribution whose variance-covariance matrix is A −1 .Exercise 41.1. [2 ]Show that the second derivative of M (w) with respect to w is given by (41.25)where f (a) is the first derivative of f (a) ≡ 1/(1 + e −a ), which is(41.27)Having computed the Hessian, our task is then to perform the integral (41.21) using our Gaussian approximation.(a)ψ(a, s 2 )The output y(x; w) depends on w only through the scalar a(x; w), so we can reduce the dimensionality of the integral by finding the probability density of a.We are assuming a locally Gaussian posterior probability distribution over w = w MP + ∆w, P (w | D, α)(1/Z Q ) exp(− 1 2 ∆w T A∆w).For our single neuron, the activation a(x; w) is a linear function of w with ∂a/∂w = x, so for any x, the activation a is Gaussian-distributed.Exercise 41.2. [2 ]Assuming w is Gaussian-distributed with mean w MP and variance-covariance matrix A −1 , show that the probability distribution of a(x) is(41.28) where a MP = a(x; w MP ) andThis means that the marginalized output is:(41.29)This is to be contrasted with y(x; w MP ) = f (a MP ), the output of the most probable network.The integral of a sigmoid times a Gaussian can be approximated by: ψ(a MP , s 2 ) φ(a MP , s 2 ) ≡ f (κ(s)a MP ) (41.30) with κ = 1/ 1 + πs 2 /8 (figure 41.10).Figure 41.11 shows the result of fitting a Gaussian approximation at the optimum w MP , and the results of using that Gaussian approximation and equa-tion (41.30) to make predictions.Comparing these predictions with those of the Langevin Monte Carlo method (figure 41.7) we observe that, whilst qualitatively the same, the two are clearly numerically different.So at least one of the two methods is not completely accurate.Exercise 41.3. [2 ]Is the Gaussian approximation to P (w | D, α) too heavy-tailed or too light-tailed, or both?It may help to consider P (w | D, α) as a function of one parameter w i and to think of the two distributions on a logarithmic scale.Discuss the conditions under which the Gaussian approximation is most accurate.If the output is immediately used to make a (0/1) decision and the costs associated with error are symmetrical, then the use of marginalized outputs under this Gaussian approximation will make no difference to the performance of the classifier, compared with using the outputs given by the most probable parameters, since both functions pass through 0.5 at a MP = 0.But these Bayesian outputs will make a difference if, for example, there is an option of saying 'I don't know', in addition to saying 'I guess 0' and 'I guess 1'.And even if there are just the two choices '0' and '1', if the costs associated with error are unequal, then the decision boundary will be some contour other than the 0.5 contour, and the boundary will be affected by marginalization.One of my students, Robert, asked:Maybe I'm missing something fundamental, but supervised neural networks seem equivalent to fitting a pre-defined function to some given data, then extrapolating -what's the difference?I agree with Robert.The supervised neural networks we have studied so far are simply parameterized nonlinear functions which can be fitted to data.Hopefully you will agree with another comment that Robert made:Unsupervised networks seem much more interesting than their supervised counterparts.I'm amazed that it works!Hopfield NetworksWe have now spent three chapters studying the single neuron.The time has come to connect multiple neurons together, making the output of one neuron be the input to another, so as to make neural networks.Neural networks can be divided into two classes on the basis of their connectivity.Feedforward networks.In a feedforward network, all the connections are directed such that the network forms a directed acyclic graph.Feedback networks.Any network that is not a feedforward network will be called a feedback network.In this chapter we will discuss a fully connected feedback network called the Hopfield network.The weights in the Hopfield network are constrained to be symmetric, i.e., the weight from neuron i to neuron j is equal to the weight from neuron j to neuron i.Hopfield networks have two applications.First, they can act as associative memories.Second, they can be used to solve optimization problems.We will first discuss the idea of associative memory, also known as content-addressable memory.In Chapter 38, we discussed the contrast between traditional digital memories and biological memories.Perhaps the most striking difference is the associative nature of biological memory.A simple model due to Donald Hebb (1949) captures the idea of associative memory.Imagine that the weights between neurons whose activities are positively correlated are increased:Now imagine that when stimulus m is present (for example, the smell of a banana), the activity of neuron m increases; and that neuron n is associated with another stimulus, n (for example, the sight of a yellow object).If these two stimuli -a yellow sight and a banana smell -co-occur in the environment, then the Hebbian learning rule (42.1) will increase the weights w nm and w mn .This means that when, on a later occasion, stimulus n occurs in isolation, making the activity x n large, the positive weight from n to m will cause neuron m also to be activated.Thus the response to the sight of a yellow object is an automatic association with the smell of a banana.We could call this 'pattern completion'.No teacher is required for this associative memory to work.No signal is needed to indicate that a correlation has been detected or that an association should be made.The unsupervised, local learning algorithm and the unsupervised, local activity rule spontaneously produce associative memory.This idea seems so simple and so effective that it must be relevant to how memories work in the brain.Convention for weights.Our convention in general will be that w ij denotes the connection from neuron j to neuron i.Architecture.A Hopfield network consists of I neurons.They are fully connected through symmetric, bidirectional connections with weights w ij = w ji .There are no self-connections, so w ii = 0 for all i.Biases w i0 may be included (these may be viewed as weights from a neuron '0' whose activity is permanently x 0 = 1).We will denote the activity of neuron i (its output) by x i .Activity rule.Roughly, a Hopfield network's activity rule is for each neuron to update its state as if it were a single neuron with the threshold activation functionSince there is feedback in a Hopfield network (every neuron's output is an input to all the other neurons) we will have to specify an order for the updates to occur.The updates may be synchronous or asynchronous.Synchronous updates -all neurons compute their activationsthen update their states simultaneously toAsynchronous updates -one neuron at a time computes its activation and updates its state.The sequence of selected neurons may be a fixed sequence or a random sequence.The properties of a Hopfield network may be sensitive to the above choices.Learning rule.The learning rule is intended to make a set of desired memories {x (n) } be stable states of the Hopfield network's activity rule.Each memory is a binary pattern, with x i ∈ {−1, 1}.The weights are set using the sum of outer products or Hebb rule,where η is an unimportant constant.To prevent the largest possible weight from growing with N we might choose to set η = 1/N .Exercise 42.1. [1 ]Explain why the value of η is not important for the Hopfield network defined above.Using the identical architecture and learning rule we can define a Hopfield network whose activities are real numbers between −1 and 1.Activity rule.A Hopfield network's activity rule is for each neuron to update its state as if it were a single neuron with a sigmoid activation function.The updates may be synchronous or asynchronous, and involve the equations a i = j w ij x j (42.6) and x i = tanh(a i ).(42.7)The learning rule is the same as in the binary Hopfield network, but the value of η becomes relevant.Alternatively, we may fix η and introduce a gain β ∈ (0, ∞) into the activation function:Exercise 42.2. [1 ]Where have we encountered equations 42.6, 42.7, and 42.8 before?The hope is that the Hopfield networks we have defined will perform associative memory recall, as shown schematically in figure 42.2.We hope that the activity rule of a Hopfield network will take a partial memory or a corrupted memory, and perform pattern completion or error correction to restore the original memory.But why should we expect any pattern to be stable under the activity rule, let alone the desired memories?We address the continuous Hopfield network, since the binary network is a special case of it.We have already encountered the activity rule (42.6, 42.8) when we discussed variational methods (section 33.2): when we approximated the spin system whose energy function waswith a separable distributionand optimized the latter so as to minimize the variational free energy were guaranteed to decrease the variational free energyIf we simply replace J by w, x by x, and h n by w i0 , we see that the equations of the Hopfield network are identical to a set of mean-field equations that minimizeThere is a general name for a function that decreases under the dynamical evolution of a system and that is bounded below: such a function is a Lyapunov function for the system.It is useful to be able to prove the existence of Lyapunov functions: if a system has a Lyapunov function then its dynamics are bound to settle down to a fixed point, which is a local minimum of the Lyapunov function, or a limit cycle, along which the Lyapunov function is a constant.Chaotic behaviour is not possible for a system with a Lyapunov function.If a system has a Lyapunov function then its state space can be divided into basins of attraction, one basin associated with each attractor.So, the continuous Hopfield network's activity rules (if implemented asynchronously) have a Lyapunov function.This Lyapunov function is a convex function of each parameter a i so a Hopfield network's dynamics will always converge to a stable fixed point.This convergence proof depends crucially on the fact that the Hopfield network's connections are symmetric.It also depends on the updates being made asynchronously.p.520] Show by constructing an example that if a feedback network does not have symmetric connections then its dynamics may fail to converge to a fixed point.p.521] Show by constructing an example that if a Hopfield network is updated synchronously that, from some initial conditions, it may fail to converge to a fixed point..  (i-m) Some initial conditions that are far from the memories lead to stable states other than the four memories; in (i), the stable state looks like a mixture of two memories, 'D' and 'J'; stable state (j) is like a mixture of 'J' and 'C'; in (k), we find a corrupted version of the 'M' memory (two bits distant); in (l) a corrupted version of 'J' (four bits distant) and in (m), a state which looks spurious until we recognize that it is the inverse of the stable state (l).units being updated asynchronously in each iteration.For an initial condition randomly perturbed from a memory, it often only takes one iteration for all the errors to be corrected.The network has more stable states in addition to the four desired memories: the inverse of any stable state is also a stable state; and there are several stable states that can be interpreted as mixtures of the memories.The network can be severely damaged and still work fine as an associative memory.If we take the 300 weights of the network shown in figure 42.3 and randomly set 50 or 100 of them to zero, we still find that the desired memories are attracting stable states.Imagine a digital computer that still works fine even when 20% of its components are destroyed!3C ] Implement a Hopfield network and confirm this amazing robust error-correcting capability.We can squash more memories into the network too.Figure 42.4a shows a set of five memories.When we train the network with Hebbian learning, all five memories are stable states, even when 26 of the weights are randomly deleted (as shown by the 'x's in the weight matrix).However, the basins of attraction are smaller than before: figures 42.4(b-f) show the dynamics resulting from randomly chosen starting states close to each of the memories (3 bits flipped).Only three of the memories are recovered correctly.If we try to store too many patterns, the associative memory fails catastrophically.When we add a sixth pattern, as shown in figure 42.5, only one of the patterns is stable; the others all flow into one of two spurious stable states.The fact that the Hopfield network's properties are not robust to the minor change from asynchronous to synchronous updates might be a cause for concern; can this model be a useful model of biological networks?It turns out that once we move to a continuous-time version of the Hopfield networks, this issue melts away.We assume that each neuron's activity x i is a continuous function of time x i (t) and that the activations a i (t) are computed instantaneously in accordance with a i (t) = j w ij x j (t).(42.16)The neuron's response to its activation is assumed to be mediated by the differential equation: (42.17) 42.6:The continuous-time continuous Hopfield network 511  Desired memories moscow------russia lima----------peru london-----england tokyo--------japan edinburgh-scotland ottawa------canada oslo--------norway stockholm---sweden paris-------franceAttracting stable states moscow------russia lima----------peru londog-----englard(1) tonco--------japan(1) edinburgh-scotland(2) oslo--------norway stockholm---sweden paris-------france wzkmhewn--xqwqwpoq(3) paris-------swedenwhere f (a) is the activation function, for example f (a) = tanh(a).For a steady activation a i , the activity x i (t) relaxes exponentially to f (a i ) with time-constant τ .Now, here is the nice result: as long as the weight matrix is symmetric, this system has the variational free energy (42.15) as its Lyapunov function.Exercise 42.6. [1 ]By computing d dt F , prove that the variational free energy F (x) is a Lyapunov function for the continuous-time Hopfield network.It is particularly easy to prove that a function L is a Lyapunov function if the system's dynamics perform steepest descent on L, with d dt x i (t) ∝ ∂ ∂x i L. In the case of the continuous-time continuous Hopfield network, it is not quite so simple, but every component of d dt x i (t) does have the same sign as ∂ ∂x i F , which means that with an appropriately defined metric, the Hopfield network dynamics do perform steepest descents on F (x).One way in which we viewed learning in the single neuron was as communication -communication of the labels of the training data set from one point in time to a later point in time.We found that the capacity of a linear threshold neuron was 2 bits per weight.Similarly, we might view the Hopfield associative memory as a communication channel (figure 42.6).A list of desired memories is encoded into a set of weights W using the Hebb rule of equation (42.5), or perhaps some other learning rule.The receiver, receiving the weights W only, finds the stable states of the Hopfield network, which he interprets as the original memories.This communication system can fail in various ways, as illustrated in the figure .1. Individual bits in some memories might be corrupted, that is, a stable state of the Hopfield network is displaced a little from the desired memory.2. Entire memories might be absent from the list of attractors of the network; or a stable state might be present but have such a small basin of attraction that it is of no use for pattern completion and error correction.3. Spurious additional memories unrelated to the desired memories might be present.4. Spurious additional memories derived from the desired memories by operations such as mixing and inversion may also be present.Of these failure modes, modes 1 and 2 are clearly undesirable, mode 2 especially so.Mode 3 might not matter so much as long as each of the desired memories has a large basin of attraction.The fourth failure mode might in some contexts actually be viewed as beneficial.For example, if a network is required to memorize examples of valid sentences such as 'John loves Mary' and 'John gets cake', we might be happy to find that 'John loves cake' was also a stable state of the network.We might call this behaviour 'generalization'.The capacity of a Hopfield network with I neurons might be defined to be the number of random patterns N that can be stored without failure-mode 2 having substantial probability.If we also require failure-mode 1 to have tiny probability then the resulting capacity is much smaller.We now study these alternative definitions of the capacity.We will first explore the information storage capabilities of a binary Hopfield network that learns using the Hebb rule by considering the stability of just one bit of one of the desired patterns, assuming that the state of the network is set to that desired pattern x (n) .We will assume that the patterns to be stored are randomly selected binary patterns.The activation of a particular neuron i iswhere the weights are, for i = j,Here we have split W into two terms, the first of which will contribute 'signal', reinforcing the desired memory, and the second 'noise'.Substituting for w ij , the activation isThe first term is (I − 1) times the desired state x(n)i .If this were the only term, it would keep the neuron firmly clamped in the desired state.The second term is a sum of (I − 1)(N − 1) random quantities x(n) j .A moment's reflection confirms that these quantities are independent random binary variables with mean 0 and variance 1.Thus, considering the statistics of a i under the ensemble of random patterns, we conclude that a i has mean (I − 1)x (n) i and variance (I − 1)(N − 1).For brevity, we will now assume I and N are large enough that we can neglect the distinction between I and I − 1, and between N and N − 1.Then we can restate our conclusion: a i is Gaussian-distributed with mean Ix What then is the probability that the selected bit is stable, if we put the network into the state x (n) ?The probability that bit i will flip on the first iteration of the Hopfield network's dynamics is i /I, which is 1 when recall is perfect and zero when the stable state has 50% of the bits flipped.There is an abrupt transition at N/I = 0.138, where the overlap drops from 0.97 to zero.whereThe important quantity N/I is the ratio of the number of patterns stored to the number of neurons.If, for example, we try to store N 0.18I patterns in the Hopfield network then there is a chance of 1% that a specified bit in a specified pattern will be unstable on the first iteration.We are now in a position to derive our first capacity result, for the case where no corruption of the desired memories is permitted.Exercise 42.7. [2 ]Assume that we wish all the desired patterns to be completely stable -we don't want any of the bits to flip when the network is put into any desired pattern state -and the total probability of any error at all is required to be less than a small number .Using the approximation to the error function for large z, (42.24) show that the maximum number of patterns that can be stored, N max , isIf, however, we allow a small amount of corruption of memories to occur, the number of patterns that can be stored increases.The analysis that led to equation (42.22) tells us that if we try to store N 0.18I patterns in the Hopfield network then, starting from a desired memory, about 1% of the bits will be unstable on the first iteration.Our analysis does not shed light on what is expected to happen on subsequent iterations.The flipping of these bits might make some of the other bits unstable too, causing an increasing number of bits to be flipped.This process might lead to an avalanche in which the network's state ends up a long way from the desired memory.In fact, when N/I is large, such avalanches do happen.When N/I is small, they tend not to -there is a stable state near to each desired memory.For the limit of large I, Amit et al. (1985) have used methods from statistical physics to find numerically the transition between these two behaviours.There is a sharp discontinuity at N crit = 0.138I.(42.26)Below this critical value, there is likely to be a stable state near every desired memory, in which a small fraction of the bits are flipped.When N/I exceeds 0.138, the system has only spurious stable states, known as spin glass states, none of which is correlated with any of the desired memories.Just below the critical value, the fraction of bits that are flipped when a desired memory has evolved to its associated stable state is 1.6%.Figure 42.8 shows the overlap between the desired memory and the nearest stable state as a function of N/I.Some other transitions in properties of the model occur at some additional values of N/I, as summarized below.For all N/I, stable spin glass states exist, uncorrelated with the desired memories.For N/I > 0.138, these spin glass states are the only stable states.For N/I ∈ (0, 0.138), there are stable states close to the desired memories.For N/I ∈ (0, 0.05), the stable states associated with the desired memories have lower energy than the spurious spin glass states.For N/I ∈ (0.05, 0.138), the spin glass states dominate -there are spin glass states that have lower energy than the stable states associated with the desired memories.For N/I ∈ (0, 0.03), there are additional mixture states, which are combinations of several desired memories.These stable states do not have as low energy as the stable states associated with the desired memories.In conclusion, the capacity of the Hopfield network with I neurons, if we define the capacity in terms of the abrupt discontinuity discussed above, is 0.138I random binary patterns, each of length I, each of which is received with 1.6% of its bits flipped.In bits, this capacity is This expression for the capacity omits a smaller negative term of order N log 2 N bits, associated with the arbitrary order of the memories.0.138I 2 × (1 − H 2 (0.016)) = 0.122 I 2 bits.(42.27)Since there are I 2 /2 weights in the network, we can also express the capacity as 0.24 bits per weight.The capacities discussed in the previous section are the capacities of the Hopfield network whose weights are set using the Hebbian learning rule.We can do better than the Hebb rule by defining an objective function that measures how well the network stores all the memories, and minimizing it.For an associative memory to be useful, it must be able to correct at least one flipped bit.Let's make an objective function that measures whether flipped bits tend to be restored correctly.Our intention is that, for every neuron i in the network, the weights to that neuron should satisfy this rule: for every pattern x (n) , if the neurons other than i are set correctly to x j = x (n) j , then the activation of neuron i should be such that its preferred output isIs this rule a familiar idea?Yes, it is precisely what we wanted the single neuron of Chapter 39 to do.Each pattern x (n) defines an input, target pair for the single neuron i.And it defines an input, target pair for all the other neurons too.Algorithm 42.9.Octave source code for optimizing the weights of a Hopfield network, so that it works as an associative memory.cf.algorithm 39.5.The data matrix x has I columns and N rows.The matrix t is identical to x except that −1s are replaced by 0s.and y, where a.(42.30)We can then steal the algorithm (algorithm 39.5, p.478) which we wrote for the single neuron, to write an algorithm for optimizing a Hopfield network, algorithm 42.9.The convenient syntax of Octave requires very few changes; the extra lines enforce the constraints that the self-weights w ii should all be zero and that the weight matrix should be symmetrical (w ij = w ji ).As expected, this learning algorithm does a better job than the one-shot Hebbian learning rule.When the six patterns of figure 42.5, which cannot be memorized by the Hebb rule, are learned using algorithm 42.9, all six patterns become stable states.4C ] Implement this learning rule and investigate empirically its capacity for memorizing random patterns; also compare its avalanche properties with those of the Hebb rule.Since a Hopfield network's dynamics minimize an energy function, it is natural to ask whether we can map interesting optimization problems onto Hopfield networks.Biological data processing problems often involve an element of constraint satisfaction -in scene interpretation, for example, one might wish to infer the spatial location, orientation, brightness and texture of each visible element, and which visible elements are connected together in objects.These inferences are constrained by the given data and by prior knowledge about continuity of objects.Hopfield and Tank (1985) suggested that one might take an interesting constraint satisfaction problem and design the weights of a binary or continuous Hopfield network such that the settling process of the network would minimize the objective function of the problem.A classic constraint satisfaction problem to which Hopfield networks have been applied is the travelling salesman problem.A set of K cities is given, and a matrix of the K(K −1)/2 distances between those cities.The task is to find a closed tour of the cities, visiting each city once, that has the smallest total distance.The travelling salesman problem is equivalent in difficulty to an NP-complete problem.The method suggested by Hopfield and Tank is to represent a tentative solution to the problem by the state of a network with I = K 2 neurons arranged in a square, with each neuron representing the hypothesis that a particular city comes at a particular point in the tour.It will be convenient to consider the states of the neurons as being between 0 and 1 rather than −1 and 1.Two solution states for a four-city travelling salesman problem are shown in figure 42.10a.The weights in the Hopfield network play two roles.First, they must define an energy function which is minimized only when the state of the network represents a valid tour.A valid state is one that looks like a permutation matrix, having exactly one '1' in every row and one '1' in every column.This rule can be enforced by putting large negative weights between any pair of neurons that are in the same row or the same column, and setting a positive bias for all neurons to ensure that K neurons do turn on.Figure 42.10b shows the negative weights that are connected to one neuron, 'B2', which represents the statement 'city B comes second in the tour'.Second, the weights must encode the objective function that we want to minimize -the total distance.This can be done by putting negative weights proportional to the appropriate distances between the nodes in adjacent columns.For example, between the B and D nodes in adjacent columns, the weight would be −d BD .The negative weights that are connected to neuron B2 are shown in figure 42.10c.The result is that when the network is in a valid state, its total energy will be the total distance of the corresponding  tour, plus a constant given by the energy associated with the biases.Now, since a Hopfield network minimizes its energy, it is hoped that the binary or continuous Hopfield network's dynamics will take the state to a minimum that is a valid tour and which might be an optimal tour.This hope is not fulfilled for large travelling salesman problems, however, without some careful modifications.We have not specified the size of the weights that enforce the tour's validity, relative to the size of the distance weights, and setting this scale factor poses difficulties.If 'large' validity-enforcing weights are used, the network's dynamics will rattle into a valid state with little regard for the distances.If 'small' validity-enforcing weights are used, it is possible that the distance weights will cause the network to adopt an invalid state that has lower energy than any valid state.Our original formulation of the energy function puts the objective function and the solution's validity in potential conflict with each other.This difficulty has been resolved by the work of Sree Aiyer (1991), who showed how to modify the distance weights so that they would not interfere with the solution's validity, and how to define a continuous Hopfield network whose dynamics are at all times confined to a 'valid subspace'.Aiyer used a graduated non-convexity or deterministic annealing approach to find good solutions using these Hopfield networks.The deterministic annealing approach involves gradually increasing the gain β of the neurons in the network from 0 to ∞, at which point the state of the network corresponds to a valid tour.A sequence of trajectories generated by applying this method to a thirtycity travelling salesman problem is shown in figure 42.11a.A solution to the 'travelling scholar problem' found by Aiyer using a continuous Hopfield network is shown in figure 42.11b.Exercise 42.9. [3 ]Storing two memories.Two binary memories m and n (m i , n i ∈ {−1, +1}) are stored by Hebbian learning in a Hopfield network usingThe biases b i are set to zero.The network is put in the state x = m.Evaluate the activation a i of neuron i and show that in can be written in the form How does this number compare with the maximum number of flipped bits that can be corrected by the optimal decoder, assuming the vector x is either a noisy version of m or of n?Exercise 42.10. [3 ]Hopfield network as a collection of binary classifiers.This exercise explores the link between unsupervised networks and supervised networks.If a Hopfield network's desired memories are all attracting stable states, then every neuron in the network has weights going to it that solve a classification problem personal to that neuron.Take the set of memories and write them in the form x (n) , xi , where x denotes all the components x i for all i = i, and let w denote the vector of weights w ii , for i = i.Using what we know about the capacity of the single neuron, show that it is almost certainly impossible to store more than 2I random memories in a Hopfield network of I neurons.Exercise 42.11.The southeast puzzle The southeast puzzle is played on a semi-infinite chess board, starting at its northwest (top left) corner.There are three rules:1.In the starting position, one piece is placed in the northwest-most square (figure 42.13a).2. It is not permitted for more than one piece to be on any given square.3. At each step, you remove one piece from the board, and replace it with two pieces, one in the square immediately to the east, and one in the the square immediately to the south, as illustrated in figure 42.13b.Every such step increases the number of pieces on the board by one.After move (b) has been made, either piece may be selected for the next move.Figure 42.13c shows the outcome of moving the lower piece.At the next move, either the lowest piece or the middle piece of the three may be selected; the uppermost piece may not be selected, since that would violate rule 2. At move (d) we have selected the middle piece.Now any of the pieces may be moved, except for the leftmost piece.Now, here is the puzzle:p.521] Is it possible to obtain a position in which all the ten squares closest to the northwest corner, marked in figure 42.13z, are empty?[Hint: this puzzle has a connection to data compression.]Solution to exercise 42.3 (p.508).Take a binary feedback network with 2 neurons and let w 12 = 1 and w 21 = −1.Then whenever neuron 1 is updated, it will match neuron 2, and whenever neuron 2 is updated, it will flip to the opposite state from neuron 1.There is no stable state.Solution to exercise 42.4 (p.508).Take a binary Hopfield network with 2 neurons and let w 12 = w 21 = 1, and let the initial condition be x 1 = 1, x 2 = −1.Then if the dynamics are synchronous, on every iteration both neurons will flip their state.The dynamics do not converge to a fixed point.Solution to exercise 42.12 (p.520).The key to this problem is to notice its similarity to the construction of a binary symbol code.Starting from the empty string, we can build a binary tree by repeatedly splitting a codeword into two.Every codeword has an implicit probability 2 −l , where l is the depth of the codeword in the binary tree.Whenever we split a codeword in two and create two new codewords whose length is increased by one, the two new codewords each have implicit probability equal to half that of the old codeword.For a complete binary code, the Kraft equality affirms that the sum of these implicit probabilities is 1.Similarly, in southeast, we can associate a 'weight' with each piece on the board.If we assign a weight of 1 to any piece sitting on the top left square; a weight of 1/2 to any piece on a square whose distance from the top left is one; a weight of 1/4 to any piece whose distance from the top left is two; and so forth, with 'distance' being the city-block distance; then every legal move in southeast leaves unchanged the total weight of all pieces on the board.Lyapunov functions come in two flavours: the function may be a function of state whose value is known to stay constant; or it may be a function of state that is bounded below, and whose value always decreases or stays constant.The total weight is a Lyapunov function of the second type.The starting weight is 1, so now we have a powerful tool: a conserved function of the state.Is it possible to find a position in which the ten highestweight squares are vacant, and the total weight is 1? What is the total weight if all the other squares on the board are occupied (figure 42  weight would be ∞ l=4 (l + 1)2 −l , which is equal to 3/4.So it is impossible to empty all ten of those squares.We have noticed that the binary Hopfield network minimizes an energy functionand that the continuous Hopfield network with activation function x n = tanh(a n ) can be viewed as approximating the probability distribution associated with that energy function,These observations motivate the idea of working with a neural network model that actually implements the above probability distribution.The stochastic Hopfield network or Boltzmann machine (Hinton and Sejnowski, 1986) has the following activity rule:Activity rule of Boltzmann machine: after computing the activation a i (42.3), set x i = +1 with probability 1 1 + e −2a i else set x i = −1.(43.3)This rule implements Gibbs sampling for the probability distribution (43.2).Given a set of examples {x (n) } N 1 from the real world, we might be interested in adjusting the weights W such that the generative modelis well matched to those examples.We can derive a learning algorithm by writing down Bayes' theorem to obtain the posterior probability of the weights given the data:We concentrate on the first term in the numerator, the likelihood, and derive a maximum likelihood algorithm (though there might be advantages in pursuing a full Bayesian approach as we did in the case of the single neuron).We differentiate the logarithm of the likelihood, lnwith respect to w ij , bearing in mind that W is defined to be symmetric with w ji = w ij .Exercise 43.1. [2 ]Show that the derivative of ln Z(W) with respect to w ij is[This exercise is similar to exercise 22.12 (p.307).]The derivative of the log likelihood is therefore:This gradient is proportional to the difference of two terms.The first term is the empirical correlation between x i and x j ,and the second term is the correlation between x i and x j under the current model,The first correlation x i x j Data is readily evaluated -it is just the empirical correlation between the activities in the real world.The second correlation, x i x j P (x | W) , is not so easy to evaluate, but it can be estimated by Monte Carlo methods, that is, by observing the average value of x i x j while the activity rule of the Boltzmann machine, equation (43.3), is iterated.In the special case W = 0, we can evaluate the gradient exactly because, by symmetry, the correlation x i x j P (x | W) must be zero.If the weights are adjusted by gradient descent with learning rate η, then, after one iteration, the weights will beprecisely the value of the weights given by the Hebb rule, equation (16.5), with which we trained the Hopfield network.One way of viewing the two terms in the gradient (43.9) is as 'waking' and 'sleeping' rules.While the network is 'awake', it measures the correlation between x i and x j in the real world, and weights are increased in proportion.While the network is 'asleep', it 'dreams' about the world using the generative model (43.4), and measures the correlations between x i and x j in the model world; these correlations determine a proportional decrease in the weights.If the second-order correlations in the dream world match the correlations in the real world, then the two terms balance and the weights do not change.Up to this point we have discussed Hopfield networks and Boltzmann machines in which all of the neurons correspond to visible variables x i .The result is a probabilistic model that, when optimized, can capture the second-order statistics of the environment.[The second-order statistics of an ensemble P (x) are the expected values x i x j of all the pairwise products x i x j .]The real world, however, often has higher-order correlations that must be included if our description of it is to be effective.Often the second-order correlations in themselves may carry little or no useful information.Consider, for example, the ensemble of binary images of chairs.We can imagine images of chairs with various designs -four-legged chairs, comfy chairs, chairs with five legs and wheels, wooden chairs, cushioned chairs, chairs with rockers instead of legs.A child can easily learn to distinguish these images from images of carrots and parrots.But I expect the second-order statistics of the raw data are useless for describing the ensemble.Second-order statistics only capture whether two pixels are likely to be in the same state as each other.Higher-order concepts are needed to make a good generative model of images of chairs.A simpler ensemble of images in which high-order statistics are important is the 'shifter ensemble', which comes in two flavours.Figure 43.1ashows a few samples from the 'plain shifter ensemble'.In each image, the bottom eight pixels are a copy of the top eight pixels, either shifted one pixel to the left, or unshifted, or shifted one pixel to the right.(The top eight pixels are set at random.)This ensemble is a simple model of the visual signals from the two eyes arriving at early levels of the brain.The signals from the two eyes are similar to each other but may differ by small translations because of the varying depth of the visual world.This ensemble is simple to describe, but its second-order statistics convey no useful information.The correlation between one pixel and any of the three pixels above it is 1/3.The correlation between any other two pixels is zero.Figure 43.1bshows a few samples from the 'labelled shifter ensemble'.Here, the problem has been made easier by including an extra three neurons that label the visual image as being an instance of either the 'shift left', 'no shift', or 'shift right' sub-ensemble.But with this extra information, the ensemble is still not learnable using second-order statistics alone.The secondorder correlation between any label neuron and any image neuron is zero.We need models that can capture higher-order statistics of an environment.So, how can we develop such models?One idea might be to create models that directly capture higher-order correlations, such as:(43.13)Such higher-order Boltzmann machines are equally easy to simulate using stochastic updates, and the learning rule for the higher-order parameters v ijk is equivalent to the learning rule for w ij .Exercise 43.2. [2 ]Derive the gradient of the log likelihood with respect to v ijk .It is possible that the spines found on biological neurons are responsible for detecting correlations between small numbers of incoming signals.However, to capture statistics of high enough order to describe the ensemble of images of chairs well would require an unimaginable number of terms.To capture merely the fourth-order statistics in a 128 × 128 pixel image, we need more than 10 7 parameters.So measuring moments of images is not a good way to describe their underlying structure.Perhaps what we need instead or in addition are hidden variables, also known to statisticians as latent variables.This is the important innovation introduced by Hinton and Sejnowski (1986).The idea is that the high-order correlations among the visible variables are described by including extra hidden variables and sticking to a model that has only second-order interactions between its variables; the hidden variables induce higher-order correlations between the visible variables.We now add hidden neurons to our stochastic model.These are neurons that do not correspond to observed variables; they are free to play any role in the probabilistic model defined by equation (43.4).They might actually take on interpretable roles, effectively performing 'feature extraction'.The activity rule of a Boltzmann machine with hidden units is identical to that of the original Boltzmann machine.The learning rule can again be derived by maximum likelihood, but now we need to take into account the fact that the states of the hidden units are unknown.We will denote the states of the visible units by x, the states of the hidden units by h, and the generic state of a neuron (either visible or hidden) by y i , with y ≡ (x, h).The state of the network when the visible neurons are clamped in state x (n) is y (n) ≡ (x (n) , h).The likelihood of W given a single data example x (n) is(43.14) whereEquation (43.14) may also be writtenwhere(43.17)Differentiating the likelihood as before, we find that the derivative with respect to any weight w ij is again the difference between a 'waking' term and a 'sleeping' term,(43.18)The first term y i y j P (h | x (n) ,W) is the correlation between y i and y j if the Boltzmann machine is simulated with the visible variables clamped to x (n) and the hidden variables freely sampling from their conditional distribution.The second term y i y j P (x,h | W) is the correlation between y i and y j when the Boltzmann machine generates samples from its model distribution.Hinton and Sejnowski demonstrated that non-trivial ensembles such as the labelled shifter ensemble can be learned using a Boltzmann machine with hidden units.The hidden units take on the role of feature detectors that spot patterns likely to be associated with one of the three shifts.The Boltzmann machine is time-consuming to simulate because the computation of the gradient of the log likelihood depends on taking the difference of two gradients, both found by Monte Carlo methods.So Boltzmann machines are not in widespread use.It is an area of active research to create models that embody the same capabilities using more efficient computations (Hinton et al., 1995;Dayan et al., 1995;Hinton and Ghahramani, 1997;Hinton, 2001;.Exercise 43.3. [3 ]Can the 'bars and stripes' ensemble (figure 43.2) be learned Each sample is generated by first picking an orientation, horizontal or vertical; then, for each row of spins in that orientation (each bar or stripe respectively), switching all spins on with probability 1/ 2. by a Boltzmann machine with no hidden units?[You may be surprised!]44 Supervised Learning in Multilayer NetworksNo course on neural networks could be complete without a discussion of supervised multilayer networks, also known as backpropagation networks.The multilayer perceptron is a feedforward network.It has input neurons, hidden neurons and output neurons.The hidden neurons may be arranged in a sequence of layers.The most common multilayer perceptrons have a single hidden layer, and are known as 'two-layer' networks, the number 'two' counting the number of layers of neurons not including the inputs.Such a feedforward network defines a nonlinear parameterized mapping from an input x to an output y = y(x; w, A).The output is a continuous function of the input and of the parameters w; the architecture of the net, i.e., the functional form of the mapping, is denoted by A. Feedforward networks can be 'trained' to perform regression and classification tasks.In the case of a regression problem, the mapping for a network with one hidden layer may have the form: Hidden layer: aOutput layer: awhere, for example, f (1) (a) = tanh(a), and f (2) (a) = a.Here l runs over the inputs x 1 , . . ., x L , j runs over the hidden units, and i runs over the outputs.The 'weights' w and 'biases' θ together make up the parameter vector w.The nonlinear sigmoid function f (1) at the hidden layer gives the neural network greater computational flexibility than a standard linear regression model.Graphically, we can represent the neural network as a set of layers of connected neurons (figure 44.1).Just as we explored the weight space of the single neuron in Chapter 39, examining the functions it could produce, let us explore the weight space of a multilayer network.In figures 44.2 and 44.3I take a network with one input and one output and a large number H of hidden units, set the biases(1) j , w(1) jl , θ(2) i and w(2) ij to random values, and plot the resulting function y(x).I set the hidden units' biases θ(1) j to random values from a Gaussian with zero mean and standard deviation σ bias ; the input-to-hidden weights w(1) jl to random values with standard deviation σ in ; and the bias and output weights θ(2) i and w(2) ij to random values with standard deviation σ out .The sort of functions that we obtain depend on the values of σ bias , σ in and σ out .As the weights and biases are made bigger we obtain more complex functions with more features and a greater sensitivity to the input variable.The vertical scale of a typical function produced by the network with random weights is of order √ Hσ out ; the horizontal range in which the function varies significantly is of order σ bias /σ in ; and the shortest horizontal length scale is of order 1/σ in .Radford Neal (1996) has also shown that in the limit as H → ∞ the statistical properties of the functions generated by randomizing the weights are independent of the number of hidden units; so, interestingly, the complexity of the functions becomes independent of the number of parameters in the model.What determines the complexity of the typical functions is the characteristic magnitude of the weights.Thus we anticipate that when we fit these models to real data, an important way of controlling the complexity of the fitted function will be to control the characteristic magnitude of the weights.This network is trained using a data set D = {x (n) , t (n) } by adjusting w so as to minimize an error function, e.g.,This objective function is a sum of terms, one for each input/target pair {x, t}, measuring how close the output y(x; w) is to the target t.This minimization is based on repeated evaluation of the gradient of E D .This gradient can be efficiently computed using the backpropagation algorithm (Rumelhart et al., 1986), which uses the chain rule to find the derivatives.Often, regularization (also known as weight decay) is included, modifying the objective function to:where, for example, E W = 1 2 i w 2 i .This additional term favours small values of w and decreases the tendency of a model to overfit noise in the training data.Rumelhart et al. (1986) showed that multilayer perceptrons can be trained, by gradient descent on M (w), to discover solutions to non-trivial problems such as deciding whether an image is symmetric or not.These networks have been successfully applied to real-world tasks as varied as pronouncing English text (Sejnowski and Rosenberg, 1987) and focussing multiple-mirror telescopes (Angel et al., 1990).The neural network learning process above can be given the following probabilistic interpretation.[Here we repeat and generalize the discussion of Chapter 41.]The error function is interpreted as defining a noise model.βE D is the negative log likelihood:Thus, the use of the sum-squared error E D (44.3) corresponds to an assumption of Gaussian noise on the target variables, and the parameter β defines a noise level σ 2 ν = 1/β.Similarly the regularizer is interpreted in terms of a log prior probability distribution over the parameters:If E W is quadratic as defined above, then the corresponding prior distribution is a Gaussian with variance σ 2 W = 1/α.The probabilistic model H specifies the architecture A of the network, the likelihood (44.5), and the prior (44.6).The objective function M (w) then corresponds to the inference of the parameters w, given the data:The w found by (locally) minimizing M (w) is then interpreted as the (locally) most probable parameter vector, w MP .The interpretation of M (w) as a log probability adds little new at this stage.But new tools will emerge when we proceed to other inferences.First, though, let us establish the probabilistic interpretation of classification networks, to which the same tools apply.If the targets t in a data set are binary classification labels (0, 1), it is natural to use a neural network whose output y(x; w, A) is bounded between 0 and 1, and is interpreted as a probability P (t = 1 | x, w, A).For example, a network with one hidden layer could be described by the feedforward equations (44.1) and (44.2), with f (2) (a) = 1/(1 + e −a ).The error function βE D is replaced by the negative log likelihood:(44.9)The total objective function is then M = G + αE W .Note that this includes no parameter β (because there is no Gaussian noise).For a multi-class classification problem, we can represent the targets by a vector, t, in which a single element is set to 1, indicating the correct class, and all other elements are set to 0. In this case it is appropriate to use a 'softmax' network having coupled outputs which sum to one and are interpreted as class probabilities y i = P (t i = 1 | x, w, A).The last part of equation ( 44.2) is replaced by:(44.10)The negative log likelihood in this case isAs in the case of the regression network, the minimization of the objective function M (w) = G + αE W corresponds to an inference of the form (44.8).A variety of useful results can be built on this interpretation.From the statistical perspective, supervised neural networks are nothing more than nonlinear curve-fitting devices.Curve fitting is not a trivial task however.The effective complexity of an interpolating model is of crucial importance, as illustrated in figure 44.5.Consider a control parameter that influences the complexity of a model, for example a regularization constant α (weight decay parameter).As the control parameter is varied to increase the complexity of the model (descending from figure 44.5a-c and going from left to right across figure 44.5d), the best fit to the training data that the model can achieve becomes increasingly good.However, the empirical performance of the model, the test error, first decreases then increases again.An over-complex model overfits the data and generalizes poorly.This problem may also complicate the choice of architecture in a multilayer perceptron, the radius of the basis functions in a radial basis function network, and the choice of the input variables themselves in any multidimensional regression problem.Finding values for model control parameters that are appropriate for the data is therefore an important and non-trivial problem.44.4: Benefits of the Bayesian approach to supervised feedforward neural networks 531 The overfitting problem can be solved by using a Bayesian approach to control model complexity.If we give a probabilistic interpretation to the model, then we can evaluate the evidence for alternative values of the control parameters.As was explained in Chapter 28, over-complex models turn out to be less probable, and the evidence P (Data | Control Parameters) can be used as an objective function for optimization of model control parameters (figure 44.5e).The setting of α that maximizes the evidence is displayed in figure 44.5b.Bayesian optimization of model control parameters has four important advantages.(1) No 'test set' or 'validation set' is involved, so all available training data can be devoted to both model fitting and model comparison.(2) Regularization constants can be optimized on-line, i.e., simultaneously with the optimization of ordinary model parameters.(3) The Bayesian objective function is not noisy, in contrast to a cross-validation measure.(4) The gradient of the evidence with respect to the control parameters can be evaluated, making it possible to simultaneously optimize a large number of control parameters.Probabilistic modelling also handles uncertainty in a natural manner.It offers a unique prescription, marginalization, for incorporating uncertainty about parameters into predictions; this procedure yields better predictions, as we saw in Chapter 41. Figure 44.6 shows error bars on the predictions of a trained neural network.As was mentioned in Chapter 41, Bayesian inference for multilayer networks may be implemented by Monte Carlo sampling, or by deterministic methods employing Gaussian approximations (Neal, 1996;MacKay, 1992c).Within the Bayesian framework for data modelling, it is easy to improve our probabilistic models.For example, if we believe that some input variables in a problem may be irrelevant to the predicted quantity, but we don't know which, we can define a new model with multiple hyperparameters that captures the idea of uncertain input variable relevance (MacKay, 1994b;Neal, 1996;MacKay, 1995b); these models then infer automatically from the data which are the relevant input variables for a problem.Exercise 44.1. [4 ]How to measure a classifier's quality.You've just written a new classification algorithm and want to measure how well it performs on a test set, and compare it with other classifiers.What performance measure should you use?There are several standard answers.Let's assume the classifier gives an output y(x), where x is the input, which we won't discuss further, and that the true target value is t.In the simplest discussions of classifiers, both y and t are binary variables, but you might care to consider cases where y and t are more general objects also.The most widely used measure of performance on a test set is the error rate -the fraction of misclassifications made by the classifier.This measure forces the classifier to give a 0/1 output and ignores any additional information that the classifier might be able to offer -for example, an indication of the firmness of a prediction.Unfortunately, the error rate does not necessarily measure how informative a classifier's output is.Consider frequency tables showing the joint frequency of the 0/1 output of a classifier (horizontal axis), and the true 0/1 variable (vertical axis).The numbers that we'll show are percentages.The error rate e is the sum of the two off-diagonal numbers, which we could call the false positive rate e + and the false negative rate e − .Of the following three classifiers, A and B have the same error rate of 10% and C has a greater error rate of 12%.But clearly classifier A, which simply guesses that the outcome is 0 for all cases, is conveying no information at all about t; whereas classifier B has an informative output: if y = 0 then we are sure that t really is zero; and if y = 1 then there is a 50% chance that t = 1, as compared to the prior probability P (t = 1) = 0.1.Classifier C is slightly less informative than B, but it is still much more useful than the information-free classifier A.How error rate ranks the classifiers:(best) A = B > C (worst).One way to improve on the error rate as a performance measure is to report the pair (e + , e − ), the false positive error rate and the false negative error rate, which are (0, 0.1) and (0.1, 0) for classifiers A and B. It is especially important to distinguish between these two error probabilities in applications where the two sorts of error have different associated costs.However, there are a couple of problems with the 'error rate pair':• First, if I simply told you that classifier A has error rates (0, 0.1) and B has error rates (0.1, 0), it would not be immediately evident that classifier A is actually utterly worthless.Surely we should have a performance measure that gives the worst possible score to A!• Second, if we turn to a multiple-class classification problem such as digit recognition, then the number of types of error increases from two to 10 × 9 = 90 -one for each possible confusion of class t with t .It would be nice to have some sensible way of collapsing these 90 numbers into a single rankable number that makes more sense than the error rate.Another reason for not liking the error rate is that it doesn't give a classifier credit for accurately specifying its uncertainty.Consider classifiers that have three outputs available, '0', '1' and a rejection class, '?', which indicates that the classifier is not sure.People often plot error-reject curves (also known as ROC curves; ROC stands for 'receiver operating characteristic') which show the total e = (e + + e − ) versus r as r is allowed to vary from 0 to 1, and use these curves to compare classifiers (figure 44.7).[In the special case of binary classification problems, e + may be plotted versus e − instead.]But as we have seen, error rates can be undiscerning performance measures.Does plotting one error rate as a function of another make this weakness of error rates go away?For this exercise, either construct an explicit example demonstrating that the error-reject curve, and the area under it, are not necessarily good ways to compare classifiers; or prove that they are.As a suggested alternative method for comparing classifiers, consider the mutual information between the output and the target,Feedforward neural networks such as multilayer perceptrons are popular tools for nonlinear regression and classification problems.From a Bayesian perspective, a choice of a neural network model can be viewed as defining a prior probability distribution over nonlinear functions, and the neural network's learning process can be interpreted in terms of the posterior probability distribution over the unknown function.(Some learning algorithms search for the function with maximum posterior probability and other Monte Carlo methods draw samples from this posterior probability.)In the limit of large but otherwise standard networks, Neal (1996) has shown that the prior distribution over nonlinear functions implied by the Bayesian neural network falls in a class of probability distributions known as Gaussian processes.The hyperparameters of the neural network model determine the characteristic lengthscales of the Gaussian process.Neal's observation motivates the idea of discarding parameterized networks and working directly with Gaussian processes.Computations in which the parameters of the network are optimized are then replaced by simple matrix operations using the covariance matrix of the Gaussian process.In this chapter I will review work on this idea by Williams and Rasmussen (1996), Neal (1997b), Barber and Williams (1997) and Gibbs and MacKay (2000), and will assess whether, for supervised regression and classification tasks, the feedforward network has been superceded.Exercise 45.1. [3 ]I regret that this chapter is rather dry.There's no simple explanatory examples in it, and few pictures.This exercise asks you to create interesting pictures to explain to yourself this chapter's ideas.Source code for computer demonstrations written in the free language octave is available at: http://www.inference.phy.cam.ac.uk/mackay/itprnn/software.html.Radford Neal's software for Gaussian processes is available at: http://www.cs.toronto.edu/~radford/.After the publication of Rumelhart, Hinton and Williams's (1986) paper on supervised learning in neural networks there was a surge of interest in the empirical modelling of relationships in high-dimensional data using nonlinear parametric models such as multilayer perceptrons and radial basis functions.In the Bayesian interpretation of these modelling methods, a nonlinear function y(x) parameterized by parameters w is assumed to underlie the data {x (n) , t n } N n=1 , and the adaptation of the model to the data corresponds to an inference of the function given the data.We will denote the set of input vectors by X N ≡ {x (n) } N n=1 and the set of corresponding target values by the vector t N ≡ {t n } N n=1 .The inference of y(x) is described by the posterior probability distributionOf the two terms on the right-hand side, the first, P (t N | y(x), X N ), is the probability of the target values given the function y(x), which in the case of regression problems is often assumed to be a separable Gaussian distribution; and the second term, P (y(x)), is the prior distribution on functions assumed by the model.This prior is implicit in the choice of parametric model and the choice of regularizers used during the model fitting.The prior typically specifies that the function y(x) is expected to be continuous and smooth, and has less high frequency power than low frequency power, but the precise meaning of the prior is somewhat obscured by the use of the parametric model.Now, for the prediction of future values of t, all that matters is the assumed prior P (y(x)) and the assumed noise model P (t N | y(x), X N ) -the parameterization of the function y(x; w) is irrelevant.The idea of Gaussian process modelling is to place a prior P (y(x)) directly on the space of functions, without parameterizing y(x).The simplest type of prior over functions is called a Gaussian process.It can be thought of as the generalization of a Gaussian distribution over a finite vector space to a function space of infinite dimension.Just as a Gaussian distribution is fully specified by its mean and covariance matrix, a Gaussian process is specified by a mean and a covariance function.Here, the mean is a function of x (which we will often take to be the zero function), and the covariance is a function C(x, x ) that expresses the expected covariance between the values of the function y at the points x and x .The function y(x) in any one data modelling problem is assumed to be a single sample from this Gaussian distribution.Gaussian processes are already well established models for various spatial and temporal problems -for example, Brownian motion, Langevin processes and Wiener processes are all examples of Gaussian processes; Kalman filters, widely used to model speech waveforms, also correspond to Gaussian process models; the method of 'kriging' in geostatistics is a Gaussian process regression method.It might be thought that it is not possible to reproduce the interesting properties of neural network interpolation methods with something so simple as a Gaussian distribution, but as we shall now see, many popular nonlinear interpolation methods are equivalent to particular Gaussian processes.(I use the term 'interpolation' to cover both the problem of 'regression' -fitting a curve through noisy data -and the task of fitting an interpolant that passes exactly through the given data points.)It might also be thought that the computational complexity of inference when we work with priors over infinite-dimensional function spaces might be infinitely large.But by concentrating on the joint probability distribution of the observed data and the quantities we wish to predict, it is possible to make predictions with resources that scale as polynomial functions of N , the number of data points.We are given N data points X N , t N = {x (n) , t n } N n=1 .The inputs x are vectors of some fixed input dimension I.The targets t are either real numbers, in which case the task will be a regression or interpolation task, or they are categorical variables, for example t ∈ {0, 1}, in which case the task is a classification task.We will concentrate on the case of regression for the time being.Assuming that a function y(x) underlies the observed data, the task is to infer the function from the given data, and predict the function's value -or the value of the observation t N +1 -at a new point x (N +1) .In a parametric approach to regression we express the unknown function y(x) in terms of a nonlinear function y(x; w) parameterized by parameters w.Example 45.2.Fixed basis functions.Using a set of basis functions {φ h (x)} H h=1 , we can write(45.2)If the basis functions are nonlinear functions of x such as radial basis functions centred at fixed points {c h } H h=1 ,.3) then y(x; w) is a nonlinear function of x; however, since the dependence of y on the parameters w is linear, we might sometimes refer to this as a 'linear' model.In neural network terms, this model is like a multilayer network whose connections from the input layer to the nonlinear hidden layer are fixed; only the output weights w are adaptive.Other possible sets of fixed basis functions include polynomials such as φ h (x) = x p i x q j where p and q are integer powers that depend on h.Example 45.3.Adaptive basis functions.Alternatively, we might make a function y(x) from basis functions that depend on additional parameters included in the vector w.In a two-layer feedforward neural network with nonlinear hidden units and a linear output, the function can be writtenhi x i + w(1) h0+ w(2) 0(45.4)where I is the dimensionality of the input space and the weight vector w consists of the input weights {w(1)hi }, the hidden unit biases {w(1) h0 }, the output weights {w(2) h } and the output bias w(2) 0 .In this model, the dependence of y on w is nonlinear.Having chosen the parameterization, we then infer the function y(x; w) by inferring the parameters w.The posterior probability of the parameters isThe factor P (t N | w, X N ) states the probability of the observed data points when the parameters w (and hence, the function y) are known.This probability distribution is often taken to be a separable Gaussian, each data point t n differing from the underlying value y(x (n) ; w) by additive noise.The factor P (w) specifies the prior probability distribution of the parameters.This too is often taken to be a separable Gaussian distribution.If the dependence of y on w is nonlinear the posterior distribution P (w | t N , X N ) is in general not a Gaussian distribution.The inference can be implemented in various ways.In the Laplace method, we minimize an objective functionwith respect to w, locating the locally most probable parameters, then use the curvature of M , ∂ 2 M (w)/∂w i ∂w j , to define error bars on w.Alternatively we can use more general Markov chain Monte Carlo techniques to create samples from the posterior distribution P (w | t N , X N ).Having obtained one of these representations of the inference of w given the data, predictions are then made by marginalizing over the parameters:(45.7)If we have a Gaussian representation of the posterior P (w | t N , X N ), then this integral can typically be evaluated directly.In the alternative Monte Carlo approach, which generates R samples w (r) that are intended to be samples from the posterior distribution P (w | t N , X N ), we approximate the predictive distribution byNonparametric approaches.In nonparametric methods, predictions are obtained without explicitly parameterizing the unknown function y(x); y(x) lives in the infinite-dimensional space of all continuous functions of x.One well known nonparametric approach to the regression problem is the spline smoothing method (Kimeldorf and Wahba, 1970).A spline solution to a one-dimensional regression problem can be described as follows: we define the estimator of y(x) to be the function ŷ(x) that minimizes the functional (45.9)where y (p) is the pth derivative of y and p is a positive number.If p is set to 2 then the resulting function ŷ(x) is a cubic spline, that is, a piecewise cubic function that has 'knots' -discontinuities in its second derivative -at the data points {x (n) }.This estimation method can be interpreted as a Bayesian method by identifying the prior for the function y(x) as: (45.10) and the probability of the data measurements t N = {t n } N n=1 assuming independent Gaussian noise as: (45.11) [The constants in equations (45.10) and (45.11) are functions of α and β respectively.Strictly the prior (45.10) is improper since addition of an arbitrary polynomial of degree (p − 1) to y(x) is not constrained.This impropriety is easily rectified by the addition of (p − 1) appropriate terms to (45.10).]Given this interpretation of the functions in equation (45.9),M (y(x)) is equal to minus the log of the posterior probability P (y(x) | t N , α, β), within an additive constant, and the splines estimation procedure can be interpreted as yielding a Bayesian MAP estimate.The Bayesian perspective allows us additionally to put error bars on the splines estimate and to draw typical samples from the posterior distribution, and it gives an automatic method for inferring the hyperparameters α and β.Splines priors are Gaussian processesThe prior distribution defined in equation (45.10) is our first example of a Gaussian process.Throwing mathematical precision to the winds, a Gaussian process can be defined as a probability distribution on a space of functions y(x) that can be written in the form (45.12) where µ(x) is the mean function and A is a linear operator, and where the inner product of two functions y(x) T z(x) is defined by, for example, dx y(x)z(x).Here, if we denote by D the linear operator that maps y(x) to the derivative of y(x), we can write equation (45.10) as (45.13) which has the same form as equation (45.12) with µ(x) = 0, and A ≡ [D p ] T D p .In order for the prior in equation ( 45.12) to be a proper prior, A must be a positive definite operator, i.e., one satisfying y(x) T Ay(x) > 0 for all functions y(x) other than y(x) = 0.Splines may be written in terms of an infinite set of fixed basis functions, as in equation ( 45.2), as follows.First rescale the x axis so that the interval (0, 2π) is much wider than the range of x values of interest.Let the basis functions be a Fourier set {cos hx, sin hx, h = 0, 1, 2, . ..}, so the function is(45.14)Use the regularizerto define a Gaussian prior on w,(45.16)If p = 2 then we have the cubic splines regularizer E W (w) = y (2) (x) 2 dx, as in equation (45.9); if p = 1 we have the regularizer E W (w) = y (1) (x) 2 dx, etc. (To make the prior proper we must add an extra regularizer on the term w 0(cos) .)Thus in terms of the prior P (y(x)) there is no fundamental difference between the 'nonparametric' splines approach and other parametric approaches.From the point of view of prediction at least, there are two objects of interest.The first is the conditional distribution P (t N +1 | t N , X N +1 ) defined in equation (45.7).The other object of interest, should we wish to compare one model with others, is the joint probability of all the observed data given the model, the evidence P (t N | X N ), which appeared as the normalizing constant in equation (45.5).Neither of these quantities makes any reference to the representation of the unknown function y(x).So at the end of the day, our choice of representation is irrelevant.The question we now address is, in the case of popular parametric models, what form do these two quantities take?We will see that for standard models with fixed basis functions and Gaussian distributions on the unknown parameters, the joint probability of all the observed data given the model, P (t N | X N ), is a multivariate Gaussian distribution with mean zero and with a covariance matrix determined by the basis functions; this implies that the conditional distribution P (t N +1 | t N , X N +1 ) is also a Gaussian distribution, whose mean depends linearly on the values of the targets t N .Standard parametric models are simple examples of Gaussian processes.Let us consider a regression problem using H fixed basis functions, for example one-dimensional radial basis functions as defined in equation ( 45.3).Let us assume that a list of N input points {x (n) } has been specified and define the N × H matrix R to be the matrix of values of the basis functions {φ h (x)} H h=1 at the points {x n },(45.17)We define the vector y N to be the vector of values of y(x) at the N points,If the prior distribution of w is Gaussian with zero mean, (45.19) then y, being a linear function of w, is also Gaussian distributed, with mean zero.The covariance matrix of y isSo the prior distribution of y is: (45.22)This result, that the vector of N function values y has a Gaussian distribution, is true for any selected points X N .This is the defining property of a Gaussian process.The probability distribution of a function y(x) is a Gaussian process if for any finite selection of points x (1) , x (2) , . . ., x (N ) , the density P (y(x (1) ), y(x (2) ), . . ., y(x (N ) )) is a Gaussian.Now, if the number of basis functions H is smaller than the number of data points N , then the matrix Q will not have full rank.In this case the probability distribution of y might be thought of as a flat elliptical pancake confined to an H-dimensional subspace in the N -dimensional space in which y lives.What about the target values?If each target t n is assumed to differ by additive Gaussian noise of variance σ 2 ν from the corresponding function value y n then t also has a Gaussian prior distribution,(45.23)We will denote the covariance matrix of t by C: (45.24)Whether or not Q has full rank, the covariance matrix C has full rank since σ 2 ν I is full rank.What does the covariance matrix Q look like?In general, the (n, n45.2: From parametric models to Gaussian processes 541 and the (n, n ) entry of C is (45.26)where δ nn = 1 if n = n and 0 otherwise.Example 45.4.Let's take as an example a one-dimensional case, with radial basis functions.The expression for Q nn becomes simplest if we assume we have uniformly-spaced basis functions with the basis function labelled h centred on the point x = h, and take the limit H → ∞, so that the sum over h becomes an integral; to avoid having a covariance that diverges with H, we had better make σ 2 w scale as S/(∆H), where ∆H is the number of basis functions per unit length of the x-axis, and S is a constant; thenIf we let the limits of integration be ±∞, we can solve this integral:We are arriving at a new perspective on the interpolation problem.Instead of specifying the prior distribution on functions in terms of basis functions and priors on parameters, the prior can be summarized simply by a covariance function, (45.30)where we have given a new name, θ 1 , to the constant out front.Generalizing from this particular case, a vista of interpolation methods opens up.Given any valid covariance function C(x, x ) -we'll discuss in a moment what 'valid' means -we can define the covariance matrix for N function values at locations X N to be the matrix Q given byand the covariance matrix for N corresponding target values, assuming Gaussian noise, to be the matrix C given by (45.32)In conclusion, the prior probability of the N target values t in the data set is:Figures 44.2 and 44.3 show some random samples from the prior distribution over functions defined by a selection of standard multilayer perceptrons with large numbers of hidden units.Those samples don't seem a million miles away from the Gaussian process samples of figure 45.1.And indeed Neal (1996) showed that the properties of a neural network with one hidden layer (as in equation (45.4)) converge to those of a Gaussian process as the number of hidden neurons tends to infinity, if standard 'weight decay' priors are assumed.The covariance function of this Gaussian process depends on the details of the priors assumed for the weights in the network and the activation functions of the hidden units.We have spent some time talking about priors.We now return to our data and the problem of prediction.How do we make predictions with a Gaussian process?Having formed the covariance matrix C defined in equation (45.32) our task is to infer t N +1 given the observed vector t N .The joint density P (t N +1 , t N ) is a Gaussian; so the conditional distributionis also a Gaussian.We now distinguish between different sizes of covariance matrix C with a subscript, such that C N +1 is the (N + 1) × (N + 1) covariancematrix for the vector t N +1 ≡ (t 1 , . . ., t N +1 ) T .We define submatrices of C N +1 as follows:The posterior distribution (45.34) is given by. (45.36)We can evaluate the mean and standard deviation of the posterior distribution of t N +1 by brute-force inversion of C N +1 .There is a more elegant expression for the predictive distribution, however, which is useful whenever predictions are to be made at a number of new points on the basis of the data set of size N .We can write C −1 N +1 in terms of C N and C −1 N using the partitioned inverse equations (Barnett, 1979): (45.37)whereWhen we substitute this matrix into equation (45.36) we findThe predictive mean at the new point is given by tN+1 and σ tN+1 defines the error bars on this prediction.Notice that we do not need to invert C N +1 in order to make predictions at x (N +1) .Only C N needs to be inverted.Thus Gaussian processes allow one to implement a model with a number of basis functions H much larger than the number of data points N , with the computational requirement being of order N 3 , independent of H. [We'll discuss ways of reducing this cost later.]The predictions produced by a Gaussian process depend entirely on the covariance matrix C. We now discuss the sorts of covariance functions one might choose to define C, and how we can automate the selection of the covariance function in response to data.The only constraint on our choice of covariance function is that it must generate a non-negative-definite covariance matrix for any set of points {x n } N n=1 .We will denote the parameters of a covariance function by θ.The covariance matrix of t has entries given bywhere C is the covariance function and N is a noise model which might be stationary or spatially varying, for example,for input-dependent noise.(45.45)The continuity properties of C determine the continuity properties of typical samples from the Gaussian process prior.An encyclopaedic paper on Gaussian processes giving many valid covariance functions has been written by Abrahamsen (1997).A stationary covariance function is one that is translation invariant in that it satisfiesfor some function D, i.e., the covariance is a function of separation only, also known as the autocovariance function.If additionally C depends only on the magnitude of the distance between x and x then the covariance function is said to be homogeneous.Stationary covariance functions may also be described in terms of the Fourier transform of the function D, which is known as the power spectrum of the Gaussian process.This Fourier transform is necessarily a positive function of frequency.One way of constructing a valid stationary covariance function is to invent a positive function of frequency and define D to be its inverse Fourier transform.Example 45.5.Let the power spectrum be a Gaussian function of frequency.Since the Fourier transform of a Gaussian is a Gaussian, the autocovariance function corresponding to this power spectrum is a Gaussian function of separation.This argument rederives the covariance function we derived at equation (45.30).Generalizing slightly, a popular form for C with hyperparameters θx is an I-dimensional vector and r i is a lengthscale associated with input x i , the lengthscale in direction i on which y is expected to vary significantly.A very large lengthscale means that y is expected to be essentially a constant function of that input.Such an input could be said to be irrelevant, as in the automatic relevance determination method for neural networks (MacKay, 1994a;Neal, 1996).The θ 1 hyperparameter defines the vertical scale of variations of a typical function.The θ 2 hyperparameter allows the whole function to be offset away from zero by some unknown constant -to understand this term, examine equation (45.25) and consider the basis function φ(x) = 1.Another stationary covariance function is  (1997).For ν = 2, this is a special case of the previous covariance function.For ν ∈ (1, 2), the typical functions from this prior are smooth but not analytic functions.For ν ≤ 1 typical functions are continuous but not smooth.A covariance function that models a function that is periodic with known period λ i in the i th input direction is(45.49)Figure 45.1 shows some random samples drawn from Gaussian processes with a variety of different covariance functions.The simplest nonstationary covariance function is the one corresponding to a linear trend.Consider the plane y(x) = i w i x i + c.If the {w i } and c have Gaussian distributions with zero mean and variances σ 2 w and σ 2 c respectively then the plane has a covariance function (45.50)An example of random sample functions incorporating the linear term can be seen in figure 45.1d.Let us assume that a form of covariance function has been chosen, but that it depends on undetermined hyperparameters θ.We would like to 'learn' these hyperparameters from the data.This learning process is equivalent to the inference of the hyperparameters of a neural network, for example, weight decay hyperparameters.It is a complexity-control problem, one that is solved nicely by the Bayesian Occam's razor.Ideally we would like to define a prior distribution on the hyperparameters and integrate over them in order to make our predictions, i.e., we would like to find(45.51)But this integral is usually intractable.There are two approaches we can take.1. We can approximate the integral by using the most probable values of hyperparameters.2. Or we can perform the integration over θ numerically using Monte Carlo methods (Williams and Rasmussen, 1996;Neal, 1997b).Either of these approaches is implemented most efficiently if the gradient of the posterior probability of θ can be evaluated.The posterior probability of θ is(45.53)The log of the first term (the evidence for the hyperparameters) is (45.54) and its derivative with respect to a hyperparameter θ is(45.55)Assuming that finding the derivatives of the priors is straightforward, we can now search for θ MP .However there are two problems that we need to be aware of.Firstly, as illustrated in figure 45.2, the evidence may be multimodal.Suitable priors and sensible optimization strategies often eliminate poor optima.Secondly and perhaps most importantly the evaluation of the gradient of the log likelihood requires the evaluation of C −1 N .Any exact inversion method (such as Cholesky decomposition, LU decomposition or Gauss-Jordan elimination) has an associated computational cost that is of order N 3 and so calculating gradients becomes time consuming for large training data sets.Approximate methods for implementing the predictions (equations (45.42) and (45.43)) and gradient computation (equation (45.55)) are an active research area.One approach based on the ideas of Skilling (1993) makes approximations to C −1 t and Trace C −1 using iterative methods with cost O(N 2 ) (Gibbs and MacKay, 1996;Gibbs, 1997).Further references on this topic are given at the end of the chapter.Gaussian processes can be integrated into classification modelling once we identify a variable that can sensibly be given a Gaussian process prior.In a binary classification problem, we can define a quantity a n ≡ a(x (n) ) such that the probability that the class is 1 rather than 0 is(45.56)Large positive values of a correspond to probabilities close to one; large negative values of a define probabilities that are close to zero.In a classification problem, we typically intend that the probability P (t n = 1) should be a smoothly varying function of x.We can embody this prior belief by defining a(x) to have a Gaussian process prior.It is not so easy to perform inferences and adapt the Gaussian process model to data in a classification model as in regression problems because the likelihood function (45.56) is not a Gaussian function of a n .So the posterior distribution of a given some observations t is not Gaussian and the normalization constant P (t N | X N ) cannot be written down analytically.Barber and Williams (1997) have implemented classifiers based on Gaussian process priors using Laplace approximations (Chapter 27).Neal (1997b) has implemented a Monte Carlo approach to implementing a Gaussian process classifier.Gibbs and MacKay (2000) have implemented another cheap and cheerful approach based on the methods of Jaakkola and Jordan (section 33.8).In this variational Gaussian process classifier, we obtain tractable upper and lower bounds for the unnormalized posterior density over a, P (t N | a)P (a).These bounds are parameterized by variational parameters which are adjusted in order to obtain the tightest possible fit.Using normalized versions of the optimized bounds we then compute approximations to the predictive distributions.Multi-class classification problems can also be solved with Monte Carlo methods (Neal, 1997b) and variational methods (Gibbs, 1997).Gaussian processes are moderately simple to implement and use.Because very few parameters of the model need to be determined by hand (generally only the priors on the hyperparameters), Gaussian processes are useful tools for automated tasks where fine tuning for each problem is not possible.We do not appear to sacrifice any performance for this simplicity.It is easy to construct Gaussian processes that have particular desired properties; for example we can make a straightforward automatic relevance determination model.One obvious problem with Gaussian processes is the computational cost associated with inverting an N × N matrix.The cost of direct methods of inversion becomes prohibitive when the number of data points N is greater than about 1000.Have we thrown the baby out with the bath water?According to the hype of 1987, neural networks were meant to be intelligent models that discovered features and patterns in data.Gaussian processes in contrast are simply smoothing devices.How can Gaussian processes possibly replace neural networks?Were neural networks over-hyped, or have we underestimated the power of smoothing methods?I think both these propositions are true.The success of Gaussian processes shows that many real-world data modelling problems are perfectly well solved by sensible smoothing methods.The most interesting problems, the task of feature discovery for example, are not ones that Gaussian processes will solve.But maybe multilayer perceptrons can't solve them either.Perhaps a fresh start is needed, approaching the problem of machine learning from a paradigm different from the supervised feedforward mapping.The study of Gaussian processes for regression is far from new.Time series analysis was being performed by the astronomer T.N.Thiele using Gaussian processes in 1880 (Lauritzen, 1981).In the 1940s, Wiener-Kolmogorov prediction theory was introduced for prediction of trajectories of military targets (Wiener, 1948).Within the geostatistics field, Matheron (1963) proposed a framework for regression using optimal linear estimators which he called 'kriging' after D.G. Krige, a South African mining engineer.This framework is identical to the Gaussian process approach to regression.Kriging has been developed considerably in the last thirty years (see Cressie (1993) for a review) including several Bayesian treatments (Omre, 1987;Kitanidis, 1986).However the geostatistics approach to the Gaussian process model has concentrated mainly on low-dimensional problems and has largely ignored any probabilistic interpretation of the model.Kalman filters are widely used to implement inferences for stationary one-dimensional Gaussian processes, and are popular models for speech and music modelling (Bar-Shalom and Fortmann, 1988).Generalized radial basis functions (Poggio and Girosi, 1989), ARMA models (Wahba, 1990) and variable metric kernel methods (Lowe, 1995) are all closely related to Gaussian processes.See also O'Hagan (1978).The idea of replacing supervised neural networks by Gaussian processes was first explored by Williams and Rasmussen (1996) and Neal (1997b).A thorough comparison of Gaussian processes with other methods such as neural networks and MARS was made by Rasmussen (1996).Methods for reducing the complexity of data modelling with Gaussian processes remain an active research area (Poggio and Girosi, 1990;Luo and Wahba, 1997;Tresp, 2000;Williams and Seeger, 2001;Smola and Bartlett, 2001;Rasmussen, 2002;Seeger et al., 2003;Opper and Winther, 2000).A longer review of Gaussian processes is in (MacKay, 1998b).A review paper on regression with complexity control using hierarchical Bayesian models is (MacKay, 1992a).Gaussian processes and support vector learning machines (Scholkopf et al., 1995;Vapnik, 1995) have a lot in common.Both are kernel-based predictors, the kernel being another name for the covariance function.A Bayesian version of support vectors, exploiting this connection, can be found in (Chu et al., 2001;Chu et al., 2002;Chu et al., 2003b;Chu et al., 2003a).In many imaging problems, the data measurements {d n } are linearly related to the underlying image f :(46.1)The vector n denotes the inevitable noise that corrupts real data.In the case of a camera which produces a blurred picture, the vector f denotes the true image, d denotes the blurred and noisy picture, and the linear operator R is a convolution defined by the point spread function of the camera.In this special case, the true image and the data vector reside in the same space; but it is important to maintain a distinction between them.We will use the subscript n = 1, . . ., N to run over data measurements, and the subscripts k, k = 1, . . ., K to run over image pixels.One might speculate that since the blur was created by a linear operation, then perhaps it might be deblurred by another linear operation.We can derive the optimal linear filter in two ways.We assume that the linear operator R is known, and that the noise n is Gaussian and independent, with a known standard deviation σ ν .We assume that the prior probability of the image is also Gaussian, with a scale parameter σ f .If we assume no correlations among the pixels then the symmetric, full rank matrix C is equal to the identity matrix I.The more sophisticated 'intrinsic correlation function' model uses C = [GG T ] −1 , where G is a convolution that takes us from an imaginary 'hidden' image, which is uncorrelated, to the real correlated image.The intrinsic correlation function should not be confused with the point spread function R which defines the image-to-data mapping.A zero-mean Gaussian prior is clearly a poor assumption if it is known that all elements of the image f are positive, but let us proceed.We can now write down the posterior probability of an image f given the data d.In words, Posterior = Likelihood × Prior Evidence .(46.5)The 'evidence' P (d | σ ν , σ f , H) is the normalizing constant for this posterior distribution.Here it is unimportant, but it is used in a more sophisticated analysis to compare, for example, different values of σ ν and σ f , or different point spread functions R.Since the posterior distribution is the product of two Gaussian functions of f , it is also a Gaussian, and can therefore be summarized by its mean, which is also the most probable image, f MP , and its covariance matrix: (46.6) which defines the joint error bars on f .In this equation, the symbol ∇ denotes differentiation with respect to the image parameters f .We can find f MP by differentiating the log of the posterior, and solving for the derivative being zero.We obtain:The operatorR T is called the optimal linear filter.When the term σ 2 ν σ 2 f C can be neglected, the optimal linear filter is the pseudoinverseThe optimal linear filter can also be manipulated into the form:The non-Bayesian derivation of the optimal linear filter starts by assuming that we will 'estimate' the true image f by a linear function of the data:The linear operator W is then 'optimized' by minimizing the expected sumsquared error between f and the unknown true image .In the following equations, summations over repeated indices k, k , n are implicit.The expectation • is over both the statistics of the random variables {n n }, and the ensemble of images f which we expect to bump into.We assume that the noise is zero mean and uncorrelated to second order with itself and everything else, with (46.11)Differentiating with respect to W, and introducing F ≡ f j f j (cf.σ 2 f C −1 in the Bayesian derivation above), we find that the optimal linear filter is:(46.12)If we identify F = σ 2 f C −1 , we obtain the optimal linear filter (46.8) of the Bayesian derivation.The ad hoc assumptions made in this derivation were the choice of a quadratic error measure, and the decision to use a linear estimator.It is interesting that without explicit assumptions of Gaussian distributions, this derivation has reproduced the same estimator as the Bayesian posterior mode, f MP .The advantage of a Bayesian approach is that we can criticize these assumptions and modify them in order to make better reconstructions.The better matched our model of images P (f | H) is to the real world, the better our image reconstructions will be, and the less data we will need to answer any given question.The Gaussian models which lead to the optimal linear filter are spectacularly poorly matched to the real world.For example, the Gaussian prior (46.3) fails to specify that all pixel intensities in an image are positive.This omission leads to the most pronounced artefacts where the image under observation has high contrast or large black patches.Optimal linear filters applied to astronomical data give reconstructions with negative areas in them, corresponding to patches of sky that suck energy out of telescopes!The maximum entropy model for image deconvolution (Gull and Daniell, 1978) was a great success principally because this model forced the reconstructed image to be positive.The spurious negative areas and complementary spurious positive areas are eliminated, and the quality of the reconstruction is greatly enhanced.The 'classic maximum entropy' model assigns an entropic priorwhere (Skilling, 1989).This model enforces positivity; the parameter α defines a characteristic dynamic range by which the pixel values are expected to differ from the default image m.The 'intrinsic-correlation-function maximum-entropy' model (Gull, 1989) introduces an expectation of spatial correlations into the prior on f by writing f = Gh, where G is a convolution with an intrinsic correlation function, and putting a classic maxent prior on the underlying hidden image h.Having found not only the most probable image f MP but also error bars on it, Σ f |d , one task is to visualize those error bars.Whether or not we use Monte Carlo methods to infer f , a correlated random walk around the posterior distribution can be used to visualize the uncertainties and correlations.For a Gaussian posterior distribution, we can create a correlated sequence of unit normal random vectors n using (46.15)where z is a unit normal random vector and c 2 + s 2 = 1 (c controls how persistent the memory of the sequence is).We then render the image sequence defined bywhereNeural network researchers often exploit the following strategy.Given a problem currently solved with a standard algorithm: interpret the computations performed by the algorithm as a parameterized mapping from an input to an output, and call this mapping a neural network; then adapt the parameters to data so as to produce another mapping that solves the task better.By construction, the neural network can reproduce the standard algorithm, so this data-driven adaptation can only make the performance better.There are several reasons why standard algorithms can be bettered in this way.1. Algorithms are often not designed to optimize the real objective function.For example, in speech recognition, a hidden Markov model is designed to model the speech signal, and is fitted so as to to maximize the generative probability given the known string of words in the training data; but the real objective is to discriminate between different words.If an inadequate model is being used, the neural-net-style training of the model will focus the limited resources of the model on the aspects relevant to the discrimination task.Discriminative training of hidden Markov models for speech recognition does improve their performance.2. The neural network can be more flexible than the standard model; some of the adaptive parameters might have been viewed as fixed features by the original designers.A flexible network can find properties in the data that were not included in the original model.A huge fraction of our brain is devoted to vision.One of the neglected features of our visual system is that the raw image falling on the retina is severely blurred: while most people can see with a resolution of about 1 arcminute (one sixtieth of a degree) under any daylight conditions, bright or dim, the image on our retina is blurred through a point spread function of width as large as 5 arcminutes (Wald and Griffin, 1947;Howarth and Bradley, 1986).It is amazing that we are able to resolve pixels that are twenty-five times smaller in area than the blob produced on our retina by any point source.Isaac Newton was aware of this conundrum.It's hard to make a lens that does not have chromatic aberration, and our cornea and lens, like a lens made of ordinary glass, refract blue light more strongly than red.Typically our eyes focus correctly for the middle of the visible spectrum (green), so if we look at a single white dot made of red, green, and blue light, the image on our retina consists of a sharply focussed green dot surrounded by a broader red blob superposed on an even broader blue blob.The width of the red and blue blobs is proportional to the diameter of the pupil, which is largest under dim lighting conditions.[The blobs are roughly concentric, though most people have a slight bias, such that in one eye the red blob is centred a tiny distanceto the left and the blue is centred a tiny distance to the right, and in the other eye it's the other way round.This slight bias explains why when we look at blue and red writing on a dark background most people perceive the blue writing to be at a slightly greater depth than the red.In a minority of people, this small bias is the other way round and the red/blue depth perception is reversed.But this effect (which many people are aware of, having noticed it in cinemas, for example) is tiny compared with the chromatic aberration we are discussing.]You can vividly demonstrate to yourself how enormous the chromatic aberration in your eye is with the help of a sheet of card and a colour computer screen.For the most impressive results -I guarantee you will be amazed -use a dim room with no light apart from the computer screen; a pretty strong effect will still be seen even if the room has daylight coming into it, as long as it is not bright sunshine.Cut a slit about 1.5 mm wide in the card.On the screen, display a few small coloured objects on a black background.I especially recommend thin vertical objects coloured pure red, pure blue, magenta (i.e., red plus blue), and white (red plus blue plus green). 1 Include a little blackand-white text on the screen too.Stand or sit sufficiently far away that you can only just read the text -perhaps a distance of four metres or so, if you have normal vision.Now, hold the slit vertically in front of one of your eyes, and close the other eye.Hold the slit near to your eye -brushing your eyelashesand look through it.Waggle the slit slowly to the left and to the right, so that the slit is alternately in front of the left and right sides of your pupil.What do you see?I see the red objects waggling to and fro, and the blue objects waggling to and fro, through huge distances and in opposite directions, while white objects appear to stay still and are negligibly distorted.Thin magenta objects can be seen splitting into their constituent red and blue parts.Measure how large the motion of the red and blue objects is -it's more than 5 minutes of arc for me, in a dim room.Then check how sharply you can see under these conditions -look at the text on the screen, for example: is it not the case that you can see (through your whole pupil) features far smaller than the distance through which the red and blue components were waggling?Yet when you are using the whole pupil, what is falling on your retina must be an image blurred with a blurring diameter equal to the waggling amplitude.One of the main functions of early visual processing must be to deconvolve this chromatic aberration.Neuroscientists sometimes conjecture that the reason why retinal ganglion cells and cells in the lateral geniculate nucleus (the main brain area to which retinal ganglion cells project) have centre-surround receptive fields with colour opponency (long wavelength in the centre and medium wavelength in the surround, for example) is in order to perform 'feature extraction' or 'edge detection', but I think this view is mistaken.The reason we have centre-surround filters at the first stage of visual processing (in the fovea at least) is for the huge task of deconvolution of chromatic aberration.I speculate that the McCollough effect, an extremely long-lasting association of colours with orientation (McCollough, 1965;MacKay and MacKay, 1974), is produced by the adaptation mechanism that tunes our chromaticaberration-deconvolution circuits.Our deconvolution circuits need to be rapidly tuneable, because the point spread function of our eye changes with our pupil diameter, which can change within seconds; and indeed the McCollough effect can be induced within 30 seconds.At the same time, the effect is long-lasting when an eye is covered, because it's in our interests that our deconvolution circuits should stay well-tuned while we sleep, so that we can see sharply the instant we wake up.I also wonder whether the main reason that we evolved colour vision was not 'in order to see fruit better' but 'so as to be able to see black and white sharper' -deconvolving chromatic aberration is easier, even in an entirely black and white world, if one has access to chromatic information in the image.And a final speculation: why do our eyes make micro-saccades when we look at things?These miniature eye-movements are of an angular size bigger than the spacing between the cones in the fovea (which are spaced at roughly 1 minute of arc, the perceived resolution of the eye).The typical size of a microsaccade is 5-10 minutes of arc (Ratliff and Riggs, 1950).Is it a coincidence that this is the same as the size of chromatic aberration?Surely micro-saccades must play an essential role in the deconvolution mechanism that delivers our high-resolution vision.3C ] Blur an image with a circular (top hat) point spread function and add noise.Then deconvolve the blurry noisy image using the optimal linear filter.Find error bars and visualize them by making a probabilistic movie.Sparse Graph Codes 47A low-density parity-check code (or Gallager code) is a block code that has a parity-check matrix, H, every row and column of which is 'sparse'.A regular Gallager code is a low-density parity-check code in which every column of H has the same weight j and every row has the same weight k; regular Gallager codes are constructed at random subject to these constraints.A low-density parity-check code with j = 3 and k = 4 is illustrated in figure 47.1.Each bit participates in j = 3 constraints, represented by squares.Each constraint forces the sum of the k = 4 bits to which it is connected to be even.Low-density parity-check codes lend themselves to theoretical study.The following results are proved in Gallager (1963) and MacKay (1999b).Low-density parity-check codes, in spite of their simple construction, are good codes, given an optimal decoder (good codes in the sense of section 11.4).Furthermore, they have good distance (in the sense of section 13.2).These two results hold for any column weight j ≥ 3. Furthermore, there are sequences of low-density parity-check codes in which j increases gradually with N , in such a way that the ratio j/N still goes to zero, that are very good, and that have very good distance.However, we don't have an optimal decoder, and decoding low-density parity-check codes is an NP-complete problem.So what can we do in practice?Given a channel output r, we wish to find the codeword t whose likelihood P (r | t) is biggest.All the effective decoding strategies for low-density paritycheck codes are message-passing algorithms.The best algorithm known is the sum-product algorithm, also known as iterative probabilistic decoding or belief propagation.We'll assume that the channel is a memoryless channel (though more complex channels can easily be handled by running the sum-product algorithm on a more complex graph that represents the expected correlations among the errors (Worthen and Stark, 1998)).For any memoryless channel, there are two approaches to the decoding problem, both of which lead to the generic problem 'find the x that maximizeswhere P (x) is a separable distribution on a binary vector x, and z is another binary vector.Each of these two approaches represents the decoding problem in terms of a factor graph (Chapter 26).t n (a) The prior distribution over codewordsThe variable nodes are the transmitted bits {t n }.Each node represents the factor [ n∈N (m) t n = 0 mod 2].The posterior distribution over codewords,Each upper function node represents a likelihood factor P (r n | t n ).The joint probability of the noise n and syndrome z,The top variable nodes are now the noise bits {n n }.The added variable nodes at the base are the syndrome values {z m }.Each definition z m = n H mn n n mod 2 is enforced by a factor.First, we note that the prior distribution over codewords,can be represented by a factor graph (figure 47.2a), with the factorization beingt n = 0 mod 2].(47.3) (We'll omit the 'mod 2's from now on.)The posterior distribution over codewords is given by multiplying this prior by the likelihood, which introduces another N factors, one for each received bit.The factor graph corresponding to this function is shown in figure 47.2b.It is the same as the graph for the prior, except for the addition of likelihood 'dongles' to the transmitted bits.In this viewpoint, the received signal r n can live in any alphabet; all that matters are the values of P (r n | t n ).Alternatively, we can view the channel output in terms of a binary received vector r and a noise vector n, with a probability distribution P (n) that can be derived from the channel properties and whatever additional information is available at the channel outputs.For example, with a binary symmetric channel, we define the noise by r = t + n, the syndrome z = Hr, and noise model P (n n = 1) = f .For other channels such as the Gaussian channel with output y, we may define a received binary vector r however we wish and obtain an effective binary noise model P (n) from y (exercises 9.18 (p.155) and 25.1 (p.325)).The joint probability of the noise n and syndrome z = Hn can be factored as(47.5)The factor graph of this function is shown in figure 47.2c.The variables n and z can also be drawn in a 'belief network' (also known as a 'Bayesian network', 'causal network', or 'influence diagram') similar to figure 47.2a, but with arrows on the edges from the upper circular nodes (which represent the variables n) to the lower square nodes (which now represent the variables z).We can say that every bit x n is the parent of j checks z m , and each check z m is the child of k bits.Both decoding viewpoints involve essentially the same graph.Either version of the decoding problem can be expressed as the generic decoding problem 'find the x that maximizes (47.6) in the codeword decoding viewpoint, x is the codeword t, and z is 0; in the syndrome decoding viewpoint, x is the noise n, and z is the syndrome.It doesn't matter which viewpoint we take when we apply the sum-product algorithm.The two decoding algorithms are isomorphic and will give equivalent outcomes (unless numerical errors intervene).I tend to use the syndrome decoding viewpoint because it has one advantage: one does not need to implement an encoder for a code in order to be able to simulate a decoding problem realistically.We'll now talk in terms of the generic decoding problem.We aim, given the observed checks, to compute the marginal posterior probabilities P (x n = 1 | z, H) for each n.It is hard to compute these exactly because the graph contains many cycles.However, it is interesting to implement the decoding algorithm that would be appropriate if there were no cycles, on the assumption that the errors introduced might be relatively small.This approach of ignoring cycles has been used in the artificial intelligence literature but is now frowned upon because it produces inaccurate probabilities.However, if we are decoding a good error-correcting code, we don't care about accurate marginal probabilities -we just want the correct codeword.Also, the posterior probability, in the case of a good code communicating at an achievable rate, is expected typically to be hugely concentrated on the most probable decoding; so we are dealing with a distinctive probability distribution to which experience gained in other fields may not apply.The sum-product algorithm was presented in Chapter 26.We now write out explicitly how it works for solving the decoding problem Hx = z (mod 2).For brevity, we reabsorb the dongles hanging off the x and z nodes in figure 47.2c and modify the sum-product algorithm accordingly.The graph in which x and z live is then the original graph (figure 47.2a) whose edges are defined by the 1s in H.The graph contains nodes of two types, which we'll call checks and bits.The graph connecting the checks and bits is a bipartite graph: bits connect only to checks, and vice versa.On each iteration, a probability ratio is propagated along each edge in the graph, and each bit node x n updates its probability that it should be in state 1.We denote the set of bits n that participate in check m by N (m) ≡ {n : H mn = 1}.Similarly we define the set of checks in which bit n participates, M(n) ≡ {m : H mn = 1}.We denote a set N (m) with bit n excluded by N (m)\n.The algorithm has two alternating parts, in which quantities q mn and r mn associated with each edge in the graph are iteratively updated.The quantity q x mn is meant to be the probability that bit n of x has the value x, given the information obtained via checks other than check m.The quantity r x mn is meant to be the probability of check m being satisfied if bit n of x is considered fixed at x and the other bits have a separable distribution given by the probabilities {q mn : n ∈ N (m)\n}.The algorithm would produce the exact posterior probabilities of all the bits after a fixed number of iterations if the bipartite graph defined by the matrix H contained no cycles.Initialization.Let p 0 n = P (x n = 0) (the prior probability that bit x n is 0), and let p 1 n = P (x n = 1) = 1 − p 0 n .If we are taking the syndrome decoding viewpoint and the channel is a binary symmetric channel then p 1 n will equal f .If the noise level varies in a known way (for example if the channel is a binary-input Gaussian channel with a real output) then p 1 n is initialized to the appropriate normalized likelihood.For every (n, m) such that H mn = 1 the variables q 0 mn and q 1 mn are initialized to the values p 0 n and p 1 n respectively.Horizontal step.In the horizontal step of the algorithm (horizontal from the point of view of the matrix H), we run through the checks m and compute for each n ∈ N (m) two probabilities: first, r 0 mn , the probability of the observed value of z m arising when x n = 0, given that the other bits {x n : n = n} have a separable distribution given by the probabilities {q 0 mn , q 1 mn }, defined by:x n mn (47.7) and second, r 1 mn , the probability of the observed value of z m arising when x n = 1, defined by:(47.8)The conditional probabilities in these summations are either zero or one, depending on whether the observed z m matches the hypothesized values for x n and the {x n }.These probabilities can be computed in various obvious ways based on equation (47.7) and (47.8).The computations may be done most efficiently (if |N (m)| is large) by regarding z m +x n as the final state of a Markov chain with states 0 and 1, this chain being started in state 0, and undergoing transitions corresponding to additions of the various x n , with transition probabilities given by the corresponding q 0 mn and q 1 mn .The probabilities for z m having its observed value given either x n = 0 or x n = 1 can then be found efficiently by use of the forward-backward algorithm (section 25.3).A particularly convenient implementation of this method uses forward and backward passes in which products of the differences δq mn ≡ q 0 mn − q 1 mn are computed.We obtain δr mn ≡ r 0 mn − r 1 mn from the identity:This identity is derived by iterating the following observation: if ζ = x µ + x ν mod 2, and x µ and x ν have probabilities q 0 µ , q 0 ν and q 1 µ , q 1 ν of being 0 and 1, then P (ζ = 1) = q 1 µ q 0 ν + q 0 µ q 1 ν and P (ζ = 0) = q 0 µ q 0 ν + q 1 µ q 1 ν .Thus P (ζ = 0) − P (ζ = 1) = (q 0 µ − q 1 µ )(q 0 ν − q 1 ν ).We recover r 0 mn and r 1 mn using(47.10)The transformations into differences δq and back from δr to {r} may be viewed as a Fourier transform and an inverse Fourier transformation.Vertical step.The vertical step takes the computed values of r 0 mn and r 1 mn and updates the values of the probabilities q 0 mn and q 1 mn .For each n we compute:where α mn is chosen such that q 0 mn +q 1 mn = 1.These products can be efficiently computed in a downward pass and an upward pass.We can also compute the 'pseudoposterior probabilities' q 0 n and q 1 n at this iteration, given by:r 0 mn , (47.13) (47.14)These quantities are used to create a tentative decoding x, the consistency of which is used to decide whether the decoding algorithm can halt.(Halt if Hx = z.)At this point, the algorithm repeats from the horizontal step.The stop-when-it's-done decoding method.The recommended decoding procedure is to set xn to 1 if q 1 n > 0.5 and see if the checks Hx = z mod 2 are all satisfied, halting when they are, and declaring a failure if some maximum number of iterations (e.g.200 or 1000) occurs without successful decoding.In the event of a failure, we may still report x, but we flag the whole block as a failure.We note in passing the difference between this decoding procedure and the widespread practice in the turbo code community, where the decoding algorithm is run for a fixed number of iterations (irrespective of whether the decoder finds a consistent state at some earlier time).This practice is wasteful of computer time, and it blurs the distinction between undetected and detected errors.In our procedure, 'undetected' errors occur if the decoder finds an x parity bitssatisfying Hx = z mod 2 that is not equal to the true x. 'Detected' errors occur if the algorithm runs for the maximum number of iterations without finding a valid decoding.Undetected errors are of scientific interest because they reveal distance properties of a code.And in engineering practice, it would seem preferable for the blocks that are known to contain detected errors to be so labelled if practically possible.Cost.In a brute-force approach, the time to create the generator matrix scales as N 3 , where N is the block size.The encoding time scales as N 2 , but encoding involves only binary arithmetic, so for the block lengths studied here it takes considerably less time than the simulation of the Gaussian channel.Decoding involves approximately 6N j floating-point multiplies per iteration, so the total number of operations per decoded bit (assuming 20 iterations) is about 120t/R, independent of blocklength.For the codes presented in the next section, this is about 800 operations.The encoding complexity can be reduced by clever encoding tricks invented by Richardson and Urbanke (2001b) or by specially constructing the paritycheck matrix (MacKay et al., 1999).The decoding complexity can be reduced, with only a small loss in performance, by passing low-precision messages in place of real numbers (Richardson and Urbanke, 2001a).Figures 47.3-47.7 illustrate visually the conditions under which low-density parity-check codes can give reliable communication over binary symmetric channels and Gaussian channels.These demonstrations may be viewed as animations on the world wide web. 1 ).The high density of the generator matrix is illustrated in figure 47.3b and c by showing the change in the transmitted vector when one of the 10 000 source bits is altered.Of course, the source images shown here are highly redundant, and such images should really be compressed before encoding.Redundant images are chosen in these demonstrations to make it easier to see the correction process during the iterative decoding.The decoding algorithm does not take advantage of the redundancy of the source vector, and it would work in exactly the same way irrespective of the choice of source vector.The transmission is sent over a channel with noise level f = 7.5% and the received vector is shown in the upper left of figure 47.5.The subsequent pictures in figure 47.5 show the iterative probabilistic decoding process.The sequence of figures shows the best guess, bit by bit, given by the iterative decoder, after 0, 1, 2, 3, 10, 11, 12, and 13 iterations.The decoder halts after the 13th iteration when the best guess violates no parity checks.This final decoding is error free.In the case of an unusually noisy transmission, the decoding algorithm fails to find a valid decoding.For this code and a channel with f = 7.5%, such failures happen about once in every 100 000 transmissions. Figure 47.6 shows this error rate compared with the block error rates of classical error-correcting codes.The received transmission over a Gaussian channel with x/σ = 1.0, which corresponds to the Shannon limit.(b2) The probability distribution of the output y of the channel with x/σ = 1.0 for each of the two possible inputs.In figure 47.7 the left picture shows the received vector after transmission over a Gaussian channel with x/σ = 1.185.The greyscale represents the value of the normalized likelihood, P (y | t = 1) P (y | t = 1)+P (y | t = 0) .This signal-to-noise ratio x/σ = 1.185 is a noise level at which this rate-1/2 Gallager code communicates reliably (the probability of error is 10 −5 ).To show how close we are to the Shannon limit, the right panel shows the received vector when the signal-tonoise ratio is reduced to x/σ = 1.0, which corresponds to the Shannon limit for codes of rate 1/2.Figure 47.8 shows how the parameters N and j affect the performance of low-density parity-check codes.As Shannon would predict, increasing the blocklength leads to improved performance.The dependence on j follows a different pattern.Given an optimal decoder, the best performance would be obtained for the codes closest to random codes, that is, the codes with largest j.However, the sum-product decoder makes poor progress in dense graphs, so the best performance is obtained for a small value of j.Among the values Figure 47.10.Monte Carlo simulation of density evolution, following the decoding process for j = 4, k = 8.Each curve shows the average entropy of a bit as a function of number of iterations, as estimated by a Monte Carlo algorithm using 10 000 samples per iteration.The noise level of the binary symmetric channel f increases by steps of 0.005 from bottom graph (f = 0.010) to top graph (f = 0.100).There is evidently a threshold at about f = 0.075, above which the algorithm cannot determine x.From MacKay (1999b).As we'll discuss later, we can do even better by making the code even more irregular.One way to study the decoding algorithm is to imagine it running on an infinite tree-like graph with the same local topology as the Gallager code's graph.The larger the matrix H, the closer its decoding properties should approach those of the infinite graph.Imagine an infinite belief network with no loops, in which every bit x n connects to j checks and every check z m connects to k bits (figure 47.11).We consider the iterative flow of information in this network, and examine the average entropy of one bit as a function of number of iterations.At each iteration, a bit has accumulated information from its local network out to a radius equal to the number of iterations.Successful decoding will occur only if the average entropy of a bit decreases to zero as the number of iterations increases.The iterations of an infinite belief network can be simulated by Monte Carlo methods -a technique first used by Gallager (1963).Imagine a network of radius I (the total number of iterations) centred on one bit.Our aim is to compute the conditional entropy of the central bit x given the state z of all checks out to radius I. To evaluate the probability that the central bit is 1 given a particular syndrome z involves an I-step propagation from the outside of the network into the centre.At the ith iteration, probabilities r at radius I − i + 1 are transformed into qs and then into rs at radius I − i in a way that depends on the states x of the unknown bits at radius I − i.In the Monte Carlo method, rather than simulating this network exactly, which would take a time that grows exponentially with I, we create for each iteration a representative sample (of size 100, say) of the values of {r, x}.In the case of a regular network with parameters j, k, each new pair {r, x} in the list at the ith iteration is created by drawing the new x from its distribution and drawing at random with replacement (j − 1)(k − 1) pairs {r, x} from the list at the (i−1)th iteration; these are assembled into a tree fragment (figure 47.12) and the sum-product algorithm is run from top to bottom to find the new r value associated with the new node.As an example, the results of runs with j = 4, k = 8 and noise densities f between 0.01 and 0.10, using 10 000 samples at each iteration, are shown in figure 47.10.Runs with low enough noise level show a collapse to zero entropy after a small number of iterations, and those with high noise level decrease to a non-zero entropy corresponding to a failure to decode.The boundary between these two behaviours is called the threshold of the decoding algorithm for the binary symmetric channel.Figure 47.10 shows by Monte Carlo simulation that the threshold for regular (j, k) = (4, 8) codes is about 0.075.Richardson and Urbanke (2001a)For practical purposes, the computational cost of density evolution can be reduced by making Gaussian approximations to the probability distributions over the messages in density evolution, and updating only the parameters of these approximations.For further information about these techniques, which produce diagrams known as EXIT charts, see (ten Brink, 1999;Chung et al., 2001;ten Brink et al., 2002).Since the rediscovery of Gallager codes, two methods have been found for enhancing their performance.Table 47.14.Translation between GF (4) and binary for message symbols.First, we can make Gallager codes in which the variable nodes are grouped together into metavariables consisting of say 3 binary variables, and the check nodes are similarly grouped together into metachecks.As before, a sparse graph can be constructed connecting metavariables to metachecks, with a lot of freedom about the details of how the variables and checks within are wired up.One way to set the wiring is to work in a finite field GF (q) such as GF (4) or GF (8), define low-density parity-check matrices using elements of GF (q), and translate our binary messages into GF (q) using a mapping such as the one for GF (4) given in table 47.14.Now, when messages are passed during decoding, those messages are probabilities and likelihoods over conjunctions of binary variables.For example if each clump contains three binary variables then the likelihoods will describe the likelihoods of the eight alternative states of those bits.With carefully optimized constructions, the resulting codes over GF (4),Transforms over GF (2 k ) can be viewed as a sequence of binary transforms in each of k dimensions.The inverse transform is identical to the Fourier transform, except that we also divide by 2 k .Figure 47.17.Comparison of regular binary Gallager codes with irregular codes, codes over GF (q), and other outstanding codes of rate 1/ 4. From left (best performance) to right: Irregular low-density parity-check code over GF (8), blocklength 48 000 bits (Davey, 1999); JPL turbo code (JPL, 1996) blocklength 65 536; Regular low-density parity-check over GF (16), blocklength 24 448 bits (Davey and MacKay, 1998); Irregular binary low-density paritycheck code, blocklength 16 000 bits (Davey, 1999) The computational cost for decoding in GF (q) scales as q log q, if the appropriate Fourier transform is used in the check nodes: the update rule for the check-to-variable message, (47.15) is a convolution of the quantities q a mj , so the summation can be replaced by a product of the Fourier transforms of q a mj for j ∈ N (m)\n, followed by an inverse Fourier transform.The Fourier transform for GF (4) is shown in algorithm 47.16.The second way of improving Gallager codes, introduced by Luby et al. (2001b), is to make their graphs irregular.Instead of giving all variable nodes the same degree j, we can have some variable nodes with degree 2, some 3, some 4, and a few with degree 20.Check nodes can also be given unequal degrees -this helps improve performance on erasure channels, but it turns out that for the Gaussian channel, the best graphs have regular check degrees.Figure 47.17 illustrates the benefits offered by these two methods for improving Gallager codes, focussing on codes of rate about 0.6 dB; and Matthew Davey's code that combines both these featuresit's irregular over GF (8) -gives a win of about 0.9 dB over the regular binary Gallager code.Methods for optimizing the profile of a Gallager code (that is, its number of rows and columns of each degree), have been developed by Richardson et al. (2001) and have led to low-density parity-check codes whose performance, when decoded by the sum-product algorithm, is within a hair's breadth of the Shannon limit.The performance of regular Gallager codes can be enhanced in a third manner: by designing the code to have redundant sparse constraints.There is a difference-set cyclic code, for example, that has N = 273 and K = 191, but the code satisfies not M = 82 but N , i.e., 273 low-weight constraints (figure 47.18).It is impossible to make random Gallager codes that have anywhere near this much redundancy among their checks.The difference-set cyclic code performs about 0.7 dB better than an equivalent random Gallager code.An open problem is to discover codes sharing the remarkable properties of the difference-set cyclic codes but with different blocklengths and rates.I call this task the Tanner challenge.We now discuss methods for fast encoding of low-density parity-check codesfaster than the standard method, in which a generator matrix G is found by Gaussian elimination (at a cost of order M 3 ) and then each block is encoded by multiplying it by G (at a cost of order M 2 ).Certain low-density parity-check matrices with M columns of weight 2 or less can be encoded easily in linear time.For example, if the matrix has a staircase structure as illustrated by the right-hand side of , (47.16) and if the data s are loaded into the first K bits, then the M parity bits p can be computed from left to right in linear time.(47.17The cost of this encoding method is linear if the sparsity of H is exploited when computing the sums in (47.17).Richardson and Urbanke (2001b) demonstrated an elegant method by which the encoding cost of any low-density parity-check code can be reduced from the straightforward method's M 2 to a cost of N + g 2 , where g, the gap, is hopefully a small constant, and in the worst cases scales as a small fraction of N .In the first step, the parity-check matrix is rearranged, by row-interchange and column-interchange, into the approximate lower-triangular form shown in figure 47.19.The original matrix H was very sparse, so the six matrices A, B, T, C, D, and E are also very sparse.The matrix T is lower triangular and has 1s everywhere on the diagonal.(47.19)This can be done in linear time.2. Find a setting of the second parity bits, p A 2 , such that the upper syndrome is zero.(47.20)This vector can be found in linear time by back-substitution, i.e., computing the first bit of p A 2 , then the second, then the third, and so forth.47.8: Further reading 5713. Compute the lower syndrome of the vector [s, 0, p A 2 ]: (47.21)This can be done in linear time.4. Now we get to the clever bit.Define the matrix (47.22) and find its inverse, F −1 .This computation needs to be done once only, and its cost is of order g 3 .This inverse F −1 is a dense g × g matrix.[If F is not invertible then either H is not of full rank, or else further column permutations of H can produce an F that is invertible.]Set the first parity bits, p 1 , to (47.23)This operation has a cost of order g 2 .Claim: At this point, we have found the correct setting of the first parity bits, p 1 .5. Discard the tentative parity bits p A 2 and find the new upper syndrome, (47.24)This can be done in linear time.6. Find a setting of the second parity bits, p 2 , such that the upper syndrome is zero, p 2 = −T −1 z C (47.25)This vector can be found in linear time by back-substitution.Low-density parity-check codes codes were first studied in 1962 by Gallager, then were generally forgotten by the coding theory community.Tanner (1981) generalized Gallager's work by introducing more general constraint nodes; the codes that are now called turbo product codes should in fact be called Tanner product codes, since Tanner proposed them, and his colleagues (Karplus and Krit, 1991) implemented them in hardware.Publications on Gallager codes contributing to their 1990s rebirth include (Wiberg et al., 1995;MacKay and Neal, 1995;MacKay and Neal, 1996;Wiberg, 1996;MacKay, 1999b;Spielman, 1996;Sipser and Spielman, 1996).Low-precision decoding algorithms and fast encoding algorithms for Gallager codes are discussed in (Richardson and Urbanke, 2001a;Richardson and Urbanke, 2001b).MacKay and  showed that low-density parity-check codes can outperform Reed-Solomon codes, even on the Reed-Solomon codes' home turf: high rate and short blocklengths.Other important papers include (Luby et al., 2001a;Luby et al., 2001b;Luby et al., 1997;Davey and MacKay, 1998;Richardson et al., 2001;Chung et al., 2001).Useful tools for the design of irregular low-density paritycheck codes include (Chung et al., 1999;Urbanke, 2001).See (Wiberg, 1996;Frey, 1998;McEliece et al., 1998) for further discussion of the sum-product algorithm.For a view of low-density parity-check code decoding in terms of group theory and coding theory, see (Forney, 2001;Offer and Soljanin, 2000;Offer and Soljanin, 2001); and for background reading on this topic see (Hartmann and Rudolph, 1976;Terras, 1999).There is a growing literature on the practical design of low-density parity-check codes (Mao and Banihashemi, 2000;Mao and Banihashemi, 2001;ten Brink et al., 2002); they are now being adopted for applications from hard drives to satellite communications.For low-density parity-check codes applicable to quantum error-correction, see MacKay et al. (2004).Exercise 47.1. [2 ]The 'hyperbolic tangent' version of the decoding algorithm.In section 47.3, the sum-product decoding algorithm for low-density paritycheck codes was presented first in terms of quantities q 0/1 mn and r 0/1 mn , then in terms of quantities δq and δr.There is a third description, in which the {q} are replaced by log probability-ratios, l mn ≡ ln q 0 mn q 1 mn .(47.26) Show that δq mn ≡ q 0 mn − q 1 mn = tanh(l mn /2).(47.27)Derive the update rules for {r} and {l}.p.572]I am sometimes asked 'why not decode other linear codes, for example algebraic codes, by transforming their parity-check matrices so that they are low-density, and applying the sum-product algorithm?' [Recall that any linear combination of rows of H, H = PH, is a valid parity-check matrix for a code, as long as the matrix P is invertible; so there are many parity check matrices for any one code.]Explain why a random linear code does not have a low-density paritycheck matrix.[Here, low-density means 'having row-weight at most k',where k is some small constant N .]Exercise 47.3. [3 ]Show that if a low-density parity-check code has more than M columns of weight 2 -say αM columns, where α > 1 -then the code will have words with weight of order log M .Exercise 47.4. [5 ]In section 13.5 we found the expected value of the weight enumerator function A(w), averaging over the ensemble of all random linear codes.This calculation can also be carried out for the ensemble of low-density parity-check codes (Gallager, 1963;MacKay, 1999b;Litsyn and Shevelev, 2002).It is plausible, however, that the mean value of A(w) is not always a good indicator of the typical value of A(w) in the ensemble.For example, if, at a particular value of w, 99% of codes have A(w) = 0, and 1% have A(w) = 100 000, then while we might say the typical value of A(w) is zero, the mean is found to be 1000.Find the typical weight enumerator function of low-density parity-check codes.Solution to exercise 47.2 (p.572).Consider codes of rate R and blocklength N , having K = RN source bits and M = (1−R)N parity-check bits.Let all the codes have their bits ordered so that the first K bits are independent, so that we could if we wish put the code in systematic form,(47.28)The number of distinct linear codes is the number of matrices P, which is R) .Can these all be expressed as distinct low-density log N 1 N 2 R(1 − R) parity-check codes?The number of low-density parity-check matrices with row-weight k isand the number of distinct codes that they define is at most (47.30) which is much smaller than N 1 , so, by the pigeon-hole principle, it is not log N 2 < N k log N possible for every random linear code to map on to a low-density H.A fourth way of describing some block codes, the algebraic approach, is not covered in this book (a) because it has been well covered by numerous other books in coding theory; (b) because, as this part of the book discusses, the state-of-the-art in error-correcting codes makes little use of algebraic coding theory; and (c) because I am not competent to teach this subject.We will now describe convolutional codes in two ways: first, in terms of mechanisms for generating transmissions t from source bits s; and second, in terms of trellises that describe the constraints satisfied by valid transmissions.We generate a transmission with a convolutional code by putting a source stream through a linear filter.This filter makes use of a shift register, linear output functions, and, possibly, linear feedback.I will draw the shift-register in a right-to-left orientation: bits roll from right to left as time goes on.Figure 48.1 shows three linear-feedback shift-registers which could be used to define convolutional codes.The rectangular box surrounding the bits z 1 . . .z 7 indicates the memory of the filter, also known as its state.All three filters have one input and two outputs.On each clock cycle, the source supplies one bit, and the filter outputs two bits t (a) and t (b) .By concatenating together these bits we can obtain from our source stream s 1 s 2 s 3 . . .a transmission stream t  these filters require k = 7 bits of memory, the codes they define are known as a constraint-length 7 codes.Convolutional codes come in three flavours, corresponding to the three types of filter in figure 48.1.The filter shown in figure 48.1a has no feedback.It also has the property that one of the output bits, t (a) , is identical to the source bit s.This encoder is thus called systematic, because the source bits are reproduced transparently in the transmitted stream, and nonrecursive, because it has no feedback.The other transmitted bit t (b) is a linear function of the state of the filter.One way of describing that function is as a dot product (modulo 2) between two binary vectors of length k + 1: a binary vector g (b) = (1, 1, 1, 0, 1, 0, 1, 1) and the state vector z = (z k , z k−1 , . . ., z 1 , z 0 ).We include in the state vector the bit z 0 that will be put into the first bit of the memory on the next cycle.The vector g (b) has g (b) κ = 1 for every κ where there is a tap (a downward pointing arrow) from state bit z κ into the transmitted bit t (b) .A convenient way to describe these binary tap vectors is in octal.Thus, this filter makes use of the tap vector 353 8 .I have drawn the delay lines from 11 101 011 ↓ ↓ ↓ 3 5 3 right to left to make it easy to relate the diagrams to these octal numbers.The filter shown in figure 48.1b also has no feedback, but it is not systematic.It makes use of two tap vectors g (a) and g (b) to create its two transmitted bits.This encoder is thus nonsystematic and nonrecursive.Because of their added complexity, nonsystematic codes can have error-correcting abilities superior to those of systematic nonrecursive codes with the same constraint length.The filter shown in figure 48.1c is similar to the nonsystematic nonrecursive filter shown in figure 48.1b, but it uses the taps that formerly made up g (a) to make a linear signal that is fed back into the shift register along with the source bit.The output t (b) is a linear function of the state vector as before.The other output is t (a) = s, so this filter is systematic.A recursive code is conventionally identified by an octal ratio, e.g., figure 48.1c's code is denoted by (247/371) 8 .The two filters in figure 48.1b,c are code-equivalent in that the sets of codewords that they define are identical.For every codeword of the nonsystematic nonrecursive code we can choose a source stream for the other encoder such that its output is identical (and vice versa).To prove this, we denote by p the quantity k κ=1 g (a)κ z κ , as shown in figure 48.3a and b, which shows a pair of smaller but otherwise equivalent filters.If the two transmissions are to be equivalent -that is, the t (a) s are equal in both figures and so are the t (b) s -then on every cycle the source bit in the systematic code must be s = t (a) .So now we must simply confirm that for this choice of s, the systematic code's shift register will follow the same state sequence as that of the nonsystematic code, assuming that the states match initially.In figure 48 (48.2) Substituting for t (a) , and using p ⊕ p = 0 we immediately findThus, any codeword of a nonsystematic nonrecursive code is a codeword of a systematic recursive code with the same taps -the same taps in the sense that there are vertical arrows in all the same places in figures 48.3(a) and (b), though one of the arrows points up instead of down in (b).Now, while these two codes are equivalent, the two encoders behave differently.The nonrecursive encoder has a finite impulse response, that is, if one puts in a string that is all zeroes except for a single one, the resulting output stream contains a finite number of ones.Once the one bit has passed through all the states of the memory, the delay line returns to the all-zero state.Figure 48.4ashows the state sequence resulting from the source string s =(0, 0, 1, 0, 0, 0, 0, 0).Figure 48.4bshows the trellis of the recursive code of figure 48.3b and the response of this filter to the same source string s =(0, 0, 1, 0, 0, 0, 0, 0).The filter has an infinite impulse response.The response settles into a periodic state with period equal to three clock cycles.Exercise 48.1. [1 ]What is the input to the recursive filter such that its state sequence and the transmission are the same as those of the nonrecursive filter?(Hint: see figure 48.5.)48.2: Linear-feedback shift-registers 577   Figure 48.6.The trellis for a k = 4 code painted with the likelihood function when the received vector is equal to a codeword with just one bit flipped.There are three line styles, depending on the value of the likelihood: thick solid lines show the edges in the trellis that match the corresponding two bits of the received string exactly; thick dotted lines show edges that match one bit but mismatch the other; and thin dotted lines show the edges that mismatch both bits.In general a linear-feedback shift-register with k bits of memory has an impulse response that is periodic with a period that is at most 2 k − 1, corresponding to the filter visiting every non-zero state in its state space.Incidentally, cheap pseudorandom number generators and cheap cryptographic products make use of exactly these periodic sequences, though with larger values of k than 7; the random number seed or cryptographic key selects the initial state of the memory.There is thus a close connection between certain cryptanalysis problems and the decoding of convolutional codes.The receiver receives a bit stream, and wishes to infer the state sequence and thence the source stream.The posterior probability of each bit can be found by the sum-product algorithm (also known as the forward-backward or BCJR algorithm), which was introduced in section 25.3.The most probable state sequence can be found using the min-sum algorithm of section 25.3 (also known as the Viterbi algorithm).The nature of this task is illustrated in figure 48.6, which shows the cost associated with each edge in the trellis for the case of a sixteen-state code; the channel is assumed to be a binary symmetric channel and the received vector is equal to a codeword except that one bit has been flipped.There are three line styles, depending on the value of the likelihood: thick solid lines show the edges in the trellis that match the corresponding two bits of the received string exactly; thick dotted lines show edges that match one bit but mismatch the other; and thin dotted lines show the edges that mismatch both bits.The min-sum algorithm seeks the path through the trellis that uses as many solid lines as possible; more precisely, it minimizes the cost of the path, where the cost is zero for a solid line, one for a thick dotted line, and two for a thin dotted line.p.581] Can you spot the most probable path and the flipped bit?When any codeword is completed, the filter state is 0000.A defect of the convolutional codes presented thus far is that they offer unequal protection to the source bits.Figure 48.7 shows two paths through the trellis that differ in only two transmitted bits.The last source bit is less well protected than the other source bits.This unequal protection of bits motivates the termination of the trellis.A terminated trellis is shown in figure 48.8.Termination slightly reduces the number of source bits used per codeword.Here, four source bits are turned into parity bits because the k = 4 memory bits must be returned to zero.An (N, K) turbo code is defined by a number of constituent convolutional encoders (often, two) and an equal number of interleavers which are K × K permutation matrices.Without loss of generality, we take the first interleaver to be the identity matrix.A string of K source bits is encoded by feeding them into each constituent encoder in the order defined by the associated interleaver, and transmitting the bits that come out of each constituent encoder.Often the first constituent encoder is chosen to be a systematic encoder, just like the recursive filter shown in figure 48.6, and the second is a non-systematic one of rate 1 that emits parity bits only.The transmitted codeword then consists of K source bits followed by M 1 parity bits generated by the first convolutional code and M 2 parity bits from the second.The resulting turbo code has rateThe turbo code can be represented by a factor graph in which the two trellises are represented by two large rectangular nodes (figure 48.9a); the K source bits and the first M 1 parity bits participate in the first trellis and the K source bits and the last M 2 parity bits participate in the second trellis.Each codeword bit participates in either one or two trellises, depending on whether it is a parity bit or a source bit.Each trellis node contains a trellis exactly like the terminated trellis shown in figure 48.8, except one thousand times as long.[There are other factor graph representations for turbo codes that make use of more elementary nodes, but the factor graph given here yields the standard version of the sum-product algorithm used for turbo codes.]If a turbo code of smaller rate such as 1/ 2 is required, a standard modification to the rate-1/ 3 code is to puncture some of the parity bits (figure 48.9b).Turbo codes are decoded using the sum-product algorithm described in Chapter 26.On the first iteration, each trellis receives the channel likelihoods, and runs the forward-backward algorithm to compute, for each bit, the relative likelihood of its being 1 or 0, given the information about the other bits.These likelihoods are then passed across from each trellis to the other, and multiplied by the channel likelihoods on the way.We are then ready for the second iteration: the forward-backward algorithm is run again in each trellis using the updated probabilities.After about ten or twenty such iterations, it's hoped that the correct decoding will be found.It is common practice to stop after some fixed number of iterations, but we can do better.As a stopping criterion, the following procedure can be used at every iteration.For each time-step in each trellis, we identify the most probable edge, according to the local messages.If these most probable edges join up into two valid paths, one in each trellis, and if these two paths are consistent with each other, it is reasonable to stop, as subsequent iterations are unlikely to take the decoder away from this codeword.If a maximum number of iterations is reached without this stopping criterion being satisfied, a decoding error can be reported.This stopping procedure is recommended for several reasons: it allows a big saving in decoding time with no loss in error probability; it allows decoding failures that are detected by the decoder to be so identified -knowing that a particular block is definitely corrupted is surely useful information for the receiver!And when we distinguish between detected and undetected errors, the undetected errors give helpful insights into the low-weight codewords of the code, which may improve the process of code design.Turbo codes as described here have excellent performance down to decoded error probabilities of about 10 −5 , but randomly-constructed turbo codes tend to have an error floor starting at that level.This error floor is caused by lowweight codewords.To reduce the height of the error floor, one can attempt to modify the random construction to increase the weight of these low-weight codewords.The tweaking of turbo codes is a black art, and it never succeeds in totalling eliminating low-weight codewords; more precisely, the low-weight codewords can be eliminated only by sacrificing the turbo code's excellent performance.In contrast, low-density parity-check codes rarely have error floors, as long as their number of weight-2 columns is not too large (cf.exercise 47.3, p.572).We close by discussing the parity-check matrix of a rate-1/ 2 convolutional code viewed as a linear block code.We adopt the convention that the N bits of one block are made up of the N/2 bits t (a) followed by the N/2 bits t (b) .Exercise 48.3. [2 ]Prove that a convolutional code has a low-density paritycheck matrix as shown schematically in figure 48.11a.Hint: It's easiest to figure out the parity constraints satisfied by a convolutional code by thinking about the nonsystematic nonrecursive encoder (figure 48.1b).Consider putting through filter a a stream that's been through convolutional filter b, and vice versa; compare the two resulting streams.Ignore termination of the trellises.The parity-check matrix of a turbo code can be written down by listing the constraints satisfied by the two constituent trellises (figure 48.11b).So turbo codes are also special cases of low-density parity-check codes.If a turbo code is punctured, it no longer necessarily has a low-density parity-check matrix, but it always has a generalized parity-check matrix that is sparse, as explained in the next chapter.For further reading about convolutional codes, Johannesson and Zigangirov (1999) is highly recommended.One topic I would have liked to include is sequential decoding.Sequential decoding explores only the most promising paths in the trellis, and backtracks when evidence accumulates that a wrong turning has been taken.Sequential decoding is used when the trellis is too big for us to be able to apply the maximum likelihood algorithm, the minsum algorithm.You can read about sequential decoding in Johannesson and Zigangirov (1999).For further information about the use of the sum-product algorithm in turbo codes, and the rarely-used but highly recommended stopping criteria for halting their decoding, Frey (1998) is essential reading.(And there's lots more good stuff in the same book!)This graph is a factor graph for the prior probability over codewords, with the circles being binary variable nodes, and the squares representing two types of factor nodes.As usual, each contributes a factor of the form [ x = 0 mod 2]; each contributes a factor of the formThe repeat-accumulate code is normally decoded using the sum-product algorithm on the factor graph depicted in figure 49.1b.The top box represents the trellis of the accumulator, including the channel likelihoods.In the first half of each iteration, the top trellis receives likelihoods for every transition in the trellis, and runs the forward-backward algorithm so as to produce likelihoods for each variable node.In the second half of the iteration, these likelihoods are multiplied together at the nodes to produce new likelihood messages to send back to the trellis.As with Gallager codes and turbo codes, the stop-when-it's-done decoding method can be applied, so it is possible to distinguish between undetected errors (which are caused by low-weight codewords in the code) and detected errors (where the decoder gets stuck and knows that it has failed to find a valid answer).Figure 49.2 shows the performance of six randomly-constructed repeataccumulate codes on the Gaussian channel.If one does not mind the error floor which kicks in at about a block error probability of 10 −4 , the performance is staggeringly good for such a simple code (cf.figure 47.17).It is interesting to study the number of iterations τ of the sum-product algorithm required to decode a sparse-graph code.Given one code and a set of channel conditions, the decoding time varies randomly from trial to trial.We find that the histogram of decoding times follows a power law, P (τ ) ∝ τ −p , for large τ .The power p depends on the signal-to-noise ratio and becomes smaller (so that the distribution is more heavy-tailed) as the signal-to-noise ratio decreases.We have observed power laws in repeat-accumulate codes and in irregular and regular Gallager codes. Figures 49.3(ii) and (iii) show the distribution of decoding times of a repeat-accumulate code at two different signal-to-noise ratios.The power laws extend over several orders of magnitude.Exercise 49.1. [5 ]Investigate these power laws.Does density evolution predict them?Can the design of a code be used to manipulate the power law in a useful way?I find that it is helpful when relating sparse-graph codes to each other to use a common representation for them all.Forney (2001) introduced the idea of a normal graph in which the only nodes are and and all variable nodes have degree one or two; variable nodes with degree two can be represented on edges that connect a node to a node.The generalized parity-check matrix is a graphical way of representing normal graphs.In a parity-check matrix, the columns are transmitted bits, and the rows are linear constraints.In a generalized parity-check matrix, additional columns may be included, which represent state variables that are not transmitted.One way of thinking of these state variables is that they are punctured from the code before transmission.State variables are indicated by a horizontal line above the corresponding columns.The other pieces of diagrammatic notation for generalized parity- check matrices are, as in (MacKay, 1999b;MacKay et al., 1998):• A diagonal line in a square indicates that that part of the matrix contains an identity matrix.• Two or more parallel diagonal lines indicate a band-diagonal matrix with a corresponding number of 1s per row.• A horizontal ellipse with an arrow on it indicates that the corresponding columns in a block are randomly permuted.• A vertical ellipse with an arrow on it indicates that the corresponding rows in a block are randomly permuted.• An integer surrounded by a circle represents that number of superposed random permutation matrices.Definition.A generalized parity-check matrix is a pair {A, p}, where A is a binary matrix and p is a list of the punctured bits.The matrix defines a set of valid vectors x, satisfying Ax = 0; (49.2) for each valid vector there is a codeword t(x) that is obtained by puncturing from x the bits indicated by p.For any one code there are many generalized parity-check matrices.The rate of a code with generalized parity-check matrix {A, p} can be estimated as follows.If A is L × M , and p punctures S bits and selects N bits for transmission (L = N + S), then the effective number of constraints on the codeword, M , is (49.3) the number of source bits is (49.4) and the rate is greater than or equal to A linear MN code is a non-systematic low-density parity-check code.The K state bits of an MN code are the source bits.Figure 49.7b shows the generalized parity-check matrix of a rate-1/2 linear MN code.Convolutional codes.In a non-systematic, non-recursive convolutional code, the source bits, which play the role of state bits, are fed into a delay-line and two linear functions of the delay-line are transmitted.In figure 49.8a, these two parity streams are shown as two successive vectors of length K. [It is common to interleave these two parity streams, a bit-reordering that is not relevant here, and is not illustrated.]Concatenation.'Parallel concatenation' of two codes is represented in one of these diagrams by aligning the matrices of two codes in such a way that the 'source bits' line up, and by adding blocks of zero-entries to the matrix such that the state bits and parity bits of the two codes occupy separate columns.An example is given by the turbo code that follows.In 'serial concatenation', the columns corresponding to the transmitted bits of the first code are aligned with the columns corresponding to the source bits of the second code.Turbo codes.A turbo code is the parallel concatenation of two convolutional codes.The generalized parity-check matrix of a rate-1/3 turbo code is shown in figure 49.8b.Repeat-accumulate codes.The generalized parity-check matrices of a rate-1/3 repeat-accumulate code is shown in figure 49.9.Repeat-accumulate codes are equivalent to staircase codes (section 47.7, p.569).Intersection.The generalized parity-check matrix of the intersection of two codes is made by stacking their generalized parity-check matrices on top of each other in such a way that all the transmitted bits' columns are correctly aligned, and any punctured bits associated with the two component codes occupy separate columns.Digital fountain codes are record-breaking sparse-graph codes for channels with erasures.Channels with erasures are of great importance.For example, files sent over the internet are chopped into packets, and each packet is either received without error or not received.A simple channel model describing this situation is a q-ary erasure channel, which has (for all inputs in the input alphabet {0, 1, 2, . . ., q−1}) a probability 1 − f of transmitting the input without error, and probability f of delivering the output '?'.The alphabet size q is 2 l , where l is the number of bits in a packet.Common methods for communicating over such channels employ a feedback channel from receiver to sender that is used to control the retransmission of erased packets.For example, the receiver might send back messages that identify the missing packets, which are then retransmitted.Alternatively, the receiver might send back messages that acknowledge each received packet; the sender keeps track of which packets have been acknowledged and retransmits the others until all packets have been acknowledged.These simple retransmission protocols have the advantage that they will work regardless of the erasure probability f , but purists who have learned their Shannon theory will feel that these retransmission protocols are wasteful.If the erasure probability f is large, the number of feedback messages sent by the first protocol will be large.Under the second protocol, it's likely that the receiver will end up receiving multiple redundant copies of some packets, and heavy use is made of the feedback channel.According to Shannon, there is no need for the feedback channel: the capacity of the forward channel is (1 − f )l bits, whether or not we have feedback.The wastefulness of the simple retransmission protocols is especially evident in the case of a broadcast channel with erasures -channels where one sender broadcasts to many receivers, and each receiver receives a random fraction (1 − f ) of the packets.If every packet that is missed by one or more receivers has to be retransmitted, those retransmissions will be terribly redundant.Every receiver will have already received most of the retransmitted packets.So, we would like to make erasure-correcting codes that require no feedback or almost no feedback.The classic block codes for erasure correction are called Reed-Solomon codes.An (N, K) Reed-Solomon code (over an alphabet of size q = 2 l ) has the ideal property that if any K of the N transmitted symbols are received then the original K source symbols can be recovered.[See Berlekamp (1968) or Lin and Costello (1983) for further information; Reed-Solomon codes exist for N < q.]But Reed-Solomon codes have the disadvantage that they are practical only for small K, N , and q: standard im-plementations of encoding and decoding have a cost of order K(N−K) log 2 N packet operations.Furthermore, with a Reed-Solomon code, as with any block code, one must estimate the erasure probability f and choose the code rate R = K/N before transmission.If we are unlucky and f is larger than expected and the receiver receives fewer than K symbols, what are we to do? We'd like a simple way to extend the code on the fly to create a lower-rate (N , K) code.For Reed-Solomon codes, no such on-the-fly method exists.There is a better way, pioneered by Michael Luby (2002) at his company Digital Fountain, the first company whose business is based on sparse-graph codes.The digital fountain codes I describe here, LT codes, were invented by Luby in 1998.The idea of a digital fountain code is as follows.The encoder is LT stands for 'Luby transform'.a fountain that produces an endless supply of water drops (encoded packets); let's say the original source file has a size of Kl bits, and each drop contains l encoded bits.Now, anyone who wishes to receive the encoded file holds a bucket under the fountain and collects drops until the number of drops in the bucket is a little larger than K.They can then recover the original file.Digital fountain codes are rateless in the sense that the number of encoded packets that can be generated from the source message is potentially limitless; and the number of encoded packets generated can be determined on the fly.Regardless of the statistics of the erasure events on the channel, we can send as many encoded packets as are needed in order for the decoder to recover the source data.The source data can be decoded from any set of K encoded packets, for K slightly larger than K (in practice, about 5% larger).Digital fountain codes also have fantastically small encoding and decoding complexities.With probability 1 − δ, K packets can be communicated with average encoding and decoding costs both of order K ln(K/δ) packet operations.Luby calls these codes universal because they are simultaneously nearoptimal for every erasure channel, and they are very efficient as the file length K grows.The overhead K − K is of order √ K(ln(K/δ)) 2 .Each encoded packet t n is produced from the source file s 1 s 2 s 3 . . .s K as follows:1. Randomly choose the degree d n of the packet from a degree distribution ρ(d); the appropriate choice of ρ depends on the source file size K, as we'll discuss later.2. Choose, uniformly at random, d n distinct input packets, and set t n equal to the bitwise sum, modulo 2 of those d n packets.This sum can be done by successively exclusive-or-ing the packets together.This encoding operation defines a graph connecting encoded packets to source packets.If the mean degree d is significantly smaller than K then the graph is sparse.We can think of the resulting code as an irregular low-density generator-matrix code.The decoder needs to know the degree of each packet that is received, and which source packets it is connected to in the graph.This information can be communicated to the decoder in various ways.For example, if the sender and receiver have synchronized clocks, they could use identical pseudo-random number generators, seeded by the clock, to choose each random degree and each set of connections.Alternatively, the sender could pick a random key, κ n , given which the degree and the connections are determined by a pseudorandom process, and send that key in the header of the packet.As long as the packet size l is much bigger than the key size (which need only be 32 bits or so), this key introduces only a small overhead cost.Decoding a sparse-graph code is especially easy in the case of an erasure channel.The decoder's task is to recover s from t = Gs, where G is the matrix associated with the graph.The simple way to attempt to solve this problem is by message-passing.We can think of the decoding algorithm as the sum-product algorithm if we wish, but all messages are either completely uncertain messages or completely certain messages.Uncertain messages assert that a message packet s k could have any value, with equal probability; certain messages assert that s k has a particular value, with probability one.This simplicity of the messages allows a simple description of the decoding process.We'll call the encoded packets {t n } check nodes.1. Find a check node t n that is connected to only one source packet s k .(If there is no such check node, this decoding algorithm halts at this point, and fails to recover all the source packets.)(a) Set s k = t n .(b) Add s k to all checks t n that are connected to s k :t n := t n + s k for all n such that G n k = 1.(50.1) (c) Remove all the edges connected to the source packet s k .2. Repeat (1) until all {s k } are determined.This decoding process is illustrated in figure 50.1 for a toy case where each packet is just one bit.There are three source packets (shown by the upper circles) and four received packets (shown by the lower check symbols), which have the values t 1 t 2 t 3 t 4 = 1011 at the start of the algorithm.At the first iteration, the only check node that is connected to a sole source bit is the first check node (panel a).We set that source bit s 1 accordingly (panel b), discard the check node, then add the value of s 1 (1) to the checks to which it is connected (panel c), disconnecting s 1 from the graph.At the start of the second iteration (panel c), the fourth check node is connected to a sole source bit, s 2 .We set s 2 to t 4 (0, in panel d), and add s 2 to the two checks it is connected to (panel e).Finally, we find that two check nodes are both connected to s 3 , and they agree about the value of s 3 (as we would hope!), which is restored in panel f.The probability distribution ρ(d) of the degree is a critical part of the design: occasional encoded packets must have high degree (i.e., d similar to K) in order to ensure that there are not some source packets that are connected to no-one.Many packets must have low degree, so that the decoding process can get started, and keep going, and so that the total number of addition operations involved in the encoding and decoding is kept small.For a given degree distribution ρ(d), the statistics of the decoding process can be predicted by an appropriate version of density evolution.Ideally, to avoid redundancy, we'd like the received graph to have the property that just one check node has degree one at each iteration.At each iteration, when this check node is processed, the degrees in the graph are reduced in such a way that one new degree-one check node appears.In expectation, this ideal behaviour is achieved by the ideal soliton distribution,for d = 2, 3, . . ., K.(50.2)The expected degree under this distribution is roughly ln K.Exercise 50.2. [2 ]Derive the ideal soliton distribution.At the first iteration (t = 0) let the number of packets of degree d be h 0 (d); show that (for d > 1) the expected number of packets of degree d that have their degree reduced to d − 1 is h 0 (d)d/K; and at the tth iteration, when t of the K packets have been recovered and the number of packets of degree d is h t (d), the expected number of packets of degree d that have their degree reduced to d − 1 is h t (d)d/(K − t).Hence show that in order to have the expected number of packets of degree 1 satisfy h t (1) = 1 for all t ∈ {0, . . .K − 1}, we must to start with have h 0 (1) = 1 and h 0 (2) = K/2; and more generally, h t (2) = (K − t)/2; then by recursion solve for h 0 (d) for d = 3 upwards.This degree distribution works poorly in practice, because fluctuations around the expected behaviour make it very likely that at some point in the decoding process there will be no degree-one check nodes; and, furthermore, a few source nodes will receive no connections at all.A small modification fixes these problems.The robust soliton distribution has two extra parameters, c and δ; it is designed to ensure that the expected number of degree-one checks is aboutrather than 1, throughout the decoding process.The parameter δ is a bound on the probability that the decoding fails to run to completion after a certain number K of packets have been received.The parameter c is a constant of order 1, if our aim is to prove Luby's main theorem about LT codes; in practice however it can be viewed as a free parameter, with a value somewhat smaller than 1 giving good results.We define a positive function as a function of the two parameters c and δ, for K = 10 000.Luby's main theorem proves that there exists a value of c such that, given K received packets, the decoding algorithm will recover the K source packets with probability 1 − δ.Luby's (2002) analysis explains how the small-d end of τ has the role of ensuring that the decoding process gets started, and the spike in τ at d = K/S is included to ensure that every source packet is likely to be connected to a check at least once.Luby's key result is that (for an appropriate value of the constant c) receiving K = K + 2 ln(S/δ)S checks ensures that all packets can be recovered with probability at least 1 − δ.In the illustrative figures I have set the allowable decoder failure probability δ quite large, because the actual failure probability is much smaller than is suggested by Luby's conservative analysis.In practice, LT codes can be tuned so that a file of original size K 10 000 packets is recovered with an overhead of about 5%. Figure 50.4shows histograms of the actual number of packets required for a couple of settings of the parameters, achieving mean overheads smaller than 5% and 10% respectively.Digital fountain codes are an excellent solution in a wide variety of situations.Let's mention two.You wish to make a backup of a large file, but you are aware that your magnetic tapes and hard drives are all unreliable in the sense that catastrophic failures, in which some stored packets are permanently lost within one device, occur at a rate of something like 10 −3 per day.How should you store your file?A digital fountain can be used to spray encoded packets all over the place, on every storage device available.Then to recover the backup file, whose size was K packets, one simply needs to find K K packets from anywhere.Corrupted packets do not matter; we simply skip over them and find more packets elsewhere.This method of storage also has advantages in terms of speed of file recovery.In a hard drive, it is standard practice to store a file in successive sectors of a hard drive, to allow rapid reading of the file; but if, as occasionally happens, a packet is lost (owing to the reading head being off track for a moment, giving a burst of errors that cannot be corrected by the packet's error-correcting code), a whole revolution of the drive must be performed to bring back the packet to the head for a second read.The time taken for one revolution produces an undesirable delay in the file system.If files were instead stored using the digital fountain principle, with the digital drops stored in one or more consecutive sectors on the drive, then one would never need to endure the delay of re-reading a packet; packet loss would become less important, and the hard drive could consequently be operated faster, with higher noise level, and with fewer resources devoted to noisychannel coding.Exercise 50.3. [2 ]Compare the digital fountain method of robust storage on multiple hard drives with RAID (the redundant array of independent disks).Imagine that ten thousand subscribers in an area wish to receive a digital movie from a broadcaster.The broadcaster can send the movie in packets over a broadcast network -for example, by a wide-bandwidth phone line, or by satellite.Imagine that not all packets are received at all the houses.Let's say f = 0.1% of them are lost at each house.In a standard approach in which the file is transmitted as a plain sequence of packets with no encoding, each house would have to notify the broadcaster of the f K missing packets, and request that they be retransmitted.And with ten thousand subscribers all requesting such retransmissions, there would be a retransmission request for almost every packet.Thus the broadcaster would have to repeat the entire broadcast twice in order to ensure that most subscribers have received the whole movie, and most users would have to wait roughly twice as long as the ideal time before the download was complete.If the broadcaster uses a digital fountain to encode the movie, each subscriber can recover the movie from any K K packets.So the broadcast needs to last for only, say, 1.1K packets, and every house is very likely to have successfully recovered the whole file.Another application is broadcasting data to cars.Imagine that we want to send updates to in-car navigation databases by satellite.There are hundreds of thousands of vehicles, and they can receive data only when they are out on the open road; there are no feedback channels.A standard method for sending the data is to put it in a carousel, broadcasting the packets in a fixed periodic sequence.'Yes, a car may go through a tunnel, and miss out on a few hundred packets, but it will be able to collect those missed packets an hour later when the carousel has gone through a full revolution (we hope); or maybe the following day. ..'If instead the satellite uses a digital fountain, each car needs to receive only an amount of data equal to the original file size (plus 5%).The encoders and decoders sold by Digital Fountain have even higher efficiency than the LT codes described here, and they work well for all blocklengths, not only large lengths such as K 10 000.Shokrollahi (2003) presents raptor codes, which are an extension of LT codes with linear-time encoding and decoding.Exercise 50.4. [2 ]Understanding the robust soliton distribution.Repeat the analysis of exercise 50.2 (p.592) but now aim to have the expected number of packets of degree 1 be h t (1) = 1 + S for all t, instead of 1. Show that the initial required number of packets isThe reason for truncating the second term beyond d = K/S and replacing it by the spike at d = K/S (see equation (50.4)) is to ensure that the decoding complexity does not grow larger than O(K ln K).Estimate the expected number of packets d h 0 (d) and the expected number of edges in the sparse graph d h 0 (d)d (which determines the decoding complexity) if the histogram of packets is as given in (50.6).Compare with the expected numbers of packets and edges when the robust soliton distribution (50.4) is used.A simple method for designing error-correcting codes for noisy channels, first pioneered by Gallager (1962), has recently been rediscovered and generalized, and communication theory has been transformed.The practical performance of Gallager's low-density parity-check codes and their modern cousins is vastly better than the performance of the codes with which textbooks have been filled in the intervening years.Which sparse-graph code is 'best' for a noisy channel depends on the chosen rate and blocklength, the permitted encoding and decoding complexity, and the question of whether occasional undetected errors are acceptable.Lowdensity parity-check codes are the most versatile; it's easy to make a competitive low-density parity-check code with almost any rate and blocklength, and low-density parity-check codes virtually never make undetected errors.For the special case of the erasure channel, the sparse-graph codes that are best are digital fountain codes.The best solution to the communication problem is: Combine a simple, pseudo-random code with a message-passing decoder.Part VII Appendices B Some PhysicsA system with states x in contact with a heat bath at temperature T = 1/β has probability distributionThe partition function isThe inverse temperature β can be interpreted as defining an exchange rate between entropy and energy.(1/β) is the amount of energy that must be given to a heat bath to increase its entropy by one nat.Often, the system will be affected by some other parameters such as the volume of the box it is in, V , in which case Z is a function of V too, Z(β, V ).For any system with a finite number of states, the function Z(β) is evidently a continuous function of β, since it is simply a sum of exponentials.Moreover, all the derivatives of Z(β) with respect to β are continuous too.What phase transitions are all about, however, is this: phase transitions correspond to values of β and V (called critical points) at which the derivatives of Z have discontinuities or divergences.Immediately we can deduce:Only systems with an infinite number of states can show phase transitions.Often, we include a parameter N describing the size of the system.Phase transitions may appear in the limit N → ∞.Real systems may have a value of N like 10 23 .If we make the system large by simply grouping together N independent systems whose partition function is Z (1) (β), then nothing interesting happens.The partition function for N independent identical systems is simplyNow, while this function Z (N ) (β) may be a very rapidly varying function of β, that doesn't mean it is showing phase transitions.The natural way to look at the partition function is in the logarithm ln Z (N ) (β) = N ln Z (1) (β).(B.4) Duplicating the original system N times simply scales up all properties like the energy and heat capacity of the system by a factor of N .So if the original system showed no phase transitions then the scaled up system won't have any either.Only systems with long-range correlations show phase transitions.Long-range correlations do not require long-range energetic couplings; for example, a magnet has only short-range couplings (between adjacent spins) but these are sufficient to create long-range order.Why are points at which derivatives diverge interesting?The derivatives of ln Z describe properties like the heat capacity of the system (that's the second derivative) or its fluctuations in energy.If the second derivative of ln Z diverges at a temperature 1/β, then the heat capacity of the system diverges there, which means it can absorb or release energy without changing temperature (think of ice melting in ice water); when the system is at equilibrium at that temperature, its energy fluctuates a lot, in contrast to the normal law-of-large-numbers behaviour, where the energy only varies by one part in √ N .A toy system that shows a phase transition This energy function describes a ground state in which all the spins are aligned in the zero direction; the energy per spin in this state is − .if any spin changes state then the energy is zero.This model is like an extreme version of a magnetic interaction, which encourages pairs of spins to be aligned.We can contrast it with an ordinary system of N independent spins whose energy is:Like the first system, the system of independent spins has a single ground state (0, 0, 0, . . ., 0) with energy −N , and it has roughly 2 N states with energy very close to 0, so the low-temperature and high-temperature properties of the independent-spin system and the coupled-spin system are virtually identical.The partition function of the coupled-spin system is at which these two asymptotes intersect.In the limit N → ∞, the graph of ln Z(β) becomes more and more sharply bent at this point (figure B.1b).The second derivative of ln Z, which describes the variance of the energy of the system, has a peak value, at β = ln 2/ , roughly equal towhich corresponds to the system spending half of its time in the ground state and half its time in the other states.At this critical point, the heat capacity of this system is thus proportional to N 2 ; the heat capacity per spin is proportional to N , which, for infinite N , is infinite, in contrast to the behaviour of systems away from phase transitions, whose capacity per atom is a finite number.For comparison, figure B.2 shows the partition function and energy-variance of the ordinary independent-spin system.Phase transitions can be categorized into 'first-order' and 'continuous' transitions.In a first-order phase transition, there is a discontinuous change of one or more order-parameters; in a continuous transition, all order-parameters change continuously.[What's an order-parameter?-a scalar function of the state of the system; or, to be precise, the expectation of such a function.]In the vicinity of a critical point, the concept of 'typicality' defined in Chapter 4 does not hold.For example, our toy system, at its critical point, has a 50% chance of being in a state with energy −N , and roughly a 1/2 N +1 chance of being in each of the other states that have energy zero.It is thus not the case that ln 1/P (x) is very likely to be close to the entropy of the system at this point, unlike a system with N i.i.d.components.Remember that information content (ln 1/P (x)) and energy are very closely related.If typicality holds, then the system's energy has negligible fluctuations, and vice versa.GF (8).We can denote the elements of GF (8) by {0, 1, A, B, C, D, E, F }.Each element can be mapped onto a polynomial over GF (2).The multiplication and addition operations are given by multiplication and addition of the polynomials, modulo x 3 + x + 1.The multiplication table is given below.element polynomial binary representationWhy are Galois fields relevant to linear codes?Imagine generalizing a binary generator matrix G and binary vector s to a matrix and vector with elements from a larger set, and generalizing the addition and multiplication operations that define the product Gs.In order to produce an appropriate input for a symmetric channel, it would be convenient if, for random s, the product Gs produced all elements in the enlarged set with equal probability.This uniform distribution is easiest to guarantee if these elements form a group under both addition and multiplication, because then these operations do not break the symmetry among the elements.When two random elements of a multiplicative group are multiplied together, all elements are produced with equal probability.This is not true of other sets such as the integers, for which the multiplication operation is more likely to give rise to some elements (the composite numbers) than others.Galois fields, by their definition, avoid such symmetry-breaking effects.A right-eigenvector of a square matrix A is a non-zero vector e R that satisfieswhere λ is the eigenvalue associated with that eigenvector.The eigenvalue may be a real number or complex number and it may be zero.Eigenvectors may be real or complex.A left-eigenvector of a matrix A is a vector e L that satisfies e T L A = λe T L .(C.2)The following statements for right-eigenvectors also apply to left-eigenvectors.• If a matrix has two or more linearly independent right-eigenvectors with the same eigenvalue then that eigenvalue is called a degenerate eigenvalue of the matrix, or a repeated eigenvalue.Any linear combination of those eigenvectors is another right-eigenvector with the same eigenvalue.• The principal right-eigenvector of a matrix is, by definition, the righteigenvector with the largest associated eigenvalue.• If a real matrix has a right-eigenvector with complex eigenvalue λ = x + yi then it also has a right-eigenvector with the conjugate eigenvalue λ * = x − yi.If A is a real symmetric N × N matrix then 1. all the eigenvalues and eigenvectors of A are real;2. every left-eigenvector of A is also a right-eigenvector of A with the same eigenvalue, and vice versa;3. a set of N eigenvectors and eigenvalues {e (a) , λ a } N a=1 can be found that are orthonormal, that is,Definition.If all the elements of a non-zero matrix C satisfy C mn ≥ 0 then C is a non-negative matrix.Similarly, if all the elements of a non-zero vector c satisfy c n ≥ 0 then c is a non-negative vector.Properties.A non-negative matrix has a principal eigenvector that is nonnegative.It may also have other eigenvectors with the same eigenvalue that are not non-negative.But if the principal eigenvalue of a non-negative matrix is not degenerate, then the matrix has only one principal eigenvector e (1) , and it is non-negative.Generically, all the other eigenvalues are smaller in absolute magnitude.[There can be several eigenvalues of identical magnitude in special cases.]An important example of a non-negative matrix is a transition probability matrix Q. Definition.A transition probability matrix Q has columns that are probability vectors, that is, it satisfies Q ≥ 0 and  This property can be rewritten in terms of the all-ones vector n = (1, 1, . . ., 1) T :So n is the principal left-eigenvector of Q with eigenvalue λ 1 = 1.(1)Because it is a non-negative matrix, Q has a principal right-eigenvector that is non-negative, eR .Generically, for Markov processes that are ergodic, this eigenvector is the only right-eigenvector with eigenvalue of magnitude 1 (see table C.6 for illustrative exceptions).This vector, if we normalize it such that e(1) R •n = 1, is called the invariant distribution of the transition probability matrix.It is the probability density that is left unchanged under Q.Unlike the principal left-eigenvector, which we explicitly identified above, we can't usually identify the principal right-eigenvector without computation.The matrix may have up to N − 1 other right-eigenvectors all of which are orthogonal to the left-eigenvector n, that is, they are zero-sum vectors.Perturbation theory is not used in this book, but it is useful in this book's fields.In this section we derive first-order perturbation theory for the eigenvectors and eigenvalues of square, not necessarily symmetric, matrices.Most presentations of perturbation theory focus on symmetric matrices, but nonsymmetric matrices (such as transition matrices) also deserve to be perturbed!We assume that we have an N × N matrix H that is a function H( ) of a real parameter , with = 0 being our starting point.We assume that a Taylor expansion of H( ) is appropriate: We assume that for all of interest, H( ) has a complete set of N righteigenvectors and left-eigenvectors, and that these eigenvectors and their eigenvalues are continuous functions of .This last assumption is not necessarily a good one: if H(0) has degenerate eigenvalues then it is possible for the eigenvectors to be discontinuous in ; in such cases, degenerate perturbation theory is needed.That's a fun topic, but let's stick with the non-degenerate case here.We write the eigenvectors and eigenvalues as follows:H( )e  L .We define these left-vectors to be row vectors, so that the 'transpose' operation is not needed and can be banished.We are free to constrain the magnitudes of the eigenvectors in whatever way we please.Each left-eigenvector and each right-eigenvector has an arbitrary magnitude.The natural constraints to use are as follows.First, we constrain the inner products with:  (C.20) Identifying the terms of order , we have:If we expand the eigenvector equation (C.11) to second order in , and assume that the equationis exact, that is, H is a purely linear function of , then we have:(H(0) + V)(e   This is as far as we will take the perturbation expansion.If we introduce the abbreviation V ba for e     ' theorem, 6, 24, 25, 27, 28, 48-50, 53, 148, 324, 344, 347, 446, 493 , 156, 157, 160, 198, 200 bit, 3, 73 bit (unit) , 4, 146, 148, 151, 206, 211, 215, 229 broadcast, 237, 239, 594 bursty, 185, 557 capacity, 14, 146, 150, , 4, 19, 205, 215, 574 coin, 1, 30, 38, 63, 76, 307, 464 coincidence, 267, 343, 351 collective, 403 collision, 200 coloured noise, 179 combination, 2, 490, 598 commander, 241 communication, v, 3, 16, 138, 146, 156, 162, 167, 178, 182, 186, 192, 205, 210, 215, 394, 556, 562, Neal,Radford M.,111,121,187,374,379,391,392,397,419,420,429,432 193 phone number, 58, 129 photon counter, 307, 342, 448 physics, 80, 85, 257, 357, 401, 422, 514 Yedidia,Jonathan,340 Z channel,148,[149][150][151]155 Zipf plot,262,263,317 Zipf's law,40,262,263 Zipf,George K.,262Copyright Cambridge University Press 2003.On-screen viewing permitted.Printing not permitted.http://www.cambridge.org/0521642981You can buy this book for 30 pounds or $50.See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.The central problem of communication theory is to construct an encoding and a decoding system that make it possible to communicate reliably over a noisy channel.During the 1990s, remarkable progress was made towards the Shannon limit, using codes that are defined in terms of sparse random graphs, and which are decoded by a simple probability-based message-passing algorithm.In a sparse-graph code, the nodes in the graph represent the transmitted bits and the constraints they satisfy.For a linear code with a codeword length N and rate R = K/N , the number of constraints is of order M = N − K. Any linear code can be described by a graph, but what makes a sparse-graph code special is that each constraint involves only a small number of variables in the graph: so the number of edges in the graph scales roughly linearly with N , rather than quadratically.In the following four chapters we will look at four families of sparse-graph codes: three families that are excellent for error-correction: low-density paritycheck codes, turbo codes, and repeat-accumulate codes; and the family of digital fountain codes, which are outstanding for erasure-correction.All these codes can be decoded by a local message-passing algorithm on the graph, the sum-product algorithm, and, while this algorithm is not a perfect maximum likelihood decoder, the empirical results are record-breaking.This chapter follows tightly on from Chapter 25.It makes use of the ideas of codes and trellises and the forward-backward algorithm.When we studied linear block codes, we described them in three ways:1.The generator matrix describes how to turn a string of K arbitrary source bits into a transmission of N bits.In Chapter 1 we discussed a very simple and not very effective method for communicating over a noisy channel: the repetition code.We now discuss a code that is almost as simple, and whose performance is outstandingly good.Repeat-accumulate codes were studied by Divsalar et al. (1998) for theoretical purposes, as simple turbo-like codes that might be more amenable to analysis than messy turbo codes.Their practical performance turned out to be just as good as other sparse-graph codes.1. Take K source bits.s 1 s 2 s 3 . . .s K 2. Repeat each bit three times, giving N = 3K bits.3. Permute these N bits using a random permutation (a fixed random permutation -the same one for every codeword).Call the permuted string u. u 1 u 2 u 3 u 4 u 5 u 6 u 7 u 8 u 9 . . .u N 4. Transmit the accumulated sum.5. That's it!Figure 49.1a shows the graph of a repeat-accumulate code, using four types of node: equality constraints , intermediate binary variables (black circles), parity constraints , and the transmitted bits (white circles).The source sets the values of the black bits at the bottom, three at a time, and the accumulator computes the transmitted bits along the top.Repetition code.The generator matrix, parity-check matrix, and generalized parity-check matrix of a simple rate-1/ 3 repetition code are shown in figure 49.4.Systematic low-density generator-matrix code.In an (N, K) systematic lowdensity generator-matrix code, there are no state variables.A transmitted codeword t of length N is given bywherewith I K denoting the K ×K identity matrix, and P being a very sparse M ×K matrix, where M = N − K.The parity-check matrix of this code isIn the case of a rate-1/ 3 code, this parity-check matrix might be represented as shown in figure 49.5.Non-systematic low-density generator-matrix code.In an (N, K) non-systematic low-density generator-matrix code, a transmitted codeword t of length N is given by t = G T s, (49.9)where G T is a very sparse N × K matrix.The generalized parity-check matrix of this code is (49.10) and the corresponding generalized parity-check equation is .11)Whereas the parity-check matrix of this simple code is typically a complex, dense matrix, the generalized parity-check matrix retains the underlying simplicity of the code.In the case of a rate-1/ 2 code, this generalized parity-check matrix might be represented as shown in figure 49.6.Low-density parity-check codes and linear MN codes.The parity-check matrix of a rate-1/3 low-density parity-check code is shown in figure 49.7a.The following exercise provides a helpful background for digital fountain codes.Exercise 50.1. [3 ]An author proofreads his K = 700-page book by inspecting random pages.He makes N page-inspections, and does not take any precautions to avoid inspecting the same page twice.(c) Show that in order for the probability that all K pages have been inspected to be 1 − δ, we require N K ln(K/δ) page-inspections.[This problem is commonly presented in terms of throwing N balls at random into K bins; what's the probability that every bin gets at least one ball?]50.5:Further exercises 595Exercise 50.5. [4 ]Show that the spike at d = K/S (equation (50.4)) is an adequate replacement for the tail of high-weight packets in (50.6).3C ] Investigate experimentally how necessary the spike at d = K/S (equation (50.4)) is for successful decoding.Investigate also whether the tail of ρ(d) beyond d = K/S is necessary.What happens if all highweight degrees are removed, both the spike at d = K/S and the tail of ρ(d) beyond d = K/S?Exercise 50.7. [4 ]Fill in the details in the proof of Luby's main theorem, that receiving K = K + 2 ln(S/δ)S checks ensures that all the source packets can be recovered with probability at least 1 − δ.4C ] Optimize the degree distribution of a digital fountain code for a file of K = 10 000 packets.Pick a sensible objective function for your optimization, such as minimizing the mean of N , the number of packets required for complete decoding, or the 95th percentile of the histogram of N (figure 50.4).Exercise 50.9. [3 ]Make a model of the situation where a data stream is broadcast to cars, and quantify the advantage that the digital fountain has over the carousel method.Exercise 50.10. [2 ]Construct a simple example to illustrate the fact that the digital fountain decoder of section 50.2 is suboptimal -it sometimes gives up even though the information available is sufficient to decode the whole file.How does the cost of the optimal decoder compare?Exercise 50.11. [2 ]If every transmitted packet were created by adding together source packets at random with probability 1/ 2 of each source packet's being included, show that the probability that K = K received packets suffice for the optimal decoder to be able to recover the K source packets is just a little below 1/2.[To put it another way, what is the probability that a random K × K matrix has full rank?]Show that if K = K + ∆ packets are received, the probability that they will not suffice for the optimal decoder is roughly 2 −∆ .4C ] Implement an optimal digital fountain decoder that uses the method of Richardson and Urbanke (2001b) derived for fast encoding of sparse-graph codes (section 47.7) to handle the matrix inversion required for optimal decoding.Now that you have changed the decoder, you can reoptimize the degree distribution, using higher-weight packets.By how much can you reduce the overhead?Confirm the assertion that this approach makes digital fountain codes viable as erasure-correcting codes for all blocklengths, not just the large blocklengths for which LT codes are excellent.Exercise 50.13. [5 ]Digital fountain codes are excellent rateless codes for erasure channels.Make a rateless code for a channel that has both erasures and noise.A What does ŝ mean?Usually, a 'hat' over a variable denotes a guess or estimator.So ŝ is a guess at the value of s.Integrals.There is no difference between f (u) du and du f (u).The integrand is f (u) in both cases.mean?This is like the summation N n=1 but it denotes a product.It's pronounced 'product over n from 1 to N'.So, for example,I like to choose the name of the free variable in a sum or a producthere, n -to be the lower case version of the range of the sum.So n usually runs from 1 to N , and m usually runs from 1 to M .This is a habit I learnt from Yaser Abu-Mostafa, and I think it makes formulae easier to understand.What does N n mean?This is pronounced 'N choose n', and it is the number of ways of selecting an unordered set of n objects from a set of size N .This function is known as the combination function.What is Γ(x)?The gamma function is defined by Γ(x) ≡ ∞ 0 du u x−1 e −u , for x > 0. The gamma function is an extension of the factorial function to real number arguments.In general, Γ(x + 1) = xΓ(x), and for integer arguments, Γ(x + 1) = x!.The digamma function is defined by Ψ (x) ≡ d dx ln Γ(x).For large x (for practical purposes, 0.A -Notation 599 and for small x (for practical purposes, 0 ≤ x ≤ 0.5):where γ e is Euler's constant.What does H −1 2 (1 − R/C) mean?Just as sin −1 (s) denotes the inverse function to s = sin(x), so H −1 2 (h) is the inverse function to h = H 2 (x).There is potential confusion when people use sin 2 x to denote (sin x) 2 , since then we might expect sin −1 s to denote 1/ sin(s); I therefore like to avoid using the notation sin 2 x.What does f (x) mean?The answer depends on the context.Often, a 'prime' is used to denote differentiation:similarly, a dot denotes differentiation with respect to time, t:However, the prime is also a useful indicator for 'another variable', for example 'a new value for a variable'.So, for example, x might denote 'the new value of x'.Also, if there are two integers that both range from 1 to N , I will often name those integers n and n .So my rule is: if a prime occurs in an expression that could be a function, such as f (x) or h (y), then it denotes differentiation; otherwise it indicates 'another variable'.What is the error function?Definitions of this function vary.I define it to be the cumulative probability of a standard (variance = 1) normal distribution,What does E(r) mean?E[r] is pronounced 'the expected value of r' or 'the expectation of r', and it is the mean value of r.Another symbol for 'expected value' is the pair of angle-brackets, r .What does |x| mean?The vertical bars '| • |' have two meanings.If A is a set, then |A| denotes the number of elements in the set; if x is a number, then |x| is the absolute value of x.What does [A|P] mean?Here, A and P are matrices with the same number of rows.[A|P] denotes the double-width matrix obtained by putting A alongside P. The vertical bar is used to avoid confusion with the product AP.What does x T mean?The superscript T is pronounced 'transpose'.Transposing a row-vector turns it into a column vector:(1, 2, 3 (A.9)The determinant of M is denoted det M.What does δ mn mean?The δ matrix is the identity matrix.Another name for the identity matrix is I or 1.Sometimes I include a subscript on this symbol -1 K -which indicates the size of the matrix (K × K).What does δ(x) mean?The delta function has the propertyAnother possible meaning for δ(S) is the truth function, which is 1 if the proposition S is true but I have adopted another notation for that.After all, the symbol δ is quite busy already, with the two roles mentioned above in addition to its role as a small real number δ and an increment operator (as in δx)![S] mean?[S] is the truth function, which is 1 if the proposition S is true and 0 otherwise.For example, the number of positive numbers in the set T = {−2, 1, 3} can be written .11)What is the difference between ':=' and '=' ?In an algorithm, x := y means that the variable x is updated by assigning it the value of y.In contrast, x = y is a proposition, a statement that x is equal to y.See Chapters 23 and 29 for further definitions and notation relating to probability distributions.Most linear codes are expressed in the language of Galois theory Why are Galois fields an appropriate language for linear codes?First, a definition and some examples.+ 0 1 0 0 1 1 1 0• 0 1 0 0 0 1 0 1 For example, the real numbers form a field, with '+' and '•' denoting ordinary addition and multiplication.A Galois field GF (q) is a field with a finite number of elements q.A unique Galois field exists for any q = p m , where p is a prime number and m is a positive integer; there are no other finite fields.GF (2).The addition and multiplication tables for GF (2) are shown in table C.1.These are the rules of addition and multiplication modulo 2.GF (p).For any prime number p, the addition and multiplication rules are those for ordinary addition and multiplication, modulo p.GF (4).The rules for GF (p m ), with m > 1, are not those of ordinary addition and multiplication.For example the tables for GF (4) (table C.2) are notAddition and multiplication tables for GF (4).the rules of addition and multiplication modulo 4. Notice that 1 + 1 = 0, for example.So how can GF (4) be described?It turns out that the elements can be related to polynomials.Consider polynomial functions of x of degree 1 and with coefficients that are elements of GF (2).The polynomials shown in table C.3 obey the addition and multiplication rules of GF (4) if addition and multiplication are modulo the polynomial x 2 + x + 1, and the coefficients of the polynomials are from GF (2).For example, B • B = x 2 + (1 + 1)x + 1 = x = A. Each element may also be represented as a bit pattern as shown in  265 Shannon,Claude,3,14,15,152,164,212,215,262, see noisy-channel coding theorem, source coding theorem, information content shattering, 485 shifter ensemble, 524 Shokrollahi,M. Amin,568 shortening,222 Siegel,Paul,262 sigmoid,473,177,178,223 significance level,51,64,457,463 simplex,173,316 Simpson'sThe central problem of communication theory is to construct an encoding and a decoding system that make it possible to communicate reliably over a noisy channel.During the 1990s, remarkable progress was made towards the Shannon limit, using codes that are defined in terms of sparse random graphs, and which are decoded by a simple probability-based message-passing algorithm.In a sparse-graph code, the nodes in the graph represent the transmitted bits and the constraints they satisfy.For a linear code with a codeword length N and rate R = K/N , the number of constraints is of order M = N − K. Any linear code can be described by a graph, but what makes a sparse-graph code special is that each constraint involves only a small number of variables in the graph: so the number of edges in the graph scales roughly linearly with N , rather than quadratically.In the following four chapters we will look at four families of sparse-graph codes: three families that are excellent for error-correction: low-density paritycheck codes, turbo codes, and repeat-accumulate codes; and the family of digital fountain codes, which are outstanding for erasure-correction.All these codes can be decoded by a local message-passing algorithm on the graph, the sum-product algorithm, and, while this algorithm is not a perfect maximum likelihood decoder, the empirical results are record-breaking.This chapter follows tightly on from Chapter 25.It makes use of the ideas of codes and trellises and the forward-backward algorithm.When we studied linear block codes, we described them in three ways:1.The generator matrix describes how to turn a string of K arbitrary source bits into a transmission of N bits.In Chapter 1 we discussed a very simple and not very effective method for communicating over a noisy channel: the repetition code.We now discuss a code that is almost as simple, and whose performance is outstandingly good.Repeat-accumulate codes were studied by Divsalar et al. (1998) for theoretical purposes, as simple turbo-like codes that might be more amenable to analysis than messy turbo codes.Their practical performance turned out to be just as good as other sparse-graph codes.1. Take K source bits.s 1 s 2 s 3 . . .s K 2. Repeat each bit three times, giving N = 3K bits.3. Permute these N bits using a random permutation (a fixed random permutation -the same one for every codeword).Call the permuted string u. u 1 u 2 u 3 u 4 u 5 u 6 u 7 u 8 u 9 . . .u N 4. Transmit the accumulated sum.5. That's it!Figure 49.1a shows the graph of a repeat-accumulate code, using four types of node: equality constraints , intermediate binary variables (black circles), parity constraints , and the transmitted bits (white circles).The source sets the values of the black bits at the bottom, three at a time, and the accumulator computes the transmitted bits along the top.Repetition code.The generator matrix, parity-check matrix, and generalized parity-check matrix of a simple rate-1/ 3 repetition code are shown in figure 49.4.Systematic low-density generator-matrix code.In an (N, K) systematic lowdensity generator-matrix code, there are no state variables.A transmitted codeword t of length N is given bywherewith I K denoting the K ×K identity matrix, and P being a very sparse M ×K matrix, where M = N − K.The parity-check matrix of this code isIn the case of a rate-1/ 3 code, this parity-check matrix might be represented as shown in figure 49.5.Non-systematic low-density generator-matrix code.In an (N, K) non-systematic low-density generator-matrix code, a transmitted codeword t of length N is given by t = G T s, (49.9)where G T is a very sparse N × K matrix.The generalized parity-check matrix of this code is (49.10) and the corresponding generalized parity-check equation is .11)Whereas the parity-check matrix of this simple code is typically a complex, dense matrix, the generalized parity-check matrix retains the underlying simplicity of the code.In the case of a rate-1/ 2 code, this generalized parity-check matrix might be represented as shown in figure 49.6.Low-density parity-check codes and linear MN codes.The parity-check matrix of a rate-1/3 low-density parity-check code is shown in figure 49.7a.The following exercise provides a helpful background for digital fountain codes.Exercise 50.1. [3 ]An author proofreads his K = 700-page book by inspecting random pages.He makes N page-inspections, and does not take any precautions to avoid inspecting the same page twice.(c) Show that in order for the probability that all K pages have been inspected to be 1 − δ, we require N K ln(K/δ) page-inspections.[This problem is commonly presented in terms of throwing N balls at random into K bins; what's the probability that every bin gets at least one ball?]50.5:Further exercises 595Exercise 50.5. [4 ]Show that the spike at d = K/S (equation (50.4)) is an adequate replacement for the tail of high-weight packets in (50.6).3C ] Investigate experimentally how necessary the spike at d = K/S (equation (50.4)) is for successful decoding.Investigate also whether the tail of ρ(d) beyond d = K/S is necessary.What happens if all highweight degrees are removed, both the spike at d = K/S and the tail of ρ(d) beyond d = K/S?Exercise 50.7. [4 ]Fill in the details in the proof of Luby's main theorem, that receiving K = K + 2 ln(S/δ)S checks ensures that all the source packets can be recovered with probability at least 1 − δ.4C ] Optimize the degree distribution of a digital fountain code for a file of K = 10 000 packets.Pick a sensible objective function for your optimization, such as minimizing the mean of N , the number of packets required for complete decoding, or the 95th percentile of the histogram of N (figure 50.4).Exercise 50.9. [3 ]Make a model of the situation where a data stream is broadcast to cars, and quantify the advantage that the digital fountain has over the carousel method.Exercise 50.10. [2 ]Construct a simple example to illustrate the fact that the digital fountain decoder of section 50.2 is suboptimal -it sometimes gives up even though the information available is sufficient to decode the whole file.How does the cost of the optimal decoder compare?Exercise 50.11. [2 ]If every transmitted packet were created by adding together source packets at random with probability 1/ 2 of each source packet's being included, show that the probability that K = K received packets suffice for the optimal decoder to be able to recover the K source packets is just a little below 1/2.[To put it another way, what is the probability that a random K × K matrix has full rank?]Show that if K = K + ∆ packets are received, the probability that they will not suffice for the optimal decoder is roughly 2 −∆ .4C ] Implement an optimal digital fountain decoder that uses the method of Richardson and Urbanke (2001b) derived for fast encoding of sparse-graph codes (section 47.7) to handle the matrix inversion required for optimal decoding.Now that you have changed the decoder, you can reoptimize the degree distribution, using higher-weight packets.By how much can you reduce the overhead?Confirm the assertion that this approach makes digital fountain codes viable as erasure-correcting codes for all blocklengths, not just the large blocklengths for which LT codes are excellent.Exercise 50.13. [5 ]Digital fountain codes are excellent rateless codes for erasure channels.Make a rateless code for a channel that has both erasures and noise.A What does ŝ mean?Usually, a 'hat' over a variable denotes a guess or estimator.So ŝ is a guess at the value of s.Integrals.There is no difference between f (u) du and du f (u).The integrand is f (u) in both cases.mean?This is like the summation N n=1 but it denotes a product.It's pronounced 'product over n from 1 to N'.So, for example,I like to choose the name of the free variable in a sum or a producthere, n -to be the lower case version of the range of the sum.So n usually runs from 1 to N , and m usually runs from 1 to M .This is a habit I learnt from Yaser Abu-Mostafa, and I think it makes formulae easier to understand.What does N n mean?This is pronounced 'N choose n', and it is the number of ways of selecting an unordered set of n objects from a set of size N .This function is known as the combination function.What is Γ(x)?The gamma function is defined by Γ(x) ≡ ∞ 0 du u x−1 e −u , for x > 0. The gamma function is an extension of the factorial function to real number arguments.In general, Γ(x + 1) = xΓ(x), and for integer arguments, Γ(x + 1) = x!.The digamma function is defined by Ψ (x) ≡ d dx ln Γ(x).For large x (for practical purposes, 0.A -Notation 599 and for small x (for practical purposes, 0 ≤ x ≤ 0.5):where γ e is Euler's constant.What does H −1 2 (1 − R/C) mean?Just as sin −1 (s) denotes the inverse function to s = sin(x), so H −1 2 (h) is the inverse function to h = H 2 (x).There is potential confusion when people use sin 2 x to denote (sin x) 2 , since then we might expect sin −1 s to denote 1/ sin(s); I therefore like to avoid using the notation sin 2 x.What does f (x) mean?The answer depends on the context.Often, a 'prime' is used to denote differentiation:similarly, a dot denotes differentiation with respect to time, t:However, the prime is also a useful indicator for 'another variable', for example 'a new value for a variable'.So, for example, x might denote 'the new value of x'.Also, if there are two integers that both range from 1 to N , I will often name those integers n and n .So my rule is: if a prime occurs in an expression that could be a function, such as f (x) or h (y), then it denotes differentiation; otherwise it indicates 'another variable'.What is the error function?Definitions of this function vary.I define it to be the cumulative probability of a standard (variance = 1) normal distribution,What does E(r) mean?E[r] is pronounced 'the expected value of r' or 'the expectation of r', and it is the mean value of r.Another symbol for 'expected value' is the pair of angle-brackets, r .What does |x| mean?The vertical bars '| • |' have two meanings.If A is a set, then |A| denotes the number of elements in the set; if x is a number, then |x| is the absolute value of x.What does [A|P] mean?Here, A and P are matrices with the same number of rows.[A|P] denotes the double-width matrix obtained by putting A alongside P. The vertical bar is used to avoid confusion with the product AP.What does x T mean?The superscript T is pronounced 'transpose'.Transposing a row-vector turns it into a column vector:(1, 2, 3 (A.9)The determinant of M is denoted det M.What does δ mn mean?The δ matrix is the identity matrix.Another name for the identity matrix is I or 1.Sometimes I include a subscript on this symbol -1 K -which indicates the size of the matrix (K × K).What does δ(x) mean?The delta function has the propertyAnother possible meaning for δ(S) is the truth function, which is 1 if the proposition S is true but I have adopted another notation for that.After all, the symbol δ is quite busy already, with the two roles mentioned above in addition to its role as a small real number δ and an increment operator (as in δx)![S] mean?[S] is the truth function, which is 1 if the proposition S is true and 0 otherwise.For example, the number of positive numbers in the set T = {−2, 1, 3} can be written .11)What is the difference between ':=' and '=' ?In an algorithm, x := y means that the variable x is updated by assigning it the value of y.In contrast, x = y is a proposition, a statement that x is equal to y.See Chapters 23 and 29 for further definitions and notation relating to probability distributions.Most linear codes are expressed in the language of Galois theory Why are Galois fields an appropriate language for linear codes?First, a definition and some examples.+ 0 1 0 0 1 1 1 0• 0 1 0 0 0 1 0 1 For example, the real numbers form a field, with '+' and '•' denoting ordinary addition and multiplication.A Galois field GF (q) is a field with a finite number of elements q.A unique Galois field exists for any q = p m , where p is a prime number and m is a positive integer; there are no other finite fields.GF (2).The addition and multiplication tables for GF (2) are shown in table C.1.These are the rules of addition and multiplication modulo 2.GF (p).For any prime number p, the addition and multiplication rules are those for ordinary addition and multiplication, modulo p.GF (4).The rules for GF (p m ), with m > 1, are not those of ordinary addition and multiplication.For example the tables for GF (4) (table C.2) are notAddition and multiplication tables for GF (4).the rules of addition and multiplication modulo 4. Notice that 1 + 1 = 0, for example.So how can GF (4) be described?It turns out that the elements can be related to polynomials.Consider polynomial functions of x of degree 1 and with coefficients that are elements of GF (2).The polynomials shown in table C.3 obey the addition and multiplication rules of GF (4) if addition and multiplication are modulo the polynomial x 2 + x + 1, and the coefficients of the polynomials are from GF (2).For example, B • B = x 2 + (1 + 1)x + 1 = x = A. Each element may also be represented as a bit pattern as shown in  265 Shannon,Claude,3,14,15,152,164,212,215,262, see noisy-channel coding theorem, source coding theorem, information content shattering, 485 shifter ensemble, 524 Shokrollahi,M. Amin,568 shortening,222 Siegel,Paul,262 sigmoid,473,177,178,223 significance level,51,64,457,463 simplex,173,316 Simpson's