Model is defined by the reward and next state probability distributions, and as we saw in section 18.4, when we know these, we can solve for the optimal policy using dynamic programming.However, these methods are costly, and we seldom have such perfect knowledge of the environment.The more interesting and realistic application of reinforcement learning is when we do not have the model.This requires exploration of the environment to query the model.We first discuss how this exploration is done and later see model-free learning algorithms for deterministic and nondeterministic cases.Though we are not going to assume a full knowledge of the environment model, we will however require that it be stationary.As we will see shortly, when we explore and get to see the value of the next state and reward, we use this information to update the value of the current state.These algorithms are called temporal difference algorithmsbecause what we do is look at the difference between our current estimate of the value of a state (or a state-action pair) and the discounted value of the next state and the reward received.To explore, one possibility is to use -greedy search where with probability , we choose one action uniformly randomly among all possible actions, namely, explore, and with probability 1 − , we choose the best action, namely, exploit.We do not want to continue exploring indefinitely but start exploiting once we do enough exploration; for this, we start with a high value and gradually decrease it.We need to make sure that our policy is soft, that is, the probability of choosing any action a ∈ A in state s ∈ S is greater than 0.We can choose probabilistically, using the softmax function to convert values to probabilitiesb∈A exp Q(s, b) (18.10) and then sample according to these probabilities.To gradually move from exploration to exploitation, we can use a "temperature" variable T and define the probability of choosing action a asWhen T is large, all probabilities are equal and we have exploration.When T is small, better actions are favored.So the strategy is to start with a large T and decrease it gradually, a procedure named annealing, which in this case moves from exploration to exploitation smoothly in time.In model-free learning, we first discuss the simpler deterministic case, where at any state-action pair, there is a single reward and next state possible.In this case, equation 18.7 reduces to Q(s t , a t ) = r t+1 + γ max a t+1 Q(s t+1 , a t+1 ) (18.12) and we simply use this as an assignment to update Q(s t , a t ).When in state s t , we choose action a t by one of the stochastic strategies we saw earlier, which returns a reward r t+1 and takes us to state s t+1 .We then update the value of previous action as Q(s t , a t ) ← r t+1 + γ max a t+1 Q(s t+1 , a t+1 ) (18.13) where the hat denotes that the value is an estimate.Q(s t+1 , a t+1 ) is a later value and has a higher chance of being correct.We discount this by γ and add the immediate reward (if any) and take this as the new estimate for the previous Q(s t , a t ).This is called a backup because it can be viewed as backup taking the estimated value of an action in the next time step and "backing it up" to revise the estimate for the value of a current action.For now we assume that all Q(s, a) values are stored in a table; we will see later on how we can store this information more succinctly when |S| and |A| are large.Initially all Q(s t , a t ) are 0, and they are updated in time as a result of trial episodes.Let us say we have a sequence of moves and at each move, we use equation 18.13 to update the estimate of the Q value of the previous state-action pair using the Q value of the current state-action pair.In the intermediate states, all rewards and therefore values are 0, so no update is done.When we get to the goal state, we get the reward r and then we can update the Q value of the previous state-action pair as γr .As for the preceding state-action pair, its immediate reward is 0 and the contribution from the next state-action pair is discounted by γ because it is one step later.Then in another episode, if we reach this .4Example to show that Q values increase but never decrease.This is a deterministic grid-world where G is the goal state with reward 100, all other immediate rewards are 0, and γ = 0.9.Let us consider the Q value of the transition marked by asterisk, and let us just consider only the two paths A and B. Let us say that path A is seen before path B, then we have γ max(0, 81) = 72.9;if afterward B is seen, a shorter path is found and the Q value becomes γ max(100, 81) = 90.If B is seen before A, the Q value is γ max(100, 0) = 90; then when A is seen, it does not change because γ max(100, 81) = 90.state, we can update the one preceding that as γ 2 r , and so on.This way, after many episodes, this information is backed up to earlier state-action pairs.Q values increase until they reach their optimal values as we find paths with higher cumulative reward, for example, shorter paths, but they never decrease (see figure 18.4).Note that we do not know the reward or next state functions here.They are part of the environment, and it is as if we query them when we explore.We are not modeling them either, though that is another possibility.We just accept them as given and learn directly the optimal policy through the estimated value function.If the rewards and the result of actions are not deterministic, then we have a probability distribution for the reward p(r t+1 |s t , a t ) from which rewards are sampled, and there is a probability distribution for the next state P (s t+1 |s t , a t ).These help us model the uncertainty in the system that may be due to forces we cannot control in the environment: for instance, our opponent in chess, the dice in backgammon, or our lack of Initialize all Q(s, a) arbitrarily For all episodes Initalize s Repeat Choose a using policy derived from Q, e.g., -greedy Take action a, observe r and s Update Q(s, a): Q(s, a) ← Q(s, a) + η(r + γ max a Q(s , a ) − Q(s, a)) s ← s Until s is terminal state knowledge of the system.For example, we may have an imperfect robot which sometimes fails to go in the intended direction and deviates, or advances shorter or longer than expected.In such a case, we haveQ(s t+1 , a t+1 ) (18.14)We cannot do a direct assignment in this case because for the same state and action, we may receive different rewards or move to different next states.What we do is keep a running average.This is known as the Q learning algorithm:Q(s t+1 , a t+1 ) − Q(s t , a t )) (18.15)We think of r t+1 +γ max a t+1 Q(s t+1 , a t+1 ) values as a sample of instances for each (s t , a t ) pair and we would like Q(s t , a t ) to converge to its mean.As usual η is gradually decreased in time for convergence, and it has been shown that this algorithm converges to the optimal Q * values (Watkins and Dayan 1992).The pseudocode of the Q learning algorithm is given in figure 18.5.We can also think of equation 18.15 as reducing the difference between the current Q value and the backed-up estimate, from one time step later.Such algorithms are called temporal difference (TD) algorithms (Sutton temporal difference 1988).This is an off-policy method as the value of the best next action is used off-policy without using the policy.In an on-policy method, the policy is used to on-policy Initialize all Q(s, a) arbitrarily For all episodes Initalize s Choose a using policy derived from Q, e.g., -greedy Repeat Take action a, observe r and s Choose a using policy derived from Q, e.g., -greedy Update Q(s, a): Q(s, a) ← Q(s, a) + η(r + γQ(s , a ) − Q(s, a)) s ← s , a ← a Until s is terminal state determine also the next action.The on-policy version of Q learning is the Sarsa algorithm whose pseudocode is given in figure 18.6.We see that Sarsa instead of looking for all possible next actions a and choosing the best, the on-policy Sarsa uses the policy derived from Q values to choose one next action a and uses its Q value to calculate the temporal difference.On-policy methods estimate the value of a policy while using it to take actions.In off-policy methods, these are separated, and the policy used to generate behavior, called the behavior policy, may in fact be different from the policy that is evaluated and improved, called the estimation policy.Sarsa converges with probability 1 to the optimal policy and stateaction values if a GLIE policy is employed to choose actions.A GLIE (greedy in the limit with infinite exploration) policy is where (1) all stateaction pairs are visited an infinite number of times, and (2) the policy converges in the limit to the greedy policy (which can be arranged, e.g., with -greedy policies by setting = 1/t).The same idea of temporal difference can also be used to learn V (s) values, instead of Q (s, a).TD learning (Sutton 1988) uses the following TD learning update rule to update a state value:V (s t ) ← V (s t ) + η[r t+1 + γV (s t+1 ) − V (s t )] (18.16)This again is the delta rule where r t+1 + γV (s t+1 ) is the better, later prediction and V (s t ) is the current estimate.Their difference is the temporal difference, and the update is done to decrease this difference.The update factor η is gradually decreased, and TD is guaranteed to converge to the optimal value function V * (s).The previous algorithms are one-step-that is, the temporal difference is used to update only the previous value (of the state or state-action pair).An eligibility trace is a record of the occurrence of past visits that eneligibility trace ables us to implement temporal credit assignment, allowing us to update the values of previously occurring visits as well.We discuss how this is done with Sarsa to learn Q values; adapting this to learn V values is straightforward.To store the eligibility trace, we require an additional memory variable associated with each state-action pair, e(s, a), initialized to 0. When the state-action pair (s, a) is visited, namely, when we take action a in state s, its eligibility is set to 1; the eligibilities of all other state-action pairs are multiplied by γλ.0 ≤ λ ≤ 1 is the trace decay parameter.e t (s, a) = 1 i f s = s t and a = a t , γλe t−1 (s, a) otherwise (18.17)If a state-action pair has never been visited, its eligibility remains 0; if it has been, as time passes and other state-actions are visited, its eligibility decays depending on the value of γ and λ (see figure 18.7).We remember that in Sarsa, the temporal error at time t isIn Sarsa with an eligibility trace, named Sarsa(λ), all state-action pairs are updated as (s, a), ∀s, a (18.19)This updates all eligible state-action pairs, where the update depends on how far they have occurred in the past.The value of λ defines the temporal credit: If λ = 0, only a one-step update is done.The algorithms we discussed in section 18.5.3are such, and for this reason they are named Q(0), Sarsa(0), or TD(0).As λ gets closer to 1, more of the previous steps are considered.When λ = 1, all previous steps are updated and the credit given to them falls only by γ per step.In online updating, all eligible values are updated immediately after each step; in offline updating, the updates are accumulated and a single update is done at TD(λ) algorithms can similarly be derived (Sutton and Barto 1998).Until now, we assumed that the Q(s, a) values (or V (s), if we are estimating values of states) are stored in a lookup table, and the algorithms we considered earlier are called tabular algorithms.There are a number of problems with this approach: (1) when the number of states and the number of actions is large, the size of the table may become quite large; (2) states and actions may be continuous, for example, turning the steering wheel by a certain angle, and to use a table, they should be discretized which may cause error; and (3) when the search space is large, too many episodes may be needed to fill in all the entries of the table with acceptable accuracy.Instead of storing the Q values as they are, we can consider this a regression problem.This is a supervised learning problem where we define a regressor Q(s, a|θ), taking s and a as inputs and parameterized by a Initialize all Q(s, a) arbitrarily, e(s, a) ← 0, ∀s, a For all episodes Initalize s Choose a using policy derived from Q, e.g., -greedy Repeat Take action a, observe r and s Choose a using policy derived from Q, e.g., -greedyvector of parameters, θ, to learn Q values.For example, this can be an artificial neural network with s and a as its inputs, one output, and θ its connection weights.A good function approximator has the usual advantages and solves the problems discussed previously.A good approximation may be achieved with a simple model without explicitly storing the training instances; it can use continuous inputs; and it allows generalization.If we know that similar (s, a) pairs have similar Q values, we can generalize from past cases and come up with good Q(s, a) values even if that state-action pair has never been encountered before.To be able to train the regressor, we need a training set.In the case of Sarsa(0), we saw before that we would like Q(s t , a t ) to get close to r t+1 + γQ(s t+1 , a t+1 ).So, we can form a set of training samples where the input is the state-action pair (s t , a t ) and the required output is r t+1 + γQ(s t+1 , a t+1 ).We can write the squared error asTraining sets can similarly be defined for Q(0) and TD(0), where in the latter case we learn V (s), and the required output is r t+1 + γV (s t+1 ).Once such a set is ready, we can use any supervised learning algorithm for learning the training set.If we are using a gradient descent method, as in training neural networks, the parameter vector is updated asThis is a one-step update.In the case of Sarsa(λ), the eligibility trace is also taken into account: 18.22) where the temporal difference error isand the vector of eligibilities of parameters are updated aswith e 0 all zeros.In the case of a tabular algorithm, the eligibilities are stored for the state-action pairs because they are the parameters (stored as a table).In the case of an estimator, eligibility is associated with the parameters of the estimator.We also note that this is very similar to the momentum method for stabilizing backpropagation (section 11.8.1).The difference is that in the case of momentum previous weight changes are remembered, whereas here previous gradient vectors are remembered.Depending on the model used for Q(s t , a t ), for example, a neural network, we plug its gradient vector in equation 18.23.In theory, any regression method can be used to train the Q function, but the particular task has a number of requirements.First, it should allow generalization; that is, we really need to guarantee that similar states and actions have similar Q values.This also requires a good coding of s and a, as in any application, to make the similarities apparent.Second, reinforcement learning updates provide instances one by one and not as a whole training set, and the learning algorithm should be able to do individual updates to learn the new instance without forgetting what has been learned before.For example, a multilayer perceptron using backpropagation can be trained with a single instance only if a small learning rate is used.Or, such instances may be collected to form a training set and learned altogether but this slows down learning as no learning happens while a sufficiently large sample is being collected.Because of these reasons, it seems a good idea to use local learners to learn the Q values.In such methods, for example, radial basis functions, information is localized and when a new instance is learned, only a local part of the learner is updated without possibly corrupting the information in another part.The same requirements apply if we are estimating the state values as V (s t |θ).In certain applications, the agent does not know the state exactly.It is equipped with sensors that return an observation, which the agent then uses to estimate the state.Let us say we have a robot that navigates in a room.The robot may not know its exact location in the room, or what else is there in the room.The robot may have a camera with which sensory observations are recorded.This does not tell the robot its state exactly but gives some indication as to its likely state.For example, the robot may only know that there is an obstacle to its right.The setting is like a Markov decision process, except that after taking an action a t , the new state s t+1 is not known, but we have an observation o t+1 that is a stochastic function of s t and a t : p(o t+1 |s t , a t ).This is called a partially observable MDP (POMDP).If o t+1 = s t+1 , then POMDP reduces to partially observable MDP the MDP.This is just like the distinction between observable and hidden Markov models and the solution is similar; that is, from the observation, we need to infer the state (or rather a probability distribution for the states) and then act based on this.If the agent believes that it is in state s 1 with probability 0.4 and in state s 2 with probability 0.6, then the value of any action is 0.4 times the value of the action in s 1 plus 0.6 times the value of the action in s 2 .The Markov property does not hold for observations.The next state observation does not only depend on the current action and observation.When there is limited observation, two states may appear the same but are different and if these two states require different actions, this can lead to a loss of performance, as measured by the cumulative reward.The agent should somehow compress the past trajectory into a current unique state estimate.These past observations can also be taken into account by taking a past window of observations as input to the policy, Figure 18.9In the case of a partially observable environment, the agent has a state estimator (SE) that keeps an internal belief state b and the policy π generates actions based on the belief states.or one can use a recurrent neural network (section 11.12.2) to maintain the state without forgetting past observations.At any time, the agent may calculate the most likely state and take an action accordingly.Or it may take an action to gather information and reduce uncertainty, for example, search for a landmark, or stop to ask for direction.This implies the importance of the value of information, value of information and indeed POMDPs can be modeled as dynamic influence diagrams (section 14.8).The agent chooses between actions based on the amount of information they provide, the amount of reward they produce, and how they change the state of the environment.To keep the process Markov, the agent keeps an internal belief state b t belief state that summarizes its experience (see figure 18.9).The agent has a state estimator that updates the belief state b t+1 based on the last action a t , current observation o t+1 , and its previous belief state b t .There is a policy π that generates the next action a t+1 based on this belief state, as opposed to the actual state that we had in a completely observable environment.The belief state is a probability distribution over states of the environment given the initial belief state (before we did any actions) and the past observation-action history of the agent (without leaving out any information that could improve agent's performance).Q learning in such a case involves the belief state-action pair values, instead of the actual state-action pairs:We now discuss an example that is a slightly different version of the Tiger problem discussed in Kaelbling, Littman, and Cassandra 1998, modified as in the example in Thrun, Burgard, and Fox 2005.Let us say we are standing in front of two doors, one to our left and the other to other right, leading to two rooms.Behind one of the two doors, we do not know which, there is a crouching tiger, and behind the other, there is a treasure.If we open the door of the room where the tiger is, we get a large negative reward, and if we open the door of the treasure room, we get some positive reward.The hidden state, z L , is the location of the tiger.Let us say p denotes the probability that tiger is in the room to the left and therefore, the tiger is in the room to the right with probability 1 − p: We can calculate the expected reward for the two actions.There are no future rewards because the episode ends once we open one of the doors.Given these rewards, if p is close to 1, if we believe that there is a high chance that the tiger is on the left, the right action will be to choose the right door, and, similarly, for p close to 0, it is better to choose the left door.The two intersect for p around 0.5, and there the expected reward is approximately −10.The fact that the expected reward is negative when p is around 0.5 (when we have uncertainty) indicates the importance of collecting information.If we can add sensors to to decrease uncertaintythat is, move p away from 0.5 to either close to 0 or close to 1-we can take actions with high positive rewards.That sensing action, a S , may have a small negative reward: R(a S ) = −1; this may be considered as the cost of sensing or equivalent to discounting future reward by γ < 1 because we are postponing taking the real action (of opening one of the doors).In such a case, the expected rewards and value of the best action are shown in figure 18.10a:Let us say as sensory input, we use microphones to check whether the tiger is behind the left or the right door.But we have unreliable sensors (so that we still stay in the realm of partial observability).Let us say we can only detect tiger's presence with 0.7 probability:If we sense o L , our belief in the tiger's position changes:The effect of this is shown in figure 18.10b where we plot R(a L |o L ).Sensing o L turns opening the right door into a better action for a wider range.The better sensors we have (if the probability of correct sensing moves from 0.7 closer to 1), the larger this range gets (exercise 9).Similarly, as we see in figure 18.10c, if we sense o R , this increases the chances of opening the left door.Note that sensing also decreases the range where there is a need to sense (once more).The expected rewards for the actions in this case are The best action is this case is the maximum of these three.Similarly, if we sense o R , the expected rewards becomeTo calculate the expected reward, we need to take average over both sensor readings weighted by their probabilities:Note that when we multiply by P (o L ), it cancels out and we get functions linear in p.These five lines and the piecewise function that corresponds to their maximum are shown in figure 18.10d.Note that the line, −40p − 5(1 − p), as well as the ones involving a S , are beneath others for all values of p and can safely be pruned.The fact that figure 18.10d is better than figure 18.10a indicates the value of information.What we calculate here is the value of the best action had we chosen a S .For example, the first line corresponds to choosing a L after a S .So to find the best decision with an episode of length two, we need to back this up by subtracting −1, which is the reward of a S , and get the expected reward for the action of sense.Equivalently, we can consider this as waiting that has an immediate reward of 0 but discounts the future reward by some γ < 1.We also have the two usual actions of a L and a R and we choose the best of three; the two immediate actions and the one discounted future action.Let us now make the problem more interesting, as in the example of Thrun, Burgard, and Fox 2005.Let us assume that there is a door between the two rooms and without us seeing, the tiger can move from one room to the other.Let us say that this is a restless tiger and it stays in the same room with probability 0.2 and moves to the other room with probability 0.8.This means that p should also be updated asand this updated p should be used in equation 18.25 while choosing the best action after having chosen a S : Figure 18.11b corresponds to figure 18.10d with the updated p .Now, when planning for episodes of length two, we have the two immediate actions of a L and a R , or we wait and sense when p changes and then we take the action and get its discounted reward (figure 18.11b):We see that figure 18.11b is better than figure 18.10a; when wrong actions may lead to large penalty, it is better to defer judgment, look for extra information, and plan ahead.We can consider longer episodes by continuing the iterative updating of p and discounting by subtracting 1 and including the two immediate actions to calculate V t , t > 2.The algorithm we have just discussed where the value is represented by piecewise linear functions works only when the number of states, actions, observations, and the episode length are all finite.Even in applications where any of these is not small, or when any is continuous-valued, the complexity becomes high and we need to resort to approximate algorithms having reasonable complexity.Reviews of such algorithms are given in Hauskrecht 2000 andThrun, Burgard, andFox 2005.More information on reinforcement learning can be found in the textbook by Sutton and Barto (1998) that discusses all the aspects, learning algorithms, and several applications.A comprehensive tutorial is Kaelbling, Littman, and Moore 1996.Recent work on reinforcement learning applied to robotics with some impressive applications is given in Thrun, Burgard, and Fox 2005.Dynamic programming methods are discussed in Bertsekas 1987 andin Bertsekas andTsitsiklis 1996, andTD(λ) and Q-learning can be seen as stochastic approximations to dynamic programming (Jaakkola, Jordan, and Singh 1994).Reinforcement learning has two advantages over classical dynamic programming: First, as they learn, they can focus on the parts of the space that are important and ignore the rest; and second, they can employ function approximation methods to represent knowledge that allows them to generalize and learn faster.A related field is that of learning automata (Narendra and Thathachar learning automata 1974), which are finite state machines that learn by trial and error for solving problems like the K-armed bandit.The setting we have here is also the topic of optimal control where there is a controller (agent) taking actions in a plant (environment) that minimize cost (maximize reward).The earliest use of temporal difference method was in Samuel's checkers program written in 1959 (Sutton and Barto 1998).For every two successive positions in a game, the two board states are evaluated by the board evaluation function that then causes an update to decrease the difference.There has been much work on games because games are both easily defined and challenging.A game like chess can easily be simulated: The allowed moves are formal, and the goal is well defined.Despite the simplicity of defining the game, expert play is quite difficult.One of the most impressive application of reinforcement learning is the TD-Gammon program that learns to play backgammon by playing TD-Gammon against itself (Tesauro 1995).This program is superior to the previous neurogammon program also developed by Tesauro, which was trained in a supervised manner based on plays by experts.Backgammon is a complex task with approximately 10 20 states, and there is randomness due to the roll of dice.Using the TD(λ) algorithm, the program achieves master level play after playing 1,500,000 games against a copy of itself.Another interesting application is in job shop scheduling, or finding a schedule of tasks satisfying temporal and resource constraints (Zhang and Dietterich 1996).Some tasks have to be finished before others can be started, and two tasks requiring the same resource cannot be done simultaneously.Zhang and Dietterich used reinforcement learning to quickly find schedules that satisfy the constraints and are short.Each state is one schedule, actions are schedule modifications, and the program finds not only one good schedule but a schedule for a class of related scheduling problems.Recently hierarchical methods have also been proposed where the problem is decomposed into a set of subproblems.This has the advantage that policies learned for the subproblems can be shared for multiple problems, which accelerates learning a new problem (Dietterich 2000).Each subproblem is simpler and learning them separately is faster.The disadvantage is that when they are combined, the policy may be suboptimal.Though reinforcement learning algorithms are slower than supervised learning algorithms, it is clear that they have a wider variety of application and have the potential to construct better learning machines (Ballard 1997).They do not need any supervision, and this may actually be better since then they are not biased by the teacher.For example, Tesauro's TD-Gammon program in certain circumstances came up with moves that turned out to be superior to those made by the best players.The field of reinforcement learning is developing rapidly, and we may expect to see other impressive results in the near future.1. Given the grid world in figure 18.12, if the reward on reaching on the goal is 100 and γ = 0.9, calculate manually Q * (s, a), V * (S), and the actions of optimal policy.2. With the same configuration given in exercise 1, use Q learning to learn the optimal policy.also stochastic in that when the robot advances in a direction, it moves in the intended direction with probability 0.5 and there is a 0.25 probability that it moves in one of the lateral directions.Learn Q(s, a) in this case.6. Assume we are estimating the value function for states V (s) and that we want to use TD(λ) algorithm.Derive the tabular value iteration update.The temporal error at time t isAll state values are updated aswhere the eligibility of states decay in time:Using equation 18.22, derive the weight update equations when a multilayer perceptron is used to estimate Q.SOLUTION: Let us say for simplicity we have one-dimensional state value s t and one-dimensional action value a t , and let us assume a linear model:We can update the three parameters w 1 , w 2 , w 3 using gradient descent (equation 16.21):In the case of a multilayer perceptron, only the last term will differ to update the weights on all layers.In the case of Sarsa(λ), e is three-dimensional: e 1 for w 1 , e 2 for w 2 , and e 3 for w 0 .We update the eligibilities (equation 18.23):and we update the weights using the eligibilities (equation 18.22):Give an example of a reinforcement learning application that can be modeled by a POMDP.Define the states, actions, observations, and reward.9. In the tiger example, show that as we get a more reliable sensor, the range where we need to sense once again decreases.10. Rework the tiger example using the following reward matrix r (A, Z) Tiger left Tiger right Open left −100 +10Open right 20 −100In previous chapters, we discussed several learning algorithms and saw that, given a certain application, more than one is applicable.Now, we are concerned with two questions:1. How can we assess the expected error of a learning algorithm on a problem?That is, for example, having used a classification algorithm to train a classifier on a dataset drawn from some application, can we say with enough confidence that later on when it is used in real life, its expected error rate will be less than, for example, 2 percent?2. Given two learning algorithms, how can we say one has less error than the other one, for a given application?The algorithms compared can be different, for example, parametric versus nonparametric, or they can use different hyperparameter settings.For example, given a multilayer perceptron (chapter 11) with four hidden units and another one with eight hidden units, we would like to be able to say which one has less expected error.Or with the k-nearest neighbor classifier (chapter 8), we would like to find the best value of k.We cannot look at the training set errors and decide based on those.The error rate on the training set, by definition, is always smaller than the error rate on a test set containing instances unseen during training.Similarly, training errors cannot be used to compare two algorithms.This is because over the training set, the more complex model having more parameters will almost always give fewer errors than the simple one.So as we have repeatedly discussed, we need a validation set that is different from the training set.Even over a validation set though, just one run may not be enough.There are two reasons for this: First, the training and validation sets may be small and may contain exceptional instances, like noise and outliers, which may mislead us.Second, the learning method may depend on other random factors affecting generalization.For example, with a multilayer perceptron trained using backpropagation, because gradient descent converges to the nearest local minimum, the initial weights affect the final weights, and given the exact same architecture and training set, starting from different initial weights, there may be multiple possible final classifiers having different error rates on the same validation set.We thus would like to have several runs to average over such sources of randomness.If we train and validate only once, we cannot test for the effect of such factors; this is only admissible if the learning method is so costly that it can be trained and validated only once.We use a learning algorithm on a dataset and generate a learner.If we do the training once, we have one learner and one validation error.To average over randomness (in training data, initial weights, etc.), we use the same algorithm and generate multiple learners.We test them on multiple validation sets and record a sample of validation errors.(Of course, all the training and validation sets should be drawn from the same application.)We base our evaluation of the learning algorithm on the distribution of these validation errors.We can use this distribution for assessing the expected error of the learning algorithm for that problem, or compare it expected error with the error rate distribution of some other learning algorithm.Before proceeding to how this is done, it is important to stress a number of points:1. We should keep in mind that whatever conclusion we draw from our analysis is conditioned on the dataset we are given.We are not comparing learning algorithms in a domain independent way but on some particular application.We are not saying anything about the expected error of a learning algorithm, or comparing one learning algorithm with another algorithm, in general.Any result we have is only true for the particular application, and only insofar as that application is rep-resented in the sample we have.And anyway, as stated by the No FreeNo Free Lunch TheoremLunch Theorem (Wolpert 1995), there is no such thing as the "best" learning algorithm.For any learning algorithm, there is a dataset where it is very accurate and another dataset where it is very poor.When we say that a learning algorithm is good, we only quantify how well its inductive bias matches the properties of the data.2. The division of a given dataset into a number of training and validation set pairs is only for testing purposes.Once all the tests are complete and we have made our decision as to the final method or hyperparameters, to train the final learner, we can use all the labeled data that we have previously used for training or validation.3. Because we also use the validation set(s) for testing purposes, for example, for choosing the better of two learning algorithms, or to decide where to stop learning, it effectively becomes part of the data we use.When after all such tests, we decide on a particular algorithm and want to report its expected error, we should use a separate test set for this purpose, unused during training this final system.This data should have never been used before for training or validation and should be large for the error estimate to be meaningful.So, given a dataset, we should first leave some part of it aside as the test set and use the rest for training and validation.Typically, we can leave one-third of the sample as the test set, then use two-thirds for cross-validation to generate multiple training/validation set pairs, as we will see shortly.So, the training set is used to optimize the parameters, given a particular learning algorithm and model structure; the validation set is used to optimize the hyperparameters of the learning algorithm or the model structure; and the test set is used at the end, once both these have been optimized.For example, with an MLP, the training set is used to optimize the weights, the validation set is used to decide on the number of hidden units, how long to train, the learning rate, and so forth.Once the best MLP configuration is chosen, its final error is calculated on the test set.With k-NN, the training set is stored as the lookup table; we optimize the distance measure and k on the validation set and test finally on the test set.4. In general, we compare learning algorithms by their error rates, but it should be kept in mind that in real life, error is only one of the criteria that affect our decision.Some other criteria are (Turney 2000):risks when errors are generalized using loss functions, instead of 0/1 loss (section 3.3), training time and space complexity, testing time and space complexity, interpretability, namely, whether the method allows knowledge extraction which can be checked and validated by experts, and easy programmability.The relative importance of these factors changes depending on the application.For example, if the training is to be done once in the factory, then training time and space complexity are not important; if adaptability during use is required, then they do become important.Most of the learning algorithms use 0/1 loss and take error as the single criterion to be minimized; recently, cost-sensitive learning variants of cost-sensitive learning these algorithms have also been proposed to take other cost criteria into account.When we train a learner on a dataset using a training set and test its accuracy on some validation set and try to draw conclusions, what we are doing is experimentation.Statistics defines a methodology to design experiments correctly and analyze the collected data in a manner so as to be able to extract significant conclusions (Montgomery 2005).In this chapter, we will see how this methodology can be used in the context of machine learning.As in other branches of science and engineering, in machine learning too, we do experiments to get information about the process under scrutiny.In our case, this is a learner, which, having been trained on a dataset, generates an output for a given input.An experiment is a test or a series experiment of tests where we play with the factors that affect the output.These factors may be the algorithm used, the training set, input features, and so on, and we observe the changes in the response to be able to extract information.The aim may be to identify the most important factors, screen the unimportant ones, or find the configuration of the factors that optimizes the response-for example, classification accuracy on a given test set.Our aim is to plan and conduct machine learning experiments and analyze the data resulting from the experiments, to be able to eliminate the effect of chance and obtain conclusions which we can consider statistically significant.In machine learning, we target a learner having the highest generalization accuracy and the minimal complexity (so that its implementation is cheap in time and space) and is robust, that is, minimally affected by external sources of variability.A trained learner can be shown as in figure 19.1; it gives an output, for example, a class code for a test input, and this depends on two type of factors.The controllable factors, as the name suggests, are those we have control on.The most basic is the learning algorithm used.There are also the hyperparameters of the algorithm, for example, the number of hidden units for a multilayer perceptron, k for k-nearest neighbor, C for support vector machines, and so on.The dataset used and the input representation, that is, how the input is coded as a vector, are other controllable factors.There are also uncontrollable factors over which we have no control, adding undesired variability to the process, which we do not want to affect our decisions.Among these are the noise in the data, the particular training subset if we are resampling from a large set, randomness in the optimization process, for example, the initial state in gradient descent with multilayer perceptrons, and so on.We use the output to generate the response variable-for example, av- erage classification error on a test set, or the expected risk using a loss function, or some other measure, such as precision and recall, as we will discuss shortly.Given several factors, we need to find the best setting for best response, or in the general case, determine their effect on the response variable.For example, we may be using principal components analyzer (PCA) to reduce dimensionality to d before a k-nearest neighbor (k-NN) classifier.The two factors are d and k, and the question is to decide which combination of d and k leads to highest performance.Or, we may be using a support vector machine classifier with Gaussian kernel, and we have the regularization parameter C and the spread of the Gaussian s 2 to fine-tune together.There are several strategies of experimentation, as shown in figure 19.2.In the best guess approach, we start at some setting of the factors that we believe is a good configuration.We test the response there and we fiddle with the factors one (or very few) at a time, testing each combination until we get to a state that we consider is good enough.If the experimenter has a good intuition of the process, this may work well; but note that there is no systematic approach to modify the factors and when we stop, we have no guarantee of finding the best configuration.Another strategy is to modify one factor at a time where we decide on a baseline (default) value for all factors, and then we try different levels for one factor while keeping all other factors at their baseline.The major disadvantage of this is that it assumes that there is no interaction between the factors, which may not always be true.In the PCA/k-NN cascade we discussed earlier, each choice for d defines a different input space for k-NN where a different k value may be appropriate.The correct approach is to use a factorial design where factors are var-To decrease the number of runs necessary, one possibility is to run a fractional factorial design where we run only a subset, another is to try to use knowledge gathered from previous runs to estimate configurations that seem likely to have high response.In searching one factor at a time, if we can assume that the response is typically quadratic (with a single maximum, assuming we are maximizing a response value, such as the test accuracy), then instead of trying all values, we can have an iterative procedure where starting from some initial runs, we fit a quadratic, find its maximum analytically, take that as the next estimate, run an experiment there, add the resulting data to the sample, and then continue fitting and sampling, until we get no further improvement.With many factors, this is generalized as the response surface design response surface design method where we try to fit a parametric response function to the factors aswhere r is the response and f i , i = 1, . . ., F are the factors.This fitted parametric function defined given the parameters φ is our empirical model estimating the response for a particular configuration of the (controllable) factors; the effect of uncontrollable factors is modeled as noise.g(•) is a (typically quadratic) regression model and after a small number of runs around some baseline (as defined by a so-called design matrix), one can have enough data to fit g(•) on.Then, we can analytically calculate the values of f i where the fitted g is maximum, which we take as our next guess, run an experiment there, get a data instance, add it to the sample, fit g once more, and so on, until there is convergence.Whether this approach will work well or not depends on whether the response can indeed be written as a quadratic function of the factors with a single maximum.Let us now talk about the three basic principles of experimental design.Randomization requires that the order in which the runs are carried randomization out should be randomly determined so that the results are independent.This is typically a problem in real-world experiments involving physical objects; for example, machines require some time to warm up until they operate in their normal range so tests should be done in random order for time not to bias the results.Ordering generally is not a problem in software experiments.Replication implies that for the same configuration of (controllable) replication factors, the experiment should be run a number of times to average over the effect of uncontrollable factors.In machine learning, this is typically done by running the same algorithm on a number of resampled versions of the same dataset; this is known as cross-validation, which we will discuss in section 19.6.How the response varies on these different replications of the same experiment allows us to obtain an estimate of the experimental error (the effect of uncontrollable factors), which we can in turn use to determine how large differences should be to be deemed statistically significant.Blocking is used to reduce or eliminate the variability due to nuisance blocking factors that influence the response but in which we are not interested.For example, defects produced in a factory may also depend on the different batches of raw material, and this effect should be isolated from the controllable factors in the factory, such as the equipment, personnel, and so on.In machine learning experimentation, when we use resampling and use different subsets of the data for different replicates, we need to make sure that for example if we are comparing learning algorithms, they should all use the same set of resampled subsets, otherwise the differences in accuracies would depend not only on the algorithms but also on the different subsets-to be able to measure the difference due to algorithms only, the different training sets in replicated runs should be identical; this is what we mean by blocking.In statistics, if there are two populations, this is called pairing and is pairing used in paired testing.Before we start experimentation, we need to have a good idea about what it is we are studying, how the data is to be collected, and how we are planning to analyze it.The steps in machine learning are the same as for any type of experimentation (Montgomery 2005).Note that at this point, it is not important whether the task is classification or regression, or whether it is an unsupervised or a reinforcement learning application.The same overall discussion applies; the difference is only in the sampling distribution of the response data that is collected.We need to start by stating the problem clearly, defining what the objectives are.In machine learning, there may be several possibilities.As we discussed before, we may be interested in assessing the expected error (or some other response measure) of a learning algorithm on a particular problem and check that, for example, the error is lower than a certain acceptable level.Given two learning algorithms and a particular problem as defined by a dataset, we may want to determine which one has less generalization error.These can be two different algorithms, or one can be a proposed improvement of the other, for example, by using a better feature extractor.In the general case, we may have more than two learning algorithms, and we may want to choose the one with the least error, or order them in terms of error, for a given dataset.In an even more general setting, instead of on a single dataset, we may want to compare two or more algorithms on two or more datasets.We need to decide on what we should use as the quality measure.Most frequently, error is used that is the misclassification error for classification and mean square error for regression.We may also use some variant; for example, generalizing from 0/1 to an arbitrary loss, we may use a risk measure.In information retrieval, we use measures such as precision and recall; we will discuss such measures in section 19.7.In a cost-sensitive setting, not only the output but also system parameters, for example, its complexity, are taken into account.What the factors are depend on the aim of the study.If we fix an algorithm and want to find the best hyperparameters, then those are the factors.If we are comparing algorithms, the learning algorithm is a factor.If we have different datasets, they also become a factor.The levels of a factor should be carefully chosen so as not to miss a good configuration and avoid doing unnecessary experimentation.It is always good to try to normalize factor levels.For example, in optimizing k of k-nearest neighbor, one can try values such as 1, 3, 5, and so on, but in optimizing the spread h of Parzen windows, we should not try absolute values such as 1.0, 2.0, and so on, because that depends on the scale of the input; it is better to find some statistic that is an indicator of scale-for example, the average distance between an instance and its nearest neighbor-and try h as different multiples of that statistic.Though previous expertise is a plus in general, it is also important to investigate all factors and factor levels that may be of importance and not be overly influenced by past experience.It is always better to do a factorial design unless we are sure that the factors do not interact, because mostly they do.Replication number depends on the dataset size; it can be kept small when the dataset is large; we will discuss this in the next section when we talk about resampling.However, too few replicates generate few data and this will make comparing distributions difficult; in the particular case of parametric tests, the assumptions of Gaussianity may not be tenable.Generally, given some dataset, we leave some part as the test set and use the rest for training and validation, probably many times by resampling.How this division is done is important.In practice, using small datasets leads to responses with high variance, and the differences will not be significant and results will not be conclusive.It is also important to avoid as much as possible toy, synthetic data and use datasets that are collected from real-world under real-life circumstances.Didactic one-or two-dimensional datasets may help provide intuition, but the behavior of the algorithms may be completely different in high-dimensional spaces.Before running a large factorial experiment with many factors and levels, it is best if one does a few trial runs for some random settings to check that all is as expected.In a large experiment, it is always a good idea to save intermediate results (or seeds of the random number generator), so that a part of the whole experiment can be rerun when desired.All the results should be reproducable.In running a large experiment with many factors and factor levels, one should be aware of the possible negative effects of software aging.It is important that an experimenter be unbiased during experimentation.In comparing one's favorite algorithm with a competitor, both should be investigated equally diligently.In large-scale studies, it may even be envisaged that testers be different from developers.One should avoid the temptation to write one's own "library" and instead, as much as possible, use code from reliable sources; such code would have been better tested and optimized.As in any software development study, the advantages of good documentation cannot be underestimated, especially when working in groups.All the methods developed for high-quality software engineering should also be used in machine learning experiments.This corresponds to analyzing data in a way so that whatever conclusion we get is not subjective or due to chance.We cast the questions that we want to answer in the framework of hypothesis testing and check whether the sample supports the hypothesis.For example, the question "Is A a more accurate algorithm than B?" becomes the hypothesis "Can we say that the average error of learners trained by A is significantly lower than the average error of learners trained by B?"As always, visual analysis is helpful, and we can use histograms of error distributions, whisker-and-box plots, range plots, and so on.Once all data is collected and analyzed, we can draw objective conclusions.One frequently encountered conclusion is the need for further experimentation.Most statistical, and hence machine learning or data mining, studies are iterative.It is for this reason that we never start with all the experimentation.It is suggested that no more than 25 percent of the available resources should be invested in the first experiment (Montgomery 2005).The first runs are for investigation only.That is also why it is a good idea not to start with high expectations, or promises to one's boss or thesis advisor.We should always remember that statistical testing never tells us if the hypothesis is correct or false, but how much the sample seems to concur with the hypothesis.There is always a risk that we do not have a conclusive result or that our conclusions be wrong, especially if the data is small and noisy.When our expectations are not met, it is most helpful to investigate why they are not.For example, in checking why our favorite algorithm A has worked awfully bad on some cases, we can get a splendid idea for some improved version of A. All improvements are due to the deficiencies of the previous version; finding a deficiency is but a helpful hint that there is an improvement we can make!But we should not go to the next step of testing the improved version before we are sure that we have completely analyzed the current data and learned all we could learn from it.Ideas are cheap, and useless unless tested, which is costly.For replication purposes, our first need is to get a number of training and validation set pairs from a dataset X (after having left out some part as the test set).To get them, if the sample X is large enough, we can randomly divide it into K parts, then randomly divide each part into two and use one half for training and the other half for validation.K is typically 10 or 30.Unfortunately, datasets are never large enough to do this.So we should do our best with small datasets.This is done by repeated use of the same data split differently; this is called crosscross-validation validation.The catch is that this makes the error percentages dependent as these different sets share data.So, given a dataset X, we would like to generate K training/validation set pairs, {T i , V i } K i=1 , from this dataset.We would like to keep the training and validation sets as large as possible so that the error estimates are robust, and at the same time, we would like to keep the overlap between different sets as small as possible.We also need to make sure that classes are represented in the right proportions when subsets of data are held out, not to disturb the class prior probabilities; this is called strat-In K-fold cross-validation, the dataset X is divided randomly into K equal-K-fold cross-validation sized parts, X i , i = 1, . . ., K. To generate each pair, we keep one of the K parts out as the validation set and combine the remaining K − 1 parts to form the training set.Doing this K times, each time leaving out another one of the K parts out, we get K pairs:There are two problems with this.First, to keep the training set large, we allow validation sets that are small.Second, the training sets overlap considerably, namely, any two training sets share K − 2 parts.K is typically 10 or 30.As K increases, the percentage of training instances increases and we get more robust estimators, but the validation set becomes smaller.Furthermore, there is the cost of training the classifier K times, which increases as K is increased.As N increases, K can be smaller; if N is small, K should be large to allow large enough training sets.One extreme case of K-fold cross-validation is leave-one-out where leave-one-out given a dataset of N instances, only one instance is left out as the validation set (instance) and training uses the N − 1 instances.We then get N separate pairs by leaving out a different instance at each iteration.This is typically used in applications such as medical diagnosis, where labeled data is hard to find.Leave-one-out does not permit stratification.Recently, with computation getting cheaper, it has also become possible to have multiple runs of K-fold cross-validation, for example, 10 × 10-fold, and use average over averages to get more reliable error estimates (Bouckaert 2003).19.6.2 5 × 2 Cross-Validation Dietterich (1998) proposed the 5 ×2 cross-validation, which uses training 5 × 2 cross-validation and validation sets of equal size.We divide the dataset X randomly into two parts, X(1) 1 and X(2) 1 , which gives our first pair of training and validation sets,1 .Then we swap the role of the two halves and get the second pair:1 .This is the first fold; X (j) i denotes half j of fold i.To get the second fold, we shuffle X randomly and divide this new fold into two, X(1) 2 and X(2) 2 .This can be implemented by drawing these from X randomly without replacement, namely, X(1)We then swap these two halves to get another pair.We do this for three more folds and because from each fold, we get two pairs, doing five folds, we get ten training and validation sets:Of course, we can do this for more than five folds and get more training/validation sets, but Dietterich (1998) points out that after five folds, the sets share many instances and overlap so much that the statistics calculated from these sets, namely, validation error rates, become too dependent and do not add new information.Even with five folds, the sets overlap and the statistics are dependent, but we can get away with this until five folds.On the other hand, if we do have fewer than five folds, we get less data (fewer than ten sets) and will not have a large enough sample to fit a distribution to and test our hypothesis on.To generate multiple samples from a single sample, an alternative to cross-validation is the bootstrap that generates new samples by drawbootstrap ing instances from the original sample with replacement.We saw the use of bootstrapping in section 17.6 to generate training sets for different learners in bagging.The bootstrap samples may overlap more than cross-validation samples and hence their estimates are more dependent; but is considered the best way to do resampling for very small datasets.In the bootstrap, we sample N instances from a dataset of size N with replacement.The original dataset is used as the validation set.The probability that we pick an instance is 1/N; the probability that we do not pick it is 1 − 1/N.The probability that we do not pick it after N draws isThis means that the training data contains approximately 63.2 percent of the instances; that is, the system will not have been trained on 36.8 percent of the data, and the error estimate will be pessimistic.The solution is replication, that is, to repeat the process many times and look at the average behavior.For classification, especially for two-class problems, a variety of measures has been proposed.There are four possible cases, as shown in table 19.1.For a positive example, if the prediction is also positive, this is a true positive; if our prediction is negative for a positive example, this is a false negative.For a negative example, if the prediction is also negative, we In some two-class problems, we make a distinction between the two classes and hence the two types of errors, false positives and false negatives.Different measures appropriate in different settings are given in table 19.2.Let us envisage an authentication application where, for example, users log on to their accounts by voice.A false positive is wrongly logging on an impostor and a false negative is refusing a valid user.It is clear that the two type of errors are not equally bad; the former is much worse.True positive rate, tp-rate, also known as hit rate, measures what proportion of valid users we authenticate and false positive rate, fp-rate, also known as false alarm rate, is the proportion of impostors we wrongly accept.Let us say the system returns P (C 1 |x), the probability of the positive class, and for the negative class, we have P (C 2 |x) = 1 − P (C 1 |x), and we choose "positive" if P (C 1 |x) > θ.If θ is close to 1, we hardly choose the positive class; that is, we will have no false positives but also few true positives.As we decrease θ to increase the number of true positives, we risk introducing false positives.For different values of θ, we can get a number of pairs of (tp-rate, fp-rate) values and by connecting them we get the receiver operating under different loss matrices (see exercise 1).Ideally, a classifier has a tp-rate of 1 and an fp-rate of 0, and hence a classifier is better the more its ROC curve gets closer to the upperleft corner.On the diagonal, we make as many true decisions as false ones, and this is the worst one can do (any classifier that is below the diagonal can be improved by flipping its decision).Given two classifiers, we can say one is better than the other one if its ROC curve is above the ROC curve of the other one; if the two curves intersect, we can say that the two classifiers are better under different loss conditions, as seen in figure 19.3b.ROC allows a visual analysis; if we want to reduce the curve to a single number we can do this by calculating the area under the curve (AUC) .A Area under the curve classifier ideally has an AUC of 1 and AUC values of different classifiers can be compared to give us a general performance averaged over different loss conditions.In information retrieval, there is a database of records; we make a information retrieval query, for example, by using some keywords, and a system (basically a two-class classifier) returns a number of records.In the database, there are relevant records and for a query, the system may retrieve some of them (true positives) but probably not all (false negatives); it may also wrongly retrieve records that are not relevant (false positives).The set of relevant and retrieved records can be visualized using a Venn diagram, as shown in figure 19.4a.Precision is the number of retrieved and relevant precision records divided by the total number of retrieved records; if precision is 1, all the retrieved records may be relevant but there may still be records that are relevant but not retrieved.Recall is the number of retrieved relrecall evant records divided by the total number of relevant records; even if recall is 1, all the relevant records may be retrieved but there may also be irrelevant records that are retrieved, as shown in figure19.4c.As in the ROC curve, for different threshold values, one can draw a curve for precision vs. recall.From another perspective but with the same aim, there are the two measures of sensitivity and specificity.Sensitivity is the same as tp-rate sensitivity specificity and recall.Specificity is how well we detect the negatives, which is the number of true negatives divided by the total number of negatives; this is equal to 1 minus the false alarm rate.One can also draw a sensitivity vs. specificity curve using different thresholds.In the case of K > 2 classes, if we are using 0/1 error, the class confuclass confusion matrix sion matrix is a K ×K matrix whose entry (i, j) contains the number of instances that belong to C i but are assigned to C j .Ideally, all off-diagonals should be 0, for no misclassification.The class confusion matrix allows us to pinpoint what types of misclassification occur, namely, if there are two classes that are frequently confused.Or, one can define K separate two-class problems, each one separating one class from the other K − 1.Let us now do a quick review of interval estimation that we will use in hyinterval estimation pothesis testing.A point estimator, for example, the maximum likelihood estimator, specifies a value for a parameter θ.In interval estimation, we specify an interval within which θ lies with a certain degree of confidence.To obtain such an interval estimator, we make use of the probability distribution of the point estimator.all the retrieved records are relevant but there may be relevant ones not retrieved.(c) Recall is 1; all the relevant records are retrieved but there may also be irrelevant records that are retrieved.For example, let us say we are trying to estimate the mean μ of a normal density from a sample X = {x t } N t=1 .m = t x t /N is the sample average and is the point estimator to the mean.m is the sum of normals and therefore is also normal, m ∼ N (μ, σ 2 /N).We define the statistic with a unit normal distribution:We know that 95 percent of Z lies in (−1.96, 1.96), namely, P {−1.96 < Z < 1.96} = 0.95, and we can write (see figure 19 That is, "with 95 percent confidence," μ will lie within 1.96σ / √ N units of the sample average.This is a two-sided confidence interval.With 99 two-sided confidence interval percent confidence, μ will lie in (m − 2.58σ / √ N, m + 2.58σ / √ N); that is, if we want more confidence, the interval gets larger.The interval gets smaller as N, the sample size, increases.This can be generalized for any required confidence as follows.Let us denote z α such thatBecause Z is symmetric around the mean, z 1−α/2 = −z α/2 , and P {X < −z α/2 } = P {X > z α/2 } = α/2.Hence for any specified level of confidence 1 − α, we haveHence a 100(1 − α) percent two-sided confidence interval for μ can be computed for any α.Similarly, knowing that P {Z < 1.64} = 0.95, we have (see figure 19.6)terval for μ, which defines a lower bound.Generalizing, a 100(1 − α) percent one-sided confidence interval for μ can be computed fromSimilarly, the one-sided lower confidence interval that defines an upper bound can also be calculated.In the previous intervals, we used σ ; that is, we assumed that the variance is known.If it is not, one can plug the sample varianceinstead of σ 2 .We know that when x t ∼ N (μ, σ 2 ), (N − 1)S 2 /σ 2 is chisquare with N − 1 degrees of freedom.We also know that m and S 2 are independent.Then,Hence for any α ∈ (0, 1/2), we can define an interval, using the values specified by the t distribution, instead of the unit normal Z Similarly, one-sided confidence intervals can be defined.The t distribution has larger spread (longer tails) than the unit normal distribution, and generally the interval given by the t is larger; this should be expected since additional uncertainty exists due to the unknown variance.Instead of explicitly estimating some parameters, in certain applications we may want to use the sample to test some particular hypothesis concerning the parameters.For example, instead of estimating the mean, we may want to test whether the mean is less than 0.02.If the random sample is consistent with the hypothesis under consideration, we "fail to reject" the hypothesis; otherwise, we say that it is "rejected."But when we make such a decision, we are not really saying that it is true or false but rather that the sample data appears to be consistent with it to a given degree of confidence or not.In hypothesis testing, the approach is as follows.We define a statistic hypothesis testing Let us say we have a sample from a normal distribution with unknown mean μ and known variance σ 2 , and we want to test a specific hypothesis about μ, for example, whether it is equal to a specified constant μ 0 .It is denoted as H 0 and is called the null hypothesis null hypothesis H 0 : μ = μ 0 against the alternative hypothesis H 1 : μ = μ 0 m is the point estimate of μ, and it is reasonable to reject H 0 if m is too far from μ 0 .This is where the interval estimate is used.We fail to reject the hypothesis with level of significance α if μ 0 lies in the 100(1 − α)We reject the null hypothesis if it falls outside, on either side.This is a two-sided test.If we reject when the hypothesis is correct, this is a type I error and type I error thus α, set before the test, defines how much type I error we can tolerate, typical values being α = 0.1, 0.05, 0.01 (see table 19.3).A type II error is type II error if we fail to reject the null hypothesis when the true mean μ is unequal to μ 0 .The probability that H 0 is not rejected when the true mean is μ is a function of μ and is given as μ) is called the power function of the test and is equal to the power function probability of rejection when μ is the true value.Type II error probability increases as μ and μ 0 gets closer, and we can calculate how large a sample we need for us to be able to detect a difference δ = |μ −μ 0 | with sufficient power.One can also have a one-sided test of the form one-sided testas opposed to the two-sided test when the alternative hypothesis is μ = μ 0 .The one-sided test with α level of significance defines the 100(1 − α) confidence interval bounded on one side in which m should lie for the hypothesis not to be rejected.We fail to reject ifand reject outside.Note that the null hypothesis H 0 also allows equality, which means that we get ordering information only if the test rejects.This tells us which of the two one-sided tests we should use.Whatever claim we have should be in H 1 so that rejection of the test would support our claim.If the variance is unknown, just as we did in the interval estimates, we use the sample variance instead of the population variance and the fact thatwhich is known as the two-sided t test.A one-sided t test can be defined t test similarly.Now that we have reviewed hypothesis testing, we are ready to see how it is used in testing error rates.We will discuss the case of classification error, but the same methodology applies for squared error in regression, log likelihoods in unsupervised learning, expected reward in reinforcement learning, and so on, as long as we can write the appropriate parametric form for the sampling distribution.We will also discuss nonparametric tests when no such parametric form can be found.We now start with error rate assessment, and, in the next section, we discuss error rate comparison.Let us start with the case where we have a single training set T and a single validation set V .We train our classifier on T and test it on V .We denote by p the probability that the classifier makes a misclassification error.We do not know p; it is what we would like to estimate or test a hypothesis about.On the instance with index t from the validation set V , let us say x t denotes the correctness of the classifier's decision: x t is a 0/1 Bernoulli random variable that takes the value 1 when the classifier commits an error and 0 when the classifier is correct.The binomial random variable X denotes the total number of errors:We would like to test whether the error probability p is less than or equal to some value p 0 we specify:If the probability of error is p, the probability that the classifier commits j errors out of N isIt is reasonable to reject p ≤ p 0 if in such a case, the probability that we see X = e errors or more is very unlikely.That is, the binomial test binomial test rejects the hypothesis ifwhere α is the significance, for example, 0.05.If p is the probability of error, our point estimate is p = X/N.Then, it is reasonable to reject the null hypothesis if p is much larger than p 0 .How large is large enough is given by the sampling distribution of p and the significance α.Because X is the sum of independent random variables from the same distribution, the central limit theorem states that for large N, X/N is approximately normal with mean p 0 and variance p 0 (1 − p 0 )/N.Then X = e is greater than z α .z 0.05 is 1.64.This approximation will work well as long as N is not too small and p is not very close to 0 or 1; as a rule of thumb, we require Np ≥ 5 and N(1 − p) ≥ 5.The two tests we discussed earlier use a single validation set.If we run the algorithm K times, on K training/validation set pairs, we get K error percentages, p i , i = 1, . . ., K on the K validation sets.Let x t i be 1 if the classifier trained on T i makes a misclassification error on instance t of V i ; x t i is 0 otherwise.Thenand the t test rejects the null hypothesis that the classification algorithm has p 0 or less error percentage at significance level α if this value is greater than t α,K−1 .Typically, K is taken as 10 or 30.t 0.05,9 = 1.83 and t 0.05,29 = 1.70.Given two learning algorithms, we want to compare and test whether they construct classifiers that have the same expected error rate.This set uses K-fold cross-validation to get K training/validation set pairs.We use the two classification algorithms to train on the training sets T i , i = 1, . . ., K, and test on the validation sets V i .The error percentages of the classifiers on the validation sets are recorded as p 1 i and p 2 i .If the two classification algorithms have the same error rate, then we expect them to have the same mean, or equivalently, that the difference of their means is 0. The difference in error rates on fold i is p i = p 1 i −p 2 i .This is a paired test; that is, for each i, both algorithms see the same training paired test and validation sets.When this is done K times, we have a distribution of p i containing K points.Given that p 1 i and p 2 i are both (approximately) normal, their difference p i is also normal.The null hypothesis is that this distribution has 0 mean:Under the null hypothesis that μ = 0, we have a statistic that is t-Thus the K-fold cv paired t test rejects the hypothesis that two clas-K-fold cv paired t test sification algorithms have the same error rate at significance level α if this value is outside the interval (−t α/2,K−1 , t α/2,K−1 ).t 0.025,9 = 2.26 and t 0.025,29 = 2.05.If we want to test whether the first algorithm has less error than the second, we need a one-sided hypothesis and use a one-tailed test:If the test rejects, our claim that the first one has significantly less error is supported.In the 5 × 2 cv t test, proposed by Dietterich (1998), we perform five replications of twofold cross-validation.In each replication, the dataset is divided into two equal-sized sets.p (j) i is the difference between the error rates of the two classifiers on fold j = 1, 2 of replication i = 1, . . ., 5. The average on replication i isUnder the null hypothesis that the two classification algorithms have the same error rate, p (j) i is the difference of two identically distributed proportions, and ignoring the fact that these proportions are not independent, p (j) i can be treated as approximately normal distributed with 0 mean and unknown variance σ 2 .Then p (j) i /σ is approximately unit normal.If we assume p (1) i and p(2) i are independent normals (which is not strictly true because their training and test sets are not drawn independently of each other), then s 2 i /σ 2 has a chi-square distribution with one degree of freedom.If each of the s 2 i are assumed to be independent (which is not true because they are all computed from the same set of available data), then their sum is chi-square with five degrees of freedom:giving us a t statistic with five degrees of freedom.The 5 × 2 cv paired t 5 × 2 cv paired t test test rejects the hypothesis that the two classification algorithms have the same error rate at significance level α if this value is outside the interval (−t α/2,5 , t α/2,5 ).t 0.025,5 = 2.57.We note that the numerator in equation 19.15, p1 , is arbitrary; actually, ten different values can be placed in the numerator, namely, p (j) i , j = 1, 2, i = 1, . . ., 5, leading to ten possible statistics:(19.16) Alpaydın (1999) proposed an extension to the 5 × 2 cv t test that combines the results of the ten possible statistics.If1 and their sum is chi-square with ten degrees of freedom:Placing this in the numerator of equation 19.15, we get a statistic that is the ratio of two chi-square distributed random variables.Two such variables divided by their respective degrees of freedom is F-distributed with ten and five degrees of freedom (section A.3.8):In many cases, we have more than two algorithms, and we would like to compare their expected error.Given L algorithms, we train them on K training sets, induce K classifiers with each algorithm, and then test them on K validation sets and record their error rates.This gives us L groups of K values.The problem then is the comparison of these L samples for statistically significant difference.This is an experiment with a single factor with L levels, the learning algorithms, and there are K replications for each level.In analysis of variance (ANOVA), we consider L independent samples, analysis of variance each of size K, composed of normal random variables of unknown mean μ j and unknown common variance σ 2 :We are interested in testing the hypothesis H 0 that all means are equal:The comparison of error rates of multiple classification algorithms fits this scheme.We have L classification algorithms, and we have their error rates on K validation folds.X ij is the number of validation errors made by the classifier, which is trained by classification algorithm j on fold i.Each X ij is binomial and approximately normal.If H 0 is not rejected, we fail to find a significant error difference among the error rates of the L classification algorithms.This is therefore a generalization of the tests we saw in section 19.11 that compared the error rates of two classification algorithms.The L classification algorithms may be different or may use different hyperparameters, for example, number of hidden units in a multilayer perceptron, number of neighbors in k-nn, and so forth.The approach in ANOVA is to derive two estimators of σ 2 .One estimator is designed such that it is true only when H 0 is true, and the second is always a valid estimator, regardless of whether H 0 is true or not.ANOVA then rejects H 0 , namely, that the L samples are drawn from the same population, if the two estimators differ significantly.Our first estimator to σ 2 is valid only if the hypothesis is true, namely,is also normal with mean μ and variance σ 2 /K.If the hypothesis is true, then m j , j = 1, . . ., L are L instances drawn from N (μ, σ 2 /K).Then their mean and variance areEach of m j is normal and (L − 1)S 2 /(σ 2 /K) is chi-square with (L − 1) degrees of freedom.Then, we haveWe define SS b , the between-group sum of squares, asSo, when H 0 is true, we haveOur second estimator of σ 2 is the average of group variances, S 2 j , defined asand their average isWe define SS w , the within-group sum of squares:Remembering that for a normal sample, we haveand that the sum of chi-squares is also a chi-square, we haveThen we have the task of comparing two variances for equality, which we can do by checking whether their ratio is close to 1.The ratio of two independent chi-square random variables divided by their respective degrees of freedom is a random variable that is F-distributed, and hence when H 0 is true, we haveFor any given significance value α, the hypothesis that the L classification algorithms have the same expected error rate is rejected if this statistic is greater than F α,L−1,L(K−1) .Note that we are rejecting if the two estimators disagree significantly.If H 0 is not true, then the variance of m j around m will be larger than what we would normally have if H 0 were true, and hence if H 0 is not true, the first estimator σ 2 b will overestimate σ 2 , and the ratio will be greater than 1.For α = 0.05, L = 5 and K = 10, F 0.05,4,45 = 2.6.If X ij vary around m with a variance of σ 2 , then if H 0 is true, m j vary around m by σ 2 /K.If it seems as if they vary more, then H 0 should be rejected because the displacement of m j around m is more than what can be explained by some constant added noise.The name analysis of variance is derived from a partitioning of the total variability in the data into its components.SS T divided by its degree of freedom, namely, K • L − 1 (there are K • L data points, and we lose one degree of freedom because m is fixed), gives us the sample variance of X ij .It can be shown that (exercise 5) the total sum of squares can be split into between-group sum of squares and within-group sum of squaresResults of ANOVA are reported in an ANOVA table as shown in table 19.4.This is the basic one-way analysis of variance where there is a single factor, for example, learning algorithm.We may consider experiments with multiple factors, for example, we can have one factor for classification algorithms and another factor for feature extraction algorithms used before, and this will be a two-factor experiment with interaction.If the hypothesis is rejected, we only know that there is some difference between the L groups but we do not know where.For this, we do post hoc post hoc testing testing, that is, an additional set of tests involving subsets of groups, for example, pairs.Fisher's least square difference test compares groups in a pairwise manleast square difference test ner.For each group, we have m i ∼ N (μ i , σ 2 w = MS w /K) and m i − m j ∼ N (μ i − μ j , 2σ 2 w ).Then, under the null hypothesis that H 0 :We reject H 0 in favor of the alternative hypothesis H 1 :. Similarly, one-sided tests can be defined to find pairwise orderings.When we do a number of tests to draw one conclusion, this is called multiple comparisons, and we need to keep in mind that if T hypotheses multiple comparisons are to be tested, each at significance level α, then the probability that at least one hypothesis is incorrectly rejected is at most T α.For example, the probability that six confidence intervals, each calculated at 95 percent individual confidence intervals, will simultaneously be correct is at least 70 percent.Thus to ensure that the overall confidence interval is at least 100(1 − α), each confidence interval should be set at 100(1 − α/T ).This is called a Bonferroni correction.Sometimes it may be the case that ANOVA rejects and none of the post hoc pairwise tests find a significant difference.In such a case, our conclusion is that there is a difference between the means but that we need more data to be able to pinpoint the source of the difference.Note that the main cost is the training and testing of L classification algorithms on K training/validation sets.Once this is done and the values are stored in a K ×L table, calculating the ANOVA or pairwise comparison test statistics from those is very cheap in comparison.Let us say we want to compare two or more algorithms on several datasets and not one.What makes this different is that an algorithm depending on how well its inductive bias matches the problem will behave differently on different datasets, and these error values on different datasets cannot be said to be normally distributed around some mean accuracy.This implies that the parametric tests that we discussed in the previous sections based on binomials being approximately normal are no longer applicable and we need to resort to nonparametric tests.The advantage of having nonparametric testssuch tests is that we can also use them for comparing other statistics that are not normal, for example, training times, number of free parameters, and so on.Parametric tests are generally robust to slight departures from normality, especially if the sample is large.Nonparametric tests are distribution free but are less efficient; that is, if both are applicable, a parametric test should be preferred.The corresponding nonparametric test will require a larger sample to achieve the same power.Nonparametric tests assume no knowledge about the distribution of the underlying population but only that the values can be compared or ordered, and, as we will see, such tests make use of this order information.When we have an algorithm trained on a number of different datasets, the average of its errors on these datasets is not a meaningful value, and, for example, we cannot use such averages to compare two algorithms A and B. To compare two algorithms, the only piece of information we can use is if on any dataset, A is more accurate than B; we can then count the number of times A is more accurate than B and check whether this could have been by chance if they indeed were equally accurate.With more than two algorithms, we will look at the average ranks of the learners trained by different algorithms.Nonparametric tests basically use this rank data and not the absolute values.Before proceeding with the details of these tests, it should be stressed that it does not make sense to compare error rates of algorithms on a whole variety of applications.Because there is no such thing as the "best learning algorithm," such tests would not be conclusive.However, we can compare algorithms on a number of datasets, or versions, of the same application.For example, we may have a number of different datasets for face recognition but with different properties (resolution, lighting, number of subjects, and so on), and we may use a nonparametric test to compare algorithms on those; different properties of the datasets would make it impossible for us to lump images from different datasets together in a single set, but we can train algorithms separately on different datasets, obtain ranks separately, and then combine these to get an overall decision.Let us say we want to compare two algorithms.We both train and validate them on i = 1, . . ., N different datasets in a paired manner-that is, all the conditions except the different algorithms should be identical.We get results e 1 i and e 2 i and if we use K-fold cross-validation on each dataset, these are averages or medians of the K values.The sign test is based on sign test the idea that if the two algorithms have equal error, on each dataset, there should be 1/2 probability that the first has less error than the second, and thus we expect the first to win on N/2 datasets.Let us defineLet us say we want to testIf the null hypothesis is correct, X is binomial in N trials with p = 1/2.Let us say that we saw that the first one wins on X = e datasets.Then, the probability that we have e or less wins when indeed p = 1/2 isand we reject the null hypothesis if this probability is too small, that is, less than α.If there are ties, we divide them equally to both sides; that is, if there are t ties, we add t/2 to e (if t is odd, we ignore the odd one and decrease N by 1).In testingwe reject if P {X ≥ e} < α.For the two-sided testwe reject the null hypothesis if e is too small or too large.If e < N/2, we reject if 2P {X ≤ e} < α; if e > N/2, we reject if 2P {X ≥ e} < α-we need to find the corresponding tail, and we multiply it by 2 because it is a two-tailed test.As we discussed before, nonparametric tests can be used to compare any measurements, for example, training times.In such a case, we see the advantage of a nonparametric test that uses order rather than averages of absolute values.Let us say we compare two algorithms on ten datasets, nine of which are small and have training times for both algorithms on the order of minutes, and one that is very large and whose training time is on the order of a day.If we use a parametric test and take the average of training times, the single large dataset will dominate the decision, but when we use the nonparameric test and compare values separately on each dataset, using the order will have the effect of normalizing separately for each dataset and hence will help us make a robust decision.We can also use the sign test as a one sample test, for example, to check if the average error on all datasets is less than two percent, by comparing μ 1 not by the mean of a second population but by a constant μ 0 .We can do this simply by plugging the constant μ 0 in place of all observations from a second sample and using the procedure used earlier; that is, we will count how many times we get more or less than 0.02 and check if this is too unlikely under the null hypothesis.For large N, normal approximation to the binomial can be used (exercise 6), but in practice, the number of datasets may be smaller than 20.Note that the sign test is a test on the median of a population, which is equal to the mean if the distribution is symmetric.The sign test only uses the sign of the difference and not its magnitude, but we may envisage a case where the first algorithm, when it wins, always wins by a large margin whereas the second algorithm, when it wins, always wins barely.The Wilcoxon signed rank test uses both the sign and Wilcoxon signed rank test the magniture of differences, as follows.Let us say, in addition to the sign of differences, we also calculate m i = |e 1i −e 2 i | and then we order them so that the smallest, min i m i , is assigned rank 1, the next smallest is assigned rank 2, and so on.If there are ties, their ranks are given the average value that they would receive if they differed slightly.For example, if the magnitudes are 2, 1, 2, 4, the ranks are 2.5, 1, 2.5, 4. We then calculate w + as the sum of all ranks whose signs are positive and w − as the sum of all ranks whose signs are negative.The null hypothesis μ 1 ≤ μ 2 can be rejected in favor of the alternative μ 1 > μ 2 only if w + is much smaller than w − .Similarly, the two-sided hypothesis μ 1 = μ 2 can be rejected in favor of the alternative μ 1 = μ 2 only if either w + or w − , that is, w = min(w + , w − ), is very small.The critical values for the Wilcoxon signed rank test are tabulated and for N > 20, normal approximations can be used.The Kruskal-Wallis test is the nonparametric version of ANOVA and is Kruskal-Wallis test a multiple sample generalization of a rank test.Given the M = L • N observations, for example, error rates, of L algorithms on N datasets, X ij , i = 1, . . ., L, j = 1, . . ., N, we rank them from the smallest to the largest and assign them ranks, R ij , between 1 and M, again taking averages in case of ties.If the null hypothesisis true, then the average of ranks of algorithm i should be approximately halfway between 1 and M, that is, (M + 1)/2.We denote the sample average rank of algorithm i by R i• and we reject the hypothesis if the average ranks seem to differ from halfway.The test statisticis approximately chi-square distributed with L − 1 degrees of freedom and we reject the null hypothesis if the statistic exceeds X α,L−1 .Just like the parametric ANOVA, if the null hypothesis is rejected, we can do post hoc testing to check for pairwise comparison of ranks.One method for this is Tukey's test, which makes use of the studentized rangewhere R max and R min are the largest and smallest means (of ranks), respectively, out of the L means, and σ 2 w is the average variance of ranks around group rank averages.We reject the null hypothesis that groups i and j have the same ranks in favor of the alternative hypothesis that they are different ifwhere q α (L, L(K − 1)) are tabulated.One-sided tests can also be defined to order algorithms in terms of average rank.Demsar (2006) proposes to use CD (critical difference) diagrams for visualization.On a scale of 1 to L, we mark the averages, R i• , and draw lines of length given by the critical difference, q α (L, L(K −1))σ w , between groups, so that lines connect groups that are not statistically significantly different.All the tests we discussed earlier in this chapter are univariate; that is, they use a single performance measure, for example, error, precision, area under the curve, and so on.However we know that different measures make different behavior explicit; for example, misclassification error is the sum of false positives and false negatives and a test on error cannot make a distinction between these two types of error.Instead, one can use a bivariate test on these two that will be more powerful than a univariate test on error because it can also check for the type of misclassification.Similarly, we can define, for example, a bivariate test on [tp-rate, fp-rate] or [precision, recall] that checks for two measures together (Yıldız, Aslan, and Alpaydın 2011).Let us say that we use p measures.If we compare in terms of (tp-rate, fp-rate) or (precision, recall), then p = 2. Actually, all of the performance measures shown in table 19.2, such as error, tp-rate, precision, and so on, are all calculated from the same four entries in table 19.1, and instead of using any predefined measure we can just go ahead and do a four-variate test on [tp, fp, fn, tn].We assume that x ij are p-variate normal distributions.We have i = 1, . . ., K folds and we start with the comparison of two algorithms, so j = 1, 2. We want to test whether the two populations have the same mean vector in the p-dimensional space:For paired testing, we calculate the paired differences: d i = x 1i − x 2i , and we test whether these have zero mean:To test for this, we calculate the sample average and covariance matrix:Under the null hypothesis, the Hotelling's multivariate test statisticHotelling's multivariate test T 2 = Km T S −1 m (19.27) is Hotelling's T 2 distributed with p and K−1 degrees of freedom (Rencher 1995).We reject the null hypothesis if T 2 > T 2 α,p,K−1 .When p = 1, this multivariate test reduces to the paired t test we discuss in section 19.11.2.In equation 19.14, √ Km/S measures the normalized distance to 0 in one dimension, whereas here, Km T S −1 m measures the squared Mahalanobis distance to 0 in p dimensions.In both cases, we reject if the distance is so large that it can only occur at most α • 100 percent of the time.If the multivariate test rejects the null hypothesis, we can do p separate post hoc univariate tests (using equation 19.14) to check which one(s) of the variates cause(s) rejection.For example, if a multivariate test on [fp, fn] rejects the null hypothesis, we can check whether the difference is due to a significant difference in false positives, false negatives, or both.It may be the case that none of the univariate differences is significant whereas the multivariate one is; this is one of the advantages of multivariate testing.The linear combination of variates that causes the maximum difference can be calculated asWe can then see the effect of the different univariate dimensions by looking at the corresponding elements of w.Actually if p = 4, we can think of w as defining for us a new performance measure from the original four values in the confusion matrix.The fact that this is the Fisher's LDA direction (section 6.8) is not accidental-we are looking for the direction that maximizes the separation of two groups of data.We can similarly get a multivariate test for comparing L > 2 algorithms by the multivariate version of ANOVA, namely, MANOVA.We test forH 1 : μ r = μ s for at least one pair r , s Let us say that x ij , i = 1, . . ., K, j = 1, . . ., L denotes the p-dimensional performance vector of algorithm j on validation fold i.The multivariate ANOVA (MANOVA) calculates the two matrices of between-and withinscatter:Then, the test statistic (Rencher 1995).We reject the null hypothesis if Λ > Λ α,p,L(K−1),L−1 .Note that rejection is for small values of Λ : If the sample mean vectors are equal, we expect H to be 0 and Λ to approach 1; as the sample means become more spread out, Λ becomes "larger" than E and Λ approaches 0.If MANOVA rejects, we can do post hoc testing in a number of ways: We can do a set of pairwise multivariate tests as we discussed previously, to see which pairs are significantly different.Or, we can do p separate univariate ANOVA on each of the individual variates (section 19.12) to see which one(s) cause a reject.If MANOVA rejects, the difference may be due to some linear combination of the variates: The mean vectors occupy a space whose dimensionality is given by s = min(p, L − 1); its dimensions are the eigenvectors of E −1 H, and by looking at these eigenvectors, we can pinpoint the directions (new performance measures) that cause MANOVA to reject.For example, if λ i / s i=1 λ i > 0.9, we get roughly one direction, and plotting the projection of data along this direction allows for a univariate ordering of the algorithms.The material related to experiment design follows the discussion from (Montgomery 2005), which here is adapted for machine learning.A more detailed discussion of interval estimation, hypothesis testing, and analysis of variance can be found in any introductory statistics book, for example, . Dietterich (1998 discusses statistical tests and compares them on a number of applications using different classification algorithms.A review of ROC use and AUC calculation is given in Fawcett 2006. Demsar (2006 reviews statistical tests for comparing classifiers over multiple datasets.When we compare two or more algorithms, if the null hypothesis that they have the same error rate is not rejected, we choose the simpler one, namely, the one with less space or time complexity.That is, we use our prior preference if the data does not prefer one in terms of error rate.For example, if we compare a linear model and a nonlinear model and if the test does not reject that they have the same expected error rate, we should go for the simpler linear model.Even if the test rejects, in choosing one algorithm over another, error rate is only one of the criteria.Other criteria like training (space/time) complexity, testing complexity, and interpretability may override in practical applications.This is how the post hoc test results are used in the MultiTest algorithm (Yıldız and Alpaydın 2006) to generate a full order.We do L(L − 1)/2 onesided pairwise tests to order the L algorithms, but it is very likely that the tests will not give a full order but only a partial order.The missing links are filled in using the prior complexity information to get a full order.A topological sort gives an ordering of algorithms using both types of information, error and complexity.There are also tests to allow checking for contrasts.Let us say 1 and 2 are neural network methods and 3 and 4 are fuzzy logic methods.We can then test whether the average of 1 and 2 differs from the average of 3 and 4, thereby allowing us to compare methods in general.Statistical comparison is needed not only to choose between learning algorithms but also for adjusting the hyperparameters of an algorithm, and the experimental design framework provides us with tools to do this efficiently; for example, response surface design can be used to learn weights in a multiple kernel learning scenario (Gönen and Alpaydın 2011).Another important point to note is that if are comparing misclassification errors, this implies that from our point of view, all misclassifications have the same cost.When this is not the case, our tests should be based on risks taking a suitable loss function into account.Not much work has been done in this area.Similarly, these tests should be generalized from classification to regression, so as to be able to assess the mean square errors of regression algorithms, or to be able to compare the errors of two regression algorithms.In comparing two classification algorithms, note that we are testing only whether they have the same expected error rate.If they do, this does not mean that they make the same errors.This is an idea that we used in chapter 17; we can combine multiple models to improve accuracy if different classifiers make different errors.We choose C 1 if the former is less than the latter and given that P (CThat is, varying the threshold decision corresponds to varying the relative cost of false positives and false negatives.2. We can simulate a classifier with error probability p by drawing samples from a Bernoulli distribution.Doing this, implement the binomial, approximate, and t tests for p 0 ∈ (0, 1).Repeat these tests at least 1,000 times for several values of p and calculate the probability of rejecting the null hypothesis.What do you expect the probability of reject to be when p 0 = p?3. Assume that x t ∼ N (μ, σ 2 ) where σ 2 is known.How can we test for H 0 : μ ≥ μ 0 vs. H 1 : μ < μ 0 ?SOLUTION: Under H 0 , we have4. The K-fold cross-validated t test only tests for the equality of error rates.If the test rejects, we do not know which classification algorithm has the lower error rate.How can we test whether the first classification algorithm does not have higher error rate than the second one?Hint: We have to test H 0 : μ ≤ 0 vs. H 1 : μ > 0.5. Show that the total sum of squares can be split into between-group sum of squares and within-group sum of squares as SS T = SS b + SS w .6. Use the normal approximation to the binomial for the sign test.SOLUTION: Under the null hypothesis that the two are equally good, we have p = 1/2 and over N datasets, we expect the number of wins X to be approximately Gaussian with μ = pN = N/2 and σ 2 = p(1 − p)N = N/4.If there are e wins, we reject if P (X < e) > α, or if P (Z < e−N/2 √ N/4) > α.7. Let us say we have three classification algorithms.How can we order these three from best to worst?8. If we have two variants of algorithm A and three variants of algorithm B, how can we compare the overall accuracies of A and B taking all their variants into account?SOLUTION: We can use contrasts (Montgomery 2005).Basically, what we would be doing is comparing the average of the two variants of A with the average of the three variants of B.We review briefly the elements of probability, the concept of a random variable, and example distributions.A random experiment is one whose outcome is not predictable with certainty in advance Casella and Berger 1990).The set of all possible outcomes is known as the sample space S. A sample space is discrete if it consists of a finite (or countably infinite) set of outcomes; otherwise it is continuous.Any subset E of S is an event.Events are sets, and we can talk about their complement, intersection, union, and so forth.One interpretation of probability is as a frequency.When an experiment is continually repeated under the exact same conditions, for any event E, the proportion of time that the outcome is in E approaches some constant value.This constant limiting frequency is the probability of the event, and we denote it as P (E).Probability sometimes is interpreted as a degree of belief.For example, when we speak of Turkey's probability of winning the World Soccer Cup in 2018, we do not mean a frequency of occurrence, since the championship will happen only once and it has not yet occurred (at the time of the writing of this book).What we mean in such a case is a subjective degree of belief in the occurrence of the event.Because it is subjective, different individuals may assign different probabilities to the same event.Axioms ensure that the probabilities assigned in a random experiment can be interpreted as relative frequencies and that the assignments are consistent with our intuitive understanding of relationships among relative frequencies:1. 0 ≤ P (E) ≤ 1.If E 1 is an event that cannot possibly occur, then P (E 1 ) = 0.If E 2 is sure to occur, P (E 2 ) = 1.2. S is the sample space containing all possible outcomes, P (S) = 1.3. If E i , i = 1, . . ., n are mutually exclusive (i.e., if they cannot occur at the same time, as in E i ∩ E j = ∅, j = i, where ∅ is the null event that does not contain any possible outcomes), we haveWhen two random variables are jointly distributed with the value of one known, the probability that the other takes a given value can be computed using Bayes' rule: Note that the denominator is obtained by summing (or integrating if y is continuous) the numerator over all possible y values.The "shape" of p(y|x) depends on the numerator with denominator as a normalizing factor to guarantee that p(y|x) sum to 1. Bayes' rule allows us to modify a prior probability into a posterior probability by taking information provided by x into account.Bayes' rule inverts dependencies, allowing us to compute p(y|x) if p(x|y) is known.Suppose that y is the "cause" of x, like y going on summer vacation and x having a suntan.Then p(x|y) is the probability that someone who is known to have gone on summer vacation has a suntan.This is the causal (or predictive) way.Bayes' rule allows us a diagnostic approach by allowing us to compute p(y|x): namely, the probability that someone who is known to have a suntan, has gone on summer vacation.Then p(y) is the general probability of anyone's going on summer vacation and p(x) is the probability that anyone has a suntan, including both those who have gone on summer vacation and those who have not.Expectation, expected value, or mean of a random variable X, denoted by E[X], is the average value of X in a large number of experiments:X is normal or Gaussian distributed with mean μ and variance σ 2 , denoted as N (μ, σ 2 ), if its density function isMany random phenomena obey the bell-shaped normal distribution, at least approximately, and many observations from nature can be seen as a continuous, slightly different versions of a typical value-that is probably why it is called the normal distribution.In such a case, μ represents the typical value and σ defines how much instances vary around the prototypical value.68.27 percent lie in (μ − σ , μ + σ ), 95.45 percent in (μ − 2σ , μ + 2σ ), and 99.73 percent in (μ − 3σ , μ + 3σ ).Thus P {|x − μ| < 3σ } ≈ 0.99.For practical purposes, p(x) ≈ 0 if x < μ − 3σ or x > μ + 3σ .Z is unit normal, namely, N (0, 1) (see figure A.1), and its density is written asBayes' formula allows us to write P (F i |E) = P (E ∩ F i ) P (E) = P (E|F i )P (F i ) j P (E|F j )P (F j )If E and F are independent, we have P (E|F) = P (E) and thusThat is, knowledge of whether F has occurred does not change the probability that E occurs.A random variable is a function that assigns a number to each outcome in the sample space of a random experiment.The probability distribution function F(•) of a random variable X for any real number a is F(a) = P {X ≤ a} (A.8) and we haveIf X is a discrete random variable F(a) = ∀x≤a P (x) (A.10) where P (•) is the probability mass function defined as P (a) = P {X = a}.If X is a continuous random variable, p(•) is the probability density function such thatIn certain experiments, we may be interested in the relationship between two or more random variables, and we use the joint probability distribution and density functions of X and Y satisfying F(x, y) = P {X ≤ x, Y ≤ y} (A.12)Individual marginal distributions and densities can be computed by marginalizing, namely, summing over the free variable:In the discrete case, we write P (X = x) = j P (x, y j ) (A.14) and in the continuous case, we haveThese can be generalized in a straightforward manner to more than two random variables.When X and Y are random variablesFor any real-valued function g(•), the expected value isMean is the first moment and is denoted by μ.Variance measures how much X varies around the expected value.If μ ≡ E[X], the variance is defined asVariance is the second moment minus the square of the first moment.Variance, denoted by σ 2 , satisfies the following property (a, b ∈ ):Var(X) is called the standard deviation and is denoted by σ .Standard deviation has the same unit as X and is easier to interpret than variance.Covariance indicates the relationship between two random variables.If the occurrence of X makes Y more likely to occur, then the covariance is positive; it is negative if X's occurrence makes Y less likely to happen and is 0 if there is no dependence.Correlation is a normalized, dimensionless quantity that is always between −1 and 1:Let X = {X t } N t=1 be a set of independent and identically distributed (iid) random variables each having mean μ and a finite variance σ 2 .Then for any > 0,That is, the average of N trials converges to the mean as N increases.There are certain types of random variables that occur so frequently that names are given to them.A trial is performed whose outcome is either a "success" or a "failure."The random variable X is a 0/1 indicator variable and takes the value 1 for a success outcome and is 0 otherwise.p is the probability that the result of trial is a success.Then P {X = 1} = p and P {X = 0} = 1 − p (A.33) which can equivalently be written asIf X is Bernoulli, its expected value and variance areIf N identical independent Bernoulli trials are made, the random variable X that represents the number of successes that occurs in N trials is binomial distributed.The probability that there are i isIf X is binomial, its expected value and variance areConsider a generalization of Bernoulli where instead of two states, the outcome of a random event is one of K mutually exclusive and exhaustive states, each of which has a probability of occurring p i where K i=1 p i = 1.Suppose that N such trials are made where outcome i occurred N i times withA special case is when N = 1; only one trial is made.Then N i are 0/1 indicator variables of which only one of them is 1 and all others are 0. Then equation A.38 reduces toX is uniformly distributed over the interval [a, b] if its density function is given byIf X is uniform, its expected value and variance areLet X 1 , X 2 , . . ., X N be a set of iid random variables all having mean μ and variance σ 2 .Then the central limit theorem states that for large N, central limit theorem the distribution ofis approximately N (Nμ, Nσ 2 ).For example, if X is binomial with parameters (N, p), X can be written as the sum of N Bernoulli trials andCentral limit theorem is also used to generate normally distributed random variables on computers.Programming languages have subroutines that return uniformly distributed (pseudo-)random numbers in the rangeLet us say X t ∼ N (μ, σ 2 ).The estimated sample meanis also normal with mean μ and variance σ 2 /N.If Z i are independent unit normal random variables, thenand we haveIt is also known that m and S 2 are independent.If Z ∼ Z and X ∼ X 2 n are independent, thenLike the unit normal density, t is symmetric around 0. As n becomes larger, t density becomes more and more like the unit normal, the difference being that t has thicker tails, indicating greater variability than does normal.If X 1 ∼ X 2 n and X 2 ∼ X 2 m are independent chi-square random variables with n and m degrees of freedom, respectively,is F-distributed with n and m degrees of freedom withA random variable is a function that assigns a number to each outcome in the sample space of a random experiment.The probability distribution function F(•) of a random variable X for any real number a is F(a) = P {X ≤ a} (A.8) and we haveIf X is a discrete random variable F(a) = ∀x≤a P (x) (A.10) where P (•) is the probability mass function defined as P (a) = P {X = a}.If X is a continuous random variable, p(•) is the probability density function such thatIn certain experiments, we may be interested in the relationship between two or more random variables, and we use the joint probability distribution and density functions of X and Y satisfying F(x, y) = P {X ≤ x, Y ≤ y} (A.12)Individual marginal distributions and densities can be computed by marginalizing, namely, summing over the free variable:In the discrete case, we write P (X = x) = j P (x, y j ) (A.14) and in the continuous case, we haveThese can be generalized in a straightforward manner to more than two random variables.When X and Y are random variablesFor any real-valued function g(•), the expected value isMean is the first moment and is denoted by μ.Variance measures how much X varies around the expected value.If μ ≡ E[X], the variance is defined asVariance is the second moment minus the square of the first moment.Variance, denoted by σ 2 , satisfies the following property (a, b ∈ ):Var(X) is called the standard deviation and is denoted by σ .Standard deviation has the same unit as X and is easier to interpret than variance.Covariance indicates the relationship between two random variables.If the occurrence of X makes Y more likely to occur, then the covariance is positive; it is negative if X's occurrence makes Y less likely to happen and is 0 if there is no dependence.Correlation is a normalized, dimensionless quantity that is always between −1 and 1:Let X = {X t } N t=1 be a set of independent and identically distributed (iid) random variables each having mean μ and a finite variance σ 2 .Then for any > 0,That is, the average of N trials converges to the mean as N increases.There are certain types of random variables that occur so frequently that names are given to them.A trial is performed whose outcome is either a "success" or a "failure."The random variable X is a 0/1 indicator variable and takes the value 1 for a success outcome and is 0 otherwise.p is the probability that the result of trial is a success.Then P {X = 1} = p and P {X = 0} = 1 − p (A.33) which can equivalently be written asIf X is Bernoulli, its expected value and variance areIf N identical independent Bernoulli trials are made, the random variable X that represents the number of successes that occurs in N trials is binomial distributed.The probability that there are i isIf X is binomial, its expected value and variance areConsider a generalization of Bernoulli where instead of two states, the outcome of a random event is one of K mutually exclusive and exhaustive states, each of which has a probability of occurring p i where K i=1 p i = 1.Suppose that N such trials are made where outcome i occurred N i times withA special case is when N = 1; only one trial is made.Then N i are 0/1 indicator variables of which only one of them is 1 and all others are 0. Then equation A.38 reduces toX is uniformly distributed over the interval [a, b] if its density function is given byIf X is uniform, its expected value and variance areLet X 1 , X 2 , . . ., X N be a set of iid random variables all having mean μ and variance σ 2 .Then the central limit theorem states that for large N, central limit theorem the distribution ofis approximately N (Nμ, Nσ 2 ).For example, if X is binomial with parameters (N, p), X can be written as the sum of N Bernoulli trials andCentral limit theorem is also used to generate normally distributed random variables on computers.Programming languages have subroutines that return uniformly distributed (pseudo-)random numbers in the rangeLet us say X t ∼ N (μ, σ 2 ).The estimated sample meanis also normal with mean μ and variance σ 2 /N.If Z i are independent unit normal random variables, thenand we haveIt is also known that m and S 2 are independent.If Z ∼ Z and X ∼ X 2 n are independent, thenLike the unit normal density, t is symmetric around 0. As n becomes larger, t density becomes more and more like the unit normal, the difference being that t has thicker tails, indicating greater variability than does normal.If X 1 ∼ X 2 n and X 2 ∼ X 2 m are independent chi-square random variables with n and m degrees of freedom, respectively,is F-distributed with n and m degrees of freedom with