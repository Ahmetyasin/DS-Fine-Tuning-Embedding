In a tree, a node has a single parent, that is, a single cause.In a polytree, a polytree node may have multiple parents, but we require that the graph be singly connected, which means that there is a single chain between any two nodes.If we remove X, the graph will split into two components.This is necessary so that we can continue splitting E X into E + X and E − X , which are independent given X (see figure 14.11).If X has multiple parents U i , i = 1, . . ., k, it receives π -messages from all of them, π X (U i ), which it combines as follows:π X (U i ) (14.24) and passes it on to its several children Y j , j = 1, . . ., m: (14.25)In this case when X has multiple parents, a λ-message X passes on to one of its parents U i combines not only the evidence X receives from its children but also the π -messages X receives from its other parents U r , r = i; they together make up E − U i X :As in a tree, to find its overall λ, the parent multiplies the λ-messages it receives from its children: (14.27)In this case of multiple parents, we need to store and manipulate the conditional probability given all the parents, p(X|U 1 , . . ., U k ), which is costly for large k.Approaches have been proposed to decrease the complexity from exponential in k to linear.For example, in a noisy OR gate, noisy OR any of the parents is sufficient to cause the event and the likelihood does not decrease when multiple parent events occur.If the probability that X happens when only cause U i happens is 1 − q i P (X|U i , ∼U p =j ) = 1 − q i (14.28) the probability that X happens when a subset T of them occur is calculated asFor example, let us say wet grass has two causes, rain and a sprinkler, with q R = q S = 0.1; that is, both singly have a 90 percent probability of causing wet grass.Then, P (W |R, ∼ S) = 0.9 and P (W |R, S) = 0.99.Another possibility is to write the conditional probability as some function given a set of parameters, for example, as a linear model P (X|U 1 , . . ., U k , w 0 , w 1 , . . ., w k ) = sigmoidwhere sigmoid guarantees that the output is a probability between 0 and 1.During training, we can learn the parameters w i , i = 0, . . ., d, for example, to maximize the likelihood on a sample.If there is a loop, that is, if there is a cycle in the underlying undirected graph-for example, if the parents of X share a common ancestor-the algorithm we discussed earlier does not work.In such a case, there is more than one path on which to propagate evidence and, for example, while evaluating the probability at X, we cannot say that X separates E into E + X and E − X as causal (upward) and diagnostic (downward) evidence; removing X does not split the graph into two.Conditioning them on X does not make them independent and the two can interact through some other path not involving X.We can still use the same algorithm if we can convert the graph to a polytree.We define clique nodes that correspond to a set of original variables and connect them so that they form a tree (see figure 14.12).We can then run the same belief propagation algorithm with some modifications.This is the basic idea behind the junction tree algorithm (Lauritzen junction tree and Spiegelhalter 1988; Jensen 1996;).Up to now, we have discussed directed graphs where the influences are undirectional and have used Bayes' rule to invert the arcs.If the influences are symmetric, we represent them using an undirected graphical model, also known as a Markov random field.For example, neighboringpixels in an image tend to have the same color-that is, are correlatedand this correlation goes both ways.Directed and undirected graphs define conditional independence differently, and, hence, there are probability distributions that are represented by a directed graph and not by an undirected graph, and vice versa (Pearl 1988).Because there are no directions and hence no distinction between the head or the tail of an arc, the treatment of undirected graphs is simpler.For example, it is much easier to check if A and B are independent given C. We just check if after removing all nodes in C, we still have a path between a node in A and a node in B. If so, they are dependent, otherwise, if all paths between nodes in A and nodes in B pass through nodes in C such that removal of C leaves nodes of A and nodes of B in separate components, we have independence.In the case of an undirected graph, we do not talk about the parent or the child but about cliques, which are sets of nodes such that there clique exists a link between any two nodes in the set.A maximal clique has the maximum number of elements.Instead of conditional probabilities (implying a direction), in undirected graphs we have potential functions potential function ψ C (X C ) where X C is the set of variables in clique C, and we define the joint distribution as the product of the potential functions of the maximal cliques of the graphwhere Z is the normalization constant to make sure that X p(X) = 1:It can be shown that a directed graph is already normalized (exercise 5).Unlike in directed graphs, the potential functions in an undirected graph do not need to have a probabilistic interpretation, and one has more freedom in defining them.In general, we can view potential functions as expressing local constraints, that is, favoring some local configurations over others.For example, in an image, we can define a pairwise potential function between neighboring pixels, which takes a higher value if their colors are similar than the case when they are different .Then, setting some of the pixels to their values given as evidence, we can estimate the values of other pixels that are not known, for example, due to occlusion.If we have the directed graph, it is easy to redraw it as an undirected graph, simply by dropping all the directions, and if a node has a single parent, we can set the pairwise potential function simply to the conditional probability.If the node has more than one parent, however, the "explaining away" phenomenon due to the head-to-head node makes the parents dependent, and hence we should have the parents in the same clique so that the clique potential includes all the parents.This is done by connecting all the parents of a node by links so that they are completely connected among them and form a clique.This is called "marrying" the parents, and the process is called moralization.Incidentally, moralization moralization is one of the steps in generating a junction tree, which is undirected.It is straightforward to adapt the belief propagation algorithm to work on undirected graphs, and it is easier because the potential function is symmetric and we do not need to make a difference between causal and diagnostic evidence.Thus, we can do inference on undirected chains and trees.But in polytrees where a node has multiple parents and moralization necessarily creates loops, this would not work.One trick is to convert it to a factor graph that uses a second kind of factor nodes in factor graph addition to the variable nodes, and we write the joint distribution as a product of factors (Kschischang, Frey, and Loeliger 2001) p(X) = 1 Z S f S (X S ) (14.33) where X s denotes a subset of the variable nodes used by factor S. Directed graphs are a special case where factors correspond to local conditional distributions, and undirected graphs are another special case where factors are potential functions over maximal cliques.The advantage is that, as we can see in figure 14.13, the tree structure can be kept even after moralization.It is possible to generalize the belief propagation algorithm to work on factor graphs; this is called the sum-product algorithm Jorsum-product algorithm dan 2004) where there is the same idea of doing local computations once and propagating them through the graph as messages.The difference now is that there are two types of messages because there are two kinds of nodes, factors and variables, and we make a distinction between their messages.Note, however, that a factor graph is bipartite, and one kind of node can have a close encounter only with the second kind.In belief propagation, or the sum-product algorithm, the aim is to find the probability of a set of nodes X given that another set of evidence nodes E are clamped to a certain value, that is, P (X|E).In some applications, we may be interested in finding the setting of all X that maximizes the full joint probability distribution p(X).For example, in the undirected case where potential functions code locally consistent configurations, such an approach would propagate local constraints over the whole graph and find a solution that maximizes global consistency.In a graph where nodes correspond to pixels and pairwise potential functions favor correlation, this approach would implement noise removal .The algorithm for this, named the max-product algorithm (Bishop max-product algorithm 2006;) is the same as the sum-product algorithm where we take the maximum (choose the most likely value) rather than the sum (marginalize).This is analogous to the difference between the forwardbackward procedure and the Viterbi algorithm in hidden Markov models that we discussed in chapter 15.Note that the nodes need not correspond to low-level concepts like pixels; in a vision application, for instance, we may have nodes for corners of different types or lines of different orientations with potential functions checking for compatibility, so as to see if they can be part of the same interpretation-remember the Necker cube, for example-so that overall consistent solutions emerge after the consolidation of local evidences.The complexity of the inference algorithms on polytrees or junction trees is determined by the maximum number of parents or the size of the largest clique, and when this is large, exact inference may be infeasible.In such a case, one needs to use an approximation or a sampling algorithm (Jordan 1999;.As in any approach, learning a graphical model has two parts.The first is the learning of parameters given a structure; this is relatively easier (Buntine 1996), and, in graphical models, conditional probability tables or their parameterizations (as in equation 14.30) can be trained to maximize the likelihood, or by using a Bayesian approach if suitable priors are known (chapter 16).The second, more difficult, and interesting part is to learn the graph structure (Cowell et al. 1999).This is basically a model selection problem, and just like the incremental approaches for learning the structure of a multilayer perceptron (section 11.9), we can see this as a search in the space of all possible graphs.One can, for example, consider operators that can add/remove arcs and/or hidden nodes and then do a search evaluating the improvement at each step (using parameter learning at each intermediate iteration).Note, however, that to check for overfitting, one should regularize properly, corresponding to a Bayesian approach with a prior that favors simpler graphs (Neapolitan 2004).However, because the state space is large, it is most helpful if there is a human expert who can manually define causal relationships among variables and creates subgraphs of small groups of variables.In chapter 16, we discuss the Bayesian approach and in section 16.8, we discuss the nonparametric Bayesian methods where model structure can be made more complex in time as more data arrives.Just as in chapter 3, we generalized from probabilities to actions with risks, influence diagrams are graphical models that allow the generalizainfluence diagrams tion of graphical models to include decisions and utilities.An influence diagram contains chance nodes representing random variables that we use in graphical models (see figure 14.14).It also has decision nodes and a utility node.A decision node represents a choice of actions.A utility node is where the utility is calculated.Decisions may be based on chance nodes and may affect other chance nodes and the utility node.Inference on an influence diagram is an extension to belief propagation on a graphical model.Given evidence on some of the chance nodes, this evidence is propagated, and for each possible decision, the utility is calculated and the decision having the highest utility is chosen.The influence diagram for classification of a given input is shown in figure 14.14.Given the input, the decision node decides on a class, and for each choice we incur a certain utility (risk).Graphical models have two advantages.One is that we can visualize the interaction of variables and have a better understanding of the process, for example, by using a causal generative model.The second is that by finding graph operations that correspond to basic probabilistic procedures such as Bayes' rule or marginalization, the task of inference can be mapped to general-purpose graph algorithms that can be efficiently represented and implemented.The idea of visual representation of variables and dependencies between them as a graph, and the related factorization of a complicated global function of many variables as a product of local functions involving a small subset of the variables for each, seems to be used in different domains in decision making, coding, and signal processing; Kschischang, Frey, and Loeliger (2001) give a review.The complexity of the inference algorithms on polytrees or junction trees is determined by the maximum number of parents or the size of the largest clique, and when this is large exact inference may be infeasible.In such a case, one needs to use an approximation or a sampling algorithm.Variational approximations and Markov chain Monte Carlo (MCMC) algorithms are discussed in , Andrieu et al. 2003 Graphical models are especially suited to represent Bayesian approaches where in addition to nodes for observed variables, we also have nodes for hidden variables as well as the model parameters.We may also introduce a hierarchy where we have nodes for hyperparameters-that is, secondlevel parameters for the priors of the first-level parameters.Thinking of data as sampled from a causal generative model that can be visualized as a graph can ease understanding and also inference in many domains.For example, in text categorization, generating a text may be thought of as the process whereby an author decides to write a document on a number of topics and then chooses a set of words for each topic.In bioinformatics, one area among many where a graphical approach used is the modeling of a phylogenetic tree; namely, it is a directed graph whose phylogenetic tree leaves are the current species, whose nonterminal nodes are past ancestors that split into multiple species during a speciation event, and whose conditional probabilities depend on the evolutionary distance between a species and its ancestor .The hidden Markov model we discuss in chapter 15 is one type of graphical model where inputs are dependent sequentially, as in speech recognition, where a word is a particular sequence of basic speech sounds called phonemes .Such dynamic graphical models find applications in many areas where there is a temporal dimension, such as speech, music, and so on (Zweig 2003;Bilmes and Bartels 2005).Graphical models are also used in computer vision-for example, in information retrieval (Barnard et al. 2003) and scene analysis (Sudderth et al. 2008).A review of the use of graphical models in bioinformatics (and related software) is given in Donkers and Tuyls 2008.For a head-to-head node, show that equation 14.10 implies P (X, Y ) = P (X) • P (Y ).We know that P (X, Y , Z) = P (Z|X, Y )P (X, Y ), and if we also know that P (X, Y , Z) = P (X)P (Y )P (Z|X, Y ), we see that P (X, Y ) = P (X)P (Y ).3. In figure 14.4, calculate P (R|W ), P (R|W , S), and P (R|W , ∼S). 4. In equation 14.30, X is binary.How do we need to modify it if X can take one of K discrete values?SOLUTION: Let us say there are j = 1, . . ., K states.Then, keeping the model linear, we need to parameterize each by a separate w j and use softmax to map to probabilities:Show that in a directed graph where the joint distribution is written as equation 14.12, x p(x) = 1.The terms cancel when we sum up over all possible values because these are probabilities.Let us, for example, take figure 14.3:We need a score function that is the sum of two parts, one quantifying a goodness of fit, that is, how likely is the data given the model, and one quantifying the complexity of the graph, to alleviate overfitting.In measuring complexity, we must take into account the total number of nodes and the number of parameters needed to represent the conditional probability distributions.For example, we should try to have nodes with as few parents as possible.Possible operators are there to add/remove an edge and add/remove a hidden node.9. Generally, in a newspaper, a reporter writes a series of articles on successive days related to the same topics as the story develops.How can we model this using a graphical model?Until now, we assumed that the instances that constitute a sample are iid.This has the advantage that the likelihood of the sample is simply the product of the likelihoods of the individual instances.This assumption, however, is not valid in applications where successive instances are dependent.For example, in a word successive letters are dependent; in English 'h' is very likely to follow 't' but not 'x'.Such processes where there is a sequence of observations-for example, letters in a word, base pairs in a DNA sequence-cannot be modeled as simple probability distributions.A similar example is speech recognition where speech utterances are composed of speech primitives called phonemes; only certain sequences of phonemes are allowed, which are the words of the language.At a higher level, words can be written or spoken in certain sequences to form a sentence as defined by the syntactic and semantic rules of the language.A sequence can be characterized as being generated by a parametric random process.In this chapter, we discuss how this modeling is done and also how the parameters of such a model can be learned from a training sample of example sequences.Consider a system that at any time is in one of a set of N distinct states: S 1 , S 2 , . . ., S N .The state at time t is denoted as q t , t = 1, 2, . .., so, for example, q t = S i means that at time t, the system is in state S i .Though we write "time" as if this should be a temporal sequence, the methodology is valid for any sequencing, be it in time, space, position on the DNA string, and so forth.At regularly spaced discrete times, the system moves to a state with a given probability, depending on the values of the previous states:For the special case of a first-order Markov model, the state at time t +1 Markov model depends only on state at time t, regardless of the states in the previous times:This corresponds to saying that, given the present state, the future is independent of the past.This is just a mathematical version of the saying, Today is the first day of the rest of your life.We further simplify the model-that is, regularize-by assuming that the transition probability from S i to S j is independent of time:So, going from S i to S j has the same probability no matter when it happens, or where it happens in the observation sequence.A = [a ij ] is a N × N matrix whose rows sum to 1.This can be seen as a stochastic automaton (see figure 15.1).From stochastic automaton each state S i , the system moves to state S j with probability a ij , and this probability is the same for any t.The only special case is the first state.We define the initial probability, π i , which is the probability that the first initial probability state in the sequence is S i :Example of a Markov model with three states.This is a stochastic automaton where π i is the probability that the system starts in state S i , and a ij is the probability that the system moves from state S i to state S j .satisfyingIn an observable Markov model, the states are observable.At any time observable Markov model t, we know q t , and as the system moves from one state to another, we get an observation sequence that is a sequence of states.The output of the process is the set of states at each instant of time where each state corresponds to a physical observable event.We have an observation sequence O that is the state sequence, whose probability is given asπ q 1 is the probability that the first state is q 1 , a q 1 q 2 is the probability of going from q 1 to q 2 , and so on.We multiply these probabilities to get the probability of the whole sequence.Let us now see an example (Rabiner and Juang 1986) to help us demonstrate.Assume we have N urns where each urn contains balls of only one color.So there is an urn of red balls, another of blue balls, and so forth.Somebody draws balls from urns one by one and shows us their color.Let q t denote the color of the ball drawn at time t.Let us say we have three states: S 1 : red, S 2 = blue, S 3 : green with initial probabilities:a ij is the probability of drawing from urn j (a ball of color j) after drawing a ball of color i from urn i.The transition matrix is, for example,Given Π and A, it is easy to generate K random sequences each of length T .Let us see how we can calculate the probability of a sequence.Assume that the first four balls are "red, red, green, green."This corresponds to the observation sequence O = {S 1 , S 1 , S 3 , S 3 }.Its probability isNow, let us see how we can learn the parameters, Π, A. Given K sequences of length T , where q k t is the state at time t of sequence k, the initial probability estimate is the number of sequences starting with S i divided by the number of sequences:As for the transition probabilities, the estimate for a ij is the number of transitions from S i to S j divided by the total number of transitions from S i over all sequences:â12 is the number of times a blue ball follows a red ball divided by the total number of red ball draws over all sequences.In a hidden Markov model (HMM), the states are not observable, but when hidden Markov model we visit a state, an observation is recorded that is a probabilistic function of the state.We assume a discrete observation in each state from the setis the observation probability, or emission probability, that we observation probability emission probability observe the value v m , m = 1, . . ., M in state S j .We again assume a homogeneous model in which the probabilities do not depend on t.The values thus observed constitute the observation sequence O.The state sequence Q is not observed, that is what makes the model "hidden," but it should be inferred from the observation sequence O.Note that there are typically many different state sequences Q that could have generated the same observation sequence O, but with different probabilities; just as, given an iid sample from a normal distribution, there are an infinite number of (μ, σ ) value pairs possible, we are interested in the one having the highest likelihood of generating the sample.Note also that in this case of a hidden Markov model, there are two sources of randomness.In addition to randomly moving from one state to another, the observation in a state is also random.Let us go back to our example.The hidden case corresponds to the urn-and-ball example where each urn contains balls of different colors.Let b j (m) denote the probability of drawing a ball of color m from urn j.We again observe a sequence of ball colors but without knowing the sequence of urns from which the balls were drawn.So it is as if now the urns are placed behind a curtain and somebody picks a ball at random from one of the urns and shows us only the ball, without showing us the urn from which it is picked.The ball is returned to the urn to keep the probabilities the same.The number of ball colors may be different from the number of urns.For example, let us say we have three urns and the observation sequence is O = {red, red, green, blue, yellow} In the previous case, knowing the observation (ball color), we knew the state (urn) exactly because there were separate urns for separate colors and each urn contained balls of only one color.The observable model is a special case of the hidden model where.2 An HMM unfolded in time as a lattice (or trellis) showing all the possible trajectories.One path, shown in thicker lines, is the actual (unknown) state trajectory that generated the observation sequence.and 0 otherwise.But in the case of a hidden model, a ball could have been picked from any urn.In this case, for the same observation sequence O, there may be many possible state sequences Q that could have generated O (see figure 15.2).To summarize and formalize, an HMM has the following elements:1. N: Number of states in the modelwhere a ij ≡ P (q t+1 = S j |q t = S i )4. Observation probabilities:5. Initial state probabilities:where π i ≡ P (q 1 = S i )N and M are implicitly defined in the other parameters so λ = (A, B, Π) is taken as the parameter set of an HMM.Given λ, the model can be used to generate an arbitrary number of observation sequences of arbitrary length, but as usual, we are interested in the other direction, that of estimating the parameters of the model given a training set of sequences.Given a number of sequences of observations, we are interested in three problems:1. Given a model λ, we would like to evaluate the probability of any given observation sequence,2. Given a model λ and an observation sequence O, we would like to find out the state sequence Q = {q 1 q 2 • • • q T }, which has the highest probability of generating O; namely, we want to find Q * that maximizes P (Q|O, λ).3. Given a training set of observation sequences, X = {O k } k , we would like to learn the model that maximizes the probability of generating X; namely, we want to find λ * that maximizes P (X|λ).Let us see solutions to these one by one, with each solution used to solve the next problem, until we get to calculating λ or learning a model from data.which we cannot calculate because we do not know the state sequence.The probability of the state sequence Q is P (Q|λ) = P (q 1 ) T t=2 P (q t |q t−1 ) = π q 1 a q 1 q 2 • • • a q T −1 q T (15.12)Then the joint probability isWe can compute P (O|λ) by marginalizing over the joint, namely, by summing up over all possible Q:However, this is not practical since there are N T possible Q, assuming that all the probabilities are nonzero.Fortunately, there is an efficient procedure to calculate P (O|λ), which is called the forward-backward pro-The nice thing about this is that it can be calculated recursively by accumulating results on the way.Initialization:Recursion (see figure 15.3a): =α t (i) explains the first t observations and ends in state S i .We multiply this by the probability a ij to move to state S j , and because there are N possible previous states, we need to sum up over all such possible previous S i .b j (O t+1 ) then is the probability we generate the (t + 1)st observation while in state S j at time t + 1.When we calculate the forward variables, it is easy to calculate the probability of the observation sequence:is the probability of generating the full observation sequence and ending up in state S i .We need to sum up over all such possible final states.Computing α t (i) is O(N 2 T ), and this solves our first evaluation problem in a reasonable amount of time.We do not need it now but let us similarly define the backward variable, β t (i), which is the probability of backward variable being in S i at time t and observing the partial sequenceThis can again be recursively computed as follows, this time going in the backward direction: Initialization (arbitrarily to 1):Recursion (see figure 15.3b):When in state S i , we can go to N possible next states S j , each with probability a ij .While there, we generate the (t + 1)st observation and β t+1 (j) explains all the observations after time t + 1, continuing from there.One word of caution about implementation is necessary here: Both α t and β t values are calculated by multiplying small probabilities, and with long sequences we risk getting underflow.To avoid this, at each time step, we normalize α t (i) by multiplying it with c t = 1 j α t (j) We also normalize β t (i) by multiplying it with the same c t (β t (i) do not sum to 1).We cannot use equation 15.17 after normalization; instead, we have (Rabiner 1989)We now move on to the second problem, that of finding the state sequence Q = {q 1 q 2 • • • q T } having the highest probability of generating the observation sequenceLet us define γ t (i) as the probability of being in state S i at time t, given O and λ, which can be computed as follows:Here we see how nicely α t (i) and β t (i) split the sequence between them: The forward variable α t (i) explains the starting part of the sequence until time t and ends in S i , and the backward variable β t (i) takes it from there and explains the ending part until time T .The numerator α t (i)β t (i) explains the whole sequence given that at time t, the system is in state S i .We need to normalize by dividing this over all possible intermediate states that can be traversed at time t, and guarantee that i γ t (i) = 1.To find the state sequence, for each time step t, we can choose the state that has the highest probability:but this may choose S i and S j as the most probable states at time t and t + 1 even when a ij = 0. To find the single best state sequence (path), we use the Viterbi algorithm, based on dynamic programming, which takes Viterbi algorithm such transition probabilities into account.Given state sequencewe define δ t (i) as the probability of the highest probability path at time t that accounts for the first t observations and ends in S i :Then we can recursively calculate δ t+1 (i) and the optimal path can be read by backtracking from T , choosing the most probable at each instant.The algorithm is as follows:1. Initialization: 4. Path (state sequence) backtracking:Using the lattice structure of figure 15.2, ψ t (j) keeps track of the state that maximizes δ t (j) at time t − 1, that is, the best previous state.The Viterbi algorithm has the same complexity with the forward phase, where instead of the sum, we take the maximum at each step.We now move on to the third problem, learning an HMM from data.The approach is maximum likelihood, and we would like to calculate λ * that maximizes the likelihood of the sample of training sequences, X = {O k } K k=1 , namely, P (X|λ).We start by defining a new variable that will become handy later on.We define ξ t (i, j) as the probability of being in S i at time t and in S j at time t + 1, given the whole observation O and λ: ξ t (i, j) ≡ P (q t = S i , q t+1 = S j |O, λ) (15.25) which can be computed as follows (see figure 15.4): ξ t (i, j) ≡ P (q t = S i , q t+1 = S j |O, λ) = P (O|q t = S i , q t+1 = S j , λ)P (q t = S i , q t+1 = S j |λ) P (O|λ) = P (O|q t = S i , q t+1 = S j , λ)P (q t+1 = S j |q t = S i , λ)P (q t = S i |λ) P (O|λ)α t (i) explains the first t observations and ends in state S i at time t.We move on to state S j with probability a ij , generate the (t +1)st observation, and continue from S j at time t + 1 to generate the rest of the observation sequence.We normalize by dividing for all such possible pairs that can be visited at time t and t + 1.If we want, we can also calculate the probability of being in state S i at time t by marginalizing over the arc probabilities for all possible next states:Note that if the Markov model were not hidden but observable, both γ t (i) and ξ t (i, j) would be 0/1.In this case when they are not, we estimate them with posterior probabilities that give us soft counts.This is just like soft counts the difference between supervised classification and unsupervised clustering where we did and did not know the class labels, respectively.In unsupervised clustering using EM (section 7.4), not knowing the class labels, we estimated them first (in the E-step) and calculated the parameters with these estimates (in the M-step).Similarly here we have the Baum-Welch algorithm, which is an EM pro-Baum-Welch algorithm cedure.At each iteration, first in the E-step, we compute ξ t (i, j) and γ t (i) values given the current λ = (A, B, Π), and then in the M-step, we recalculate λ given ξ t (i, j) and γ t (i).These two steps are alternated until convergence during which, it has been shown, P (O|λ) never decreases.Assume indicator variables z t i as1 if q t = S i 0 otherwise (15.28) and1 if q t = S i and q t+1 = S j 0 otherwise (15.29)These are 0/1 in the case of an observable Markov model and are hidden random variables in the case of an HMM.In this latter case, we estimate them in the E-step asIn the M-step, we calculate the parameters given these estimated values.The expected number of transitions from S i to S j is t ξ t (i, j) and the total number of transitions from S i is t γ t (i).The ratio of these two gives us the probability of transition from S i to S j at any time:Note that this is the same as equation 15.9, except that the actual counts are replaced by estimated soft counts.The probability of observing v m in S j is the expected number of times v m is observed when the system is in S j over the total number of times the system is inWhen there are multiple observation sequenceswhich we assume to be independentthe parameters are now averages over all observations in all sequences:In our discussion, we assumed discrete observations modeled as a multinomialIf the inputs are continuous, one possibility is to discretize them and then use these discrete values as observations.Typically, a vector quantizer (section 7.3) is used for this purpose of converting continuous values to the discrete index of the closest reference vector.For example, in speech recognition, a word utterance is divided into short speech segments corresponding to phonemes or part of phonemes; after preprocessing, these are discretized using a vector quantizer and an HMM is then used to model a word utterance as a sequence of them.We remember that k-means used for vector quantization is the hard version of a Gaussian mixture model:and the observations are kept continuous.In this case of Gaussian mixtures, EM equations can be derived for the component parameters (with suitable regularization to keep the number of parameters in check) and the mixture proportions (Rabiner 1989)We discussed graphical models in chapter 14, and the hidden Markov model can also be depicted as a graphical model.The three successive states q t−2 , q t−1 , q t correspond to the three states on a chain in a firstorder Markov model.The state at time t, q t , depends only on the state at time t − 1, q t−1 , and given q t−1 , q t is independent of q t−2 P (q t |q t−1 , q t−2 ) = P (q t |q t−1 )as given by the state transition probability matrix A (see figure 15.5).Each hidden variable generates a discrete observation that is observed, as given by the observation probability matrix B. The forward-backward procedure of hidden Markov models we discuss in this chapter is an application of belief propagation that we discussed in section 14.5.Continuing with the graphical formalism, different HMM types can be devised and depicted as different graphical models.In figure 15.6a, an input-output HMM is shown where there are two separate observed inputinput-output HMM output sequences and there is also a sequence of hidden states (Bengio and Frasconi 1996).In some applications this is the case, namely, additional to the observation sequence O t , we have an input sequence, x t , and we know that the observation depends also on the input.In such a case, we condition the observation O t in state S j on the input x t and write P (O t |q t = S j , x t ).When the observations are numeric, for example, we replace equation 15.38 with a generalized model p(O t |q t = S j , x t , λ) ∼ N (g j (x t |θ j ), σ 2 j ) (15.40) where, for example, assuming a linear model, we have g j (x t |w j , w j0 ) = w j x t + w j0 (15.41)If the observations are discrete and multinomial, we have a classifier taking x t as input and generating a 1-of-M output, or we can generate posterior class probabilities and keep the observations continuous.Similarly, the state transition probabilities can also be conditioned on the input, namely, P (q t+1 = S j |q t = S i , x t ), which is implemented by a classifier choosing the state at time t + 1 as a function of the state at time t and the input.This is a Markov mixture of experts (Meila and Jordan Markov mixture of experts 1996) and is a generalization of the mixture of experts architecture (section 12.8) where the gating network keeps track of the decision it made in the previous time step.This has the advantage that the model is no longer homogeneous; different observation and transition probabilities are used at different time steps.There is still a single model for each state, parameterized by θ j , but it generates different transition or observation probabilities depending on the input seen.It is possible that the input is not a single value but a window around time t making the input a vector; this allows handling applications where the input and observation sequences have different lengths.Even if there is no other explicit input sequence, an HMM with input can be used by generating an "input" through some prespecified function of previous observationsthereby providing a window of size τ of contextual input.Another HMM type that can be easily visualized is a factorial HMM, factorial HMMwhere there are multiple separate hidden sequences that interact to generate a single observation sequence.An example is a pedigree that dispedigree plays the parent-child relationship ); figure 15.6b models meiosis where the two sequences correspond to the chromosomes of the father and the mother (which are independent), and at each locus (gene), the offspring receives one allele from the father and the other allele from the mother.A coupled HMM, shown in figure 15.6c, models two parallel but interactcoupled HMM ing hidden sequences that generate two parallel observation sequences.For example, in speech recognition, we may have one observed acoustic sequence of uttered words and one observed visual sequence of lip images, each having its hidden states where the two are dependent.In a switching HMM, shown in figure 15.6d, there are K parallel indeswitching HMM pendent hidden state sequences, and the state variable S at any one time picks one of them and the chosen one generates the output.That is, we switch between state sequences as we go along.In HMM proper, though the observation may be continuous, the state variable is discrete; in a linear dynamical system, also known as theKalman filter, both the state and the observations are continuous.In the Kalman filter basic case, the state at time t is a linear function of the state at t − 1 with additive zero-mean Gaussian noise, and, at each state, the observation is another linear function of the state with additive zero-mean Gaussian noise.The two linear mappings and the covariances of the two noise sources make up the parameters.All HMM variants we discussed earlier can similarly be generalized to use continuous states.By suitably modifying the graphical model, we can adapt the architecture to the characteristics of the process that generates the data.This process of matching the model to the data is a model selection procedure to best trade off bias and variance.The disadvantage is that exact inference may no longer be possible on such extended HMMs, and we would need approximation or sampling methods .Just like any model, the complexity of an HMM should be tuned so as to balance its complexity with the size and properties of the data at hand.One possibility is to tune the topology of the HMM.In a fully connected (ergodic) HMM, there is transition from a state to any other state, which makes A a full N × N matrix.In some applications, only certain transitions are allowed, with the disallowed transitions having their a ij = 0.When there are fewer possible next states, N < N, the complexity of forward-backward passes and the Viterbi procedure is O(NN T ) instead of O(N 2 T ).For example, in speech recognition, left-to-right HMMs are used, which left-to-right HMMs have their states ordered in time so that as time increases, the state index increases or stays the same.Such a constraint allows modeling sequences whose properties change over time as in speech, and when we get to a state, we know approximately the states preceding it.There is the property that we never move to a state with a smaller index, namely, a ij = 0, for j < i.Large changes in state indices are not allowed either, namely, a ij = 0, for j > i + τ.The example of the left-to-right HMM given in figure 15.7 with τ = 2 has the state transition matrixAnother factor that determines the complexity of an HMM is the number of states N. Because the states are hidden, their number is not known and should be chosen before training.This is determined using prior information and can be fine-tuned by cross-validation, namely, by checking the likelihood of validation sequences.When used for classification, we have a set of HMMs, each one modeling the sequences belonging to one class.For example, in spoken word recognition, examples of each word train a separate model, λ i .Given a new word utterance O to classify, all of the separate word models are evaluated to calculate P (O|λ i ).We then use Bayes' rule to get the posterior probabilitieswhere P (λ i ) is the prior probability of word i.The utterance is assigned to the word having the highest posterior.This is the likelihood-based approach; there is also work on discriminative HMM trained directly to maximize the posterior probabilities.When there are several pronunciations of the same word, these are defined as parallel paths in the HMM for the word.In the case of a continuous input like speech, the difficult task is that of segmenting the signal into small discrete observations.Typically, phones phones are used that are taken as the primitive parts, and combining them, longer sequences (e.g., words) are formed.Each phone is recognized in parallel (by the vector quantizer), then the HMM is used to combine them serially.If the speech primitives are simple, then the HMM becomes complex and vice versa.In connected speech recognition where the words are not uttered one by one with clear pauses between them, there is a hierarchy of HMMs at several levels; one combines phones to recognize words, another combines words to recognize sentences by building a language model, and so forth.Hybrid neural network/HMM models were also used for speech recognition (Morgan and Bourlard 1995).In such a model, a multilayer perceptron (chapter 11) is used to capture temporally local but possibly complex and nonlinear primitives, for example, phones, while the HMM is used to learn the temporal structure.The neural network acts as a preprocessor and translates the raw observations in a time window to a form that is easier to model than the output of a vector quantizer.An HMM can be visualized as a graphical model and evaluation in an HMM is a special case of the belief propagation algorithm that we discuss in chapter 14.The reason we devote a special chapter is the widespread successful use of this particular model, especially in automatic speech recognition.But the basic HMM architecture can be extended-for example, by having multiple sequences, or by introducing hidden (latent) variables, as we discuss in section 15.9.In chapter 16, we discuss the Bayesian approach and in section 16.8, we discuss the nonparametric Bayesian methods where the model structure can be made more complex over time as more data arrives.One application of that is the infinite HMM .The HMM is a mature technology, and there are HMM-based commercial speech recognition systems in actual use (Rabiner and Juang 1993;Jelinek 1997).In section 11.12, we discussed how to train multilayer perceptrons for recognizing sequences.HMMs have the advantage over time delay neural networks in that no time window needs to be defined a priori, and they train better than recurrent neural networks.HMMs are applied to diverse sequence recognition tasks.Applications of HMMs to bioinformatics is given in Baldi and Brunak 1998, and to natural language processing in Manning and Schütze 1999.It is also applied to online handwritten character recognition, which differs from optical recognition in that the writer writes on a touch-sensitive pad and the input is a sequence of (x, y) coordinates of the pen tip as it moves over the pad and is not a static image.Bengio et al. (1995)  In any such recognition system, one critical point is to decide how much to do things in parallel and what to leave to serial processing.In speech recognition, phonemes may be recognized by a parallel system that corresponds to assuming that all the phoneme sound is uttered in one time step.The word is then recognized serially by combining the phonemes.In an alternative system, phonemes themselves may be designed as a sequence of simpler speech sounds, if the same phoneme has many versions, for example, depending on the previous and following phonemes.Doing things in parallel is good but only to a degree; one should find the ideal balance of parallel and serial processing.To be able to call anyone at the touch of a button, we would need millions of buttons on our telephone; instead, we have ten buttons and we press them in a sequence to dial the number.We discussed graphical models in chapter 14, and we know that HMMs can be considered a special class of graphical models and inference and learning operations on HMMs are analogous to their counterparts in graphical models (Smyth, Heckerman, and Jordan 1997).There are various extensions to HMMs, such as factorial HMMs, where at each time step, there are a number of states that collectively generate the observation and treestructured HMMs where there is a hierarchy of states.The general formalism also allows us to treat continuous as well as discrete states, known as linear dynamical systems.For some of these models, exact inference is not possible and one needs to use approximation or sampling methods .Actually, any graphical model can be extended in time by unfolding it in time and adding dependencies between successive copies.In fact, a hidden Markov model is nothing but a sequence of clustering problems where the cluster index at time t is dependent not only on observation at time t but also on the index at time t − 1, and the Baum-Welch algorithm is expectation-maximization extended to also include this dependency in time.In section 6.5, we discussed factor analysis where a small number of hidden factors generate the observation; similarly, a linear dynamical system may be viewed as a sequence of such factor analysis models where the current factors also depend on the previous factors.This dynamic dependency may be added when needed.For example, figure 14.5 models the cause of wet grass for a particular day; if we believe that yesterday's weather has an influence on today's weather (and we should-it tends to be cloudy on successive days, then sunny for a number of days, and so on), we can have the dynamic graphical model shown in figure 15.8 where we model this dependency.1. Given the observable Markov model with three states, S 1 , S 2 , S 3 , initial probabilitiesand transition probabilitiesgenerate 100 sequences of 1,000 states.2. Using the data generated by the previous exercise, estimate Π, A and compare with the parameters used to generate the data.3. Formalize a second-order Markov model.What are the parameters?How can we calculate the probability of a given state sequence?How can the parameters be learned for the case of an observable model?SOLUTION: In a second-order model, the current state depends on the two previous states:Initial state probability defines the probability of the first state:We also need parameters to define the probability of the second state given the first state:Given a second-order observable MM with parameters λ = (Π, Θ, A), the probability of an observed state sequence isThe probabilities are estimated as proportions:t γ t (j) 8. Consider the urn-and-ball example where we draw without replacement.How will it be different?SOLUTION: If we draw without replacement, then at each iteration, the number of balls change, which means that the observation probabilities, B, change.We will no longer have a homogenous model.9. Let us say at any time we have two observations from two different alphabets; for example, let us say we are observing the values of two currencies every day.How can we implement this using HMM?SOLUTION: In such a case, what we have is a hidden state generating two different observations.That is, we have two B, each trained with its own observation sequence.These two observations then need to be combined to estimate A and π .10. How can we have an incremental HMM where we add new hidden states when necessary?SOLUTION: Again, this is a state space search.Our aim may be to maximize validation log likelihood, and an operator allows us to add a hidden state.We do then a forward search.There are structure learning algorithms for the more general case of graphical models, which we discussed in chapter 14.In the Bayesian approach, we consider parameters as random variables with a distribution allowing us to model our uncertainty in estimating them.We continue from where we left off in section 4.4 and discuss estimating both the parameters of a distribution and the parameters of a model for regression, classification, clustering, or dimensionality reduction.We also discuss nonparametric Bayesian modeling where model complexity is not fixed but depends on the data.Bayesian estimation, which we introduced in section 4.4, treats a parameter θ as a random variable with a probability distribution.The maximum likelihood approach we discussed in section 4.2 treats a parameter as an unknown constant.For example, if the parameter we want to estimate is the mean μ, its maximum likelihood estimator is the sample average X.We calculate X over our training set, plug it in our model, and use it, for example, for classification.However, we know that especially with small samples, the maximum likelihood estimator can be a poor estimator and has variance-as the training set varies, we may calculate different values of X, which in turn may lead to different discriminants with different generalization accuracies.In Bayesian estimation, we make use of the fact that we have uncertainty in estimating θ and instead of a single θ ML , we use all θ weighted by our estimated distribution, p(θ|X).That is, we average over our uncertainty in estimating θ.While estimating p(θ|X), we can make use of the prior information we The rectangular plate contains N independent instances drawn, and they make up the training set X.The new x is independently drawn given θ.This is the iid assumption.If θ is not known, they are dependent.We infer θ from the past instances using Bayes' rule, which is then used to make inference about the new x .may have regarding the value of the parameter.Such prior beliefs are especially important when we have a small sample (and when the variance of the maximum likelihood estimator is high).In such a case, we are interested in combining what the data tells us, namely, the value calculated from the sample, and our prior information.As we first discussed in section 4.4, we code this information using a prior probability distribution.For example, before looking at a sample to estimate the mean, we may have some prior belief that it is close to 2, between 1 and 3, and in such a case, we write p(μ) in such a way that the bulk of the density lies in the interval [1,3].Using Bayes' rule, we combine the prior and the likelihood and calculate the posterior probability distribution:Here, p(θ) is the prior density; it is what we know regarding the possible values that θ may take before looking at the sample.p(X|θ) is the sample likelihood; it tells us how likely our sample X is if the parameter of the distribution takes the value θ.For example, if the instances in our sample are between 5 and 10, such a sample is likely if μ is 7 but is less likely if μ is 3 and even less likely if μ is 1. p(X) in the denominator is a normalizer to make sure that the posterior p(θ|X) integrates to 1.It is called the posterior probability because it tells us how likely θ takes a certain value after looking at the sample.The Bayes' rule takes the prior distribution, combines it with what the data reveals, and generates the posterior distribution.We then use this posterior distribution later for making inference.Let us say that we have a past sample X = {x t } N t=1 drawn from sone distribution with unknown parameter θ.We can then draw one more instance x , and we would like to calculate its probability distribution.We can visualize this as a graphical model (chapter 14)  We write the joint asWe can estimate the probability distribution for the new x given the sample X:In calculating p(θ|X), Bayes' rule inverts the direction of the arc and makes a diagnostic inference.This inferred (posterior) distribution is then used to derive a predictive distribution for new x.We see that our estimate is a weighted sum (we replace dθ by θ if θ is discrete valued) of estimates using all possible values of θ weighted by how likely each θ is, given the sample X.This is the full Bayesian treatment and it may not be possible if the posterior is not easy to integrate.As we saw in section 4.4, in the case of the maximum a posteriori (MAP) estimate, we use the mode of the maximum a posteriori (MAP) estimate posterior:The MAP estimate corresponds to assuming that the posterior makes a very narrow peak around a single point, that is, the mode.If the prior p(θ) is uniform over all θ, then the mode of the posterior p(θ|X) and the mode of the likelihood p(X|θ) are at the same point, and the MAP estimate is equal to the maximum likelihood (ML) estimate:This implies that using ML corresponds to assuming no a priori distinction between different values of θ.Basically, the Bayesian approach has two advantages:1.The prior helps us ignore the values that θ is unlikely to take and concentrate on the region where it is likely to lie.Even a weak prior with long tails can be very helpful.2. Instead of using a single θ estimate in prediction, we generate a set of possible θ values (as defined by the posterior) and use all of them in prediction, weighted by how likely they are.If we use the MAP estimate instead of integrating over θ, we make use of the first advantage but not the second-if we use the ML estimate, we lose both advantages.If we use an uninformative (uniform) prior, we make use of the second advantage but not the first.Actually it is this second advantage, rather than the first, that makes the Bayesian approach interesting, and in chapter 17, we discuss combining multiple models where we see methods that are very similar, though not always Bayesian.This approach can be used in different types of distributions and for different types of applications.The parameter θ can be the parameter of a distribution.For example, in classification, it can be the unknown class mean, for which we define a prior and get its posterior; then, we get a different discriminant for each possible value of the mean and hence the Bayesian approach will average over all possible discriminants whereas in the ML approach there is a single mean estimate and hence a single discriminant.The unknown parameter, as we will see shortly, can also be the parameters of a fitted model.For example, in linear regression, we can define a prior distribution on the slope and the intercept parameters and calculate a posterior on them, that is, a distribution over lines.We will then be averaging over the prediction of all possible lines, weighted by how likely they are as specified by their prior weights and how well they fit the given data.One of the most critical aspects of Bayesian estimation is evaluating the integral in equation 16.2.For some cases, we can calculate it, but mostly we cannot, and in such cases, we need to approximate it, and we will see methods for this in the next few sections, namely, Laplace and variational approximations, and Markov chain Monte Carlo (MCMC) sampling.Now, let us see these and other applications of the Bayesian approach in more detail, starting from simple and incrementally making them more complex.Let us say that each instance is a multinomial variable taking one of K distinct states (section 4.2.2).We say x t i = 1 if instance t is in state i and x t j = 0, ∀j = i.The parameters are the probabilities of states, q = [q 1 , q 2 , . . ., q K ] T with q i , i = 1, . . ., K satisfying q i ≥ 0, ∀i and i q i = 1.For example, x t may correspond to news documents and states may correspond to K different news categories: sports, politics, arts, and so on.The probabilities q i then correspond to the proportions of different news categories, and priors on them allow us to code our prior beliefs in these proportions; for example, we may expect to have more news related to sports than news related to arts.The sample likelihood isThe prior distribution of q is the Dirichlet distribution:where α = [α 1 , . . ., α K ] T and α 0 = i α i .α i , the parameters of the prior, are called the hyperparameters.Γ (x) is the gamma function defined asGiven the prior and the likelihood, we can derive the posterior:where N i = N t=1 x t i .We see that the posterior has the same form as the prior, and we call such a prior a conjugate prior.Both the prior and the conjugate prior likelihood have the form of product of powers of q i , and we combine them to make up the posterior:Looking at equation 16.3, we can bring an interpretation to the hyperparameters α i .Just as n i are counts of occurrences of state i in a sample of N, we can view α i as counts of occurences of state i in some imaginary sample of α 0 instances.In defining the prior, we are subjectively saying the following: In a sample of α 0 , I would expect α i of them to belong to state i.Note that larger α 0 implies that we have a higher confidence (a more peaked distribution) in our subjective proportions: Saying that I expect to have 60 out of 100 occurrences belong to state 1 has higher confidence than saying that I expect to have 6 out of 10.The posterior then is another Dirichlet that sums up the counts of the occurences of states, imagined and actual, given by the prior and the likelihood, respectively.The conjugacy has a nice implication.In a sequential setting where we receive a sequence of instances, because the posterior and the prior have the same form, the current posterior accumulates information from all past instances and becomes the prior for the next instance.When the variable is binary, x t ∈ {0, 1}, the multinomial sample becomes Bernoulli:and the Dirichlet prior reduces to the beta distribution:← beta(20,30) For example, x t may be 0 or 1 depending on whether email with index t in a random sample of size N is legitimate or spam, respectively.Then defining a prior on q allows us to define a prior belief on the spam probability: I would expect, on the average, α/(α + β) of my emails to be spam.Beta is a conjugate prior, and for the posterior we getwhere A = t x t , and we see again that we combine the occurrences in the imaginary and the actual samples.Note that when α = β = 1, we have a uniform prior and the posterior has the same shape as the likelihood.As the two counts, whether α and β for the prior or α+A and β+N−A for the posterior, increase and their difference increases, we get a distribution that is more peaked with smaller variance (see figure 16.2).As we see more data (imagined or actual), the variance decreases.We now consider the case where instances are Gaussian distributed.Let us start with the univariate case, p(x) ∼ N (μ, σ 2 ), where the parame-ters are μ and σ 2 ; we discussed this briefly in section 4.4.The sample likelihood isThe conjugate prior for μ is Gaussian, p(μ) ∼ N (μ 2 0 , σ 2 0 ), and we write the posterior aswherewhere m = t x t /N is the sample average.We see that the mean of the posterior density (which is the MAP estimate), μ N , is a weighted average of the prior mean μ 0 and the sample mean m, with weights being inversely proportional to their variances (see figure 16.3 for an example).Note that because both coefficients are between 0 and 1 and sum to 1, μ N is always between μ 0 and m.When the sample size N or the variance of the prior σ 2 0 is large, the posterior mean is close to m, relying more on the information provided by the sample.When σ 2 0 is small-that is, when we have little prior uncertainty regarding the correct value of μ, or when we have a small sample-our prior guess μ 0 has higher effect.σ N gets smaller when either of σ 0 or σ gets smaller or if N is larger.Note also that σ N is smaller than both σ 0 and σ / √ N, that is, the posterior variance is smaller than both prior variance and that of m.Incorporating both results in a better posterior estimate than using any of the prior or sample alone.If σ 2 is known, for new x, we can integrate over this posterior to make a prediction:We see that x is still Gaussian, that it is centered at the posterior mean, and that its variance now includes the uncertainty due to the estimation of the mean and the new sampled instance x.We can write, where this last follows from the fact that the new x is an independent draw.Once we get a distribution for p(x|X), we can use it for different purposes.For example in classification, this approach corresponds to assuming Gaussian classes where means have a Gaussian prior and they are trained using X i , the subset of X labeled by class C i .Then, p(x|X i ) as calculated above, corresponds to p(x|C i ), which we combine with prior P (C i ) to get the posterior and hence a discriminant.If we do not know σ 2 , we also need to estimate it.For the case of variance, we work with the precision, the reciprocal of the variance, λ ≡ 1/σ 2 .precision Using this, the sample likelihood is written asThe conjugate prior for the precision is the gamma distribution:where we define a 0 ≡ ν 0 /2 and b 0 ≡ (ν 0 /2)s 2 0 such that s 2 0 is our prior estimate of variance and ν 0 is our confidence in this prior-it may be thought of as the size of the imaginary sample on which we believe s 2 0 is estimated.The posterior then is also gamma:wherewhere s 2 = t (x t − μ) 2 /N is the sample variance.Again, we see that posterior estimates are weighted sum of priors and sample statistics.To make a prediction for new x, when both μ and σ 2 are unknown, we need the joint posterior that we write aswhere p(λ) ∼ gamma(a 0 , b 0 ) and p(μ|λ) ∼ N (μ 0 , 1/(κ 0 λ)).Here again, κ 0 may be thought of as the size of the imaginary sample and as such it defines our confidence in the prior.The conjugate prior for the joint in this case is called the normal-gamma distributionThe posterior iswhereTo make a prediction for new x, we integrate over the posterior:That is, we get a (nonstandardized) t distribution having the given mean and variance values with 2a N degrees of freedom.In equation 16.8, we have a Gaussian distribution; here the mean is the same but because σ 2 is unknown, its estimation adds uncertainty, and we get a t distribution with wider tails.Sometimes, equivalently, instead of modeling the precision λ, we model σ 2 and for this, we can use the inverse gamma or the inverse chi-squared distribution; see Murphy 2007.If we have multivariate x ∈ d , we use exactly the same approach, except for the fact that we need to use the multivariate versions of the distributions ).We havewhere Λ ≡ Σ −1 is the precision matrix.We use a Gaussian prior (condiprecision matrix tioned on Λ) for the mean:and for the precision matrix, the multivariate version of the gamma distribution is called the Wishart distribution:where ν 0 , as with κ 0 , corresponds to the strength of our prior belief.The conjugate joint prior is the normal-Wishart distribution:and the posterior iswhere T is the scatter matrix.To make a prediction for new x, we integrate over the joint posterior:That is, we get a (nonstandardized) multivariate t distribution having this mean and covariance with ν N − d + 1 degrees of freedom.We now discuss the case where we estimate the parameters, not of a distribution, but some function of the input, for regression or classification.Again, our approach is to consider these parameters as random variables with a prior distribution and use Bayes' rule to calculate a posterior distribution.We can then either evaluate the full integral, approximate it, or use the MAP estimate.Let us take the case of a linear regression model:where β is the precision of the additive noise (assume that one of the d inputs is always +1).The parameters are the weights w and we have a sample X = {x t , r t } N t=1 where x ∈ d and r t ∈ .We can break it down into a matrix of inputs and a vector of desired outputs as X = [X, r] where X is N × d and r is N × 1.From equation 16.19, we haveWe saw previously in section 4.6 that the log likelihood iswhere the second term is a constant, independent of the parameters.We expand the first term asFor the case of the ML estimate, we find w that maximizes this, or equivalently, minimizes the last term that is the sum of the squared error.It can be rewritten asTaking the derivative with respect to w and setting it to 0we get the maximum likelihood estimator (we have previously derived this in section 5.8):Having calculated the parameters, we can now do prediction.Given new input x , the response is calculated asIn the general case, for any model, g(x|w), for example, a multilayer perceptron where w are all the weights, we minimize, for example, using gradient descent:and w LSQ that minimizes it is called the least squares estimator.Then for new x , the prediction is calculated asIn the case of the Bayesian approach, we define a Gaussian prior for the parameters:which is a conjugate prior, and for the posterior, we getwhere μ N = βΣ N X T r (16.23)To calculate the output for new x , we integrate over the full posterior r = (w T x )p(w|X, r)dwThe graphical model for this is shown in figure 14.7.If we want to use a point estimate, the MAP estimator isand in calculating the output for input x , we replace the density with a single point, namely, the mean:We can also calculate the variance of our estimate:Comparing equation 16.24 with the ML estimate of equation 16.21, this can be seen as regularization-that is, we add a constant α to the diagonal to better condition the matrix to be inverted.The prior, p(w) ∼ N (0, (1/α)I), says that we expect the parameters to be close to 0 with spread inversely proportional to α.When α → 0, we have a flat prior and the MAP estimate converges to the ML estimate.We see in figure 16.4 that if we increase α, we force parameters to be closer to 0 and the posterior distribution moves closer to the origin and shrinks.If we decrease β, we assume noise with higher variance and the posterior also has higher variance.If we take the log of the posterior, we have log p(w|X, r) ∝ log p(r|w, X) + log p(w) The MAP solution with one standard deviation error bars are also shown dashed.Center: prior density centered at 0 and variance 1/α.To the right: posterior density whose mean is the MAP solution.We see that when α is increased, the variance of the prior shrinks and the line moves closer to the flat 0 line.When β is decreased, more noise is assumed and the posterior density has higher variance.which we maximize to find the MAP estimate.In the general case, given our model g(x|w), we can write an augmented error functionwith λ ≡ α/β.This is known as parameter shrinkage or ridge regression ridge regression in statistics.In section 4.8, we called this regularization, and in section 11.9, we called this weight decay in neural networks.The first term is the negative log of the likelihood, and the second term penalizes w i away from 0 (as dictated by α of the prior).Though this approach reduces i w 2 i , it does not force individual w i to 0; that is, it cannot be used for feature selection, namely, to determine which x i are redundant.For this, one can use a Laplacian prior that uses Laplacian prior the L 1 norm instead of the L 2 norm (Figueiredo 2003):The posterior probability is no longer Gaussian and the MAP estimate is found by minimizingwhere σ 2 is the variance of noise (for which we plug in our estimate).This is known as lasso (least absolute shrinkage and selection operalasso tor) (Tibshirani 1996).To see why L 1 induces sparseness, let us consider the case with two weights [w 1 , w 2 ] T (Figueiredo 2003)2, and therefore L 1 prefers to set w 2 to 0 and use a large w 1 , rather than having small values for both.Above, we assume that β, the precision of noise, is known and w is the only parameter we integrated on.If we do not know β, we can also define a prior on it.Just as we do in section 16.3, we can define a gamma prior:and a prior on w conditioned on β: p(w|β) ∼ N (μ 0 , βΣ 0 )If μ 0 = 0 and Σ 0 = αI, we get ridge regression, as we discussed above.We now can write a conjugate normal-gamma prior on parameters w and β:It can be shown (Hoff 2009) that the posterior iswhereAn example is given in figure 16.5 where we fit a polynomial of different degrees on a small set of instances-w corresponds to the vector of coefficients of the polynomial.We see that the maximum likelihood starts to overfit as the degree is increased.We use Markov chain Monte Carlo sampling to get the Bayesian fitas follows: We draw a β value from p(β) ∼ gamma(a N , b N ), and then we draw a w from p(w|β) ∼ N (μ N , βΣ N ), which gives us one sampled model from the posterior p(w, β).Ten such samples are drawn for each degree, as shown in figure 16.5.The thick line is the average of those ten models and is an approximation of the full integral; we see that even with ten samples, we get a reasonable and very smooth fit to the data.Note that any of the sampled models from the posterior is not necessarily any better than the maximum likelihood estimator; it is the averaging that leads to a smoother and hence better fit.Using the Bayes' estimate of equation 16.23, the prediction is written asThis is the dual representation.When we can write the parameter in Figure 16.5 Bayesian polynomial regression example.Circles are the data points and the dashed line is the maximum likelihood fit, which overfits as the degree of the polynomial is increased.Thin lines are ten samples from the posterior p(w, β) and the thick line is their average.terms of the training data, or a subset of it as in support vector machines (chapter 13), we can write the prediction as a function of the current input and past data.We can rewrite this aswhere we defineWe know that we can generalize the linear kernel of equation 16.28 by using a nonlinear basis function φ(x) to map to a new space where wewhere we defineas the equivalent kernel.This is the dual representation in the space of φ(x).We see that we can write our estimate as a weighted sum of the effects of instances in the training set where the effect is given by the kernel kernel function function K(x , x t ); this is similar to the nonparametric kernel smoothers we discussed in chapter 8, or the kernel machines of chapter 13.Error bars can be defined usingAn example is given in figure 16.6 for the linear, quadratic, and sixthdegree kernels.This is equivalent to the polynomial regression we see in figure 16.5, except that here we use the dual representation and the polynomial coefficients w are embedded in the kernel function.We see that just as in regression proper where we can work on the original x or φ(x), in Bayesian regression too we can work on the preprocessed φ(x), defining parameters in that space.Later on in this chapter, we are going to see Gaussian processes where we can define and use K(x, x t ) directly without needing to calculate φ(x).In a two-class problem, we have a single output, and assuming a linear model, we have The log likelihood of a Bernoulli sample is given aswhich we maximize, or minimize its negative log-the cross-entropy-to find the ML estimate, for example, using gradient descent.This is called logistic discrimination (section 10.7).In the case of the Bayesian approach, we assume a Gaussian priorand the log of the posterior is given as log p(w|r, X) ∝ log p(w) + log p(r|w, X)This posterior distribution is no longer Gaussian, and we cannot integrate exactly.We can use Laplace approximation, which works as follows Laplace approximation .Let us say we want to approximate some distribution f (x), not necessarily normalized (to integrate to 1).In Laplace approximation, we find the mode of f (x), x 0 , fit a Gaussian q(x) centered there with covariance given by the curvature of f (x) around that mean, and then if we want to integrate, we integrate this fitted Gaussian instead.To find the variance of the Gaussian, we consider the Taylor expansion of f (•)Note that the first, linear term disappears because the first derivative is 0 at the mode.Taking exp, we haveTo normalize f (x), we consider that in a Gaussian distributionand thereforeIn the multivariate setting where x ∈ d , we havewhere A is the (Hessian) matrix of second derivatives:The Laplace approximation is thenHaving now discussed how to approximate, we can now use it for the posterior density.The MAP estimate, w MAP -the mode of p(w|r, X)-is taken as the mean, and the covariance matrix is given by the inverse of the matrix of the second derivatives of the negative log likelihood:We then integrate over this Gaussian to estimate the class probability:where q(w) ∼ N (w MAP , S −1 N ).A further complication is that we cannot integrate analytically over a Gaussian convolved with a sigmoid.If we use the probit function instead, which has the same S-shape as the sigmoid, probit function an analytical solution is possible .Defining the prior is the subjective part of Bayesian estimation and as such should be done with care.It is best to define robust priors with heavy tails so as not to limit the parameter space too much; in the extreme case of no prior preference, one can use an uninformative prior and methods have been proposed for this purpose, for example, Jeffreys prior .Sometimes our choice of a prior is also motivated by simplicity-for example, a conjugate prior makes inference quite easy.One critical decision is when to take a parameter as a constant and when to define it as a random variable with a prior and to be integrated (averaged) out.For example, in section 16.4.1,we assume that we know the noise precision whereas in section 16.4.2,we assume we do not and define a gamma prior on it.Similarly for the spread of weights in linear regression, we assume a constant α value but can also define a prior on it and average it out if we want.Of course, this makes the prior more complicated and the whole inference more difficult but averaging over α should be preferred if we do not know what the good value for α is.Another decision is how high to go in defining the priors.Let us say we have parameter θ and we define a posterior on it.In prediction, we havewhere p(θ|X) ∝ p(X|θ)p(θ).If we believe that we cannot define a good p(θ) but that it depends on some other variable, we can condition θ on a hyper parameter α and integrate it out:This is called a hierarchical prior.This really makes the inference rather difficult because we need to integrate on two levels.One shortcut is to test different values α on the data, choose the best α * , and just use that value:This is called level II maximum likelihood or empirical Bayes.Assume we have many models M j , each with its own set of parameters θ j , and we want to compare these models.For example, in figure 16.5, we have polynomials of different degrees and let us say we want to check how well they fit the data.For a given model M and parameter θ, the likelihood of data is p(X|M, θ).To get the Bayesian marginal likelihood for a given model, we average marginal likelihood over θ: p(X|M) = p(X|θ, M)p(θ|M)dθ (16.33)This is also called model evidence.For example, in the polynomial remodel evidence gression example above, for a given degree, we havewhere p(w, β|M) is the prior assumed for model M. We can then calculate the posterior probability of a model given the data:where P (M) is the prior distribution defined over models.The nice property of the Bayesian approach is that even if those priors are taken uniform, the marginal likelihood, because it averages over all θ, favors simpler models.Let us assume we have models in increasing complexity, for example, polynomials with increasing degree.M 1 , M 2 , and M 3 are three models in increasing complexity.The x axis is the space of all datasets with N instances.A complex model can fit more datasets but spreads itself thin over the space of all possible datasets of size N; a simpler model can fit fewer datasets but each with a heavier probability.For a particular dataset X , if both can fit, the simpler model will have higher marginal likelihood .Let us say we have a dataset X with N instances.A more complex model will be able to fit more of such datasets reasonably well compared with a simpler model-consider choosing randomly three points in a plane; the number of such triples that can be fitted by a line is much fewer than the number of triples that can be fitted by a quadratic.Given that X p(X|M) = 1, because for a complex model there are more possible X where it can make a reasonable fit, if there is a fit, the value of p(X |M) for some particular X is going to be smaller-see figure 16.7.Hence for a simpler model p(M|X) will be higher (even if we assume that the priors, p(M), are equal); this is the Bayesian interpretation of Occam's razor .For the polynomial fitting example of figure 16.5, a comparison of likelihood and the marginal likelihood is shown in figure 16.8.We see that likelihood increases when complexity increases, which implies overfitting, but the marginal likelihood increases until the correct degree and then starts decreasing; this is because there are many more complex models that fit badly to the data and they pull the likelihood down as we average over them.If we have two models M 0 and M 1 , we can compare themand we have higher belief in M 1 if this ratio is higher than 1, and in M 0 otherwise.There are two important points here: One, the ratio of the two marginal likelihoods is called the Bayes factor and is enough for model selection Bayes factor even if the two priors are taken equal.Second, in the Bayesian approach, we do not choose among models and we do not do model selection; but in keeping with the spirit of the Bayesian approach, we average over their predictions rather than choosing one and discarding the rest.For instance, in the polynomial regression example above, rather than choosing one degree, it is best to take a weighted average over all degrees weighted by their marginal likelihoods.A related approach is the Bayesian information criterion (BIC) where The first term is the likelihood using the ML estimator and the second term is a penalty for complex models: |M| is a measure of model complexity, in other words, the degrees of freedom in the model-for example, the number of coefficients in a linear regression model.As model complexity increases, the first term may be higher but the second penalty term compensates for this.A related, but not Bayesian, approach is Akaike's information criterionAkaike's information criterion (AIC), which is written as (16.36)where again we see a penalty term that is proportional to the model complexity.It is important to note here that in such criteria, |M| represents the "effective" degrees of freedom and not simply the number of adjustable parameters in the model.For example in a multilayer perceptron (chapter 11), the effective degrees of freedom is much less than the number of adjustable connection weights.One interpretation of the penalty term is as a term of "optimism" (Hastie, Tibshirani, and Friedman 2011).In a complex model, the ML estimator would overfit and hence be a very optimistic indicator of model performance; therefore, it should be cut back proportional to the model complexity.In section 7.2, we discuss the mixture model where we write the density as a weighted sum of component densities.Let us remember equation 7.1:where P (G i ) are the mixture proportions and p(x|G i ) are the component densities.For example, in Gaussian mixtures, we have p(x|G i ) ∼ N (μ i , Σ i ), and defining π i ≡ P (G i ), we have the parameter vector Φ = {π i , μ i , Σ i } k i=1 that we need to learn from data X = {x t } N t=1 .In section 7.4, we discussed the EM algorithm that is a maximum likelihood procedure:If we have a prior distribution p(Φ), we can devise a Bayesian approach.For example, the MAP estimator isLet us now write down the prior.Π i are multinomial variables and for them, we can use a Dirichlet prior as we discuss in section 16.2.1.For the Gaussian components, for the mean and precision (inverse covariance) matrix, we can use a normal-Wishart prior as we discuss in section 16.3:So in using EM in this case, the E-step does not change, but in the Mstep we maximize the posterior with this prior .Adding log of the posterior, equation 7.10 becomeswhere h t i ≡ E[z t i ] are the soft labels estimated in the E-step using the current values of Φ.The M-step MAP estimate for the mixture proportions are as follows (based on equation 16.4):where N i = i h t i .The M-step MAP estimates for the Gaussian component density parameters are as follows (based on equation 16.16): T is the within-scatter matrix for component i, and T is the between-scatter of component i around the prior mean.If we take α i = 1/K, this is a uniform prior.We can take κ 0 = 0 not to bias the mean estimates unless we do have some prior information about them.We can take V 0 as the identity matrix and hence the MAP estimate has a regularizing effect.The mixture density is shown as a generative graphical model in figure 16.9.Once we know how to do basic blocks in a Bayesian manner, we can combine them to get more complicated models.For example, combining the mixture model we have here and the linear regression model we discuss in section 16.4.1,we can write the Bayesian version of the mixture of experts model (section 12.8) where we cluster the data into components and learn a separate linear regression model in each component at the same time.The posterior turns out to be rather nasty and Waterhouse et al. 1996 use variational approximation, which, roughly speaking, works as follows.We remember that in Laplace approximation, we approximate p(θ|X) by a Gaussian and integrate over the Gaussian instead.In variational variational approximation approximation, we approximate the posterior by a density q(Z|ψ) whose parameters ψ are adjustable ).Hence, it is more general because we are not restricted to use a Gaussian density.Here, Z contains all the latent variables in the model and the parameters θ, and ψ of the approximating model q(Z|ψ) are adjusted such that q(Z|ψ) is as close as possible to p(Z|X).We define as the Kullback-Leibler distance between the two:To make life easier, the set of latent variables (including the parameters) is assumed to be partitioned into subsets Z i , i = 1, . . ., k, such that the variational distribution can be factorized:Adjustment of the parameters ψ i in each factor is iterative, rather like the expectation-maximization algorithm we discussed in section 7.4.We start from (possibly random) initial values and while adjusting each, we use the expected values of the Z j , j = i in a circular manner.This is called the mean-field approximation.This factorization is an approximation.For example, in section 16.4.2when we discuss regression, we writebecause w is conditioned on β.A variational approximation would assumeFor example, in the mixture of experts model, the latent parameters are the component indices and the parameters are the parameters in the gating model, the regression weights in the local experts, the variance of the noise, and the hyperparameters of the priors for gating and regression weights; they are all factors (Waterhouse, MacKay, and Robinson 1996).The models we discuss earlier in this chapter are all parametric, in the sense that we have models of fixed complexity with a set of parameters and these parameters are optimized using the data and the prior information.In chapter 8, we discussed nonparametric models where the training data makes up the model and hence model complexity depends on the size of the data.Now we address how such a nonparametric approach can be used in the Bayesian setting.A nonparametric model does not mean that the model has no parameters; it means that the number of parameters is not fixed and that their number can grow depending on the size of the data, or better still, depending on the complexity of the regularity that underlies the data.Such models are also sometimes called infinite models, in the sense that their complexity can keep on increasing with more data.In section 11.9, we discuss incremental neural network models where new hidden units are added when necessary and network is grown during training, but usually in parametric learning, adjusting model complexity is handled in an outer loop by checking performance on a separate validation set.The nonparametric Bayesian approach includes model adjustment in parameter training by using a suitable prior (Gershman and Blei 2012).This makes such models more flexible, and would have normally made them prone to overfitting if not for the Bayesian approach that alleviates this risk.Because it is the parameters that grow, the priors on such parameters should be able to handle that growth and we will discuss three example prior distributions for three different type of machine learning applications, namely, Gaussian processes for supervised learning, Dirichlet processes for clustering, and beta processes for dimensionality reduction.Let us say we have the linear model y = w T x.Then, for each w, we have one line.Given a prior distribution p(w), we get a distribution of lines, or to be more specific, for any w, we get a distribution of y values calculated at x as y(x|w) when w is sampled from p(w), and this is what we mean when we talk about a Gaussian process.We know that if p(w) isGaussian, each y is a linear combination of Gaussians and is also Gaussian; in particular, we are interested in the joint distribution of y values calculated at the N input data points, x t , t = 1, . . ., N (MacKay 1998).We assume a zero-mean Gaussian prior p(w) ∼ N (0, (1/α)I)Given the N × d data points X and the d × 1 weight vector, we write the y outputs as y = Xw (16.44) which is N-variate Gaussian withwhere K is the (Gram) matrix with elementsThis is known as the covariance function in the literature of Gaussian covariance functionprocesses and the idea is the same as in kernel functions: If we use a set of basis functions φ(x), we generalize from the dot product of the original inputs to the dot product of basis functions by a kernelThe actual observed output r is given by the line with added noise, r = y + where ∼ N (0, β −1 ).For all N data points, we write it asTo make a prediction, we consider the new data as the (N + 1)st data point pair (x , r ), and write the joint using all N + 1 data points.We havewherewith k being the N × 1 dimensional vector of K(x , x t ), t = 1, . . ., N and c = K(x , x ) + β −1 .Then to make a prediction, we calculate p(r |x , X, r), which is Gaussian with An example shown in figure 16.10 uses linear, quadratic, and Gaussian kernels.The first two are defined as the dot product of their corresponding basis functions; the Gaussian kernel is defined directly asThe mean, which is our point estimate (if we do not integrate over the full distribution), can also be written as a weighted sum of the kernel effects where w t is the tth component of k T C −1 N .Note that we can also calculate the variance of a prediction at a point to get an idea about uncertainty in there, and it depends on the instances that affect the prediction in there.In the case of a Gaussian kernel, only instances within a locality are effective and prediction variance is high where there is little data in the vicinity (see figure 16.11).Kernel functions can be defined and used for any application, as we have previously discussed in the context of kernel machines in chapter 13.The possibility of using kernel functions directly without needing to calculate or store the basis functions offers a great flexibility.Normally, given a training set, we first calculate the parameters, for example using equation 16.21, and then use the parameters to make predictions using equation 16.22, never needing the training set any more.This makes sense because generally the dimensionality of the parameters, which is generally O(d), is much lower than the size of the training set N.When we work with basis functions, however, calculating the parameter explicitly may no longer be the case, because the dimensionality of the basis functions may be very high, even infinite.In such a case, it is cheaper to use the dual representation, taking into account the effects of training instances using kernel functions, as we do here.This idea is also used in nonparametric smoothers (chapter 8) and kernel machines (chapter 13).The requirement here is that C N be invertible and hence positive definite.For this, K should be semidefinite so that after adding β −1 > 0 to the diagonals, we get positive definiteness.We also see that the costliest operation is this inversion of the N × N matrix, which fortunately needs to be calculated only once (during training) and stored.Still, for large N, one may need an approximation.When we use it for classification for a two-class problem, the output is filtered through a sigmoid, y = sigmoid(w T x), and the distribution of y is no longer Gaussian.The derivation is similar except that the conditional p(r N+1 |x N+1 , X, r) is not Gaussian either and we need to approximate, for example, using Laplace approximation Rasmussen and Williams 2006).To explain a Dirichlet process, let us start with a metaphor: There is a Chinese restaurant with a lot of tables.Customers enter one by one; we start with the first customer who sits at the first table, and any subsequent customer can either sit at one of the occupied tables or go and start a new table.The probability that a customer sits at an occupied table is proportional to the number of customers already sitting at the table, and the probability that he or she sits at a new table depends on a parameter α.This is called a Chinese restaurant process:where n i is the number of customers already starting at table i, n = k i=1 n i is the total number of customers.α is the propensity to start a new table and is the parameter of the process.Note that at each step, the sitting arrangement of customers define a partition of integers 1 to n into k subsets.This is called a Dirichlet process with parameter α.We can apply this to clustering by making customer choices not only dependent on the table occupancies but also on the input.Let us say that this is not a Chinese restaurant but the dinner of a large conference, for example, NIPS.There is a large dining lounge with many tables, and in the evening, the conference participants enter the lounge one by one.They want to eat, but they also want to participate in interesting conversation.For that, they want to sit at a table where there are already many people sitting, but they also want to sit next to people having similar research interests.If they see no such table, they start a new table and expect incoming similar participants to find and join them.Assume that instance/participant t is represented by a d-dimensional vector x t , and let us assume that such x t are locally Gaussian distributed.This defines a Gaussian mixture over the whole space/dining lounge, and to have it Bayesian, we define priors on the parameters of the Gaussian components, as we discuss in section 16.7.To make it nonparametric, we define a Dirichlet process as the prior so a new component can be added when necessary, as follows:X i is the set of instances previously assigned to component i; using their data and the priors, we can calculate a posterior and integrating over it, we can calculate p(x t |X i ).Roughly speaking, the probability this new instance is assigned to component i will be high if there are already many instances in the component, that is, due to a high prior, or if x t is similar to the instances already in X i .If none of the existing components have a high probability, a new component is added: p(x t ) is the marginal probability (integrated over the component parameter priors because there is no data).Different α may lead to different numbers of clusters.To adjust α, we can use empirical Bayes, or also define a prior on it and average it out.In chapter 7, when we talk about k-means clustering (section 7.3), we discuss leader-cluster algorithms where new clusters are added during training and as an example of that, in section 12.2.2,we discuss adaptive resonance theory where we add a new cluster if the distance to the center of the nearest cluster is more than a vigilance value.What we have here is very similar: Assuming Gaussian components and diagonal covariance matrices, if the Euclidean distance to all clusters is high, all posteriors will be small and a new component will be added.Let us see an application of the Bayesian approach in text processing, namely topic modeling (Blei 2012).In this age, there are digital repositopic modeling tories containing a very large number of documents-scientific articles, web pages, emails, blog posts, and so on-but finding a relevant topic for a query is very difficult, unless documents are manually annotated with topics, such as "arts," "sports," and so on.What we would like to is do this annotation automatically.Assume we have a vocabulary with M words.Each document contains N words chosen from a number of topics in different proportions-that is, each document is a probability distribution over topics.A document may be partially "arts" and partially "politics," for example.Each topic in turn is defined as a mixture of the M words-that is, each topic corresponds to a probability distribution over words.For example, for the topic arts, the words "painting" and "sculpture" have a high probability, but the word "knee" has a low probability.In latent Dirichlet allocation, we define a generative process as follows latent Dirichlet allocation (figure 16.12)-there are K topics, a vocabulary of M words, and all documents contain N words (Blei, Ng, and Jordan 2003):To generate each document d, we first decide on the topics it will be about.These topic probabilities, π d k , k = 1, . . ., K, define a multinomial distribution and are drawn from a Dirichlet prior with hyperparameter α (section 16.2.1):Once we know the topic distribution for document d, we generate its N words using it.In generating word i, first we decide on its particular topic by sampling from π: We roll a die with K faces where face k has probability π k .We define z d i as the outcome, it will be a value between 1 and K:Now we know that in document d, the ith word will be about topic z d i ∈ {1, . . ., K}.We have a K × M matrix of probabilities W whose row k, w k ≡ [w k1 , . . ., w KM ] T gives us the probabilities of occurrences of the M words in topic k.So knowing that the topic for word i needs to come from topic z d i , we will sample from the multinomial distribution whose parameters are given by row z d i of W to get the word x d i (which is a value between 1 and M):This is a multinomial draw, and we define a Dirichlet prior with hyperparameter β on these rows of multinomial probabilities:This completes the process to generate one word.To generate the N words for the document, we do this N times; namely, for each word, we decide on a topic, then given the topic, we choose a word (inner plate in the figure).When we get to the next document, we sample another topic distribution π (outer plate), and then sample N words from that topic distribution.On all the documents, we always use the same W, and in learning, we are given a large corpus of documents, that is, only x d i values are observed.We can then write a posterior distribution as usual and learn W, the word probabilities for topics shared across all documents.Once W is learned, each of its rows correspond to one topic.By looking at the words with high probabilities, we can assign some meaning to these topics.Note, however, that we will always learn some W; whether the rows will be meaningful or not is another matter.The model we have just discussed is parametric, and its size is fixed; we can make it nonparametric by making K, the number of topics, which is the hidden complexity parameter, increase as necessary and adapt to data using a Dirichlet process.We need to be careful though.Each document contains N words that come from some topics, but we have several documents and they all need to share the same set of topics; that is, we need to tie the Dirichlets that generate the topics.For this, we define a hierarchy; we define a higher Dirichlet process from which we draw the Dirichlets for individual documents.This is a hierarchical Dirichlet prohierarchical Dirichlet process cess (Teh et al. 2006) that allows topics learned for one document be shared by all.Now let us see an application of the Bayesian approach to dimensionality reduction in factor analysis.Remember that given the N × d matrix of data X, we want to find k features or latent factors, each of which are d-dimensional such that the data can be written as a linear combination of them.That is, we want to find Z and A such thatwhere A is the k × d matrix whose row j is the d-dimensional feature vector (similar to the eigenvector in PCA (section 6.3) and Z is N × k matrix whose row t defines instance t as a vector of features.Let us assume that z t j are binary and are drawn from Bernoulli distributions with probability μ j :1 with probability μ j 0 with probability 1 − μ j (16.50)So z t j indicates the absence/presence of hidden factor j in constructing instance t.If the corresponding factor is present, row j of A is chosen and the sum of all such rows chosen make up row t of X.We are being Bayesian so we define priors.We define a Gaussian prior on A and a beta conjugate prior on μ j of Bernoulli z t j :where α is the hyperparameter.We can write down the posterior and estimate the matrix A. Looking at the rows of A, we can get an idea about what the hidden factors represent; for example, if k is small (e.g., 2), we can plot and visualize the data.We assume a certain k; hence this model is parametric.We can make it nonparametric and allow k increase with more data (Griffiths and Ghahramani 2011).This defines a beta process and the corresponding metaphor beta process is called the Indian buffet process, which defines a generative model thatThere is an Indian restaurant with a buffet that contains k dishes and each customer can take a serving of any subset of these dishes.The first customer (instance) enters and takes servings of the first m dishes; we assume m is a random variable generated from a Poisson distribution with parameter α.Then each subsequent customer n can take a serving of any existing dish j with probability n j /n where n j is the number of customers before who took a serving of dish j, and once he or she is done sampling the existing dishes, that customer can also ask for Poisson(α/n) additional new dishes, hence growing the model.When applied to the context of latent factor model earlier, this corresponds to a model where the number of factors need not be fixed and instead grows as the complexity inherent in data grows.Bayesian approaches are becoming more popular recently.The use of generative graphical models corresponds quite well to the Bayesian formalism, and we are seeing interesting applications in various domains from natural language processing to computer vision to bioinformatics.The recent field of Bayesian nonparametrics is also interesting in that adapting model complexity is now a part of training and is not an outer loop of model complexity adjustment; we expect to see more work along this direction in the near future.One example of this is the infinite hidden Markov models  where the number of hidden states is automatically adjusted with more data.Due to lack of space and the need to keep the chapter to a reasonable length, the approximation and sampling methods are not discussed in detail in this chapter; see  for more information about variational methods and Markov chain Monte Carlo sampling.Bayesian approach is interesting and promising, and has already worked successfully in many cases, but it is far from completely supplanting the nonBayesian, or frequentist, approach.For tractability, generative models may be quite simple-for example, latent Dirichlet analysis loses the ordering or words-or the approximation methods may be hard to derive, and sampling methods slow to converge; hence frequentist shortcuts, (e.g., empirical Bayes), may be preferred in certain cases.Hence, it is best to look for an ideal compromise between the two worlds rather than fully committing to one.1.For the setting of figure 16.3, observe how the posterior changes as we change N, σ 2 , and σ 2 0 .2. Let us denote by x the number of spam emails I receive in a random sample of n.Assume that the prior for q, the proportion of spam emails is uniform in [0, 1].Find the posterior distribution for p(q|x).3. As above, except that assume that p(q) ∼ N (μ 0 , σ 2 0 ).Also assume n is large so that you can use central limit theorem and approximate binomial by a Gaussian.Derive p(q|x).4. What is Var(r ) when the maximum likelihood estimator is used?Compare it with equation 16.25.5. In figure 16.10, how does the fit change when we change s 2 ?SOLUTION: As usual, s is the smoothing parameter and we get smoother fits as we increase s.6. Propose a filtering algorithm to choose a subset of the training set in Gaussian processes.SOLUTION: One nice property of Gaussian processes is that we can calculate the variance at a certain point.For any instance from the training set, we can calculate the leave-one-out estimate there and check whether the actual output is in, for example, the 95 percent prediction interval.If it is, this means that we do not need that instance and it can be left out.Those that cannot be pruned will be just like the support vectors in a kernel machine, namely, those instances that are stored and needed, to bound the total error of the fit.7. Active learning is when the learner is able to generate x itself and ask a suactive learning pervisor to provide the corresponding r value during learning one by one, instead of passively being given a training set.How can we implement active learning using Gaussian processes?(Hint: Where do we have the largest uncertainty?)SOLUTION: This is just like the previous exercise, except that we add instead of prune.Using the same logic, we can see that we need instances where the prediction interval is large.Given the variance as a function of x, we search for its local maxima.In the case of a Gaussian kernel, we expect points that are distant from training data to have high variance, but this need not be the case for all kernels.While searching, we need to make sure that we do not go out of the valid input bounds.8. Let us say we have a set of documents where for each document, we have one copy in English and one in French.How can we extend latent Dirichlet allocation for this case?We discussed many different learning algorithms in the previous chapters.Though these are generally successful, no one single algorithm is always the most accurate.Now, we are going to discuss models composed of multiple learners that complement each other so that by combining them, we attain higher accuracy.In any application, we can use one of several learning algorithms, and with certain algorithms, there are hyperparameters that affect the final learner.For example, in a classification setting, we can use a parametric classifier or a multilayer perceptron, and, for example, with a multilayer perceptron, we should also decide on the number of hidden units.The No Free Lunch Theorem states that there is no single learning algorithm that in any domain always induces the most accurate learner.The usual approach is to try many and choose the one that performs the best on a separate validation set.Each learning algorithm dictates a certain model that comes with a set of assumptions.This inductive bias leads to error if the assumptions do not hold for the data.Learning is an ill-posed problem and with finite data, each algorithm converges to a different solution and fails under different circumstances.The performance of a learner may be fine-tuned to get the highest possible accuracy on a validation set, but this finetuning is a complex task and still there are instances on which even the best learner is not accurate enough.The idea is that there may be another learner that is accurate on these.By suitably combining multiple basebase-learner learners then, accuracy can be improved.Recently with computation and memory getting cheaper, such systems composed of multiple learners have become popular (Kuncheva 2004).There are basically two questions here:1. How do we generate base-learners that complement each other?2. How do we combine the outputs of base-learners for maximum accuracy?Our discussion in this chapter will answer these two related questions.We will see that model combination is not a trick that always increases accuracy; model combination does always increase time and space complexity of training and testing, and unless base-learners are trained carefully and their decisions combined smartly, we will only pay for this extra complexity without any significant gain in accuracy.Since there is no point in combining learners that always make similar decisions, the aim is to be able to find a set of diverse learners who differ diversity in their decisions so that they complement each other.At the same time, there cannot be a gain in overall success unless the learners are accurate, at least in their domain of expertise.We therefore have this double task of maximizing individual accuracies and the diversity between learners.Let us now discuss the different ways to achieve this.We can use different learning algorithms to train different base-learners.Different algorithms make different assumptions about the data and lead to different classifiers.For example, one base-learner may be parametric and another may be nonparametric.When we decide on a single algorithm, we give emphasis to a single method and ignore all others.Combining multiple learners based on multiple algorithms, we free ourselves from taking a decision and we no longer put all our eggs in one basket.We can use the same learning algorithm but use it with different hyperparameters.Examples are the number of hidden units in a multilayer perceptron, k in k-nearest neighbor, error threshold in decision trees, the kernel function in support vector machines, and so forth.With a Gaussian parametric classifier, whether the covariance matrices are shared or not is a hyperparameter.If the optimization algorithm uses an iterative procedure such as gradient descent whose final state depends on the initial state, such as in backpropagation with multilayer perceptrons, the initial state, for example, the initial weights, is another hyperparameter.When we train multiple base-learners with different hyperparameter values, we average over this factor and reduce variance, and therefore error.Separate base-learners may be using different representations of the same input object or event, making it possible to integrate different types of sensors/measurements/modalities.Different representations make different characteristics explicit allowing better identification.In many applications, there are multiple sources of information, and it is desirable to use all of these data to extract more information and achieve higher accuracy in prediction.For example, in speech recognition, to recognize the uttered words, in addition to the acoustic input, we can also use the video image of the speaker's lips and shape of the mouth as the words are spoken.This is similar to sensor fusion where the data from different sensors are intesensor fusion grated to extract more information for a specific application.Another example is information, for example, image retrieval where in addition to the image itself, we may also have text annotation in the form of keywords.In such a case, we want to be able to combine both of these sources to find the right set of images; this is also sometimes called multi-view learning.The simplest approach is to concatenate all data vectors and treat it as one large vector from a single source, but this does not seem theoretically appropriate since this corresponds to modeling data as sampled from one multivariate statistical distribution.Moreover, larger input dimensionalities make the systems more complex and require larger samples for the estimators to be accurate.The approach we take is to make separate predictions based on different sources using separate base-learners, then combine their predictions.Even if there is a single input representation, by choosing random subsets from it, we can have classifiers using different input features; this is called the random subspace method (Ho 1998).This has the effect that random subspace different learners will look at the same problem from different points of view and will be robust; it will also help reduce the curse of dimensionality because inputs are fewer dimensional.Another possibility is to train different base-learners by different subsets of the training set.This can be done randomly by drawing random training sets from the given sample; this is called bagging.Or, the learners can be trained serially so that instances on which the preceding base-learners are not accurate are given more emphasis in training later base-learners; examples are boosting and cascading, which actively try to generate complementary learners, instead of leaving this to chance.The partitioning of the training sample can also be done based on locality in the input space so that each base-learner is trained on instances in a certain local part of the input space; this is what is done by the mixture of experts that we discussed in chapter 12 but that we revisit in this context of combining multiple learners.Similarly, it is possible to define the main task in terms of a number of subtasks to be implemented by the base-learners, as is done by error-correcting output codes.One important note is that when we generate multiple base-learners, we want them to be reasonably accurate but do not require them to be very accurate individually, so they are not, and need not be, optimized separately for best accuracy.The base-learners are not chosen for their accuracy, but for their simplicity.We do require, however, that the baselearners be diverse, that is, accurate on different instances, specializing in subdomains of the problem.What we care for is the final accuracy when the base-learners are combined, rather than the accuracies of the base-learners we started from.Let us say we have a classifier that is 80 percent accurate.When we decide on a second classifier, we do not care for the overall accuracy; we care only about how accurate it is on the 20 percent that the first classifier misclassifies, as long as we know when to use which one.This implies that the required accuracy and diversity of the learners also depend on how their decisions are to be combined, as we will dis-cuss next.If, as in a voting scheme, a learner is consulted for all inputs, it should be accurate everywhere and diversity should be enforced everywhere; if we have a partioning of the input space into regions of expertise for different learners, diversity is already guaranteed by this partitioning and learners need to be accurate only in their own local domains.There are also different ways the multiple base-learners are combined to generate the final output:Multiexpert combination methods have base-learners that work in parmultiexpert combination allel.These methods can in turn be divided into two:In the global approach, also called learner fusion, given an input, all base-learners generate an output and all these outputs are used.Examples are voting and stacking.In the local approach, or learner selection, for example, in mixture of experts, there is a gating model, which looks at the input and chooses one (or very few) of the learners as responsible for generating the output.Multistage combination methods use a serial approach where the next multistage combination base-learner is trained with or tested on only the instances where the previous base-learners are not accurate enough.The idea is that the base-learners (or the different representations they use) are sorted in increasing complexity so that a complex base-learner is not used (or its complex representation is not extracted) unless the preceding simpler base-learners are not confident.An example is cascading.Let us say that we have L base-learners.We denote by d j (x) the prediction of base-learner M j given the arbitrary dimensional input x.In the case of multiple representations, each M j uses a different input representation x j .The final prediction is calculated from the predictions of the base-learners:where f (•) is the combining function with Φ denoting its parameters.When there are K outputs, for each learner there are d ji (x), i = 1, . . ., K, j = 1, . . ., L, and, combining them, we also generate K values,.1 Base-learners are d j and their outputs are combined using f (•).This is for a single output; in the case of classification, each base-learner has K outputs that are separately used to calculate y i , and then we choose the maximum.Note that here all learners observe the same input; it may be the case that different learners observe different representations of the same input object or event.1, . . ., K and then for example in classification, we choose the class with the maximum y i value:The simplest way to combine multiple classifiers is by voting, which corvoting responds to taking a linear combination of the learners (see figure 17.1):This is also known as ensembles and linear opinion pools.In the simensembles linear opinion pools plest case, all learners are given equal weight and we have simple voting   (Kittler et al. 1998).If the outputs are not posterior probabilities, these rules require that outputs be normalized to the same scale (Jain, Nandakumar, and Ross 2005).An example of the use of these rules is shown in table 17.2, which demonstrates the effects of different rules.Sum rule is the most intuitive and is the most widely used in practice.Median rule is more robust to outliers; minimum and maximum rules are pessimistic and optimistic, respectively.With the product rule, each learner has veto power; regardless of the other ones, if one learner has an output of 0, the overall output goes to 0. Note that after the combination rules, y i do not necessarily sum up to 1.In weighted sum, d ji is the vote of learner j for class C i and w j is the weight of its vote.Simple voting is a special case where all voters have equal weight, namely, w j = 1/L.In classification, this is called plurality voting where the class having the maximum number of votes is the winner.When there are two classes, this is majority voting where the winning class gets more than half of the votes (exercise 1).If the voters can also supply the additional information of how much they vote for each class (e.g., by the posterior probability), then after normalization, these can be used as weights in a weighted voting scheme.Equivalently, if d ji are the class posterior probabilities, P (C i |x, M j ), then we can just sum them up (w j = 1/L) and choose the class with maximum y i .In the case of regression, simple or weighted averaging or median can be used to fuse the outputs of base-regressors.Median is more robust to noise than the average.Another possible way to find w j is to assess the accuracies of the learners (regressor or classifier) on a separate validation set and use that information to compute the weights, so that we give more weights to more accurate learners.These weights can also be learned from data, as we will discuss when we discuss stacked generalization in section 17.9.Voting schemes can be seen as approximations under a Bayesian framework with weights approximating prior model probabilities, and model decisions approximating model-conditional likelihoods.This is Bayesian Bayesian model combination model combination-see section 16.6.For example, in classification we have w j ≡ P (M j ), d ji = P (C i |x, M j ), and equation 17.2 corresponds toSimple voting corresponds to a uniform prior.If we have a prior distribution preferring simpler models, this would give larger weights to them.We cannot integrate over all models; we only choose a subset for which we believe P (M j ) is high, or we can have another Bayesian step and calculate P (M j |X), the probability of a model given the sample, and sample high probable models from this density.Hansen and Salamon (1990) have shown that given independent twoclass classifiers with success probability higher than 1/2, namely, better than random guessing, by taking a majority vote, the accuracy increases as the number of voting classifiers increases.Let us assume that d j are iid with expected value E[d j ] and variance Var(d j ), then when we take a simple average with w j = 1/L, the expected value and variance of the output areWe see that the expected value does not change, so the bias does not change.But variance, and therefore mean square error, decreases as the number of independent voters, L, increases.In the general case,which implies that if learners are positively correlated, variance (and error) increase.We can thus view using different algorithms and input features as efforts to decrease, if not completely eliminate, the positive correlation.In section 17.10, we discuss pruning methods to remove learners with high positive correlation fron an ensemble.We also see here that further decrease in variance is possible if the voters are not independent but negatively correlated.The error then decreases if the accompanying increase in bias is not higher because these aims are contradictory; we cannot have a number of classifiers that are all accurate and negatively correlated.In mixture of experts for example, where learners are localized, the experts are negatively correlated but biased (Jacobs 1997).If we view each base-learner as a random noise function added to the true discriminant/regression function and if these noise functions are uncorrelated with 0 mean, then the averaging of the individual estimates is like averaging over the noise.In this sense, voting has the effect of smoothing in the functional space and can be thought of as a regularizer with a smoothness assumption on the true function (Perrone 1993).We saw an example of this in figure 4.5d, where, averaging over models with large variance, we get a better fit than those of the individual models.This is the idea in voting: We vote over models with high variance and low bias so that after combination, the bias remains small and we reduce the variance by averaging.Even if the individual models are biased, the decrease in variance may offset this bias and still a decrease in error is possible.In error-correcting output codes (ECOC) (Dietterich and Bakiri 1995), the error-correcting output codes main classification task is defined in terms of a number of subtasks that are implemented by the base-learners.The idea is that the original task of separating one class from all other classes may be a difficult problem.Instead, we want to define a set of simpler classification problems, each specializing in one aspect of the task, and combining these simpler classifiers, we get the final classifier.Base-learners are binary classifiers having output −1/ + 1, and there is a code matrix W of K × L whose K rows are the binary codes of classes in terms of the L base-learners d j .For example, if the second row of W is [−1, +1, +1, −1], this means that for us to say an instance belongs to C 2 , the instance should be on the negative side of d 1 and d 4 , and on the positive side of d 2 and d 3 .Similarly, the columns of the code matrix defines the task of the base-learners.For example, if the third column is [−1, +1, +1] T , we understand that the task of the third base-learner, d 3 , is to separate the instances of C 1 from the instances of C 2 and C 3 combined.This is how we form the training set of the base-learners.For example in this case, all instances labeled with C 2 and C 3 form X + 3 and instances labeled with C 1 form X − 3 , and d 3 is trained so that x t ∈ X + 3 give output +1 and x t ∈ X − 3 give output −1.The code matrix thus allows us to define a polychotomy (K > 2 classification problem) in terms of dichotomies (K = 2 classification problem), and it is a method that is applicable using any learning algorithm to implement the dichotomizer base-learners-for example, linear or multilayer perceptrons (with a single output), decision trees, or SVMs whose original definition is for two-class problems.The typical one discriminant per class setting corresponds to the diagonal code matrix where L = K.For example, for K = 4, we haveThe problem here is that if there is an error with one of the baselearners, there may be a misclassification because the class code words are so similar.So the approach in error-correcting codes is to have L > K and increase the Hamming distance between the code words.One pos-sibility is pairwise separation of classes where there is a separate baselearner to separate C i from C j , for i < j (section 10.4).In this case, L = K(K − 1)/2 and with K = 4, the code matrix iswhere a 0 entry denotes "don't care."That is, d 1 is trained to separate C 1 from C 2 and does not use the training instances belonging to the other classes.Similarly, we say that an instance belongs to C 2 if d 1 = −1 and d 4 = d 5 = +1, and we do not consider the values of d 2 , d 3 , and d 6 .The problem here is that L is O(K 2 ), and for large K pairwise separation may not be feasible.If we can have L high, we can just randomly generate the code matrix with −1/ + 1 and this will work fine, but if we want to keep L low, we need to optimize W. The approach is to set L beforehand and then find W such that the distances between rows, and at the same time the distances between columns, are as large as possible, in terms of Hamming distance.With K classes, there are 2 (K−1) − 1 possible columns, namely, two-class problems.This is because K bits can be written in 2 K different ways and complements (e.g., "0101" and "1010," from our point of view, define the same discriminant) dividing the possible combinations by 2 and then subtracting 1 because a column of all 0s (or 1s) is useless.For example, when K = 4, we haveWhen K is large, for a given value of L, we look for L columns out of the 2 (K−1) −1.We would like these columns of W to be as different as possible so that the tasks to be learned by the base-learners are as different from each other as possible.At the same time, we would like the rows of W to be as different as possible so that we can have maximum error correction in case one or more base-learners fail.ECOC can be written as a voting scheme where the entries of W, w ij , are considered as vote weights:w ij d j (17.6) and then we choose the class with the highest y i .Taking a weighted sum and then choosing the maximum instead of checking for an exact match allows d j to no longer need to be binary but to take a value between −1 and +1, carrying soft certainties instead of hard decisions.Note that a value p j between 0 and 1, for example, a posterior probability, can be converted to a value d j between −1 and +1 simply asThe difference between equation 17.6 and the generic voting model of equation 17.2 is that the weights of votes can be different for different classes, namely, we no longer have w j but w ij , and also that w j ≥ 0 whereas w ij are −1, 0, or +1.One problem with ECOC is that because the code matrix W is set a priori, there is no guarantee that the subtasks as defined by the columns of W will be simple.Dietterich and Bakiri (1995) report that the dichotomizer trees may be larger than the polychotomizer trees and when multilayer perceptrons are used, there may be slower convergence by backpropagation.Bagging is a voting method whereby base-learners are made different by bagging training them over slightly different training sets.Generating L slightly different samples from a given sample is done by bootstrap, where given a training set X of size N, we draw N instances randomly from X with replacement.Because sampling is done with replacement, it is possible that some instances are drawn more than once and that certain instances are not drawn at all.When this is done to generate L samples X j , j = 1, . . ., L, these samples are similar because they are all drawn from the same original sample, but they are also slightly different due to chance.The base-learners d j are trained with these L samples X j .A learning algorithm is an unstable algorithm if small changes in the unstable algorithm training set causes a large difference in the generated learner, namely, the learning algorithm has high variance.Bagging, short for bootstrap aggregating, uses bootstrap to generate L training sets, trains L base-learners using an unstable learning procedure, and then, during testing, takes an average (Breiman 1996).Bagging can be used both for classification and regression.In the case of regression, to be more robust, one can take the median instead of the average when combining predictions.We saw before that averaging reduces variance only if the positive correlation is small; an algorithm is stable if different runs of the same algorithm on resampled versions of the same dataset lead to learners with high positive correlation.Algorithms such as decision trees and multilayer perceptrons are unstable.Nearest neighbor is stable, but condensed nearest neighbor is unstable (Alpaydın 1997).If the original training set is large, then we may want to generate smaller sets of size N < N from them using bootstrap, since otherwise the bootstrap replicates X j will be too similar, and d j will be highly correlated.In bagging, generating complementary base-learners is left to chance and to the unstability of the learning method.In boosting, we actively try to generate complementary base-learners by training the next learner on the mistakes of the previous learners.The original boosting algoboosting rithm (Schapire 1990) combines three weak learners to generate a strong learner.A weak learner has error probability less than 1/2, which makes weak learner it better than random guessing on a two-class problem, and a strong strong learner learner has arbitrarily small error probability.Given a large training set, we randomly divide it into three.We use X 1 and train d 1 .We then take X 2 and feed it to d 1 .We take all instances misclassified by d 1 and also as many instances on which d 1 is correct from X 2 , and these together form the training set of d 2 .We then take X 3 and feed it to d 1 and d 2 .The instances on which d 1 and d 2 disagree form the training set of d 3 .During testing, given an instance, we give it to d 1 and d 2 ; if they agree, that is the response, otherwise the response of d 3 is taken as the output.Schapire (1990) has shown that this overall system has reduced error rate, and the error rate can arbitrarily be reduced by using such systems recursively, that is, a boosting system of three models used as d j in a higher system.Though it is quite successful, the disadvantage of the original boost-Training: For all {x t , r t } N t=1 ∈ X, initialize p t 1 = 1/N For all base-learners j = 1, . . ., L Randomly draw X j from X with probabilities p t j Train d j using X j For each (x t , r t ), calculate y t j ← d j (x t ) Calculate error rate: j ← t p t j • 1(y t j = r t )  ing method is that it requires a very large training sample.The sample should be divided into three and furthermore, the second and third classifiers are only trained on a subset on which the previous ones err.So unless one has a quite large training set, d 2 and d 3 will not have training sets of reasonable size.Drucker et al. (1994) use a set of 118,000 instances in boosting multilayer perceptrons for optical handwritten digit recognition.Freund and Schapire (1996) proposed a variant, named AdaBoost, short AdaBoost for adaptive boosting, that uses the same training set over and over and thus need not be large, but the classifiers should be simple so that they do not overfit.AdaBoost can also combine an arbitrary number of baselearners, not three.Many variants of AdaBoost have been proposed; here, we discuss the original algorithm AdaBoost.M1 (see figure 17.2).The idea is to modify the probabilities of drawing the instances as a function of the error.Let us say p t j denotes the probability that the instance pair (x t , r t ) is drawn to train the jth base-learner.Initially, all p t 1 = 1/N.Then we add new base-learners as follows, starting from j = 1: j denotes the error rate of d j .AdaBoost requires that learners are weak, that is, j < 1/2, ∀j; if not, we stop adding new base-learners.Note that this error rate is not on the original problem but on the dataset used at step j.We define β j = j /(1 − j ) < 1, and we set p t j+1 = β j p t j if d j correctly classifies x t ; otherwise, p t j+1 = p t j .Because p t j+1 should be probabilities, there is a normalization where we divide p t j+1 by t p t j+1 , so that they sum up to 1.This has the effect that the probability of a correctly classified instance is decreased, and the probability of a misclassified instance increases.Then a new sample of the same size is drawn from the original sample according to these modified probabilities, p t j+1 , with replacement, and is used to train d j+1 .This has the effect that d j+1 focuses more on instances misclassified by d j ; that is why the base-learners are chosen to be simple and not accurate, since otherwise the next training sample would contain only a few outlier and noisy instances repeated many times over.For example, with decision trees, decision stumps, which are trees grown only one or two levels, are used.So it is clear that these would have bias but the decrease in variance is larger and the overall error decreases.An algorithm like the linear discriminant has low variance, and we cannot gain by AdaBoosting linear discriminants.Once training is done, AdaBoost is a voting method.Given an instance, all d j decide and a weighted vote is taken where weights are proportional to the base-learners' accuracies (on the training set): w j = log(1/β j ).Freund and Schapire (1996) showed improved accuracy in twenty-two benchmark problems, equal accuracy in one problem, and worse accuracy in four problems.Schapire et al. (1998) explain that the success of AdaBoost is due to its property of increasing the margin.If the margin increases, the training margin instances are better separated and an error is less likely.This makes AdaBoost's aim similar to that of support vector machines (chapter 13).In AdaBoost, although different base-learners have slightly different training sets, this difference is not left to chance as in bagging, but is a function of the error of the previous base-learner.The actual performance of boosting on a particular problem is clearly dependent on the data and the base-learner.There should be enough training data and the base-learner should be weak but not too weak, and boosting is especially susceptible to noise and outliers.AdaBoost has also been generalized to regression: One straightforward way, proposed by Avnimelech and Intrator (1997), checks for whether the prediction error is larger than a certain threshold, and if so marks it as error, then uses AdaBoost proper.In another version (Drucker 1997), probabilities are modified based on the magnitude of error, such that instances where the previous base-learner commits a large error, have a higher probability of being drawn to train the next base-learner.Weighted average, or median, is used to combine the predictions of the base-learners.In voting, the weights w j are constant over the input space.In the mixture mixture of experts of experts architecture, which we previously discussed in section 12.8) as a local method, as an extension of radial basis functions, there is a gating network whose outputs are weights of the experts.This architecture can then be viewed as a voting method where the votes depend on the input, and may be different for different inputs.The competitive learning algorithm used by the mixture of experts localizes the base-learners such that each of them becomes an expert in a different part of the input space and have its weight, w j (x), close to 1 in its region of expertise.The final output is a weighted average as in voting y = L j=1 w j (x)d j (17.7) except in this case, both the base-learners and the weights are a function of the input (see figure 17.3).Jacobs (1997) has shown that in the mixture of experts architecture, experts are biased but are negatively correlated.As training proceeds, bias decreases and expert variances increase but at the same time as experts localize in different parts of the input space, their covariances get more and more negative, which, due to equation 17.5, decreases the total variance, and thus the error.In section 12.8, we considered the case where both experts and gating are linear functions but a nonlinear method, for example, a multilayer perceptron with hidden units, can also be used for both.This may decrease the expert biases but risks increasing expert variances and overfitting.In dynamic classifier selection, similar to the gating network of mixture dynamic classifier selection of experts, there is first a system which takes a test input and estimates3 Mixture of experts is a voting method where the votes, as given by the gating system, are a function of the input.The combiner system f also includes this gating system.the competence of base-classifiers in the vicinity of the input.It then picks the most competent to generate output and that output is given as the overall output.Woods, Kegelmeyer, and Bowyer (1997) find the k nearest training points of the test input, look at the accuracies of the base classifiers on those, and choose the one that performs the best on them.Only the selected base-classifier need be evaluated for that test input.To decrease variance, at the expense of more computation, one can take a vote over a few competent base-classifiers instead of using just a single one.Note that in such a scheme, one should make sure that for any region of the input space, there is a competent base-classifier; this implies that there should be some partitioning of the learning of the input space among the base-classifiers.This is the nice property of mixture of experts, namely, the gating model that does the selection and the expert base-learners that it selects from are trained in a coupled manner.It would be straightforward to have a regression version of this dynamic learner selection algorithm (exercise 5).Stacked generalization is a technique proposed by Wolpert (1992) that exstacked generalization tends voting in that the way the output of the base-learners is combined need not be linear but is learned through a combiner system, f (•|Φ), which is another learner, whose parameters Φ are also trained (see figure 17.4):The combiner learns what the correct output is when the base-learners give a certain output combination.We cannot train the combiner function on the training data because the base-learners may be memorizing the training set; the combiner system should actually learn how the baselearners make errors.Stacking is a means of estimating and correcting for the biases of the base-learners.Therefore, the combiner should be trained on data unused in training the base-learners.If f (•|w 1 , . . ., w L ) is a linear model with constraints, w i ≥ 0, j w j = 1, the optimal weights can be found by constrained regression, but of course we do not need to enforce this; in stacking, there is no restriction on the combiner function and unlike voting, f (•) can be nonlinear.For example, it may be implemented as a multilayer perceptron with Φ its connection weights.The outputs of the base-learners d j define a new L-dimensional space in which the output discriminant/regression function is learned by the combiner function.In stacked generalization, we would like the base-learners to be as different as possible so that they will complement each other, and, for this, it is best if they are based on different learning algorithms.If we are combining classifiers that can generate continuous outputs, for example, posterior probabilities, it is better that they be the combined rather than hard decisions.When we compare a trained combiner as we have in stacking, with a fixed rule such as in voting, we see that both have their advantages: A trained rule is more flexible and may have less bias, but adds extra parameters, risks introducing variance, and needs extra time and data for training.Note also that there is no need to normalize classifier outputs before stacking.Model combination is not a magical formula that is always guaranteed to decrease error; base-learners should be diverse and accurate-that is, they should provide useful information.If a base-learner does not add to accuracy, it can be discarded; also, of the two base-learners that are highly correlated, one is not needed.Note that an inaccurate learner can worsen accuracy, for example, majority voting assumes more than half of the classifiers to be accurate for an input.Therefore, given a set of candidate base-learners, it may not be a good idea to use them as they are, and instead, we may want to do some preprocessing.We can actually think of the outputs of our base-learners as forming a feature vector for the later stage of combination, and we remember from chapter 6 that we have the same problem with features.Some may be just useless, and some may be highly correlated.Hence, we can use the same ideas of feature selection and extraction here too.Our first approach is to select a subset from the set of base-learners, keeping some and discarding the rest, and the second approach is to define few, new, uncorrelated metalearners from the original base-learners.Choosing a subset from an ensemble of base-learners is similar to input feature selection, and the possible approaches for ensemble selection are ensemble selection the same.We can have a forward/incremental/growing approach where at each iteration, from a set of candidate base-learners, we add to the ensemble the one that most improves accuracy, we can have a backward/decremental/pruning approach where at each iteration, we remove the base-learner whose absence leads to highest improvement, or we can have a floating approach where both additions and removals are allowed.The combination scheme can be a fixed rule, such as voting, or it can be a trained stacker.Such a selection scheme would not include inaccurate learners, ones that are not diverse enough or are correlated (Caruana et al. 2004;Ruta and Gabrys 2005).So discarding the useless also decreases the overall complexity.Different learners may be using different representations, and such an approach also allows choosing the best complementary representations (Demir and Alpaydın 2005).Note that if we use a decision tree as the combiner, it acts both as a selector and a combiner (Ulaş et al. 2009).No matter how we vary the learning algorithms, hyperparameters, resampled folds, or input features, we get positively correlated classifiers (Ulaş, Yıldız, and Alpaydın 2012), and postprocessing is needed to remove this correlation that may be harmful.One possibility is to discard some of the correlated ones, as we discussed earlier; another is to apply a feature extraction method where from the space of the outputs of base-learners, we go to a new, lower-dimensional space where we define uncorrelated metalearners that will also be fewer in number.Merz (1999) proposes the SCANN algorithm that uses correspondence analysis-a variant of principal components analysis (section 6.3)-on the crisp outputs of base classifiers and combines them using the nearest mean classifier.Actually, any linear or nonlinear feature extraction method we discussed in chapter 6 can be used and its (preferrably continuous) output can be fed to any learner, as we do in stacking.Let us say we have L learners each having K outputs.Then, for example, using principal component analysis, we can map from the K • Ldimensional space to a new space of lower-dimensional, uncorrelated space of "eigenlearners" (Ulaş, Yıldız, and Alpaydın 2012).We can then train the combiner in this new space (using a separate dataset unused to train the base-learners and the dimensionality reducer).Actually, by looking at the coefficients of the eigenvectors, we can also understand the contribution of the base-learners and assess their utility.It has been shown by Jacobs (1995) that L dependent learners are worth the same as L independent learners where L ≤ L, and this is exactly the idea here.Another point to note is that rather than drastically discarding or keeping a subset of the ensemble, this approach uses all the base-learners, and hence all the information, but at the expense of more computation.The idea in cascaded classifiers is to have a sequence of base-classifiers d j sorted in terms of their space or time complexity, or the cost of the representation they use, so that d j+1 is costlier than d j (Kaynak and Alpaydın 2000).Cascading is a multistage method, and we use d j only if all cascading preceding learners, d k , k < j are not confident (see figure 17.5).For this, associated with each learner is a confidence w j such that we say d j is confident of its output and can be used if w j > θ j where 1/K < θ j ≤ θ j+1 < 1 is the confidence threshold.In classification, the confidence function is set to the highest posterior: w j ≡ max i d ji ; this is the strategy used for rejections (section 3.3).We use learner d j if all the preceding learners are not confident:Starting with j = 1, given a training set, we train d j .Then we find all instances from a separate validation set on which d j is not confident, and these constitute the training set of d j+1 .Note that unlike in AdaBoost, we choose not only the misclassified instances but the ones for which the previous base-learner is not confident.This covers the misclassifications as well as the instances for which the posterior is not high enough; these are instances on the right side of the boundary but for which the distance to the discriminant, namely, the margin, is not large enough.The idea is that an early simple classifier handles the majority of instances, and a more complex classifier is used only for a small percentage, thereby not significantly increasing the overall complexity.This is contrary to the multiexpert methods like voting where all base-learners generate their output for any instance.If the problem space is complex, a few base-classifiers may be cascaded increasing the complexity at each stage.In order not to increase the number of base-classifiers, the few instances not covered by any are stored as they are and are treated by a nonparametric classifier, such as k-NN.The inductive bias of cascading is that the classes can be explained by a small number of "rules" in increasing complexity, with an additional small set of "exceptions" not covered by the rules.The rules are implemented by simple base-classifiers, for example, perceptrons of increasing complexity, which learn general rules valid over the whole input space.Exceptions are localized instances and are best handled by a nonparametric model.Cascading thus stands between the two extremes of parametric and nonparametric classification.The former-for example, a linear modelfinds a single rule that should cover all the instances.A nonparametric classifier-for example, k-NN-stores the whole set of instances without generating any simple rule explaining them.Cascading generates a rule (or rules) to explain a large part of the instances as cheaply as possible and stores the rest as exceptions.This makes sense in a lot of learning applications.For example, most of the time the past tense of a verb in English is found by adding a "-d" or "-ed" to the verb; there are also irregular verbs-for example, "go"/"went"-that do not obey this rule.The idea in combining learners is to divide a complex task into simpler tasks that are handled by separately trained base-learners.Each baselearner has its own task.If we had a large learner containing all the base-learners, then it would risk overfitting.For example, consider taking a vote over three multilayer perceptrons, each with a single hidden layer.If we combine them all together with the linear model combining their outputs, this is a large multilayer perceptron with two hidden layers.If we train this large model with the whole sample, it very probably overfits.When we train the three multilayer perceptrons separately, for example, using ECOC, bagging, and so forth, it is as if we define a required output for the second-layer hidden nodes of the large multilayer perceptron.This puts a constraint on what the overall learner should learn and simplifies learning.One disadvantage of combining is that the combined system is not interpretable.For example, even though decision trees are interpretable, bagged or boosted trees are not interpretable.Error-correcting codes with their weights as −1/0/ + 1 allow some form of interpretability.Mayoraz and Moreira (1997) discuss incremental methods for learning the errorcorrecting output codes where base-learners are added when needed.Allwein, Schapire, and Singer (2000) discuss various methods for coding multiclass problems as two-class problems.Alpaydın and Mayoraz (1999) consider the application of ECOC where linear base-learners are combined to get nonlinear discriminants, and they also propose methods to learn the ECOC matrix from data.The earliest and most intuitive approach is voting.Kittler et al. (1998) give a review of fixed rules and also discuss an application where multi-ple representations are combined.The task is person identification using three representations: frontal face image, face profile image, and voice.The error rate of the voting model is lower than the error rates when a single representation is used.Another application is given in Alimoglu and Alpaydın 1997 where for improved handwritten digit recognition, two sources of information are combined: One is the temporal pen movement data as the digit is written on a touch-sensitive pad, and the other is the static two-dimensional bitmap image once the digit is written.In that application, the two classifiers using either of the two representations have around 5 percent error, but combining the two reduces the error rate to 3 percent.It is also seen that the critical stage is the design of the complementary learners and/or representations, the way they are combined is not as critical.Combining different modalities is used in biometrics, where the aim is biometrics authentication using different input sources, fingerprint, signature, face, and so on.In such a case, different classifiers use these modalities separately and their decisions are combined.This both improves accuracy and makes spoofing more difficult.Noble (2004) makes a distinction between three type of combination strategies when we have information coming from multiple sources in different representations or modalities:In early integration, all these inputs are concatenated to form a single vector that is then fed to a single classifier.Previously we discussed why this is not a very good idea.In late integration, which we advocated in this chapter, different inputs are fed to separate classifiers whose outputs are then combined, by voting, stacking, or any other method we discussed.Kernel algorithms, which we discussed in chapter 13, allow a different method of integration that Noble (2004) calls intermediate integration, as being between early and late integration.This is the multiple kermultiple kernel learning nel learning approach (see section 13.8) where there is a single kernel machine classifier that uses multiple kernels for different inputs and the combination is not in the input space as in early integration, or in the space of decisions as in late integration, but in the space of the basis functions that define the kernels.For different sources, there are different notions of similarity calculated by their kernels, and the classifier accumulates and uses them.Some ensemble methods such as voting are similar to Bayesian averaging (chapter 16).For example when we do bagging and train the same model on different resampled training sets, we may consider them as being samples from the posterior distribution, but other combination methods such as mixture of experts and stacking go much beyond averaging over parameters or models.When we are combining multiple views/representations, concatenating them is not really a good idea but one interesting possibility is to do some combined dimensionality reduction.We can consider a generative model (section 14.3) where we assume that there is a set of latent factors that generate these multiple views in parallel, and from the observed views, we can go back to that latent space and do classification there (Chen et al. 2012).Combining multiple learners has been a popular topic in machine learning since the early 1990s, and research has been going on ever since.Kuncheva (2004) discusses different aspects of classifier combination; the book also includes a section on combination of multiple clustering results.AdaBoosted decision trees are considered to be one of the best machine learning algorithms.There are also versions of AdaBoost where the next base-learner is trained on the residual of the previous base-learner (Hastie, Tibshirani, and Friedman 2001).Recently, it has been noticed that ensembles do not always improve accuracy and research has started to focus on the criteria that a good ensemble should satisfy or how to form a good one.A survey of the role of diversity in ensembles is given in Kuncheva 2005.6.What is the difference between voting and stacking using a linear perceptron as the combiner function?SOLUTION: If the voting system is also trained, the only difference would be that with stacking, the weights need not be positive or sum up to 1, and there is also a bias term.Of course, the main advantage of stacking is when the combiner is nonlinear.7. In cascading, why do we require θ j+1 ≥ θ j ?SOLUTION: Instances on which the confidence is less than θ j have already been filtered out by d j ; we require the threshold to increase so that we can have higher confidences.8. To be able to use cascading for regression, during testing, a regressor should be able to say whether it is confident of its output.How can we implement this?9. How can we combine the results of multiple clustering solutions?SOLUTION: The easiest is the following: Let us take any two training instances.Each clustering solution either places them in the same cluster or not; denote it as 1 and 0. The average of these counts over all clustering solutions is the overall probability that those two are in the same cluster (Kuncheva 2004).10.In section 17.10, we discuss that if we use a decision tree as a combiner in stacking, it works both as a selector and a combiner.What are the other advantages and disadvantages?SOLUTION: A tree uses only a subset of the classifiers and not the whole.Using the tree is fast, and we need to evaluate only the nodes on our path, which may be short.See Ulaş et al. 2009 for more detail.The disadvantage is that the combiner cannot look at combinations of classifier decisions (assuming that the tree is univariate).Using a subset may also be harmful; we do not get the redundancy we need if some classifiers are faulty.of the robot.At any time, the environment is in a certain state that is one of a set of possible states-for example, the state of the board, the position of the robot in the maze.The decision maker has a set of actions possible: legal movement of pieces on the chess board, movement of the robot in possible directions without hitting the walls, and so forth.Once an action is chosen and taken, the state changes.The solution to the task requires a sequence of actions, and we get feedback, in the form of a reward rarely, generally only when the complete sequence is carried out.The reward defines the problem and is necessary if we want a learning agent.The learning agent learns the best sequence of actions to solve a problem where "best" is quantified as the sequence of actions that has the maximum cumulative reward.Such is the setting of reinforcement learning.Reinforcement learning is different from the learning methods we discussed before in a number of respects.It is called "learning with a critic," as opposed to learning with a teacher which we have in supervised learning.A critic differs from a teacher in that it does not tell us what to do critic but only how well we have been doing in the past; the critic never informs in advance.The feedback from the critic is scarce and when it comes, it comes late.This leads to the credit assignment problem.After taking credit assignment several actions and getting the reward, we would like to assess the individual actions we did in the past and find the moves that led us to win the reward so that we can record and recall them later on.As we see shortly, what a reinforcement learning program does is that it learns to generate an internal value for the intermediate states or actions in terms of how good they are in leading us to the goal and getting us to the real reward.Once such an internal reward mechanism is learned, the agent can just take the local actions to maximize it.The solution to the task requires a sequence of actions, and from this perspective, we remember the Markov models we discussed in chapter 15.Indeed, we use a Markov decision process to model the agent.The difference is that in the case of Markov models, there is an external process that generates a sequence of signals, for example, speech, which we observe and model.In the current case, however, it is the agent that generates the sequence of actions.Previously, we also made a distinction between observable and hidden Markov models where the states are observed or hidden (and should be inferred) respectively.Similarly here, sometimes we have a partially observable Markov decision process in cases where the agent does not know its state exactly but should infer it with some uncertainty through observations using sensors.For example, in the case of a robot moving in a room, the robot may not know its exact position in the room, nor the exact location of obstacles nor the goal, and should make decisions through a limited image provided by a camera.We start with a simple example.The K-armed bandit is a hypothetical K-armed bandit slot machine with K levers.The action is to choose and pull one of the levers, and we win a certain amount of money that is the reward associated with the lever (action).The task is to decide which lever to pull to maximize the reward.This is a classification problem where we choose one of K.If this were supervised learning, then the teacher would tell us the correct class, namely, the lever leading to maximum earning.In this case of reinforcement learning, we can only try different levers and keep track of the best.This is a simplified reinforcement learning problem because there is only one state, or one slot machine, and we need only decide on the action.Another reason why this is simplified is that we immediately get a reward after a single action; the reward is not delayed, so we immediately see the value of our action.Let us say Q(a) is the value of action a.Initially, Q(a) = 0 for all a.When we try action a, we get reward r a ≥ 0. If rewards are deterministic, we always get the same r a for any pull of a and in such a case, we can just set Q(a) = r a .If we want to exploit, once we find an action a such that Q(a) > 0, we can keep choosing it and get r a at each pull.However, it is quite possible that there is another lever with a higher reward, so we need to explore.We can choose different actions and store Q(a) for all a. Whenever we want to exploit, we can choose the action with the maximum value, that is, choose a * if Q(a * ) = max a Q(a) (18.1)If rewards are not deterministic but stochastic, we get a different reward each time we choose the same action.The amount of the reward is defined by the probability distribution p(r |a).In such a case, we define Q t (a) as the estimate of the value of action a at time t.It is an average of all rewards received when action a was chosen before time t.An online update can be defined aswhere r t+1 (a) is the reward received after taking action a at time (t + 1)st time.Note that equation 18.2 is the delta rule that we have used on many occasions in the previous chapters: η is the learning factor (gradually decreased in time for convergence), r t+1 is the desired output, and Q t (a) is the current prediction.Q t+1 (a) is the expected value of action a at time t + 1 and converges to the mean of p(r |a) as t increases.The full reinforcement learning problem generalizes this simple case in a number of ways.First, we have several states.This corresponds to having several slot machines with different reward probabilities, p(r |s i , a j ), and we need to learn Q(s i , a j ), which is the value of taking action a j when in state s i .Second, the actions affect not only the reward but also the next state, and we move from one state to another.Third, the rewards are delayed and we need to be able to estimate immediate values from delayed rewards.The learning decision maker is called the agent.The agent interacts with the environment that includes everything outside the agent.The agent has sensors to decide on its state in the environment and takes an action that modifies its state.When the agent takes an action, the environment provides a reward.Time is discrete as t = 0, 1, 2, . .., and s t ∈ S denotes the state of the agent at time t where S is the set of all possible states.a t ∈ A(s t ) denotes the action that the agent takes at time t where A(s t ) is the set of possible actions in state s t .When the agent in state s t takes the action a t , the clock ticks, reward r t+1 ∈ is received, and the agent moves to the next state, s t+1 .The problem is modeled using a Markovdecision process (MDP).The reward and next state are sampled from their respective probability distributions, p(r t+1 |s t , a t ) and P (s t+1 |s t , a t ).Note that what we have is a Markov system where the state and reward in the next time step depend only on the current state and action.In some applications, reward and next state are deterministic, and for a certain state and action taken, there is one possible reward value and next state.Depending on the application, a certain state may be designated as the initial state and in some applications, there is also an absorbing terminal (goal) state where the search ends; all actions in this terminal state transition to itself with probability 1 and without any reward.The sequence of actions from the start to the terminal state is an episode, or a trial.The policy, π , defines the agent's behavior and is a mapping from the policy states of the environment to actions: π : S → A. The policy defines the action to be taken in any state s t : a t = π(s t ).The value of a policy π , V π (s t ), is the expected cumulative reward that will be received while the agent follows the policy, starting from state s t .In the finite-horizon or episodic model, the agent tries to maximize the finite-horizon expected reward for the next T steps:r t+i ⎤ ⎦ (18.3)Certain tasks are continuing, and there is no prior fixed limit to the episode.In the infinite-horizon model, there is no sequence limit, but infinite-horizon future rewards are discounted:where 0 ≤ γ < 1 is the discount rate to keep the return finite.If γ = 0, discount rate then only the immediate reward counts.As γ approaches 1, rewards further in the future count more, and we say that the agent becomes more farsighted.γ is less than 1 because there generally is a time limit to the sequence of actions needed to solve the task.The agent may be a robot that runs on a battery.We prefer rewards sooner rather than later because we are not certain how long we will survive.For each policy π , there is a V π (s t ), and we want to find the optimal optimal policy policy π * such that V * (s t ) = max π V π (s t ), ∀s t (18.5)In some applications, for example, in control, instead of working with the values of states, V (s t ), we prefer to work with the values of stateaction pairs, Q(s t , a t ).V (s t ) denotes how good it is for the agent to be in state s t , whereas Q(s t , a t ) denotes how good it is to perform action a t when in state s t .We define Q * (s t , a t ) as the value, that is, the expected cumulative reward, of action a t taken in state s t and then obeying the optimal policy afterward.The value of a state is equal to the value of the best possible action: To each possible next state s t+1 , we move with probability P (s t+1 |s t , a t ), and continuing from there using the optimal policy, the expected cumulative reward is V * (s t+1 ).We sum over all such possible next states, and we discount it because it is one time step later.Adding our immediate expected reward, we get the total expected cumulative reward for action a t .We then choose the best of possible actions.Equation 18.6 is known as Bellman's equation (Bellman 1957).Similarly, we can also write This means that if we have the Q * (s t , a t ) values, then by using a greedy search at each local step we get the optimal sequence of steps that maximizes the cumulative reward.We start with model-based learning where we completely know the environment model parameters, p(r t+1 |s t , a t ) and P (s t+1 |s t , a t ).In such a case, we do not need any exploration and can directly solve for the optimal value function and policy using dynamic programming.The optimal value function is unique and is the solution to the simultaneous equations given in equation 18.6.Once we have the optimal value function, the optimal policy is to choose the action that maximizes the value in the next state:To find the optimal policy, we can use the optimal value function, and there is an iterative algorithm called value iteration that has been shown We say that the values converged if the maximum value difference between two iterations is less than a certain threshold δ:where l is the iteration counter.Because we care only about the actions with the maximum value, it is possible that the policy converges to the optimal one even before the values converge to their optimal values.Each iteration is O(|S| 2 |A|), but frequently there is only a small number k < |S| of next possible states, so complexity decreases to O(k|S||A|).In policy iteration, we store and update the policy rather than doing this indirectly over the values.The pseudocode is given in figure 18.3.The idea is to start with a policy and improve it repeatedly until there is no change.The value function can be calculated by solving for the linear equations.We then check whether we can improve the policy by taking these into account.This step is guaranteed to improve the policy, and when no improvement is possible, the policy is guaranteed to be optimal.Each iteration of this algorithm takes O(|A||S| 2 + |S| 3 ) time that is more than that of value iteration, but policy iteration needs fewer iterations than value iteration.In reinforcement learning, the learner is a decision-making agent that takes actions in an environment and receives reward (or penalty) for its actions in trying to solve a problem.After a set of trial-anderror runs, it should learn the best policy, which is the sequence of actions that maximize the total reward.Let us say we want to build a machine that learns to play chess.In this case we cannot use a supervised learner for two reasons.First, it is very costly to have a teacher that will take us through many games and indicate us the best move for each position.Second, in many cases, there is no such thing as the best move; the goodness of a move depends on the moves that follow.A single move does not count; a sequence of moves is good if after playing them we win the game.The only feedback is at the end of the game when we win or lose the game.Another example is a robot that is placed in a maze.The robot can move in one of the four compass directions and should make a sequence of movements to reach the exit.As long as the robot is in the maze, there is no feedback and the robot tries many moves until it reaches the exit and only then does it get a reward.In this case there is no opponent, but we can have a preference for shorter trajectories, implying that in this case we play against time.These two applications have a number of points in common: There is a decision maker, called the agent, that is placed in an environment (see figure 18.1).In chess, the game-player is the decision maker and the environment is the board; in the second case, the maze is the environmentIn reinforcement learning, the learner is a decision-making agent that takes actions in an environment and receives reward (or penalty) for its actions in trying to solve a problem.After a set of trial-anderror runs, it should learn the best policy, which is the sequence of actions that maximize the total reward.Let us say we want to build a machine that learns to play chess.In this case we cannot use a supervised learner for two reasons.First, it is very costly to have a teacher that will take us through many games and indicate us the best move for each position.Second, in many cases, there is no such thing as the best move; the goodness of a move depends on the moves that follow.A single move does not count; a sequence of moves is good if after playing them we win the game.The only feedback is at the end of the game when we win or lose the game.Another example is a robot that is placed in a maze.The robot can move in one of the four compass directions and should make a sequence of movements to reach the exit.As long as the robot is in the maze, there is no feedback and the robot tries many moves until it reaches the exit and only then does it get a reward.In this case there is no opponent, but we can have a preference for shorter trajectories, implying that in this case we play against time.These two applications have a number of points in common: There is a decision maker, called the agent, that is placed in an environment (see figure 18.1).In chess, the game-player is the decision maker and the environment is the board; in the second case, the maze is the environment