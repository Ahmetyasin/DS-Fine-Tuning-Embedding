This chapter is organized as follows.The next section introduces single-layer and multi-layer networks.The different types of activation functions, output nodes, and loss functions are discussed.The backpropagation algorithm is introduced in Section 1.3.Practical issues in neural network training are discussed in Section 1.4.Some key points on how neural networks gain their power with specific choices of activation functions are discussed in Section 1.5.The common architectures used in neural network design are discussed in Section 1.6.Advanced topics in deep learning are discussed in Section 1.7.Some notable benchmarks used by the deep learning community are discussed in Section 1.8.A summary is provided in Section 1.9.Although Chapters 1 and 2 provide an overview of the training methods for neural networks, a more detailed understanding of the training challenges is provided in Chapters 3 and 4. Chapters 5 and 6 present radialbasis function (RBF) networks and restricted Boltzmann machines.A lot of the recent success of deep learning is a result of the specialized architectures for various domains, such as recurrent neural networks and convolutional neural networks.Chapters 7 and 8 discuss recurrent and convolutional neural networks.Several advanced topics like deep reinforcement learning, neural Turing mechanisms, and generative adversarial networks are discussed in Chapters 9 and 10.We have taken care to include some of the "forgotten" architectures like RBF networks and Kohonen self-organizing maps because of their potential in many applications.The book is written for graduate students, researchers, and practitioners.Numerous exercises are available along with a solution manual to aid in classroom teaching.Where possible, an application-centric view is highlighted in order to give the reader a feel for the technology.Throughout this book, a vector or a multidimensional data point is annotated with a bar, such as X or y.A vector or multidimensional point may be denoted by either small letters or capital letters, as long as it has a bar.Vector dot products are denoted by centered dots, such as X • Y .A matrix is denoted in capital letters without a bar, such as R. Throughout the book, the n × d matrix corresponding to the entire training data set is denoted by D, with n documents and d dimensions.The individual data points in D are therefore d-dimensional row vectors.On the other hand, vectors with one component for each dataArtificial neural networks are popular machine learning techniques that simulate the mechanism of learning in biological organisms.The human nervous system contains cells, which are referred to as neurons.The neurons are connected to one another with the use of axons and dendrites, and the connecting regions between axons and dendrites are referred to as synapses.These connections are illustrated in Figure 1.1(a).The strengths of synaptic connections often change in response to external stimuli.This change is how learning takes place in living organisms.This biological mechanism is simulated in artificial neural networks, which contain computation units that are referred to as neurons.Throughout this book, we will use the term "neural networks" to refer to artificial neural networks rather than biological ones.The computational units are connected to one another through weights, which serve the same role as the strengths of synaptic connections in biological organisms.Each input to a neuron is scaled with a weight, which affects the function computed at that unit.This architecture is illustrated in Figure 1.1(b).An artificial neural network computes a function of the inputs by propagating the computed values from the input neurons to the output neuron(s) and using the weights as intermediate parameters.Learning occurs by changing the weights connecting the neurons.Just as external stimuli are needed for learning in biological organisms, the external stimulus in artificial neural networks is provided by the training data containing examples of input-output pairs of the function to be learned.For example, the training data might contain pixel representations of images (input) and their annotated labels (e.g., carrot, banana) as the output.These training data pairs are fed into the neural network by using the input representations to make predictions about the output labels.The training data provides feedback to the correctness of the weights in the neural network depending on how well the predicted output (e.g., probability of carrot) for a particular input matches the annotated output label in the training data.One can view the errors made by the neural network in the computation of a function as a kind of unpleasant feedback in a biological organism, leading to an adjustment in the synaptic strengths.Similarly, the weights between neurons are adjusted in a neural network in response to prediction errors.The goal of changing the weights is to modify the computed function to make the predictions more correct in future iterations.Therefore, the weights are changed carefully in a mathematically justified way so as to reduce the error in computation on that example.By successively adjusting the weights between neurons over many input-output pairs, the function computed by the neural network is refined over time so that it provides more accurate predictions.Therefore, if the neural network is trained with many different images of bananas, it will eventually be able to properly recognize a banana in an image it has not seen before.This ability to accurately compute functions of unseen inputs by training over a finite set of input-output pairs is referred to as model generalization.The primary usefulness of all machine learning models is gained from their ability to generalize their learning from seen training data to unseen examples.The biological comparison is often criticized as a very poor caricature of the workings of the human brain; nevertheless, the principles of neuroscience have often been useful in designing neural network architectures.A different view is that neural networks are built as higher-level abstractions of the classical models that are commonly used in machine learning.In fact, the most basic units of computation in the neural network are inspired by traditional machine learning algorithms like least-squares regression and logistic regression.Neural networks gain their power by putting together many such basic units, and learning the weights of the different units jointly in order to minimize the prediction error.From this point of view, a neural network can be viewed as a computational graph of elementary units in which greater power is gained by connecting them in particular ways.When a neural network is used in its most basic form, without hooking together multiple units, the learning algorithms often reduce to classical machine learning models (see Chapter 2).The real power of a neural model over classical methods is unleashed when these elementary computational units are combined, and the weights of the elementary models are trained using their dependencies on one another.By combining multiple units, one is increasing the power of the model to learn more complicated functions of the data than are inherent in the elementary models of basic machine learning.The way in which these units are combined also plays a role in the power of the architecture, and requires some understanding and insight from the analyst.Furthermore, sufficient training data is also required in order to learn the larger number of weights in these expanded computational graphs.Figure 1.2:An illustrative comparison of the accuracy of a typical machine learning algorithm with that of a large neural network.Deep learners become more attractive than conventional methods primarily when sufficient data/computational power is available.Recent years have seen an increase in data availability and computational power, which has led to a "Cambrian explosion" in deep learning technology.Humans and computers are inherently suited to different types of tasks.For example, computing the cube root of a large number is very easy for a computer, but it is extremely difficult for humans.On the other hand, a task such as recognizing the objects in an image is a simple matter for a human, but has traditionally been very difficult for an automated learning algorithm.It is only in recent years that deep learning has shown an accuracy on some of these tasks that exceeds that of a human.In fact, the recent results by deep learning algorithms that surpass human performance [184] in (some narrow tasks on) image recognition would not have been considered likely by most computer vision experts as recently as 10 years ago.Many deep learning architectures that have shown such extraordinary performance are not created by indiscriminately connecting computational units.The superior performance of deep neural networks mirrors the fact that biological neural networks gain much of their power from depth as well.Furthermore, biological networks are connected in ways we do not fully understand.In the few cases that the biological structure is understood at some level, significant breakthroughs have been achieved by designing artificial neural networks along those lines.A classical example of this type of architecture is the use of the convolutional neural network for image recognition.This architecture was inspired by Hubel and Wiesel's experiments [212] in 1959 on the organization of the neurons in the cat's visual cortex.The precursor to the convolutional neural network was the neocognitron [127], which was directly based on these results.The human neuronal connection structure has evolved over millions of years to optimize survival-driven performance; survival is closely related to our ability to merge sensation and intuition in a way that is currently not possible with machines.Biological neuroscience [232] is a field that is still very much in its infancy, and only a limited amount is known about how the brain truly works.Therefore, it is fair to suggest that the biologically inspired success of convolutional neural networks might be replicated in other settings, as we learn more about how the human brain works [176].A key advantage of neural networks over traditional machine learning is that the former provides a higher-level abstraction of expressing semantic insights about data domains by architectural design choices in the computational graph.The second advantage is that neural networks provide a simple way to adjust theIn this section, we will introduce single-layer and multi-layer neural networks.In the singlelayer network, a set of inputs is directly mapped to an output by using a generalized variation of a linear function.This simple instantiation of a neural network is also referred to as the perceptron.In multi-layer neural networks, the neurons are arranged in layered fashion, in which the input and output layers are separated by a group of hidden layers.This layer-wise architecture of the neural network is also referred to as a feed-forward network.This section will discuss both single-layer and multi-layer networks.The simplest neural network is referred to as the perceptron.This neural network contains a single input layer and an output node.The basic architecture of the perceptron is shown in Figure 1.3(a).Consider a situation where each training instance is of the form (X, y), where each X = [x 1 , . . .x d ] contains d feature variables, and y ∈ {−1, +1} contains the observed value of the binary class variable.By "observed value" we refer to the fact that it is given to us as a part of the training data, and our goal is to predict the class variable for cases in which it is not observed.For example, in a credit-card fraud detection application, the features might represent various properties of a set of credit card transactions (e.g., amount and frequency of transactions), and the class variable might represent whether or not this set of transactions is fraudulent.Clearly, in this type of application, one would have historical cases in which the class variable is observed, and other (current) cases in which the class variable has not yet been observed but needs to be predicted.The input layer contains d nodes that transmit the d features X = [x 1 . . .x d ] with edges of weight W = [w 1 . . .w d ] to an output node.The input layer does not perform any computation in its own right.The linear function W • X = d i=1 w i x i is computed at the output node.Subsequently, the sign of this real value is used in order to predict the dependent variable of X.Therefore, the prediction ŷ is computed as follows:The sign function maps a real value to either +1 or −1, which is appropriate for binary classification.Note the circumflex on top of the variable y to indicate that it is a predicted value rather than an observed value.The error of the prediction is therefore E(X) = y − ŷ, which is one of the values drawn from the set {−2, 0, +2}.In cases where the error value E(X) is nonzero, the weights in the neural network need to be updated in the (negative) direction of the error gradient.As we will see later, this process is similar to that used in various types of linear models in machine learning.In spite of the similarity of the perceptron with respect to traditional machine learning models, its interpretation as a computational unit is very useful because it allows us to put together multiple units in order to create far more powerful models than are available in traditional machine learning.The architecture of the perceptron is shown in Figure 1.3(a), in which a single input layer transmits the features to the output node.The edges from the input to the output contain the weights w 1 . . .w d with which the features are multiplied and added at the output node.Subsequently, the sign function is applied in order to convert the aggregated value into a class label.The sign function serves the role of an activation function.Different choices of activation functions can be used to simulate different types of models used in machine learning, like least-squares regression with numeric targets, the support vector machine, or a logistic regression classifier.Most of the basic machine learning models can be easily represented as simple neural network architectures.It is a useful exercise to model traditional machine learning techniques as neural architectures, because it provides a clearer picture of how deep learning generalizes traditional machine learning.This point of view is explored in detail in Chapter 2. It is noteworthy that the perceptron contains two layers, although the input layer does not perform any computation and only transmits the feature values.The input layer is not included in the count of the number of layers in a neural network.Since the perceptron contains a single computational layer, it is considered a single-layer network.In many settings, there is an invariant part of the prediction, which is referred to as the bias.For example, consider a setting in which the feature variables are mean centered, but the mean of the binary class prediction from {−1, +1} is not 0.This will tend to occur in situations in which the binary class distribution is highly imbalanced.In such a case, the aforementioned approach is not sufficient for prediction.We need to incorporate an additional bias variable b that captures this invariant part of the prediction:The bias can be incorporated as the weight of an edge by using a bias neuron.This is achieved by adding a neuron that always transmits a value of 1 to the output node.The weight of the edge connecting the bias neuron to the output node provides the bias variable.An example of a bias neuron is shown in Figure 1.3(b).Another approach that works well with single-layer architectures is to use a feature engineering trick in which an additional feature is created with a constant value of 1.The coefficient of this feature provides the bias, and one can then work with Equation 1.1.Throughout this book, biases will not be explicitly used (for simplicity in architectural representations) because they can be incorporated with bias neurons.The details of the training algorithms remain the same by simply treating the bias neurons like any other neuron with a fixed activation value of 1.Therefore, the following will work with the predictive assumption of Equation 1.1, which does not explicitly uses biases.At the time that the perceptron algorithm was proposed by Rosenblatt [405], these optimizations were performed in a heuristic way with actual hardware circuits, and it was not presented in terms of a formal notion of optimization in machine learning (as is common today).However, the goal was always to minimize the error in prediction, even if a formal optimization formulation was not presented.The perceptron algorithm was, therefore, heuristically designed to minimize the number of misclassifications, and convergence proofs were available that provided correctness guarantees of the learning algorithm in simplified settings.Therefore, we can still write the (heuristically motivated) goal of the perceptron algorithm in least-squares form with respect to all training instances in a data set D con-taining feature-label pairs:This type of minimization objective function is also referred to as a loss function.As we will see later, almost all neural network learning algorithms are formulated with the use of a loss function.As we will learn in Chapter 2, this loss function looks a lot like leastsquares regression.However, the latter is defined for continuous-valued target variables, and the corresponding loss is a smooth and continuous function of the variables.On the other hand, for the least-squares form of the objective function, the sign function is nondifferentiable, with step-like jumps at specific points.Furthermore, the sign function takes on constant values over large portions of the domain, and therefore the exact gradient takes on zero values at differentiable points.This results in a staircase-like loss surface, which is not suitable for gradient-descent.The perceptron algorithm (implicitly) uses a smooth approximation of the gradient of this objective function with respect to each example:(1.3) Note that the above gradient is not a true gradient of the staircase-like surface of the (heuristic) objective function, which does not provide useful gradients.Therefore, the staircase is smoothed out into a sloping surface defined by the perceptron criterion.The properties of the perceptron criterion will be described in Section 1.2.1.1.It is noteworthy that concepts like the "perceptron criterion" were proposed later than the original paper by Rosenblatt [405] in order to explain the heuristic gradient-descent steps.For now, we will assume that the perceptron algorithm optimizes some unknown smooth function with the use of gradient descent.Although the above objective function is defined over the entire training data, the training algorithm of neural networks works by feeding each input data instance X into the network one by one (or in small batches) to create the prediction ŷ.The weights are then updated, based on the error value E(X) = (y − ŷ).Specifically, when the data point X is fed into the network, the weight vector W is updated as follows:The parameter α regulates the learning rate of the neural network.The perceptron algorithm repeatedly cycles through all the training examples in random order and iteratively adjusts the weights until convergence is reached.A single training data point may be cycled through many times.Each such cycle is referred to as an epoch.One can also write the gradientdescent update in terms of the error E(X) = (y − ŷ) as follows:The basic perceptron algorithm can be considered a stochastic gradient-descent method, which implicitly minimizes the squared error of prediction by performing gradient-descent updates with respect to randomly chosen training points.The assumption is that the neural network cycles through the points in random order during training and changes the weights with the goal of reducing the prediction error on that point.It is easy to see from Equation 1.5 that non-zero updates are made to the weights only when y = ŷ, which occurs only when errors are made in prediction.In mini-batch stochastic gradient descent, the aforementioned updates of Equation 1.5 are implemented over a randomly chosen subset of training points S: The advantages of using mini-batch stochastic gradient descent are discussed in Section 3.2.8 of Chapter 3.An interesting quirk of the perceptron is that it is possible to set the learning rate α to 1, because the learning rate only scales the weights.The type of model proposed in the perceptron is a linear model, in which the equation W • X = 0 defines a linear hyperplane.Here, W = (w 1 . . .w d ) is a d-dimensional vector that is normal to the hyperplane.Furthermore, the value of W • X is positive for values of X on one side of the hyperplane, and it is negative for values of X on the other side.This type of model performs particularly well when the data is linearly separable.Examples of linearly separable and inseparable data are shown in Figure 1.4.The perceptron algorithm is good at classifying data sets like the one shown on the left-hand side of Figure 1.4, when the data is linearly separable.On the other hand, it tends to perform poorly on data sets like the one shown on the right-hand side of Figure 1.4.This example shows the inherent modeling limitation of a perceptron, which necessitates the use of more complex neural architectures.Since the original perceptron algorithm was proposed as a heuristic minimization of classification errors, it was particularly important to show that the algorithm converges to reasonable solutions in some special cases.In this context, it was shown [405] that the perceptron algorithm always converges to provide zero error on the training data when the data are linearly separable.However, the perceptron algorithm is not guaranteed to converge in instances where the data are not linearly separable.For reasons discussed in the next section, the perceptron might sometimes arrive at a very poor solution with data that are not linearly separable (in comparison with many other learning algorithms).As discussed earlier in this chapter, the original perceptron paper by Rosenblatt [405] did not formally propose a loss function.In those years, these implementations were achieved using actual hardware circuits.The original Mark I perceptron was intended to be a machine rather than an algorithm, and custom-built hardware was used to create it (cf.Figure 1.5).The general goal was to minimize the number of classification errors with a heuristic update process (in hardware) that changed weights in the "correct" direction whenever errors were made.This heuristic update strongly resembled gradient descent but it was not derived as a gradient-descent method.Gradient descent is defined only for smooth loss functions in algorithmic settings, whereas the hardware-centric approach was designed in a more Can we find a smooth loss function, whose gradient turns out to be the perceptron update?The number of classification errors in a binary classification problem can be written in the form of a 0/1 loss function for training data point (X i , y i ) as follows:The simplification to the right-hand side of the above objective function is obtained by setting both y 2 i and sign{W •X i } 2 to 1, since they are obtained by squaring a value drawn from {−1, +1}.However, this objective function is not differentiable, because it has a staircaselike shape, especially when it is added over multiple points.Note that the 0/1 loss above is dominated by the term −y i sign{W • X i }, in which the sign function causes most of the problems associated with non-differentiability.Since neural networks are defined by gradient-based optimization, we need to define a smooth objective function that is responsible for the perceptron updates.It can be shown [41] that the updates of the perceptron implicitly optimize the perceptron criterion.This objective function is defined by dropping the sign function in the above 0/1 loss and setting negative values to 0 in order to treat all correct predictions in a uniform and lossless way:(1.8)The reader is encouraged to use calculus to verify that the gradient of this smoothed objective function leads to the perceptron update, and the update of the perceptron is essentiallyThe modified loss function to enable gradient computation of a nondifferentiable function is also referred to as a smoothed surrogate loss function.Almost all continuous optimization-based learning methods (such as neural networks) with discrete outputs (such as class labels) use some type of smoothed surrogate loss function.Although the aforementioned perceptron criterion was reverse engineered by working backwards from the perceptron updates, the nature of this loss function exposes some of the weaknesses of the updates in the original algorithm.An interesting observation about the perceptron criterion is that one can set W to the zero vector irrespective of the training data set in order to obtain the optimal loss value of 0. In spite of this fact, the perceptron updates continue to converge to a clear separator between the two classes in linearly separable cases; after all, a separator between the two classes provides a loss value of 0 as well.However, the behavior for data that are not linearly separable is rather arbitrary, and the resulting solution is sometimes not even a good approximate separator of the classes.The direct sensitivity of the loss to the magnitude of the weight vector can dilute the goal of class separation; it is possible for updates to worsen the number of misclassifications significantly while improving the loss.This is an example of how surrogate loss functions might sometimes not fully achieve their intended goals.Because of this fact, the approach is not stable and can yield solutions of widely varying quality.Several variations of the learning algorithm were therefore proposed for inseparable data, and a natural approach is to always keep track of the best solution in terms of the number of misclassifications [128].This approach of always keeping the best solution in one's "pocket" is referred to as the pocket algorithm.Another highly performing variant incorporates the notion of margin in the loss function, which creates an identical algorithm to the linear support vector machine.For this reason, the linear support vector machine is also referred to as the perceptron of optimal stability.The perceptron criterion is a shifted version of the hinge-loss used in support vector machines (see Chapter 2).The hinge loss looks even more similar to the zero-one loss criterion of Equation 1.7, and is defined as follows:(1.9)Note that the perceptron does not keep the constant term of 1 on the right-hand side of Equation 1.7, whereas the hinge loss keeps this constant within the maximization function.This change does not affect the algebraic expression for the gradient, but it does change which points are lossless and should not cause an update.The relationship between the perceptron criterion and the hinge loss is shown in Figure 1.6.This similarity becomes particularly evident when the perceptron updates of Equation 1.6 are rewritten as follows:Here, S + is defined as the set of all misclassified training points X ∈ S that satisfy the condition y(W • X) < 0. This update seems to look somewhat different from the perceptron, because the perceptron uses the error E(X) for the update, which is replaced with y in the update above.A key point is that the (integer) error value E(X) = (y − sign{W • X}) ∈ {−2, +2} can never be 0 for misclassified points in S + .Therefore, we have E(X) = 2y for misclassified points, and E(X) can be replaced with y in the updates after absorbing the factor of 2 within the learning rate.This update is identical to that used by the primal support vector machine (SVM) algorithm [448], except that the updates are performed only for the misclassified points in the perceptron, whereas the SVM also uses the marginally correct points near the decision boundary for updates.Note that the SVM uses the condition y(W • X) < 1 [instead of using the condition y(W • X) < 0] to define S + , which is one of the key differences between the two algorithms.This point shows that the perceptron is fundamentally not very different from well-known machine learning algorithms like the support vector machine in spite of its different origins.Freund and Schapire provide a beautiful exposition of the role of margin in improving stability of the perceptron and also its relationship with the support vector machine [123].It turns out that many traditional machine learning models can be viewed as minor variations of shallow neural architectures like the perceptron.The relationships between classical machine learning models and shallow neural networks are described in detail in Chapter 2.The choice of activation function is a critical part of neural network design.In the case of the perceptron, the choice of the sign activation function is motivated by the fact that a binary class label needs to be predicted.However, it is possible to have other types of situations where different target variables may be predicted.For example, if the target variable to be predicted is real, then it makes sense to use the identity activation function, and the resulting algorithm is the same as least-squares regression.If it is desirable to predict a probability of a binary class, it makes sense to use a sigmoid function for activating the output node, so that the prediction ŷ indicates the probability that the observed value, y, of the dependent variable is 1.The negative logarithm of |y/2 − 0.5 + ŷ| is used as the loss, assuming that y is coded from {−1, 1}.If ŷ is the probability that y is 1, then |y/2 − 0.5 + ŷ| is the probability that the correct value is predicted.This assertion is easy to verify by examining the two cases where y is 0 or 1.This loss function can be shown to be representative of the negative log-likelihood of the training data (see Section 2.2.3 of Chapter 2).The importance of nonlinear activation functions becomes significant when one moves from the single-layered perceptron to the multi-layered architectures discussed later in this chapter.Different types of nonlinear functions such as the sign, sigmoid, or hyperbolic tangents may be used in various layers.We use the notation Φ to denote the activation function:Therefore, a neuron really computes two functions within the node, which is why we have incorporated the summation symbol Σ as well as the activation symbol Φ within a neuron.The break-up of the neuron computations into two separate values is shown in Figure 1.7.The value computed before applying the activation function Φ(•) will be referred to as the pre-activation value, whereas the value computed after applying the activation function is referred to as the post-activation value.The output of a neuron is always the post-activation value, although the pre-activation variables are often used in different types of analyses, such as the computations of the backpropagation algorithm discussed later in this chapter.The pre-activation and post-activation values of a neuron are shown in Figure 1.7.The most basic activation function Φ(•) is the identity or linear activation, which provides no nonlinearity:The linear activation function is often used in the output node, when the target is a real value.It is even used for discrete outputs when a smoothed surrogate loss function needs to be set up.The classical activation functions that were used early in the development of neural networks were the sign, sigmoid, and the hyperbolic tangent functions:While the sign activation can be used to map to binary outputs at prediction time, its non-differentiability prevents its use for creating the loss function at training time.For example, while the perceptron uses the sign function for prediction, the perceptron criterion in training only requires linear activation.The sigmoid activation outputs a value in (0, 1), which is helpful in performing computations that should be interpreted as probabilities.Furthermore, it is also helpful in creating probabilistic outputs and constructing loss functions derived from maximum-likelihood models.The tanh function has a shape similar to that of the sigmoid function, except that it is horizontally re-scaled and vertically translated/re-scaled to [−1, 1].The tanh and sigmoid functions are related as follows (see Exercise 3):The tanh function is preferable to the sigmoid when the outputs of the computations are desired to be both positive and negative.Furthermore, its mean-centering and larger gradient (because of stretching) with respect to sigmoid makes it easier to train.The sigmoid and the tanh functions have been the historical tools of choice for incorporating nonlinearity in the neural network.In recent years, however, a number of piecewise linear activation functions have become more popular:The ReLU and hard tanh activation functions have largely replaced the sigmoid and soft tanh activation functions in modern neural networks because of the ease in training multilayered neural networks with these activation functions.Pictorial representations of all the aforementioned activation functions are illustrated in Figure 1.8.It is noteworthy that all activation functions shown here are monotonic.Furthermore, other than the identity activation function, most 1 of the other activation functions saturate at large absolute values of the argument at which increasing further does not change the activation much.As we will see later, such nonlinear activation functions are also very useful in multilayer networks, because they help in creating more powerful compositions of different types of functions.Many of these functions are referred to as squashing functions, as they map the outputs from an arbitrary range to bounded outputs.The use of a nonlinear activation plays a fundamental role in increasing the modeling power of a network.If a network used only linear activations, it would not provide better modeling power than a single-layer linear network.This issue is discussed in Section 1.5.The choice and number of output nodes is also tied to the activation function, which in turn depends on the application at hand.For example, if k-way classification is intended, k output values can be used, with a softmax activation function with respect to outputs v = [v 1 , . . ., v k ] at the nodes in a given layer.Specifically, the activation function for the ith output is defined as follows:It is helpful to think of these k values as the values output by k nodes, in which the inputs are v 1 . . .v k .An example of the softmax function with three outputs is illustrated in Figure 1.9, and the values v 1 , v 2 , and v 3 are also shown in the same figure.Note that the three outputs correspond to the probabilities of the three classes, and they convert the three outputs of the final hidden layer into probabilities with the softmax function.The final hidden layer often uses linear (identity) activations, when it is input into the softmax layer.Furthermore, there are no weights associated with the softmax layer, since it is only converting real-valued outputs into probabilities.The use of softmax with a single hidden layer of linear activations exactly implements a model, which is referred to as multinomial logistic regression [6].Similarly, many variations like multi-class SVMs can be easily implemented with neural networks.Another example of a case in which multiple output nodes are used is the autoencoder, in which each input data point is fully reconstructed by the output layer.The autoencoder can be used to implement matrix factorization methods like singular value decomposition.This architecture will be discussed in detail in Chapter 2. The simplest neural networks that simulate basic machine learning algorithms are instructive because they lie on the continuum between traditional machine learning and deep networks.By exploring these architectures, one gets a better idea of the relationship between traditional machine learning and neural networks, and also the advantages provided by the latter.The choice of the loss function is critical in defining the outputs in a way that is sensitive to the application at hand.For example, least-squares regression with numeric outputs requires a simple squared loss of the form (y − ŷ) 2 for a single training instance with target y and prediction ŷ.One can also use other types of loss like hinge loss for y ∈ {−1, +1} and real-valued prediction ŷ (with identity activation):The hinge loss can be used to implement a learning method, which is referred to as a support vector machine.For multiway predictions (like predicting word identifiers or one of multiple classes), the softmax output is particularly useful.However, a softmax output is probabilistic, and therefore it requires a different type of loss function.In fact, for probabilistic predictions, two different types of loss functions are used, depending on whether the prediction is binary or whether it is multiway:1. Binary targets (logistic regression): In this case, it is assumed that the observed value y is drawn from {−1, +1}, and the prediction ŷ is a an arbitrary numerical value on using the identity activation function.In such a case, the loss function for a single instance with observed value y and real-valued prediction ŷ (with identity activation) is defined as follows:This type of loss function implements a fundamental machine learning method, referred to as logistic regression.Alternatively, one can use a sigmoid activation function to output ŷ ∈ (0, 1), which indicates the probability that the observed value y is 1.Then, the negative logarithm of |y/2 − 0.5 + ŷ| provides the loss, assuming that y is coded from {−1, 1}.This is because |y/2 − 0.5 + ŷ| indicates the probability that the prediction is correct.This observation illustrates that one can use various combinations of activation and loss functions to achieve the same result.In this case, if ŷ1 . . .ŷk are the probabilities of the k classes (using the softmax activation of Equation 1.9), and the rth class is the ground-truth class, then the loss function for a single instance is defined as follows:This type of loss function implements multinomial logistic regression, and it is referred to as the cross-entropy loss.Note that binary logistic regression is identical to multinomial logistic regression, when the value of k is set to 2 in the latter.The key point to remember is that the nature of the output nodes, the activation function, and the loss function depend on the application at hand.Furthermore, these choices also depend on one another.Even though the perceptron is often presented as the quintessential representative of single-layer networks, it is only a single representative out of a very large universe of possibilities.In practice, one rarely uses the perceptron criterion as the loss function.For discrete-valued outputs, it is common to use softmax activation with crossentropy loss.For real-valued outputs, it is common to use linear activation with squared loss.Generally, cross-entropy loss is easier to optimize than squared loss.Most neural network learning is primarily related to gradient-descent with activation functions.For this reason, the derivatives of these activation functions are used repeatedly in this book, and gathering them in a single place for future reference is useful.This section provides details on the derivatives of these loss functions.Later chapters will extensively refer to these results.The derivative of the linear activation function is 1 at all places.The derivative of sign(v) is 0 at all values of v other than at v = 0, where it is discontinuous and non-differentiable.Because of the zero gradient and non-differentiability of this activation function, it is rarely used in the loss function even when it is used for prediction at testing time.The derivatives of the linear and sign activations are illustrated in Figure 1.10(a) and (b), respectively.The derivative of sigmoid activation is particularly simple, when it is expressed in terms of the output of the sigmoid, rather than the input.Let o be the output of the sigmoid function with argument v:Then, one can write the derivative of the activation as follows:(1.17)The key point is that this sigmoid can be written more conveniently in terms of the outputs:The derivative of the sigmoid is often used as a function of the output rather than the input.The derivative of the sigmoid activation function is illustrated in Figure 1.10(c).As in the case of the sigmoid activation, the tanh activation is often used as a function of the output o rather than the input v:One can then compute the gradient as follows:One can also write this derivative in terms of the output o:The derivative of the tanh activation is illustrated in Figure 1.10(d).The ReLU takes on a partial derivative value of 1 for non-negative values of its argument, and 0, otherwise.The hard tanh function takes on a partial derivative value of 1 for values of the argument in [−1, +1] and 0, otherwise.The derivatives of the ReLU and hard tanh activations are illustrated in Figure 1.10(e) and (f), respectively.Multilayer neural networks contain more than one computational layer.The perceptron contains an input and output layer, of which the output layer is the only computationperforming layer.The input layer transmits the data to the output layer, and all computations are completely visible to the user.Multilayer neural networks contain multiple computational layers; the additional intermediate layers (between input and output) are referred to as hidden layers because the computations performed are not visible to the user.The specific architecture of multilayer neural networks is referred to as feed-forward networks, because successive layers feed into one another in the forward direction from input to output.The default architecture of feed-forward networks assumes that all nodes in one layer are connected to those of the next layer.Therefore, the architecture of the neural network is almost fully defined, once the number of layers and the number/type of nodes in each layer have been defined.The only remaining detail is the loss function that is optimized in the output layer.Although the perceptron algorithm uses the perceptron criterion, this is not the only choice.It is extremely common to use softmax outputs with cross-entropy loss for discrete prediction and linear outputs with squared loss for real-valued prediction.As in the case of single-layer networks, bias neurons can be used both in the hidden layers and in the output layers.Examples of multilayer networks with or without the bias neurons are shown in Figure 1.11(a) and (b), respectively.In each case, the neural network contains three layers.Note that the input layer is often not counted, because it simply transmits the data and no computation is performed in that layer.If a neural network contains p 1 . . .p k units in each of its k layers, then the (column) vector representations of these outputs, denoted by h 1 . . .h k have dimensionalities p 1 . . .p k .Therefore, the number of units in each layer is referred to as the dimensionality of that layer.The basic architecture of a feed-forward network with two hidden layers and a single output layer.Even though each unit contains a single scalar variable, one often represents all units within a single layer as a single vector unit.Vector units are often represented as rectangles and have connection matrices between them.The weights of the connections between the input layer and the first hidden layer are contained in a matrix W 1 with size d × p 1 , whereas the weights between the rth hidden layer and the (r + 1)th hidden layer are denoted by the p r × p r+1 matrix denoted by W r .If the output layer contains o nodes, then the final matrix W k+1 is of size p k × o.The d-dimensional input vector x is transformed into the outputs using the following recursive equations:Here, the activation functions like the sigmoid function are applied in element-wise fashion to their vector arguments.However, some activation functions such as the softmax (which are typically used in the output layers) naturally have vector arguments.Even though each unit of a neural network contains a single variable, many architectural diagrams combine the units in a single layer to create a single vector unit, which is represented as a rectangle rather than a circle.For example, the architectural diagram in Figure 1.11(c) (with scalar units) has been transformed to a vector-based neural architecture in Figure 1.11(d).Note that the connections between the vector units are now matrices.Furthermore, an implicit assumption in the vector-based neural architecture is that all units in a layer use the same activation function, which is applied in element-wise fashion to that layer.This constraint is usually not a problem, because most neural architectures use the same activation function throughout the computational pipeline, with the only deviation caused by the nature of the output layer.Throughout this book, neural architectures in which units contain vector variables will be depicted with rectangular units, whereas scalar variables will correspond to circular units.Note that the aforementioned recurrence equations and vector architectures are valid only for layer-wise feed-forward networks, and cannot always be used for unconventional architectural designs.It is possible to have all types of unconventional designs in which inputs might be incorporated in intermediate layers, or the topology might allow connections between non-consecutive layers.Furthermore, the functions computed at a node may not always be in the form of a combination of a linear function and an activation.It is possible to have all types of arbitrary computational functions at nodes.Although a very classical type of architecture is shown in Figure 1.11, it is possible to vary on it in many ways, such as allowing multiple output nodes.These choices are often determined by the goals of the application at hand (e.g., classification or dimensionality reduction).A classical example of the dimensionality reduction setting is the autoencoder, which recreates the outputs from the inputs.Therefore, the number of outputs and inputs is equal, as shown in Figure 1.12.The constricted hidden layer in the middle outputs the reduced representation of each instance.As a result of this constriction, there is some loss in the representation, which typically corresponds to the noise in the data.The outputs of the hidden layers correspond to the reduced representation of the data.In fact, a shallow variant of this scheme can be shown to be mathematically equivalent to a well-known dimensionality reduction method known as singular value decomposition.As we will learn in Chapter 2, increasing the depth of the network results in inherently more powerful reductions.Although a fully connected architecture is able to perform well in many settings, better performance is often achieved by pruning many of the connections or sharing them in an insightful way.Typically, these insights are obtained by using a domain-specific understanding of the data.A classical example of this type of weight pruning and sharing is that of the convolutional neural network architecture (cf.Chapter 8), in which the architecture is carefully designed in order to conform to the typical properties of image data.Such an approach minimizes the risk of overfitting by incorporating domain-specific insights (or bias).As we will discuss later in this book (cf.Chapter 4), overfitting is a pervasive problem in neural network design, so that the network often performs very well on the training data, but it generalizes poorly to unseen test data.This problem occurs when the number of free parameters, (which is typically equal to the number of weight connections), is too large compared to the size of the training data.In such cases, the large number of parameters memorize the specific nuances of the training data, but fail to recognize the statistically significant patterns for classifying unseen test data.Clearly, increasing the number of nodes in the neural network tends to encourage overfitting.Much recent work has been focused both on the architecture of the neural network as well as on the computations performed within each node in order to minimize overfitting.Furthermore, the way in which the neural network is trained also has an impact on the quality of the final solution.Many clever methods, such as pretraining (cf.Chapter 4), have been proposed in recent years in order to improve the quality of the learned solution.This book will explore these advanced training methods in detail.It is helpful to view a neural network as a computational graph, which is constructed by piecing together many basic parametric models.Neural networks are fundamentally more powerful than their building blocks because the parameters of these models are learned jointly to create a highly optimized composition function of these models.The common use of the term "perceptron" to refer to the basic unit of a neural network is somewhat misleading, because there are many variations of this basic unit that are leveraged in different settings.In fact, it is far more common to use logistic units (with sigmoid activation) and piecewise/fully linear units as building blocks of these models.A multilayer network evaluates compositions of functions computed at individual nodes.A path of length 2 in the neural network in which the function f (•) follows g(•) can be considered a composition function f (g(•)).Furthermore, if g 1 (•), g 2 (•) . . .g k (•) are the functions computed in layer m, and a particular layer-(m + 1) node computes f (•), then the composition function computed by the layer-(m + 1) node in terms of the layer-m inputs is f (g 1 (•), . . .g k (•)).The use of nonlinear activation functions is the key to increasing the power of multiple layers.If all layers use an identity activation function, then a multilayer network can be shown to simplify to linear regression.It has been shown [208] that a network with a single hidden layer of nonlinear units (with a wide ranging choice of squashing functions like the sigmoid unit) and a single (linear) output layer can compute almost any "reasonable" function.As a result, neural networks are often referred to as universal function approximators, although this theoretical claim is not always easy to translate into practical usefulness.The main issue is that the number of hidden units required to do so is rather large, which increases the number of parameters to be learned.This results in practical problems in training the network with a limited amount of data.In fact, deeper networks are often preferred because they reduce the number of hidden units in each layer as well as the overall number of parameters.The "building block" description is particularly appropriate for multilayer neural networks.Very often, off-the-shelf softwares for building neural networks 2 provide analysts with access to these building blocks.The analyst is able to specify the number and type of units in each layer along with an off-the-shelf or customized loss function.A deep neural network containing tens of layers can often be described in a few hundred lines of code.All the learning of the weights is done automatically by the backpropagation algorithm that uses dynamic programming to work out the complicated parameter update steps of the underlying computational graph.The analyst does not have to spend the time and effort to explicitly work out these steps.This makes the process of trying different types of architectures relatively painless for the analyst.Building a neural network with many of the off-the-shelf softwares is often compared to a child constructing a toy from building blocks that appropriately fit with one another.Each block is like a unit (or a layer of units) with a particular type of activation.Much of this ease in training neural networks is attributable to the backpropagation algorithm, which shields the analyst from explicitly working out the parameter update steps of what is actually an extremely complicated optimization problem.Working out these steps is often the most difficult part of most machine learning algorithms, and an important contribution of the neural network paradigm is to bring modular thinking into machine learning.In other words, the modularity in neural network design translates to modularity in learning its parameters; the specific name for the latter type of modularity is "backpropagation."This makes the design of neural networks more of an (experienced) engineer's task rather than a mathematical exercise.In the single-layer neural network, the training process is relatively straightforward because the error (or loss function) can be computed as a direct function of the weights, which allows easy gradient computation.In the case of multi-layer networks, the problem is that the loss is a complicated composition function of the weights in earlier layers.The gradient of a composition function is computed using the backpropagation algorithm.The backpropagation algorithm leverages the chain rule of differential calculus, which computes the error gradients in terms of summations of local-gradient products over the various paths from a node to the output.Although this summation has an exponential number of components (paths), one can compute it efficiently using dynamic programming.The backpropagation algorithm is a direct application of dynamic programming.It contains two main phases, referred to as the forward and backward phases, respectively.The forward phase is required to compute the output values and the local derivatives at various nodes, and the backward phase is required to accumulate the products of these local values over all paths from the node to the output:1. Forward phase: In this phase, the inputs for a training instance are fed into the neural network.This results in a forward cascade of computations across the layers, using the current set of weights.The final predicted output can be compared to that of the training instance and the derivative of the loss function with respect to the output is computed.The derivative of this loss now needs to be computed with respect to the weights in all layers in the backwards phase.The main goal of the backward phase is to learn the gradient of the loss function with respect to the different weights by using the chain rule of differential calculus.These gradients are used to update the weights.Since these gradients are learned in the backward direction, starting from the output node, this learning process is referred to as the backward phase.Consider a sequence of hidden units h 1 , h 2 , . . ., h k followed by output o, with respect to which the loss function L is computed.Furthermore, assume that the weight of the connection from hidden unit h r to h r+1 is w (hr,hr+1) .Then, in the case that a single path exists from h 1 to o, one can derive the gradient of the loss function with respect to any of these edge weights using the chain rule:The aforementioned expression assumes that only a single path from h 1 to o exists in the network, whereas an exponential number of paths might exist in reality.A generalized variant of the chain rule, referred to as the multivariable chain rule, computes the gradient in a computational graph, where more than one path might exist.This is achieved by adding the composition along each of the paths from h 1 to o.An example of the chain rule in a computational graph with two paths is shown in Figure 1.13.Therefore, one generalizes the above expression to the case where a set P of paths exist from h r to o:The computation of ∂hr ∂w (h r−1 ,hr ) on the right-hand side is straightforward and will be discussed below (cf.Equation 1.27).However, the path-aggregated term above [annotated by Δ(h r , o) = ∂L ∂hr ] is aggregated over an exponentially increasing number of paths (with respect to path length), which seems to be intractable at first sight.A key point is that the computational graph of a neural network does not have cycles, and it is possible to compute such an aggregation in a principled way in the backwards direction by first computing Δ(h k , o) for nodes h k closest to o, and then recursively computing these values for nodes in earlier layers in terms of the nodes in later layers.Furthermore, the value of Δ(o, o) for each output node is initialized as follows:This type of dynamic programming technique is used frequently to efficiently compute all types of path-centric functions in directed acyclic graphs, which would otherwise require an exponential number of operations.The recursion for Δ(h r , o) can be derived using the multivariable chain rule:Since each h is in a later layer than h r , Δ(h, o) has already been computed while evaluating Δ(h r , o).However, we still need to evaluate ∂h ∂hr in order to compute Equation 1.25.Consider a situation in which the edge joining h r to h has weight w (hr,h) , and let a h be the value computed in hidden unit h just before applying the activation function Φ(•).In other words, we have h = Φ(a h ), where a h is a linear combination of its inputs from earlier-layer units incident on h.Then, by the univariate chain rule, the following expression for ∂h ∂hr can be derived:This value of ∂h ∂hr is used in Equation 1.25, which is repeated recursively in the backwards direction, starting with the output node.The corresponding updates in the backwards direction are as follows:Therefore, gradients are successively accumulated in the backwards direction, and each node is processed exactly once in a backwards pass.Note that the computation of Equation 1.25 (which requires proportional operations to the number of outgoing edges) needs to be repeated for each incoming edge into the node to compute the gradient with respect to all edge weights.Finally, Equation 1.23 requires the computation of ∂hr ∂w (h r−1 ,hr ) , which is easily computed as follows:Here, the key gradient that is backpropagated is the derivative with respect to layer activations, and the gradient with respect to the weights is easy to compute for any incident edge on the corresponding unit.It is noteworthy that the dynamic programming recursion of Equation 1.26 can be computed in multiple ways, depending on which variables one uses for intermediate chaining.All these recursions are equivalent in terms of the final result of backpropagation.In the following, we give an alternative version of the dynamic programming recursion, which is more commonly seen in textbooks.Note that Equation 1.23 uses the variables in the hidden layers as the "chain" variables for the dynamic programming recursion.One can also use the pre-activation values of the variables for the chain rule.The pre-activation variables in a neuron are obtained after applying the linear transform (but before applying the activation variables) as the intermediate variables.The pre-activation value of the hidden variable h = Φ(a h ) is a h .The differences between the pre-activation and post-activation values within a neuron are shown in Figure 1.7.Therefore, instead of Equation 1.23, one can use the following chain rule:Here, we have introduced the notation δ(h r , o) = ∂L ∂a hr instead of Δ(h r , o) = ∂L ∂hr for setting up the recursive equation.The value of δ(o, o) = ∂L ∂ao is initialized as follows:Then, one can use the multivariable chain rule to set up a similar recursion:This recursion condition is found more commonly in textbooks discussing backpropagation.The partial derivative of the loss with respect to the weight is then computed using δ(h r , o) as follows:As with the single-layer network, the process of updating the nodes is repeated to convergence by repeatedly cycling through the training data in epochs.A neural network may sometimes require thousands of epochs through the training data to learn the weights at the different nodes.A detailed description of the backpropagation algorithm and associated issues is provided in Chapter 3. In this chapter, we provide a brief discussion of these issues.In spite of the formidable reputation of neural networks as universal function approximators, considerable challenges remain with respect to actually training neural networks to provide this level of performance.These challenges are primarily related to several practical problems associated with training, the most important one of which is overfitting.The problem of overfitting refers to the fact that fitting a model to a particular training data set does not guarantee that it will provide good prediction performance on unseen test data, even if the model predicts the targets on the training data perfectly.In other words, there is always a gap between the training and test data performance, which is particularly large when the models are complex and the data set is small.In order to understand this point, consider a simple single-layer neural network on a data set with five attributes, where we use the identity activation to learn a real-valued target variable.This architecture is almost identical to that of Figure 1.3, except that the identity activation function is used in order to predict a real-valued target.Therefore, the network tries to learn the following function:Consider a situation in which the observed target value is real and is always twice the value of the first attribute, whereas other attributes are completely unrelated to the target.However, we have only four training instances, which is one less than the number of features (free parameters).For example, the training instances could be as follows:The correct parameter vector in this case is W = [2, 0, 0, 0, 0] based on the known relationship between the first feature and target.The training data also provides zero error with this solution, although the relationship needs to be learned from the given instances since it is not given to us a priori.However, the problem is that the number of training points is fewer than the number of parameters and it is possible to find an infinite number of solutions with zero error.For example, the parameter set [0,2,4,6,8] also provides zero error on the training data.However, if we used this solution on unseen test data, it is likely to provide very poor performance because the learned parameters are spuriously inferred and are unlikely to generalize well to new points in which the target is twice the first attribute (and other attributes are random).This type of spurious inference is caused by the paucity of training data, where random nuances are encoded into the model.As a result, the solution does not generalize well to unseen test data.This situation is almost similar to learning by rote, which is highly predictive for training data but not predictive for unseen test data.Increasing the number of training instances improves the generalization power of the model, whereas increasing the complexity of the model reduces its generalization power.At the same time, when a lot of training data is available, an overly simple model is unlikely to capture complex relationships between the features and target.A good rule of thumb is that the total number of training data points should be at least 2 to 3 times larger than the number of parameters in the neural network, although the precise number of data instances depends on the specific model at hand.In general, models with a larger number of parameters are said to have high capacity, and they require a larger amount of data in order to gain generalization power to unseen test data.The notion of overfitting is often understood in the trade-off between bias and variance in machine learning.The key take-away from the notion of bias-variance trade-off is that one does not always win with more powerful (i.e., less biased) models when working with limited training data, because of the higher variance of these models.For example, if we change the training data in the table above to a different set of four points, we are likely to learn a completely different set of parameters (from the random nuances of those points).This new model is likely to yield a completely different prediction on the same test instance as compared to the predictions using the first training data set.This type of variation in the prediction of the same test instance using different training data sets is a manifestation of model variance, which also adds to the error of the model; after all, both predictions of the same test instance could not possibly be correct.More complex models have the drawback of seeing spurious patterns in random nuances, especially when the training data are insufficient.One must be careful to pick an optimum point when deciding the complexity of the model.These notions are described in detail in Chapter 4. Neural networks have always been known to theoretically be powerful enough to approximate any function [208].However, the lack of data availability can result in poor performance; this is one of the reasons that neural networks only recently achieved prominence.The greater availability of data has revealed the advantages of neural networks over traditional machine learning (cf. Figure 1.2).In general, neural networks require careful design to minimize the harmful effects of overfitting, even when a large amount of data is available.This section provides an overview of some of the design methods used to mitigate the impact of overfitting.Since a larger number of parameters causes overfitting, a natural approach is to constrain the model to use fewer non-zero parameters.In the previous example, if we constrain the vector W to have only one non-zero component out of five components, it will correctly obtain the solution [2, 0, 0, 0, 0].Smaller absolute values of the parameters also tend to overfit less.Since it is hard to constrain the values of the parameters, the softer approach of adding the penalty λ||W || p to the loss function is used.The value of p is typically set to 2, which leads to Tikhonov regularization.In general, the squared value of each parameter (multiplied with the regularization parameter λ > 0) is added to the objective function.The practical effect of this change is that a quantity proportional to λw i is subtracted from the update of the parameter w i .An example of a regularized version of Equation 1.6 for mini-batch S and update step-size α > 0 is as follows:Here, E[X] represents the current error (y − ŷ) between observed and predicted values of training instance X.One can view this type of penalization as a kind of weight decay during the updates.Regularization is particularly important when the amount of available data is limited.A neat biological interpretation of regularization is that it corresponds to gradual forgetting, as a result of which "less important" (i.e., noisy) patterns are removed.In general, it is often advisable to use more complex models with regularization rather than simpler models without regularization.As a side note, the general form of Equation 1.33 is used by many regularized machine learning models like least-squares regression (cf.Chapter 2), where E(X) is replaced by the error-function of that specific model.Interestingly, weight decay is only sparingly used in the single-layer perceptron 3 because it can sometimes cause overly rapid forgetting with a small number of recently misclassified training points dominating the weight vector; the main issue is that the perceptron criterion is already a degenerate loss function with a minimum value of 0 at W = 0 (unlike its hinge-loss or least-squares cousins).This quirk is a legacy of the fact that the single-layer perceptron was originally defined in terms of biologically inspired updates rather than in terms of carefully thought-out loss functions.Convergence to an optimal solution was never guaranteed other than in linearly separable cases.For the single-layer perceptron, some other regularization techniques, which are discussed below, are more commonly used.The most effective way of building a neural network is by constructing the architecture of the neural network after giving some thought to the underlying data domain.For example, the successive words in a sentence are often related to one another, whereas the nearby pixels in an image are typically related.These types of insights are used to create specialized architectures for text and image data with fewer parameters.Furthermore, many of the parameters might be shared.For example, a convolutional neural network uses the same set of parameters to learn the characteristics of a local block of the image.The recent advancements in the use of neural networks like recurrent neural networks and convolutional neural networks are examples of this phenomena.Another common form of regularization is early stopping, in which the gradient descent is ended after only a few iterations.One way to decide the stopping point is by holding out a part of the training data, and then testing the error of the model on the held-out set.The gradient-descent approach is terminated when the error on the held-out set begins to rise.Early stopping essentially reduces the size of the parameter space to a smaller neighborhood within the initial values of the parameters.From this point of view, early stopping acts as a regularizer because it effectively restricts the parameter space.As discussed earlier, a two-layer neural network can be used as a universal function approximator [208], if a large number of hidden units are used within the hidden layer.It turns out that networks with more layers (i.e., greater depth) tend to require far fewer units per layer because the composition functions created by successive layers make the neural network more powerful.Increased depth is a form of regularization, as the features in later layers are forced to obey a particular type of structure imposed by the earlier layers.Increased constraints reduce the capacity of the network, which is helpful when there are limitations on the amount of available data.A brief explanation of this type of behavior is given in Section 1.5.The number of units in each layer can typically be reduced to such an extent that a deep network often has far fewer parameters even when added up over the greater number of layers.This observation has led to an explosion in research on the topic of deep learning.Even though deep networks have fewer problems with respect to overfitting, they come with a different family of problems associated with ease of training.In particular, the loss derivatives with respect to the weights in different layers of the network tend to have vastly different magnitudes, which causes challenges in properly choosing step sizes.Different manifestations of this undesirable behavior are referred to as the vanishing and exploding gradient problems.Furthermore, deep networks often take unreasonably long to converge.These issues and design choices will be discussed later in this section and at several places throughout the book.A variety of ensemble methods like bagging are used in order to increase the generalization power of the model.These methods are applicable not just to neural networks but to any type of machine learning algorithm.However, in recent years, a number of ensemble methods that are specifically focused on neural networks have also been proposed.Two such methods include Dropout and Dropconnect.These methods can be combined with many neural network architectures to obtain an additional accuracy improvement of about 2% in many real settings.However, the precise improvement depends to the type of data and the nature of the underlying training.For example, normalizing the activations in hidden layers can reduce the effectiveness of Dropout methods, although one can gain from the normalization itself.Ensemble methods are discussed in Chapter 4.While increasing depth often reduces the number of parameters of the network, it leads to different types of practical issues.Propagating backwards using the chain rule has its drawbacks in networks with a large number of layers in terms of the stability of the updates.In particular, the updates in earlier layers can either be negligibly small (vanishing gradient) or they can be increasingly large (exploding gradient) in certain types of neural network architectures.This is primarily caused by the chain-like product computation in Equation 1.23, which can either exponentially increase or decay over the length of the path.In order to understand this point, consider a situation in which we have a multi-layer network with one neuron in each layer.Each local derivative along a path can be shown to be the product of the weight and the derivative of the activation function.The overall backpropagated derivative is the product of these values.If each such value is randomly distributed, and has an expected value less than 1, the product of these derivatives in Equation 1.23 will drop off exponentially fast with path length.If the individual values on the path have expected values greater than 1, it will typically cause the gradient to explode.Even if the local derivatives are randomly distributed with an expected value of exactly 1, the overall derivative will typically show instability depending on how the values are actually distributed.In other words, the vanishing and exploding gradient problems are rather natural to deep networks, which makes their training process unstable.Many solutions have been proposed to address this issue.For example, a sigmoid activation often encourages the vanishing gradient problem, because its derivative is less than 0.25 at all values of its argument (see Exercise 7), and is extremely small at saturation.A ReLU activation unit is known to be less likely to create a vanishing gradient problem because its derivative is always 1 for positive values of the argument.More discussions on this issue are provided in Chapter 3. Aside from the use of the ReLU, a whole host of gradient-descent tricks are used to improve the convergence behavior of the problem.In particular, the use of adaptive learning rates and conjugate gradient methods can help in many cases.Furthermore, a recent technique called batch normalization is helpful in addressing some of these issues.These techniques are discussed in Chapter 3.Sufficiently fast convergence of the optimization process is difficult to achieve with very deep networks, as depth leads to increased resistance to the training process in terms of letting the gradients smoothly flow through the network.This problem is somewhat related to the vanishing gradient problem, but has its own unique characteristics.Therefore, some "tricks" have been proposed in the literature for these cases, including the use of gating networks and residual networks [184].These methods are discussed in Chapters 7 and 8, respectively.The optimization function of a neural network is highly nonlinear, which has lots of local optima.When the parameter space is large, and there are many local optima, it makes sense to spend some effort in picking good initialization points.One such method for improving neural network initialization is referred to as pretraining.The basic idea is to use either supervised or unsupervised training on shallow sub-networks of the original network in order to create the initial weights.This type of pretraining is done in a greedy and layerwise fashion in which a single layer of the network is trained at one time in order to learn the initialization points of that layer.This type of approach provides initialization points that ignore drastically irrelevant parts of the parameter space to begin with.Furthermore, unsupervised pretraining often tends to avoid problems associated with overfitting.The basic idea here is that some of the minima in the loss function are spurious optima because they are exhibited only in the training data and not in the test data.Using unsupervised pretraining tends to move the initialization point closer to the basin of "good" optima in the test data.This is an issue associated with model generalization.Methods for pretraining are discussed in Section 4.7 of Chapter 4.Interestingly, the notion of spurious optima is often viewed from the lens of model generalization in neural networks.This is a different perspective from traditional optimization.In traditional optimization, one does not focus on the differences in the loss functions of the training and test data, but on the shape of the loss function in only the training data.Surprisingly, the problem of local optima (from a traditional perspective) is a smaller issue in neural networks than one might normally expect from such a nonlinear function.Most of the time, the nonlinearity causes problems during the training process itself (e.g., failure to converge), rather than getting stuck in a local minimum.A significant challenge in neural network design is the running time required to train the network.It is not uncommon to require weeks to train neural networks in the text and image domains.In recent years, advances in hardware technology such as Graphics Processor Units (GPUs) have helped to a significant extent.GPUs are specialized hardware processors that can significantly speed up the kinds of operations commonly used in neural networks.In this sense, some algorithmic frameworks like Torch are particularly convenient because they have GPU support tightly integrated into the platform.Although algorithmic advancements have played a role in the recent excitement around deep learning, a lot of the gains have come from the fact that the same algorithms can do much more on modern hardware.Faster hardware also supports algorithmic development, because one needs to repeatedly test computationally intensive algorithms to understand what works and what does not.For example, a recent neural model such as the long shortterm memory has changed only modestly [150] since it was first proposed in 1997 [204].Yet, the potential of this model has been recognized only recently because of the advances in computational power of modern machines and algorithmic tweaks associated with improved experimentation.One convenient property of the vast majority of neural network models is that most of the computational heavy lifting is front loaded during the training phase, and the prediction phase is often computationally efficient, because it requires a small number of operations (depending on the number of layers).This is important because the prediction phase is often far more time-critical compared to the training phase.For example, it is far more important to classify an image in real time (with a pre-built model), although the actual building of that model might have required a few weeks over millions of images.Methods have also been designed to compress trained networks in order to enable their deployment in mobile and space-constrained settings.These issues are discussed in Chapter 3.Even though the biological metaphor sounds like an exciting way to intuitively justify the computational power of a neural network, it does not provide a complete picture of the settings in which neural networks perform well.At its most basic level, a neural network is a computational graph that performs compositions of simpler functions to provide a more complex function.Much of the power of deep learning arises from the fact that repeated composition of multiple nonlinear functions has significant expressive power.Even though the work in [208] shows that the single composition of a large number of squashing functions can approximate almost any function, this approach will require an extremely large number of units (i.e., parameters) of the network.This increases the capacity of the network, which causes overfitting unless the data set is extremely large.Much of the power of deep learning arises from the fact that the repeated composition of certain types of functions increases the representation power of the network, and therefore reduces the parameter space required for learning.Not all base functions are equally good at achieving this goal.In fact, the nonlinear squashing functions used in neural networks are not arbitrarily chosen, but are carefully designed because of certain types of properties.For example, imagine a situation in which the identity activation function is used in each layer, so that only linear functions are computed.In such a case, the resulting neural network is no stronger than a single-layer, linear network: Theorem 1.5.1 A multi-layer network that uses only the identity activation function in all its layers reduces to a single-layer network performing linear regression.Proof: Consider a network containing k hidden layers, and therefore contains a total of (k + 1) computational layers (including the output layer).The corresponding (k + 1) weight matrices between successive layers are denoted by W 1 . . .W k+1 .Let x be the d-dimensional column vector corresponding to the input, h 1 . . .h k be the column vectors corresponding to the hidden layers, and o be the m-dimensional column vector corresponding to the output.Then, we have the following recurrence condition for multi-layer networks:In all the cases above, the activation function Φ(•) has been set to the identity function.Then, by eliminating the hidden layer variables, it is easy to show the following:Note that one can replace the matrix W 1 W 2 . . .W k+1 with the new d × m matrix W xo , and learn the coefficients of W xo instead of those of all the matrices W 1 , W 2 . . .W k+1 , without loss of expressivity.In other words, we have the following:However, this condition is exactly identical to that of linear regression with multiple outputs [6].In fact, it is a bad idea to learn the redundant matrices W 1 . . .W k+1 instead of W xo , because doing so increases the number of parameters to be learned without increasing the power of the model in any way.Therefore, a multilayer neural network with identity activations does not gain over a single-layer network in terms of expressivity.The aforementioned result is for the case of regression modeling with numeric target variables.A similar result holds true for binary target variables.In the special case, where all layers use identity activation and the final layer uses a single output with sign activation for prediction, the multilayer neural network reduces to the perceptron.Lemma 1.5.1 Consider a multilayer network in which all hidden layers use identity activation and the single output node uses the perceptron criterion as the loss function and the sign activation for prediction.This neural network reduces to the single-layer perceptron.The proof of this result is almost identical to that of the one discussed above.In fact, as long as the hidden layers are linear, nothing is gained using the additional layers.This result shows that deep networks largely make sense only when the activation functions in intermediate layers are non-linear.Typically, the functions like sigmoid and tanh are squashing functions in which the output is bounded within an interval, and the gradients are largest near zero values.For large absolute values of their arguments, these functions are said to reach saturation where increasing the absolute value of the argument further does not change its value significantly.This type of function in which values do not vary significantly at large absolute values of their arguments is shared by another family of functions, referred to as Gaussian kernels, which are commonly used in non-parametric density estimation:The only difference is that Gaussian kernels saturate to 0 at large values of their argument, whereas functions like sigmoid and tanh can also saturate to values of +1 and −1.It is well known in the literature on density estimation [451] that the sum of many small Gaussian kernels can be used to approximate any density function.Density functions have a special nonnegative structure in which extremes of the data distribution always saturate to zero density, and therefore the underlying kernels also show the same behavior.A similar principle holds true (more generally) for squashing functions in which the linear combination of many small activation functions can be used to approximate an arbitrary function; however, squashing functions do not saturate to zero in order to handle arbitrary behavior at extreme values.The universal approximation result of neural networks [208] posits that a linear combination of sigmoid units (and/or most other reasonable squashing functions) in a single hidden layer can be used to approximate any function well.Note that the linear combination can be performed by a single output node.Therefore, a two-layer network is sufficient as long as the number of hidden units is large enough.However, some kind of basic non-linearity in the activation function is always required in order to model the turns and twists in an arbitrary function.To understand this point, note that all 1-dimensional functions can be approximated as a sum of scaled/translated step functions and most of the activation functions discussed in this chapter (e.g., sigmoid) look awfully like step functions (see Figure 1.8).This basic idea is the essence of the universal approximation theorem of neural networks.In fact, the proof of the ability of squashing functions to approximate any function is conceptually similar to that of kernels at least at an intuitive level.However, the number of base functions required to reach a high level of approximation can be extremely large in both cases, potentially increasing the data-centric requirements to an unmanageable level.For this reason, shallow networks face the persistent problem of overfitting.The universal approximation theorem asserts the ability to well-approximate the function implicit in the training data, but makes no guarantee about whether the function can be generalized to unseen test data.The previous section provides a concrete proof of the fact that a neural network with only linear activations does not gain from increasing the number of layers in it.For example, consider the two-class data set illustrated in Figure 1.14, which is represented in two dimensions denoted by x 1 and x 2 .There are two instances, A and B, of the class denoted by '*' with coordinates (1, 1) and (−1, 1), respectively.There is also a single instance B of the class denoted by '+' with coordinates (0, 1), A neural network with only linear activations will never be able to classify the training data perfectly because the points are not linearly separable.On the other hand, consider a situation in which the hidden units have ReLU activation, and they learn the two new features h 1 and h 2 , which are as follows:Note that these goals can be achieved by using appropriate weights from the input to hidden layer, and also applying a ReLU activation unit.The latter achieves the goal of thresholding negative values to 0. We have indicated the corresponding weights in the neural network shown in Figure 1.14.We have shown a plot of the data in terms of h 1 and h 2 in the same figure.The coordinates of the three points in the 2-dimensional hidden layer are {(1, 0), (0, 1), (0, 0)}.It is immediately evident that the two classes become linearly separable in terms of the new hidden representation.In a sense, the task of the first layer was representation learning to enable the solution of the problem with a linear classifier.Therefore, if we add a single linear output layer to the neural network, it will be able to The key point is that the use of the nonlinear ReLU function is crucial in ensuring this linear separability.Activation functions enable nonlinear mappings of the data, so that the embedded points can become linearly separable.In fact, if both the weights from hidden to output layer are set to 1 with a linear activation function, the output O will be defined as follows:This simple linear function separates the two classes because it always takes on the value of 1 for the two points labeled '*' and takes on 0 for the point labeled '+'.Therefore, much of the power of neural networks is hidden in the use of activation functions.The weights shown in Figure 1.14 will need to be learned in a data-driven manner, although there are many alternative choices of weights that can make the hidden representation linearly separable.Therefore, the learned weights may be different than the ones shown in Figure 1.14 if actual training is performed.Nevertheless, in the case of the perceptron, there is no choice of weights at which one could hope to classify this training data set perfectly because the data set is not linearly separable in the original space.In other words, the activation functions enable nonlinear transformations of the data, that become increasingly powerful with multiple layers.A sequence of nonlinear activations imposes a specific type of structure on the learned model, whose power increases with the depth of the sequence (i.e., number of layers in the neural network).Another classical example is the XOR function in which the two points {(0, 0), (1, 1)} belong to one class, and the other two points {(1, 0), (0, 1)} belong to the other class.It is possible to use ReLU activation to separate these two classes as well, although bias neurons will be needed in this case (see Exercise 1).The original backpropagation paper [409] discusses the XOR function, because this function was one of the motivating factors for designing multilayer networks and the ability to train them.The XOR function is considered a litmus test to determine the basic feasibility of a particular family of neural networks to properly predict nonlinearly separable classes.Although we have used the ReLU activation function above for simplicity, it is possible to use most of the other nonlinear activation functions to achieve the same goals.The basic idea of deep learning is that repeated composition of functions can often reduce the requirements on the number of base functions (computational units) by a factor that is exponentially related to the number of layers in the network.Therefore, even though the number of layers in the network increases, the number of parameters required to approximate the same function reduces drastically.This increases the generalization power of the network.The idea behind deeper architectures is that they can better leverage repeated regularities in the data patterns in order to reduce the number of computational units and therefore generalize the learning even to areas of the data space where one does not have examples.Often these repeated regularities are learned by the neural network within the weights as the basis vectors of hierarchical features.Although a detailed proof [340] of this fact is beyond the scope of this book, we provide a simple example to elucidate this point.Consider a situation in which a 1-dimensional function is defined by 1024 repeated steps of the same size and height.A shallow network with one hidden layer and step activation functions would require at least 1024 units in order to model the function.However, a multilayer network would model a pattern of 1 step in the first layer, 2 steps in the next, 4 steps in the third, and 2 r steps in the rth layer.This situation is illustrated in Figure 1.15.Note that the pattern of 1 step is the simplest feature because it is repeated 1024 times, whereas a pattern of 2 steps is more complex.Therefore, the features (and the functions learned) in successive layers are hierarchically related.In this case, a total of 10 layers are required and a small number of constant nodes are required in each layer to model the joining of the two patterns from the previous layer.Another way to understand this point is as follows.Consider a 1-dimensional function which takes one the value of 1 and −1 in alternate intervals, and this value switches 1024 times at regular intervals of the argument.The only way to simulate this function with a linear combination of step activation functions (containing only one switch in value) is to use 1024 of them (or a small constant factor of this number).However, a neural network with 10 hidden layers and only 2 units per layer has 2 10 = 1024 paths from the source to the output.As long as the function to be learned is regular in some way, it is often possible to learn parameters for the layers so that these 1024 paths are able to capture the complexity of 1024 different value switches in the function.The earlier layers learn more detailed patterns, whereas the later layers learn higher-level patterns.Therefore, the overall number of nodes required is an order of magnitude less than that required in the singlelayer network.This means that the amount of data required for learning is also an order of magnitude less.The reason for this is that the multilayer network implicitly looks for the repeated regularities and learns them with less data, rather than trying to explicitly learn every turn and twist of the target function.When using convolutional neural networks with image data, this behavior becomes intuitively obvious in which earlier layers model simple features like lines, a middle layer might model elementary shapes, and a later layer might model a complex shape like a face.On the other hand, a single layer would have difficulty in modeling every twist and turn of a face.This provides the deeper model with better generalization power and also the ability to learn with less data.However, increasing the depth of the network is not without its disadvantages.Deeper networks are often harder to train, and they show all types of unstable behavior such as the vanishing and exploding gradient problems.Deep networks are also notoriously unstable to parameter choice.These issues are often addressed with careful design of the functions computed within nodes, as well as the use of pretraining procedures to improve performance.The aforementioned discussion provides an overview of the most common ways in which the operations and structures of typical neural networks are constructed.However, there are many variations of this common theme.The following will discuss some of these variations.In general, there is a heavy emphasis on layer-wise feed-forward networks in the neural network domain with a sequential arrangement between input, hidden, and output layers.In other words, all input nodes feed into the first hidden layer, the hidden layers successively feed into one another, and the final hidden layer feeds into the output layer.The compu-Figure 1.16:An example of an unconventional architecture in which inputs occur to layers other than the first hidden layer.As long as the neural network is acyclic (or can be transformed into an acyclic representation), the weights of the underlying computation graph can be learned using dynamic programming (backpropagation).tational units are often defined by squashing functions applied to linear combinations of input.The hidden layer generally does not take inputs, and the loss is generally not computed over the values in the hidden layers.Because of this focus, it is easy to forget that a neural network can be defined as any type of parameterized computation graph, where these restrictions are not necessary for the backpropagation algorithm to work.In general, it is possible to have input and loss computation in intermediate layers, although this is less common.For example, a neural network is proposed in [515] that is inspired by the notion of a random forest [49], and it allows input in different layers of the network.An example of this type of network is shown in Figure 1.16.In this case, it is clear that the distinction between the input layers and the hidden layers has been blurred.In other variations of the basic feed-forward architecture, loss functions are computed not just at the output nodes, but also at the hidden nodes.The contributions at the hidden nodes are often in the form of penalties that act as regularizers.For example, these types of methods are used to perform sparse feature learning by imposing penalties on the hidden nodes (cf.Chapters 2 and 4).In this case, the distinction between the hidden layers and output layers is blurred.Another recent example of a design choice is the use of skip connections [184] in which the inputs from a particular layer are allowed to connect to layers beyond the immediate next layer.This type of approach leads to truly deep models.For example, a 152-layer architecture, referred to as ResNet [184], has reached human-level performance in the image recognition task.Although this architecture does not blur the distinction between input, hidden, and output layers, its structure differs from a traditional feed-forward network in which connections are placed only between successive layers.These networks have an iterative view of feature engineering [161], in which the features in later layers are iterative refinements of those in previous layers.In contrast, the traditional approach to feature engineering is hierarchical, in which features in later layers are increasingly abstract representations obtained from those in previous layers.Some neural networks like long short-term memory and convolutional neural networks define various types of multiplicative "forgetting," convolution, and pooling operations between variables that are not strictly in any of the forms discussed in this chapter.In fact, these architectures are now used so heavily in the text and image domains that they are no longer considered unusual.Another unique type of architecture is the sum-product network [383].In this case, the nodes are either summation nodes or product nodes.Summation nodes are similar to the traditional linear transformation with a set of weighted edges.However, the weights are constrained to be positive.The product nodes simply multiply its inputs without the need for weights.It is noteworthy that there are many variations in terms of how products can be computed.For example, if the inputs are two scalars, then one can simply compute their product.If the inputs are two vectors of equal length, one can compute their element-wise product.Several deep learning libraries do support these types of product operations.It is natural for the summation layers and the product layers to alternate in order to maximize expressivity.Sum-product networks are quite expressive, and it is often possible to build deep variations with a high level of expressivity [30,93].A key point is that almost any mathematical function can be approximately written as a polynomial function of its inputs.Therefore, almost any function can be expressed using the sum-product architecture, although deeper architectures allow modeling with greater structure.Unlike traditional neural networks in which nonlinearity is incorporated with activation functions, the product operation is the key to nonlinearity in the sum-product network.It is often helpful to be flexible in using different types of computational operations within the nodes beyond the known transformations and activation functions.Furthermore, the connections between nodes need not be structured in layer-wise fashion and nodes in the hidden layer can be included in the loss computation.As long as the underlying computational graph is acyclic, it is easy to generalize the backpropagation algorithm to any type of architecture and computational operation.After all, a dynamic programming algorithm (like backpropagation) can be used on virtually any type of directed acyclic graph in which multiple nodes can be used for initializing the dynamic programming recursion.It is important to keep in mind that architectures that are designed with a proper domain-specific understanding can often provide superior results to black-box methods that use fully connected feed-forward networks.There are several types of neural architectures that are used commonly in various machine learning applications.This section will provide a brief overview of some of these architectures, which will be discussed in greater detail in later chapters.Most of the basic machine learning models like linear regression, classification, support vector machines, logistic regression, singular value decomposition, and matrix factorization can be simulated with shallow neural networks containing no more than one or two layers.It is instructive to explore these basic architectures, because it indirectly showcases the power of neural networks; most of what we know about machine learning can be simulated with relatively simple models!Furthermore, many basic neural network models like the Widrow-Hoff learning model are directly related to traditional machine learning models like the Fisher's discriminant, even though they were proposed independently.A noteworthy observation is that deeper architectures are often created by stacking these simpler models in a creative way.The neural architectures for basic machine learning models are discussed in Chapter 2. A number of applications to text mining, graphs, and recommender systems will also be discussed in this chapter.Radial basis function (RBF) networks represent the forgotten architecture from the rich history of neural networks.They are not commonly used in the modern era, although they do have significant potential for specific types of problems.One limiting issue is that these networks are not deep, and they typically use only two layers.The first layer is constructed in an unsupervised way, whereas the second layer is trained using supervised methods.These networks are fundamentally different from feed-forward networks, and gain their power from the larger number of nodes in the unsupervised layer.The basic principles of using RBF networks are fundamentally very different from those of feed-forward networks, in the sense that the former gains its power from expanding the size of the feature space rather than depth.This approach is based on Cover's theorem on separability of patterns [84], which states that pattern classification problems are more likely to be linearly separable when cast into a high-dimensional space with a nonlinear transformation.The second layer of the network contains a prototype in each node and the activation is defined by the similarity of the input data to the prototype.These activations are then combined with trained weights of the next layer to create a final prediction.This approach is very similar to that of nearestneighbor classifiers, except that the weights in the second layer provide an additional level of supervision.In other words, the approach is a supervised nearest-neighbor method.Notably, support vector machines are known to be supervised variants of nearestneighbor classifiers in which a kernel function is combined with supervised weights to weight the neighboring points in the final prediction [6].Radial basis function networks can be used to simulate kernel methods like support vector machines.For specific types of problems like classification, one can use these architectures more effectively than an off-the-shelf kernel support vector machine.This is because these models are more general, providing more opportunities for experimentation than a kernel support vector machine.Furthermore, it is sometimes possible to gain some advantages from increased depth in the supervised layers.The full potential of radial basis function networks remains unexplored in the literature, because this architecture has largely been forgotten with the increased focus on vanilla feedforward methods.A discussion of radial basis function networks is provided in Chapter 5.Restricted Boltzmann machines (RBMs) use the notion of energy minimization in order to create neural network architectures for modeling data in an unsupervised way.These methods are particularly useful for creating generative models of the data, and they are closely related to probabilistic graphical models [251].Restricted Boltzmann machines owe their origins to the use of Hopfield networks [207], which can be used to store memories.Stochastic variants of these networks were generalized to Boltzmann machines, in which hidden layers modeled generative aspects of the data.Restricted Boltzmann machines are often used for unsupervised modeling and dimensionality reduction, although they can also be used for supervised modeling.However, since they were not naturally suited to supervised modeling, the supervised training was often preceded by an unsupervised phase.This naturally led to the discovery of the notion of pretraining, which was found to be extremely beneficial for supervised learning.RBMs were among the first models that were used for deep learning, especially in the unsupervised setting.The pretraining approach was eventually adopted by other types of models.Therefore, RBMs also have a historical significance in terms of motivating some training methodologies for deep models.The training process of a restricted Boltzmann machine is quite different from that of a feed-forward network.In particular, these models cannot be trained using backpropagation, and they require Monte Carlo sampling in order to perform the training.The particular algorithm that is used commonly for training an RBM is the contrastive divergence algorithm.A discussion of restricted Boltzmann machines is provided in Chapter 6.Recurrent neural networks are designed for sequential data like text sentences, time-series, and other discrete sequences like biological sequences.In these cases, the input is of the form x 1 . . .x n , where x t is a d-dimensional point received at the time-stamp t.For example, the vector x t might contain the d values at the tth tick of a multivariate time-series (with d different series).In a text-setting, the vector x t will contain the one-hot encoded word at the tth time-stamp.In one-hot encoding, we have a vector of length equal to the lexicon size, and the component for the relevant word has a value of 1.All other components are 0.An important point about sequences is that successive words are dependent on one another.Therefore, it is helpful to receive a particular input x t only after the earlier inputs have already been received and converted into a hidden state.The traditional type of feedforward network in which all inputs feed into the first layer does not achieve this goal.Therefore, the recurrent neural network allows the input x t to interact directly with the hidden state created from the inputs at previous time stamps.The basic architecture of the recurrent neural network is illustrated in Figure 1.17(a).The key point is that there is an input x t at each time-stamp, and a hidden state h t that changes at each time stamp as new data points arrive.Each time-stamp also has an output value y t .For example, in a time-series setting, the output y t might be the forecasted prediction of x t+1 .When used in the text-setting of predicting the next word, this approach is referred to as language modeling.In some applications, we do not output y t at each time stamp, but only at the end of the sequence.For example, if one is trying the classify the sentiment of a sentence as "positive" or "negative," the output will occur only at the final time stamp.The hidden state at time t is given by a function of the input vector at time t and the hidden vector at time (t − 1):) is used to learn the output probabilities from the hidden states.Note that the functions f (•) and g(•) are the same at each time stamp.The implicit assumption is that the time-series exhibits a certain level of stationarity; the underlying properties do not change with time.Although this property is not exactly true in real settings, it is a good assumption to use for regularization.A key point here is the presence of the self-loop in Figure 1.17(a), which will cause the hidden state of the neural network to change after the input of each x t .In practice, one only works with sequences of finite length, and it makes sense to unfurl the loop into a "time-layered" network that looks more like a feed-forward network.This network is shown in Figure 1.17(b).Note that in this case, we have a different node for the hidden This representation is mathematically equivalent to Figure 1.17(a), but is much easier to comprehend because of its similarity to a traditional network.Note that unlike traditional feed-forward networks, the inputs also occur to intermediate layers in this unfurled network.The weight matrices of the connections are shared by multiple connections in the timelayered network to ensure that the same function is used at each time stamp.This sharing is the key to the domain-specific insights that are learned by the network.The backpropagation algorithm takes the sharing and temporal length into account when updating the weights during the learning process.This special type of backpropagation algorithm is referred to as backpropagation through time (BPTT).Because of the recursive nature of Equation 1.36, the recurrent network has the ability to compute a function of variable-length inputs.In other words, one can expand the recurrence of Equation 1.36 to define the function for h t in terms of t inputs.For example, starting at h 0 , which is typically fixed to some constant vector, we have, whereas h 2 is a function of both x 1 and x 2 .Since the output y t is a function of h t , these properties are inherited by y t as well.In general, we can write the following:Note that the function F t (•) varies with the value of t.Such an approach is particularly useful for variable-length inputs like text sentences.More details of recurrent neural networks are provided in Chapter 7; this chapter will also discuss the applications of recurrent neural networks in various domains.An interesting theoretical property of recurrent neural networks is that they are Turing complete [444].What this means is that given enough data and computational resources, a recurrent neural network can simulate any algorithm.In practice, however, this theoretical property is not useful because recurrent networks have significant practical problems with generalization for long sequences.The amount of data and the size of the hidden states required for longer sequences increases in a way that is not realistic.Furthermore, there are practical issues in finding the optimum choices of parameters because of the vanishing and exploding gradient problems.As a result, specialized variants of the recurrent neural network architecture have been proposed, such as the use of long short-term memory.These advanced architectures will also be discussed in Chapter 7. Furthermore, some advanced variants of the recurrent architecture, such as neural Turing machines, have shown improvements over the recurrent neural network in some applications.Convolutional neural networks are biologically inspired networks that are used in computer vision for image classification and object detection.The basic motivation for the convolutional neural network was obtained from Hubel and Wiesel's understanding [212] of the workings of the cat's visual cortex, in which specific portions of the visual field seemed to excite particular neurons.This broader principle was used to design a sparse architecture for convolutional neural networks.The first basic architecture based on this biological inspiration was the neocognitron, which was then generalized to the LeNet-5 architecture [279].In the convolutional neural network architecture, each layer of the network is 3-dimensional, which has a spatial extent and a depth corresponding to the number of features.The notion of depth of a single layer in a convolutional neural network is distinct 4 from the notion of depth in terms of the number of layers.In the input layer, these features correspond to the color channels like RGB (i.e., red, green, blue), and in the hidden channels these features represent hidden feature maps that encode various types of shapes in the image.If the input is in grayscale (like LeNet-5), then the input layer will have a depth of 1, but later layers will still be 3-dimensional.The architecture contains two types of layers, referred to as the convolution and subsampling layers, respectively.For the convolution layers, a convolution operation is defined, in which a filter is used to map the activations from one layer to the next.A convolution operation uses a 3-dimensional filter of weights with the same depth as the current layer but with a smaller spatial extent.The dot product between all the weights in the filter and any choice of spatial region (of the same size as the filter) in a layer defines the value of the hidden state in the next layer (after applying an activation function like ReLU).The operation between the filter and the spatial regions in a layer is performed at every possible position in order to define the next layer (in which the activations retain their spatial relationships from the previous layer).The connections in a convolutional neural network are very sparse, because any activation in a particular layer is a function of only a small spatial region in the previous layer.All layers other than the final set of two of three layers maintain their spatial structure.Therefore, it is possible to spatially visualize what parts of the image affect particular portions of the activations in a layer.The features in lower-level layers capture lines or other primitive shapes, whereas the features in higher-level layers capture more complex shapes like loops (which commonly occur in many digits).Therefore, later layers can create digits by composing the shapes in these intuitive features.This is a classical example of the way in which semantic insights about specific data domains are used to design clever architectures.In addition, a subsampling layer simply averages the values in the local regions of size 2 × 2 in order to compress the spatial footprints of the layers by a factor of 2. An illustration of the architecture of LeNet-5 is shown in Figure 1.18.In the early years, LeNet-5 was used by several banks to recognize hand-written numbers on checks.Convolutional neural networks have historically been the most successful of all types of neural networks.They are used widely for image recognition, object detection/localization, and even text processing.The performance of these networks has recently exceeded that of humans in the problem of image classification [184].Convolutional neural networks provide a very good example of the fact that architectural design choices in a neural network should be performed with semantic insight about the data domain at hand.In the particular case of the convolutional neural network, this insight was obtained by observing the biological workings of a cat's visual cortex, and heavily using the spatial relationships among pixels.This fact also provides some evidence that a further understanding of neuroscience might also be helpful for the development of methods in artificial intelligence.Pretrained convolutional neural networks from publicly available resources like ImageNet are often available for use in an off-the-shelf manner for other applications and data sets.This is achieved by using most of the pretrained weights in the convolutional network without any change except for the final classification layer.The weights of the final classification layer are learned from the data set at hand.The training of the final layer is necessary because the class labels in a particular setting may be different from those of ImageNet.Nevertheless, the weights in the early layers are still useful because they learn various types of shapes in the images that can be useful for virtually any type of classification application.Furthermore, the feature activations in the penultimate layer can even be used for unsupervised applications.For example, one can create a multidimensional representation of an arbitrary image data set by passing each image through the convolutional neural network and extracting the activations of the penultimate layer.Subsequently, any type of indexing can be applied to this representation for retrieving images that are similar to a specific target image.Such an approach often provides surprisingly good results in image retrieval because of the semantic nature of the features learned by the network.It is noteworthy that the use of pretrained convolutional networks is so popular that training is rarely started from scratch.Convolutional neural networks are discussed in detail in Chapter 8.Many deeper architectures with feed-forward architectures have multiple layers in which successive transformations of the inputs from the previous layer lead to increasingly sophisticated representations of the data.The values of each hidden layer for a particular input contain a transformed representation of the input point, which becomes increasingly informative about the target value we are trying to learn, as the layer gets closer to the output node.As shown in Section 1.5.1, appropriately transformed feature representations are more amenable to simple types of predictions in the output layer.This sophistication is a result of the nonlinear activations in intermediate layers.Traditionally, the sigmoid and tanh activations were the most popular choices in the hidden layers, but the ReLU activation has become increasingly popular in recent years because of the desirable property that it is better at avoiding the vanishing and exploding gradient problems (cf.Section 3.4.2 of Chapter 3).For classification, the final layer can be viewed as a relatively simple prediction layer which contains a single linear neuron in the case of regression, and is a sigmoid/sign function in the case of binary classification.More complex outputs might require multiple nodes.One way of viewing this division of labor between the hidden layers and final prediction layer is that the early layers create a feature representation that is more amenable to the task at hand.The final layer then leverages this learned feature representation.This division of labor is shown in Figure 1.19.A key point is that the features learned in the hidden layers are often (but not always) generalizable to other data sets and problem settings in the same domain (e.g., text, images, and so on).This property can be leveraged in various ways by simply replacing the output node(s) of a pretrained network with a different application-specific output layer (e.g., linear regression layer instead of sigmoid classification layer) for the data set and problem at hand.Subsequently, only the weights of the newly replaced output layer may need to be learned for the new data set and application, whereas the weights of other layers are fixed.The output of each hidden layer is a transformed feature representation of the data, in which the dimensionality of the representation is defined by the number of units in that layer.One can view this process as a kind of hierarchical feature engineering in which the features in earlier layers represent primitive characteristics of the data, whereas those in later layers represent complex characteristics with semantic significance to the class labels.Data represented in the terms of the features of later layers are often more well behaved (e.g., linearly separable) because of the semantic nature of the features learned by the transformation.This type of behavior is particularly evident in a visually interpretable way in some domains like convolutional neural networks for image data.In convolutional neural networks, the features in earlier layers capture detailed but primitive shapes like lines or edges from the data set of images.On the other hand, the features in later layers capture shapes of greater complexity like hexagons, honeycombs, and so forth, depending on the type of images provided as training data.Note that such semantically interpretable shapes often have closer correlations with class labels in the image domain.For example, almost any image will contain lines or edges, but images belonging to particular classes will be more likely to have hexagons or honeycombs.This property tends to make the representations of later layers easier to classify with simple models like linear classifiers.This process is illustrated in Figure 1.19.The features in earlier layers are used repeatedly as building blocks to create more complex features.This general principle of "putting together" simple features to create more complex features lies at the core of the successes achieved with neural networks.As it turns out, this property is also useful in leveraging pretrained models in a carefully calibrated way.The practice of using pretrained models is also referred to as transfer learning.A particular type of transfer learning, which is used commonly in neural networks, is that the data and structure available in a given data set are used to learn features for that entire domain.A classical example of this setting is that of text or image data.In text data, the representations of text words are created using standardized benchmark data sets like Wikipedia [594] and models like word2vec.These can be used in almost any text application, since the nature of text data does not change very much with the application.A similar approach is often used for image data, in which the ImageNet data set (cf. Section 1.8.2) is used to pretrain convolutional neural networks, and provide ready-to-use features.One can download a pretrained convolutional neural network model and convert any image data set into a multidimensional representation by passing the image through the pretrained network.Furthermore, if additional application-specific data is available, one can regulate the level of transfer learning depending on the amount of available data.This is achieved by fine-tuning a subset of the layers in the pretrained neural network with this additional data.If a small amount of application-specific data is available, one can fix the weights of the early layers to their pretrained values and fine-tune only the last few layers of the neural network.The early layers often contain primitive features, which are more easily generalizable to arbitrary applications.For example, in a convolutional neural network, the early layers learn primitive features like edges, which are useful across diverse images like trucks or carrots.On the other hand, the later layers contain complex features which might depend on the image collection at hand (e.g., truck wheel versus carrot top).Finetuning only the weights of the later layers makes sense in such cases.If a large amount of application-specific data is available, one can fine-tune a larger number of layers.Therefore, deep networks provide significant flexibility in terms of how transfer learning is done with pretrained neural network models.Several topics in deep learning have increasingly gained attention, and have had significant successes.Although some of these methods are limited by current computational considerations, their potential is quite significant.This section will discuss some of these topics.In general forms of artificial intelligence, the neural network must learn to take actions in ever-changing and dynamic situations.Examples include learning robots and self-driving cars.In these cases, a critical assumption is that the learning system has no knowledge of the appropriate sequence of actions up front, and it learns through reward-based reinforcement as it takes various actions.These types of learning correspond to dynamic sequences of actions that are hard to model using traditional machine learning methods.The key assumption here is that these systems are too complex to explicitly model, but they are simple enough to evaluate, so that a reward value can be assigned for each action of the learner.Imagine a setting in which one wishes to train a learning system to play a video game from scratch without any prior knowledge of the rules.Video games are excellent test beds for reinforcement learning methods because they are microcosms of living the "game" of life.As in real-world settings, the number of possible states (i.e., unique position in game) might be too large to even enumerate, and the optimal choice of move depends critically on the knowledge of what is truly important to model from a particular state.Furthermore, since one does not start with any knowledge of the rules, the learning system would need to collect the data through its actions much as a mouse explores a maze to learn its structure.Therefore, the collected data is highly biased by the user actions, which provides a particularly challenging landscape for learning.The successful training of reinforcement learning methods is a critical gateway for self-learning systems, which is the holy grail of artificial intelligence.Although the field of reinforcement learning was developed independently of the field of neural networks, the strong complementarity of the two fields has brought them together.Deep learning methods can be useful in learning feature representations from high-dimensional sensory inputs (e.g., the video screens of pixels in a video game or the screen of pixels in a robot's "vision").Furthermore, reinforcement learning methods are often used to support various types of neural network algorithms like attention mechanisms.Reinforcement learning methods are discussed in Chapter 9.An important aspect of neural networks is that the data storage and computations are tightly integrated.For example, the states in a neural network can be considered a type of transient memory, which behave much like the ever-changing registers in the central processing unit of a computer.But what if we want to construct a neural network where one can control where to read data from, and where to write the data to.This goal is achieved with the notion of attention and external memory.Attention mechanisms can be used in various applications like image processing where one focuses on small parts of the image to gain successive insights.These techniques are also used for machine translation.Neural networks that can tightly control access in reading and writing to an external memory are referred to as neural Turing machines [158] or memory networks [528].Although these methods are advanced variants of recurrent neural networks, they show significantly improved potential than their predecessors in terms of the types of problems they can handle.These methods are discussed in Chapter 10.Generative adversarial networks are a model of data generation that can create a generative model of a base data set by using an adversarial game between two players.The two players correspond to a generator and a discriminator.The generator takes Gaussian noise as input and produces an output, which is a generated sample like the base data.The discriminator is typically a probabilistic classifier like logistic regression whose job is to distinguish real samples from the base data set and the generated sample.The generator tries to create samples that are as realistic as possible; its job is to fool the discriminator, whereas the job of the discriminator is to identify the fake samples irrespective of how well the generator tries to fool it.The problem can be understood as an adversarial game between the generator and discriminator, and the formal optimization model is a minimax learning problem.The Nash equilibrium of this minimax game provides the final trained model.Typically, this equilibrium point is one at which the discriminator is unable to distinguish between real and fake samples.Such methods can create realistic fantasy samples using a base data set, and are used commonly in the image domain.For example, if the approach is trained using a data set containing images of bedrooms, it will produce realistic looking bedrooms that are not actually a part of the base data.Therefore, the approach can be used for artistic or creative endeavors.These methods can also be conditioned on specific types of context, which could be any type of object such as label, text caption, or an image with missing details.In these cases, pairs of related training objects are used.A typical pair could be a caption (context) and an image (base object).Similarly, one might have pairs corresponding to sketches of objects and actual photographs.Therefore, starting with a captioned image data set of various types of animals, it is possible to create a fantasy image that is not a part of the base data by using a contextual caption such as "blue bird with sharp claws."Similarly, starting with an artist's sketch of a purse, the approach can create a realistic and colored image of a purse.Generative adversarial networks are discussed in Chapter 10.The benchmarks used in the neural network literature are dominated by data from the domain of computer vision.Although traditional machine learning data sets like the UCI repository [601] can be used for testing neural networks, the general trend is towards using data sets from perceptually oriented data domains that can be visualized well.Although there are a variety of data sets drawn from the text and image domains, two of them stand out because of their ubiquity in deep learning papers.Although both are data sets drawn from computer vision, the first of them is simple enough that it can also be used for testing generic applications beyond the field of vision.In the following, we provide a brief overview of these two data sets.The MNIST database, which stands for Modified National Institute of Standards and Technology database, is a large database of handwritten digits [281].As the name suggests, this data set was created by modifying an original database of handwritten digits provided by NIST.The data set contains 60,000 training images and 10,000 testing images.Each image is a scan of a handwritten digit from 0 to 9, and the differences between different images are a result of the differences in the handwriting of different individuals.These individuals were American Census Bureau employees and American high school students.The original black and white images from NIST were size normalized to fit in a 20 × 20 pixel box while preserving their aspect ratio and centered in a 28 × 28 image by computing the center of mass of the pixels.The images were translated to position this point at the center of the 28 × 28 field.Each of these 28 × 28 pixel values takes on a value from 0 to 255, depending on where it lies in the grayscale spectrum.The labels associated with the images correspond to the ten digit values.Examples of the digits in the MNIST database are illustrated in Figure 1.20.The size of the data set is rather small, and it contains only a simple object corresponding to a digit.Therefore, one might argue that the MNIST database is a toy data Figure 1.20: Examples of handwritten digits in the MNIST database set.However, its small size and simplicity is also an advantage because it can be used as a laboratory for quick testing of machine learning algorithms.Furthermore, the simplification of the data set by virtue of the fact that the digits are (roughly) centered makes it easy to use it to test algorithms beyond computer vision.Computer vision algorithms require specialized assumptions such as translation invariance.The simplicity of this data set makes these assumptions unnecessary.It has been remarked by Geoff Hinton [600] that the MNIST database is used by neural network researchers in much the same way as biologists use fruit flies for early and quick results (before serious testing on more complex organisms).Although the matrix representation of each image is suited to a convolutional neural network, one can also convert it into a multidimensional representation of 28 × 28 = 784 dimensions.This conversion loses some of the spatial information in the image, but this loss is not debilitating (at least in the case of the MNIST data set) because of its relative simplicity.In fact, the use of a simple support vector machine on the 784-dimensional representation can provide an impressive error rate of about 0.56%.A straightforward 2-layer neural network on the multidimensional representation (without using the spatial structure in the image) generally does worse than the support vector machine across a broad range of parameter choices!A deep neural network without any special convolutional architecture can achieve an error rate of 0.35% [72].Deeper neural networks and convolutional neural networks (that do use spatial structure) can reduce the error rate to as low as 0.21% by using an ensemble of five convolutional networks [402].Therefore, even on this simple data set, one can see that the relative performance of neural networks with respect to traditional machine learning is sensitive to the specific architecture used in the former.Finally, it should be noted that the 784-dimensional non-spatial representation of the MNIST data is used for testing all types of neural network algorithms beyond the domain of computer vision.Even though the use of the 784-dimensional (flattened) representation is not appropriate for a vision task, it is still useful for testing the general effectiveness of non-vision oriented (i.e., generic) neural network algorithms.For example, the MNIST data is frequently used to test generic autoencoders and not just convolutional ones.Even when the non-spatial representation of an image is used to reconstruct it with an autoencoder, one can still visualize the results with the original spatial positions of the reconstructed pixels to obtain a feel of what the algorithm is doing with the data.This visual exploration often gives the researcher some insights that are not available with arbitrary data sets like those obtained from the UCI Machine Learning Repository [601].In this sense, the MNIST data set tends to have broader usability than many other types of data sets.The ImageNet database [581] is a huge database of over 14 million images drawn from 1000 different categories.Its class coverage is exhaustive enough that it covers most types of images that one would encounter in everyday life.This database is organized according to a WordNet hierarchy of nouns [329].The WordNet database is a data set containing the relationships among English words using the notion of synsets.The WordNet hierarchy has been successfully used for machine learning in the natural language domain, and therefore it is natural to design an image data set around these relationships.The ImageNet database is famous for the fact that an annual ImageNet Large Scale Visual Recognition Challenge (ILSVRC) [582] is held using this dataset.This competition has a very high profile in the vision community and receives entries from most major research groups in computer vision.The entries to this competition have resulted in many of the state-of-the-art image recognition architectures today, including the methods that have surpassed human performance on some narrow tasks like image classification [184].Because of the wide availability of known results on these data sets, it is a popular alternative for benchmarking.We will discuss some of the state-of-the-art algorithms submitted to the ImageNet competition in Chapter 8 on convolutional neural networks.Another important significance of the ImageNet data set is that it is large and diverse enough to be representative of the key visual concepts within the image domain.As a result, convolutional neural networks are often trained on this data set; the pretrained network can be used to extract features from an arbitrary image.This image representation is defined by the hidden activations in the penultimate layer of the neural network.Such an approach creates new multidimensional representations of image data sets that are amenable for use with traditional machine learning methods.One can view this approach as a kind of transfer learning in which the visual concepts in the ImageNet data set are transferred to unseen data objects for other applications.Although a neural network can be viewed as a simulation of the learning process in living organisms, a more direct understanding of neural networks is as computational graphs.Such computational graphs perform recursive composition of simpler functions in order to learn more complex functions.Since these computational graphs are parameterized, the problem generally boils down to learning the parameters of the graph in order to optimize a loss function.The simplest types of neural networks are often basic machine learning models like least-squares regression.The real power of neural networks is unleashed by using more complex combinations of the underlying functions.The parameters of such networks are learned by using a dynamic programming method, referred to as backpropagation.There are several challenges associated with learning neural network models, such as overfitting and training instability.In recent years, numerous algorithmic advancements have reduced these problems.The design of deep learning methods in specific domains such as text and images requires carefully crafted architectures.Examples of such architectures include recurrent neural networks and convolutional neural networks.For dynamic settings in which a sequence of decisions need to be learned by a system, methods like reinforcement learning are useful.A proper understanding of neural network design requires a solid understanding of machine learning algorithms, and especially the linear models based on gradient descent.The reader is recommended to refer to [2,3,40,177] for basic knowledge on machine learning methods.Numerous surveys and overviews of neural networks in different contexts may be found in [27,28,198,277,345,431]. Classical books on neural networks for pattern recognition may be found in [41,182], whereas more recent perspectives on deep learning may be found in [147].A recent text mining book [6] also discusses recent advances in deep learning for text analytics.An overview of the relationships between deep learning and computational neuroscience may be found in [176,239].The perceptron algorithm was proposed by Rosenblatt [405].To address the issue of stability, the pocket algorithm [128], the Maxover algorithm [523], and other margin-based methods [123].Other early algorithms of a similar nature included the Widrow-Hoff [531] and the Winnow algorithms [245].The Winnow algorithm uses multiplicative updates instead of additive updates, and is particularly useful when many features are irrelevant.The original idea of backpropagation was based on the idea of differentiation of composition of functions as developed in control theory [54,237].The use of dynamic programming to perform gradient-based optimization of variables that are related via a directed acyclic graph has been a standard practice since the sixties.However, the ability to use these methods for neural network training had not yet been observed at the time.In 1969, Minsky and Papert published a book on perceptrons [330], which was largely negative about the potential of being able to properly train multilayer neural networks.The book showed that a single perceptron had limited expressiveness, and no one knew how to train multiple layers of perceptrons anyway.Minsky was an influential figure in artificial intelligence, and the negative tone of his book contributed to the first winter in the field of neural networks.The adaptation of dynamic programming methods to backpropagation in neural networks was first proposed by Paul Werbos in his PhD thesis in 1974 [524].However, Werbos's work could not overcome the strong views against neural networks that had already become entrenched at the time.The backpropagation algorithm was proposed again by Rumelhart et al. in 1986 [408, 409].Rumelhart et al.'s work is significant for the beauty of its presentation, and it was able to address at least some of the concerns raised earlier by Minsky and Papert.This is one of the reasons that the Rumelhart et al. paper is considered very influential from the perspective of backpropagation, even though it was certainly not the first to propose the method.A discussion of the history of the backpropagation algorithm may be found in the book by Paul Werbos [525].At this point, the field of neural networks was only partially resurrected, as there were still problems with training neural networks.Nevertheless, pockets of researchers continued to work in the area, and had already set up most of the known neural architectures, such as convolution neural networks, recurrent neural networks, and LSTMs, before the year 2000.The accuracy of these methods was still quite modest because of data and computation limitations.Furthermore, backpropagation turned out to be less effective at training deeper networks because of the vanishing and exploding gradient problems.However, by this time, it was already hypothesized by several prominent researchers that existing algorithms would yield large performance improvements with increases in data, computational power, and algorithmic experimentation.The coupling of big data frameworks with GPUs turned out to be a boon for neural network research in the late 2000s.With reduced cycle times for experimentation enabled by increased computational power, tricks like pretraining started showing up in the late 2000s [198].The publicly obvious resurrection of neural networks occurred after the year 2011 with the resounding victories [255] of neural networks in deep learning competitions for image classification.The consistent victories of deep learning algorithms in these competitions laid the foundation for the explosion in popularity we see today.Notably, the differences of these winning architectures from the ones that were developed more than two decades earlier are modest (but essential).Paul Werbos was a pioneer of recurrent neural networks, and proposed the original version of backpropagation through time [526].The basics of the convolutional neural network were proposed in the context of the neocognitron in [127].This idea was then generalized to LeNet-5, which was one of the first convolutional neural networks.The ability of neural networks to perform universal function approximation is discussed in [208].The beneficial effect of depth on reducing the number of parameters is discussed in [340].The theoretical expressiveness of neural networks was recognized early in its development.For example, early work recognized that a neural network with a single hidden layer can be used to approximate any function [208].A further result is that certain neural architectures like recurrent networks are Turing complete [444].The latter means that neural networks can potentially simulate any algorithm.Of course, there are numerous practical issues associated with neural network training, as to why these exciting theoretical results do not always translate into real-world performance.The foremost problem among them is the data-hungry nature of shallow architectures, which is ameliorated with increased depth.Increased depth can be viewed as a form of regularization in which one is forcing the neural network to identify and learn repeating patterns in data points.Increased depth, however, makes the neural network harder to train from an optimization point of view.A discussion on some of these issues may be found in [41,140,147].An experimental evaluation showing the advantages of deeper architectures is provided in [267].Deep learning has a significant number of free video lectures available on resources such as YouTube and Coursera.TensorFlow [574] is also strongly oriented towards computational graphs, and is the framework proposed by Google.Torch [572] is written in a high-level language called Lua, and it is relatively friendly to use.In recent years, Torch has gained some ground compared to other frameworks.Support for GPUs is tightly integrated in Torch, which makes it relatively easy to deploy Torch-based applications on GPUs.Many of these frameworks contain pretrained models from computer vision and text mining, which can be used to extract features.Many off-the-shelf tools for deep learning are available from the DeepLearning4j repository [590].IBM has a PowerAI platform that offers many machine learning and deep learning frameworks on top of IBM Power Systems [599].Notably, as of the writing of this book, this platform also has a free edition available for certain uses.For the data set in Exercise 4, where the two features are denoted by (x 1 , x 2 ), define a new 1-dimensional representation z denoted by the following:Is the data set linearly separable in terms of the 1-dimensional representation corresponding to z? Explain the importance of nonlinear transformations in classification problems.6. Implement the perceptron in a programming language of your choice.Show that the derivative of the sigmoid activation function is at most 0.25, irrespective of the value of its argument.At what value of its argument does the sigmoid activation function take on its maximum value?8. Show that the derivative of the tanh activation function is at most 1, irrespective of the value of its argument.At what value of its argument does the tanh activation take on its maximum value?9. Consider a network with two inputs x 1 and x 2 .It has two hidden layers, each of which contain two units.Assume that the weights in each layer are set so that top unit in each layer applies sigmoid activation to the sum of its inputs and the bottom unit in each layer applies tanh activation to the sum of its inputs.Finally, the single output node applies ReLU activation to the sum of its two inputs.Write the output of this neural network in closed form as a function of x 1 and x 2 .This exercise should give you an idea of the complexity of functions computed by neural networks.10. Compute the partial derivative of the closed form computed in the previous exercise with respect to x 1 .Is it practical to compute derivatives for gradient descent in neural networks by using closed-form expressions (as in traditional machine learning)?11. Consider a 2-dimensional data set in which all points with x 1 > x 2 belong to the positive class, and all points with x 1 ≤ x 2 belong to the negative class.Therefore, the true separator of the two classes is linear hyperplane (line) defined by x 1 − x 2 = 0. Now create a training data set with 20 points randomly generated inside the unit square in the positive quadrant.Label each point depending on whether or not the first coordinate x 1 is greater than its second coordinate x 2 .(a) Implement the perceptron algorithm without regularization, train it on the 20 points above, and test its accuracy on 1000 randomly generated points inside the unit square.Generate the test points using the same procedure as the training points.(b) Change the perceptron criterion to hinge-loss in your implementation for training, and repeat the accuracy computation on the same test points above.Regularization is not used.(c) In which case do you obtain better accuracy and why?(d) In which case do you think that the classification of the same 1000 test instances will not change significantly by using a different set of 20 training points?Chapter 2"Simplicity is the ultimate sophistication."-Leonardoda VinciConventional machine learning often uses optimization and gradient-descent methods for learning parameterized models.Examples of such models include linear regression, support vector machines, logistic regression, dimensionality reduction, and matrix factorization.Neural networks are also parameterized models that are learned with continuous optimization methods.This chapter will show that a wide variety of optimization-centric methods in machine learning can be captured with very simple neural network architectures containing one or two layers.In fact, neural networks can be viewed as more powerful versions of these simple models, with this power being achieved by combining the basic models into a comprehensive neural architecture (i.e., computational graph).It is useful to show these parallels early on, as this allows the understanding of the design of a deep network as a composition of the basic units that one often uses in machine learning.Furthermore, showing this relationship provides an appreciation of the specific way in which traditional machine learning is different from neural networks, and of the cases in which one can hope to do better with neural networks.In many cases, minor variations of these simple neural network architectures (corresponding to traditional machine learning methods) provide useful variations of machine learning models that have not been studied elsewhere.In a sense, the number of ways in which one can combine the different elements of a computational graph is far greater than what is studied in traditional machine learning, even when shallow models are used.Complex or deep neural architectures are often an overkill in instances where only a small amount of data are available.Additionally, it is easier to optimize traditional machinelearning models in data-lean settings as these models are more interpretable.On the other hand, as the amount of data increases, neural networks have an advantage because they retain the flexibility to model more complex functions with the addition of neurons to the computational graph.Figure 2.1 illustrates this point.One way of viewing deep learning models is as a stacking of simpler models like logistic or linear regression.The coupling of a linear neuron with the sigmoid activation leads to logistic regression, which will be discussed in detail in this chapter.The coupling of a linear unit with sigmoid activation is also used 1 extensively for building complex neural networks.Therefore, it is natural to ask the following question [312]:Although many neural networks can be viewed in this way, this point of view does not fully capture the complexity and the style of thinking involved in deep learning models.For example, several models (such as recurrent neural networks or convolutional neural networks) perform this stacking in a particular way with a domain-specific understanding of the input data.Furthermore, the parameters of different units are sometimes shared in order to force the solution to obey specific types of properties.The ability to put together the basic units in a clever way is a key architectural skill required by practitioners in deep learning.Nevertheless, it is also important to learn the properties of the basic models in machine learning, since they are used repeatedly in deep learning as elementary units of computation.This chapter will, therefore, explore these basic models.It is noteworthy that there are close relationships between some of the earliest neural networks (e.g., perceptron and Widrow-Hoff learning) and traditional machine learning models (e.g., support vector machine and Fisher discriminant).In some cases, these relationships remained unnoticed for several years, as these models were proposed independently by different communities.As a specific example, the loss function of the L 2 -support vector machine was proposed by Hinton [190] in the context of a neural architecture in 1989.When used with regularization, the resulting neural network would behave identically to an L 2 -support vector machine.In comparison, Cortes and Vapnik's paper on the support vector machine [82] appeared several years later with an L 1 -loss function.These relationships are not surprising because the best way to define a shallow neural network is often closely related to a known machine learning algorithm.Therefore, it is important to explore these basic neural models in order to develop an integrated view of neural networks and traditional machine learning.This chapter will primarily discuss two classes of models for machine learning:1. Supervised models: The supervised models discussed in this chapter primarily correspond to linear models and their variants.These include methods like least-squares regression, support vector machines, and logistic regression.Multiclass variants of these models will also be studied.The unsupervised models discussed in this chapter primarily correspond to dimensionality reduction and matrix factorization.Traditional methods like principal component analysis can also be presented as simple neural network architectures.Minor variations of these models can provide reductions of vastly different properties, which will be discussed later.The neural network framework also provides a way of understanding the relationships between widely different unsupervised methods like linear dimensionality reduction, nonlinear dimensionality reduction, and sparse feature learning, thereby providing an integrated view of traditional machine learning algorithms.This chapter assumes that the reader has a basic familiarity with the classical machine learning models.Nevertheless, a brief overview of each model will also be provided to the uninitiated reader.The next section will discuss some basic models for classification and regression, such as least-squares regression, binary Fisher discriminant, support vector machine, and logistic regression.The multiway variants of these models will be discussed in Section 2.3.Feature selection methods for neural networks are discussed in Section 2.4.The use of autoencoders for matrix factorization is discussed in Section 2.5.As a specific application of simple neural architectures, the word2vec method is discussed in Section 2.6.Simple methods for creating node embeddings in graphs are introduced in Section 2.7.A summary is given in Section 2.8.In this section, we will discuss some basic architectures for machine learning models such as least-squares regression and classification.As we will see, the corresponding neural architectures are minor variations of the perceptron model in machine learning.The main difference is in the choice of the activation function used in the final layer, and the loss function used on these outputs.This will be a recurring theme throughout this chapter, where we will see that small changes in neural architectures can result in distinct models from traditional machine learning.Presenting traditional machine learning models in the form of neural architectures also helps one appreciate the true closeness among various machine learning models.Throughout this section, we will work with a single-layer network with d input nodes and a single output node.The coefficients of the connections from the d input nodes to the output node are denoted by W = (w 1 . . .w d ).Furthermore, the bias will not be explicitly shown because it can be seamlessly modeled as the coefficient of an additional dummy input with a constant value of 1.Let (X i , y i ) be a training instance, in which the observed value y i is predicted from the feature variables X i using the following relationship:Here, W is the d-dimensional coefficient vector learned by the perceptron.Note the circumflex on top of ŷi to indicate that it is a predicted value rather than an observed value.In general, the goal of training is to ensure that the prediction ŷi is as close as possible to the observed value y i .The gradient-descent steps of the perceptron are focused on reducing the number of misclassifications, and therefore the updates are proportional to the difference (y i − ŷi ) between the observed and predicted values based on Equation 1.33 of Chapter 1:A gradient-descent update that is proportional to the difference between the observed and predicted values is naturally caused by a squared loss function such as (y i − ŷi ) 2 .Therefore, one possibility is to consider the squared loss between the predicted and observed values as the loss function.This architecture is shown in Figure 2.3(a), and the output is a discrete value.However, the problem is that this loss function is discrete because it takes on the value of either 0 or 4. Such a loss function is not differentiable because of its staircase-like jumps.The perceptron is one of the few learning models in which the gradient-descent updates were proposed historically before the loss function was proposed.What differentiable objective function does the perceptron really optimize?The answer to this question may be found in Section 1.2.1.1 of Chapter 1 by observing that the updates are performed only for misclassified training instances (i.e., y i ŷi < 0), and may be written using the indicator function I(•) ∈ {0, 1} that takes on 1 when the condition in its argument is satisfied:This rewrite from Equation 2.2 to Equation 2.3 uses the fact that y i = (y i − ŷi )/2 for misclassified points, and one can absorb a constant factor of 2 within the learning rate.This update can be shown to be consistent with the loss function L i (specific to the ith training example) as follows:  In many discrete variable prediction settings, the output is often a predicted score (e.g., probability of class or the value of W • X i ), which is then converted into a discrete prediction.Nevertheless, the final prediction need not always be converted into a discrete value, and one can simply output the relevant score for the class (which is often used for computing the loss function anyway).The sign activation is rarely used in most neural-network implementations, as most class-variable predictions of neural-network implementations are continuous scores.One can, in fact, create an extended architecture for the perceptron (cf. Figure 2.2), in which both discrete and continuous values are output.However, since the discrete part is not relevant to the loss computation and most outputs are reported as scores anyway, one rarely uses this type of extended representation.Therefore, throughout the remainder of this book, the activation in the output node is based on the score output (and how the loss function is computed), rather than on how a test instance is predicted as a discrete value.In least-squares regression, the training data contains n different training pairs (X 1 , y 1 ) . . .(X n , y n ), where each X i is a d-dimensional representation of the data points, and each y i is a real-valued target.The fact that the target is real-valued is important, because the underlying problem is then referred to as regression rather than classification.Least-squares regression is the oldest of all learning problems, and the gradient-descent methods proposed by Tikhonov and Arsenin in the 1970s [499] are very closely related to the gradient-descent updates of Rosenblatt [405] for the perceptron algorithm.In fact, as we will see later, one can also use least-squares regression on binary targets by "pretending" that these targets are real-valued.The resulting approach is equivalent to the Widrow-Hoff learning algorithm, which is famous in the neural network literature as the second learning algorithm proposed after the perceptron.In least-squares regression, the target variable is related to the feature variables using the following relationship:Note the presence of the circumflex on top of ŷi to indicate that it is a predicted value.The bias is missing in the relationship of Equation 2.5.Throughout this section, it will be assumed that one of the features in the training data has a constant value of 1, and the coefficient of this dummy feature is the bias.This is a standard feature engineering trick borrowed from conventional machine learning.In neural networks, the bias is often represented with the use of a bias neuron (cf.Section 1.2.1 of Chapter 1) with a constant output of 1.Although the bias neuron is almost always used in real settings, we avoid showing it explicitly throughout this book in order to maintain simplicity in presentation.The error of the prediction, e i , is given by e i = (y i − ŷi ).Here, W = (w 1 . . .w d ) is a d-dimensional coefficient vector that needs to be learned so as to minimize the total squared error on the training data, which is n i=1 e 2 i .The portion of the loss that is specific to the ith training instance is given by the following:This loss can be simulated with the use of an architecture similar to the perceptron except that the squared loss is paired with the identity activation function.This architecture is shown in Figure 2.3(c), whereas the perceptron architecture is shown in Figure 2.3(a).Both the perceptron and least-squares regression have the same goal of minimizing the prediction error.However, since the loss function in classification is inherently discrete, the perceptron algorithm uses a smooth approximation of the desired goal.This results in the smoothed perceptron criterion shown in Figure 2.3(b).As we will see below, the gradient-descent update in least-squares regression is very similar to that in the perceptron, with the main difference being that real-valued errors are used in regression rather than discrete errors drawn from {−2, +2}.As in the perceptron algorithm, the stochastic gradient-descent steps are determined by computing the gradient of e 2 i with respect to W , when the training pair (X i , y i ) is presented to the neural network.This gradient can be computed as follows:Therefore, the gradient-descent updates for W are computed using the above gradient and step-size α:One can rewrite the above update as follows:It is possible to modify the gradient-descent updates of least-squares regression to incorporate forgetting factors.Adding regularization is equivalent to penalizing the loss function of least-squares classification with the additional term proportional to λ • ||W || 2 , where λ > 0 is the regularization parameter.With regularization, the update can be written as follows:Note that the update above looks identical to the perceptron update of Equation 2.2.The updates are, however, not exactly identical because of how the predicted value ŷi is computed in the two cases.In the case of the perceptron, the sign function is applied to W • X i in order to compute the binary value ŷi and therefore the error (y i − ŷi ) can only be drawn from {−2, +2}.In least-squares regression, the prediction ŷi is a real value without the application of the sign function.This observation naturally leads to the following question; what if we applied leastsquares regression directly to minimize the squared distance of the real-valued prediction ŷi from the observed binary targets y i ∈ {−1, +1}?The direct application of least-squares regression to binary targets is referred to as least-squares classification.The gradient-descent update is the same as the one shown in Equation 2.9, which looks identical to that of the perceptron.However, the least-squares classification method does not yield the same result as the perceptron algorithm, because the real-valued training errors (y i − ŷi ) in least-squares classification are computed differently from the integer error (y i − ŷi ) in the perceptron.This direct application of least-squares regression to binary targets is referred to as Widrow-Hoff learning.Following the perceptron, the Widrow-Hoff learning rule was proposed in 1960.However, the method was not a fundamentally new one, as it is a direct application of least-squares regression to binary targets.Although the sign function is applied to the real-valued prediction of unseen test instances to convert them to binary predictions, the error of training instances is computed directly using real-valued predictions (unlike the perceptron).Therefore, it is also referred to as least-squares classification or linear least-squares method [6].Remarkably, a seemingly unrelated method proposed in 1936, known as the Fisher discriminant, also reduces to Widrow-Hoff learning in the special case of binary targets.The Fisher discriminant is formally defined as a direction W along which the ratio of inter-class variance to the intra-class variance is maximized in the projected data.By choosing a scalar b in order to define the hyperplane W • X = b, it is possible to model the separation between the two classes.This hyperplane is used for classification.Although the definition of the Fisher discriminant seems quite different from least-squares regression/classification at first sight, a remarkable result is that the Fisher discriminant for binary targets is identical to the least-squares regression as applied to binary targets (i.e., least-squares classification).Both the data and the targets need to be mean-centered, which allows the bias variable b to be set to 0. Several proofs of this result are available in the literature [3,6,40,41].The neural architecture for classification with the Widrow-Hoff method is illustrated in Figure 2.3(c).The gradient-descent steps in both the perceptron and the Widrow-Hoff would be given by Equation 2.8, except for differences in how (y i − ŷi ) is computed.In the case of the perceptron, this value will always be drawn from {−2, +2}.In the case of Widrow-Hoff, these errors can be arbitrary real values, since ŷi is set to W • X i without using the sign function.This difference is important because the perceptron algorithm never penalizes a positive class point for W • X i being "too correct" (i.e., larger than 1), whereas using real-valued predictions to compute the error has the unfortunate effect of penalizing such points.The inappropriate penalization of over-performance is the Achilles heel of Widrow-Hoff learning and the Fisher discriminant [6].It is noteworthy that least-squares regression/classification, Widrow-Hoff learning, and the Fisher discriminant were proposed independently in very different eras and by different communities of researchers.Indeed, the Fisher discriminant, which is oldest of these methods and dates back to 1936, is often viewed as a method for finding class-sensitive directions rather than as a classifier.It can, however, also be used as a classifier by using the resulting direction W to create a linear prediction.The completely different origins and seemingly different motives of all these methods make the equivalence in their solutions all the more noticeable.The Widrow-Hoff learning rule is also referred to as Adaline, which is short for adaptive linear neuron.It is also referred to as the delta rule.To recap, the learning rule of Equation 2.8, when applied to binary targets in {−1, +1}, can be alternatively referred to as least-squares classification, least mean-squares algorithm (LMS), Fisher 2 discriminant classifier, the Widrow-Hoff learning rule, delta rule, or Adaline.Therefore, the family of least-squares classification methods has been rediscovered several times in the literature under different names and with different motivations.The loss function of the Widrow-Hoff method can be rewritten slightly from least-squares regression because of its binary responses:This type of encoding is possible when the target variable y i is drawn from {−1, +1} because we can use y 2 i = 1.It is helpful to convert the Widrow-Hoff objective function to this form because it can be more easily related to other objective functions like the perceptron and the support vector machine.For example, the loss function of the support vector machine is obtained by "repairing" the above loss so that over-performance is not penalized.One can repair the loss function by changing the objective function to [max{(1−ŷ i y i ), 0}] 2 , which was Hinton's L 2 -loss support vector machine (SVM) [190].Almost all the binary classification models discussed in this chapter can be shown to be closely related to the Widrow-Hoff loss function by using different ways of repairing the loss, so that over-performance is not penalized.The gradient-descent updates (cf.Equation 2.9) of least-squares regression can be rewritten slightly for Widrow-Hoff learning because of binary response variables:The second form of the update is helpful in relating it to perceptron and SVM updates, in each of which (1 − y i ŷi ) is replaced with an indicator variable that is a function of y i ŷi .This point will be discussed in a later section.The special case of least-squares regression and classification is solvable in closed form (without gradient-descent) by using the pseudo-inverse of the n × d training data matrix D, whose rows are X 1 . . .X n .Let the n-dimensional column vector of dependent variables be denoted by y = [y 1 . . .y n ] T .The pseudo-inverse of matrix D is defined as follows:Then, the row-vector W is defined by the following relationship:If regularization is incorporated, the coefficient vector W is given by the following:Here, λ > 0 is the regularization parameter.However, inverting a matrix like (D T D + λI) is typically done using numerical methods that require gradient descent anyway.One rarely inverts large matrices like D T D. In fact, the Widrow-Hoff updates provide a very efficient way of solving the problem without using the closed-form solution.Logistic regression is a probabilistic model that classifies the instances in terms of probabilities.Because the classification is probabilistic, a natural approach for optimizing the parameters is to ensure that the predicted probability of the observed class for each training instance is as large as possible.This goal is achieved by using the notion of maximumlikelihood estimation in order to learn the parameters of the model.The likelihood of the training data is defined as the product of the probabilities of the observed labels of each training instance.Clearly, larger values of this objective function are better.By using the negative logarithm of this value, one obtains an a loss function in minimization form.Therefore, the output node uses the negative log-likelihood as a loss function.This loss function replaces the squared error used in the Widrow-Hoff method.The output layer can be formulated with the sigmoid activation function, which is very common in neural network design.Let (X 1 , y 1 ), (X 2 , y 2 ), . . .(X n , y n ) be a set of n training pairs in which X i contains the d-dimensional features and y i ∈ {−1, +1} is a binary class variable.As in the case of a perceptron, a single-layer architecture with weights W = (w 1 . . .w d ) is used.Instead of using the hard sign activation on W • X i to predict y i , logistic regression applies the soft sigmoid function to W • X i in order to estimate the probability that y i is 1:For a test instance, it can be predicted to the class whose predicted probability is greater than 0.5.Note that P (y i = 1) is 0.5 when W • X i = 0, and X i lies on the separating hyperplane.Moving X i in either direction from the hyperplane results in different signs of W • X i and corresponding movements in the probability values.Therefore, the sign of W • X i also yields the same prediction as picking the class with probability larger than 0.5.We will now describe how the loss function corresponding to likelihood estimation is set up.This methodology is important because it is used widely in many neural models.For positive samples in the training data, we want to maximize P (y i = 1) and for negative samples, we want to maximize P (y i = −1).For positive samples satisfying y i = 1, one wants to maximize ŷi and for negative samples satisfying y i = −1, one wants to maximize 1 − ŷi .One can write this casewise maximization in the form of a consolidated expression of always maximizing |y i /2 − 0.5 + ŷi |.The products of these probabilities must be maximized over all training instances to maximize the likelihood L:Therefore, the loss function is set tofor each training instance, so that the product-wise maximization is converted to additive minimization over training instances.Additive forms of the objective function are particularly convenient for the types of stochastic gradient updates that are common in neural networks.The overall architecture and loss function is illustrated in Figure 2.3(d).For each training instance, the predicted probability ŷi is computed by passing it through the neural network, and the loss is used to determine the gradient for each training instance.Let the loss for the ith training instance be denoted by L i , which is also annotated in Equation 2.15.Then, the gradient of L i with respect to the weights in W can be computed as follows:Note that one can concisely write the above gradient as follows:Therefore, the gradient-descent updates of logistic regression are given by the following (including regularization):(2.17)Just as the perceptron and the Widrow-Hoff algorithms use the magnitudes of the mistakes to make updates, the logistic regression method uses the probabilities of the mistakes to make updates.This is a natural extension of the probabilistic nature of the loss function to the update.It is possible to implement the same model by using different choices of activation and loss in the output node as long as they combine to yield the same result.Instead of using sigmoid activation to create the output ŷi ∈ (0, 1), it is also possible to use identity activation to create the output ŷi ∈ (−∞, +∞), and then apply the following loss function:The alternative architecture for logistic regression is shown in Figure 2.3(e).For the final prediction of the test instance, the sign function can be applied to ŷi , which is equivalent to predicting it to the class for which its probability is greater than 0.5.This example shows that it is possible to implement the same model using different combinations of activation and loss functions, as long as they combine to yield the same result.One desirable property of using the identity activation to define ŷi is that it is consistent with how the loss functions of other models like the perceptron and Widrow-Hoff learning are defined.Furthermore, the loss function of Equation 2.18 contains the product of y i and ŷi as in other models.This makes it possible to directly compare the loss functions of various models, which will be explored later in this chapter.The loss function in support vector machines is closely related to that in logistic regression.However, instead of using a smooth loss function (like that in Equation 2.18), the hinge-loss is used instead.Consider the training data set of n instances denoted by (X 1 , y 1 ), (X 2 , y 2 ), . . .(X n , y n ).The neural architecture of the support-vector machine is identical to that of least-squares classification (Widrow-Hoff).The main difference is in the choice of loss function.As in the case of least-squares classification, the prediction ŷi for the training point X i is obtained by applying the identity activation function on W • X i .Here, W = (w 1 , . . .w d ) contains the vector of d weights for the d different inputs into the single-layer network.Therefore, the output of the neural network is ŷi = W • X i for computing the loss function, although a test instance is predicted by applying the sign function to the output.The loss function L i for the ith training instance in the support-vector machine is defined as follows:This loss is referred to as the hinge-loss, and the corresponding neural architecture is illustrated in Figure 2.3(f).The overall idea behind this loss function is that a positive training instance is only penalized for being less than 1, and a negative training instance is only penalized for being greater than −1.In both cases, the penalty is linear, and abruptly flattens out at the aforementioned thresholds.It is helpful to compare this loss function with the Widrow-Hoff loss value of (1 − y i ŷi ) 2 , in which predictions are penalized for being different from the target values.As we will see later, this difference is an important advantage for the support vector machine over the Widrow-Hoff loss function.In order to explain the difference in loss functions between the perceptron, Widrow-Hoff, logistic regression, and the support vector machine, we have shown the loss for a single positive training instance at different values of ŷi = W • X i in Figure 2.4.In the case of the perceptron, only the smoothed surrogate loss function (cf.Section 1.2.1.1 of Chapter 1) is shown.Since the target value is +1, the loss function shows diminishing The SVM loss is shifted from the perceptron (surrogate) loss by exactly one unit to the right;(ii) the logistic loss is a smooth variant of the SVM loss; (iii) the Widrow-Hoff/Fisher loss is the only case in which points are increasingly penalized for classifying points "too correctly" (i.e., increasing W • X beyond +1 for X in positive class).Repairing the Widrow-Hoff loss function by setting it to 0 for W • X > 1 yields the quadratic loss SVM [190].improvement by increasing W • X i beyond +1 in the case of logistic regression.In the case of the support-vector machine the hinge-loss function flattens out beyond this point.In other words, only misclassified points or points that are too close to the decision boundary W • X = 0 are penalized.The perceptron criterion is identical in shape to the hinge loss, except that it is shifted by one unit to the left.The Widrow-Hoff method is the only case in which a positive training point is penalized for having too large a positive value of W • X i .In other words, the Widrow-Hoff method penalizes points for being properly classified in a very strong way.This is a potential problem with the Widrow-Hoff objective function, in which well-separated points cause problems in training.The stochastic gradient-descent method computes the partial derivative of the point-wise loss function L i with respect to the elements in W .The gradient is computed as follows:Therefore, the stochastic gradient method samples a point and checks whether y i ŷi < 1.If this is the case, an update is performed that is proportional to y i X i :Here, I(•) ∈ {0, 1} is the indicator function that takes on the value of 1 when the condition in its argument is satisfied.This approach is the simplest version of the primal update forSVMs [448].The reader should also convince herself is that this update is identical to that of a (regularized) perceptron (cf.Equation 2.3), except that the condition for making this update in the perceptron is y i ŷi < 0. Therefore, a perceptron makes the update only when a point is misclassified, whereas the support vector machine also makes updates for points that are classified correctly, albeit not very confidently.This neat relationship is because the loss function of the perceptron criterion shown in Figure 2.4 is shifted from the hinge-loss in the SVM.To emphasize the similarities and differences in the loss functions used by the different methods, we tabulate the loss functions below:It is noteworthy that all the derived updates in this section typically correspond to stochastic gradient-descent updates that are encountered both in traditional machine learning and in neural networks.The updates are the same whether or not we use a neural architecture to represent the models for these algorithms.Our main point in going through this exercise is to show that rudimentary special cases of neural networks are instantiations of well-known algorithms in the machine learning literature.The key point is that with greater availability of data one can incorporate additional nodes and depth to increase the model's capacity, explaining the superior behavior of neural networks with larger data sets (cf. Figure 2.1).All the models discussed so far in this chapter are designed for binary classification.In this section, we will discuss how one can design multiway classification models by changing the architecture of the perceptron slightly, and allowing multiple output nodes.Consider a setting with k different classes.Each training instance (X i , c(i)) contains a ddimensional feature vector X i and the index c(i) ∈ {1 . . .k} of its observed class.In such a case, we would like to find k different linear separators W 1 . . .W k simultaneously so that the value of W c(i) • X i is larger than W r • X i for each r = c(i).This is because one always predicts a data instance X i to the class r with the largest value of W r • X i .Therefore, the loss function for the ith training instance in the case of the multiclass perceptron is defined as follows:The multiclass perceptron is illustrated in Figure 2.5(a).As in all neural network models, one can use gradient-descent in order to determine the updates.For a correctly classified instance, the gradient is always 0, and there are no updates.For a misclassified instance, the gradients are as follows: Therefore, the stochastic gradient-descent method is applied as follows.Each training instance is fed into the network.If the correct class r = c(i) receives the largest of output W r • X i , then no update needs to be executed.Otherwise, the following update is made to each separator W r for learning rate α > 0:Only two of the separators are always updated at a given time.In the special case that k = 2, these gradient updates reduce to the perceptron because both the separators W 1 and W 2 will be related asAnother quirk that is specific to the unregularized perceptron is that it is possible to use a learning rate of α = 1 without affecting the learning because the value of α only has the effect of scaling the weight when starting with W j = 0 (see Exercise 2).This property is, however, not true for other linear models in which the value of α does affect the learning.The Weston-Watkins SVM [529] varies on the multiclass perceptron in two ways:1.The multiclass perceptron only updates the linear separator of a class that is predicted most incorrectly along with the linear separator of the true class.On the other hand, the Weston-Watkins SVM updates the separator of any class that is predicted more favorably than the true class.In both cases, the separator of the observed class is updated by the same aggregate amount as the incorrect classes (but in the opposite direction).2.Not only does the Weston-Watkins SVM update the separator in the case of misclassification, it updates the separators in cases where an incorrect class gets a prediction that is "uncomfortably close" to the true class.This is based on the notion of margin.As in the case of the multiclass perceptron, it is assumed that the ith training instance is denoted by (X i , c(i)), where X i contains the d-dimensional feature variables, and c(i) contains the class index drawn from {1, . . ., k}.One wants to learn d-dimensional coefficients W 1 . . .W k of the k linear separators so that the class index r with the largest value of W r •X i is predicted to be the correct class c(i).The loss function L i for the ith training instance (X i , c(i)) in the Weston-Watkins SVM is as follows:The neural architecture of the Weston-Watkins SVM is illustrated in Figure 2.5(b).It is instructive to compare the objective function of the Weston-Watkins SVM (Equation 2.25) with that of the multiclass perceptron (Equation 2.22).First, for each class r = c(i), if the prediction W r • X i lags behind that of the true class by less than a margin amount of 1, then a loss is incurred for that class.Furthermore, the losses over all such classes r = c(i) are added, rather than taking the maximum of the losses.These two differences accomplish the two intuitive goals discussed above.In order to determine the gradient-descent updates, one can find the gradient of the loss function with respect to each W r .In the event that the loss function L i is 0, the gradient of the loss function is 0 as well.Therefore, no update is required when the training instance is classified correctly with sufficient margin with respect to the second-best class.However, if the loss function is non-zero we have either a misclassified or a "barely correct" prediction in which the second-best and best class prediction are not sufficiently separated.In such cases, the gradient of the loss is non-zero.The loss function of Equation 2.25 is created by adding up the contributions of the (k − 1) separators belonging to the incorrect classes.Let δ(r, X i ) be a 0/1 indicator function, which is 1 when the rth class separator contributes positively to the loss function in Equation 2.25.In such a case, the gradient of the loss function is as follows:This results in the following stochastic gradient-descent step for the rth separator W r at learning rate α:For training instances X i in which the loss L i is zero, the above update can be shown to simplify to a regularization update of each hyperplane W r :The regularization uses the parameter λ > 0. Regularization is considered essential to the proper functioning of a support vector machine.Multinomial logistic regression can be considered the multi-way generalization of logistic regression, just as the Weston-Watkins SVM is the multiway generalization of the binary SVM.Multinomial logistic regression uses negative log-likelihood loss, and is therefore a probabilistic model.As in the case of the multiclass perceptron, it is assumed that the input to the model is a training data set containing pairs of the form (X i , c(i)), where c(i) ∈ {1 . . .k} is the index of the class of d-dimensional data point X i .As in the case of the previous two models, the class r with the largest value of W r • X i is predicted to be the label of the data point X i .However, in this case, there is an additional probabilistic interpretation of W r • X i in terms of the posterior probability P (r|X i ) that the data point X i takes on the label r.This estimation can be naturally accomplished with the softmax activation function:In other words, the model predicts the class membership in terms of probabilities.The loss function L i for the ith training instance is defined by the cross-entropy, which is the negative logarithm of the probability of the true class.The neural architecture of the softmax classifier is illustrated in Figure 2.5(c).The cross-entropy loss may be expressed in terms of either the input features or in terms of the softmax pre-activation values v r = W r • X i as follows:Therefore, the partial derivative of L i with respect to v r can be computed as follows:The gradient of the loss of the ith training instance with respect to the separator of the rth class is computed by using the chain rule of differential calculus in terms of its pre-activation value v j = W j • X i :In the above simplification, we used the fact that v j has a zero gradient with respect to W r for j = r.The value of ∂Li ∂vr in Equation 2.35 can be substituted from Equation 2.34 to obtain the following result:Note that we have expressed the gradient indirectly using probabilities (based on Equation 2.29) both for brevity and for intuitive understanding of how the gradient is related to the probability of making different types of mistakes.Each of the terms [1 − P (r|X i )] and P (r|X i ) is the probability of making a mistake for an instance with label c(i) with respect to the predictions for the rth class.After including similar regularization impact as other models, the separator for the rth class is updated as follows:Here, α is the learning rate, and λ is the regularization parameter.The softmax classifier updates all the k separators for each training instance, unlike the multiclass perceptron and the Weston-Watkins SVM, each of which updates only a small subset of separators (or no separator) for each training instance.This is a consequence of probabilistic modeling, in which correctness is defined in a soft way.Consider a classification problem in which we have an extremely large number of classes.In such a case, learning becomes too slow, because of the large number of separators that need to be updated for each training instance.This situation can occur in applications like text mining, where the prediction is a target word.Predicting target words is particularly common in neural language models, which try to predict the next word given the immediate history of previous words.The cardinality of the number of classes will typically be larger than 10 5 in such cases.Hierarchical softmax is a way of improving learning efficiency by decomposing the classification problem hierarchically.The idea is to group the classes hierarchically into a binary tree-like structure, and then perform log 2 (k) binary classifications from the root to the leaf for k-way classification.Although the hierarchical classification can compromise the accuracy to some extent, the efficiency improvements can be significant.How is the hierarchy of classes obtained?The naïve approach is to create a random hierarchy.However, the specific grouping of classes has an effect on performance.Grouping similar classes tends to improve performance.It is possible to use domain-specific insights to improve the quality of the hierarchy.For example, if the prediction is a target word, one can use the WordNet hierarchy [329] to guide the grouping.Further reorganization may be needed [344] because the WordNet hierarchy is not exactly a binary tree.Another option is to use Huffman encoding in order to create the binary tree [325,327].Refer to the bibliographic notes for more pointers.One of the common refrains about neural networks has been their lack of interpretability [97].However, it turns out that one can use backpropagation in order to determine the features that contribute the most to the classification of a particular test instance.This provides the analyst with an understanding of the relevance of each feature to classification.This approach also has the useful property that it can be used for feature selection [406].Consider a test instance X = (x 1 , . . .x d ), for which the multilabel output scores of the neural network are o 1 . . .o k .Furthermore, let the output of the winning class among the k outputs be o m , where m ∈ {1 . . .k}.Our goal is to identify the features that are most relevant to the classification of this test instance.In general, for each attribute x i , we would like to determine the sensitivity of the output o m to x i .Features with large absolute magnitudes of this sensitivity are obviously relevant to the classification of this test instance.In order to achieve this goal, we would like to compute the absolute magnitude of ∂om ∂xi .The features with the largest absolute value of the partial derivative have the greatest influence on the classification to the winning class.The sign of this derivative also tells us whether increasing x i slightly from its current value increases or decreases the score of the winning class.For classes other than the winning class, the derivative also provides some understanding of the sensitivity, but this is less important, particularly when the number of classes is large.The value of ∂om ∂xi can be computed by a straightforward application of the backpropagation algorithm, in which one does not stop backpropagating at the first hidden layer but applies the process all the way to the input layer.One can also use this approach for feature selection by aggregating the absolute value of the gradient over all classes and all correctly classified training instances.The features with the largest aggregate sensitivity over the whole training data are the most relevant.Strictly speaking, one does not need to aggregate this value over all classes, but one can simply use only the winning class for correctly classified training instances.However, the original work in [406] aggregates this value over all classes and all instances.Similar methods for interpreting the effects of different portions of the input are also used in computer vision with convolutional neural networks [466].A discussion of some of these methods is provided in Section 8.5.1 of Chapter 8.In the case of computer vision, the visual effects of this type of saliency analysis are sometimes spectacular.For example, for an image of a dog, the analysis will tell us which features (i.e., pixels) results in the image being considered a dog.As a result, we can create a black-and-white saliency image in which the portion corresponding to a dog is emphasized in light color against a dark background (cf. Figure 8.12 of Chapter 8).Autoencoders represent a fundamental architecture that is used for various types of unsupervised learning, including matrix factorization, principal component analysis, and dimensionality reduction.Natural architectural variations of the autoencoder can also be used for matrix factorization of incomplete data to create recommender systems.Furthermore, some recent feature engineering methods in the natural language domain like word2vec can also be viewed as variations of autoencoders, which perform nonlinear matrix factorizations of word-context matrices.The nonlinearity is achieved with the activation function in the output layer, which is usually not available with traditional matrix factorization.Therefore, one of our goals will be to demonstrate how small changes to the underlying building blocks of the neural network can be used to implement sophisticated variations of a given family of methods.This is particularly convenient for the analyst, who only has to experiment with small variations of the architecture to test different types of models.Such variations would require more effort to construct in traditional machine learning, because one does not have the benefit of learning abstractions like backpropagation.First, we begin with a simple simulation of a traditional matrix factorization method with a shallow neural architecture.Then, we discuss how this basic setup provides the path to generalizations to nonlinear dimensionality reduction methods by adding layers and/or nonlinear activation functions.Therefore, the goal of this section is to show two things:1. Classical dimensionality reduction methods like singular value decomposition and principal component analysis are special cases of neural architectures.2. By adding different types of complexities to the basic architecture, one can generate complex nonlinear embeddings of the data.While nonlinear embeddings are also available in machine learning, neural architectures provide unprecedented flexibility in controlling the properties of the embedding by making various types of architectural changes (and allowing backpropagation to take care of the changes in the underlying learning algorithms).We will also discuss a number of applications such as recommender systems and outlier detection.The basic idea of an autoencoder is to have an output layer with the same dimensionality as the inputs.The idea is to try to reconstruct each dimension exactly by passing it through the network.An autoencoder replicates the data from the input to the output, and is therefore sometimes referred to as a replicator neural network.Although reconstructing the data might seem like a trivial matter by simply copying the data forward from one layer to another, this is not possible when the number of units in the middle are constricted.In other words, the number of units in each middle layer is typically fewer than that in the input (or output).As a result, these units hold a reduced representation of the data, and the final layer can no longer reconstruct the data exactly.Therefore, this type of reconstruction is inherently lossy.The loss function of this neural network uses the sum-of-squared differences between the input and the output in order to force the output to be as similar as possible to the input.This general representation of the autoencoder is given in Figure 2.6(a), where an architecture is shown with three constricted layers.It is noteworthy that the representation of the innermost hidden layer will be hierarchically related to those in the two outer hidden layers.Therefore, an autoencoder is capable of performing hierarchical data reduction.It is common (but not necessary) for an M -layer autoencoder to have a symmetric architecture between the input and output, where the number of units in the kth layer is the same as that in the (M − k + 1)th layer.Furthermore, the value of M is often odd, as a result of which the (M + 1)/2th layer is often the most constricted layer.Here, we are counting the (non-computational) input layer as the first layer, and therefore the minimum number of layers in an autoencoder would be three, corresponding to the input layer, constricted layer, and the output layer.As we will see later, this simplest form of the autoencoder is used in traditional machine learning for singular value decomposition.The symmetry in the architecture often extends to the fact that the weights outgoing from the kth layer are tied to those incoming to the (M − k)th layer in many architectures.For now, we will not make this assumption for simplicity in presentation.Furthermore, the symmetry is never absolute because of the effect of nonlinear activation functions.For example, if a nonlinear activation function is used in the output layer, there is no way to symmetrically mirror that fact in the (non-computational) input layer.The reduced representation of the data is also sometimes referred to as the code, and the number of units in this layer is the dimensionality of the reduction.The initial part of the neural architecture before the bottleneck is referred to as the encoder (because it creates a reduced code), and the final part of the architecture is referred to as the decoder (because it reconstructs from the code).The general schematic of the autoencoder is shown in Figure 2.6(b).In the following, we describe the simplest version of an autoencoder, which is used for matrix factorization.This autoencoder only has a single hidden layer of k d units between the input and output layers of d units each.For the purpose of discussion, assume that we have an n × d matrix denoted by D, which we would like to factorize into an n × k matrix U and a d × k matrix V :Here, k is the rank of the factorization.The matrix U contains the reduced representation of the data, and the matrix V contains the basis vectors.Matrix factorization is one of the most widely studied problems in supervised learning, and it is used for dimensionality reduction, clustering, and predictive modeling in recommender systems.In traditional machine learning, this problem is solved by minimizing the Frobenius norm of the residual matrix denoted by (D − UV T ).The squared Frobenius norm of a matrix is the sum of the squares of the entries in the matrix.Therefore, one can write the objective function of the optimization problem as follows:Here, the notation || • || F indicates the Frobenius norm.The parameter matrices U and V need to be learned in order to optimize the aforementioned error.This objective function has an infinite number of optima, one of which has mutually orthogonal basis vectors.That particular solution is referred to as truncated singular value decomposition.Although it is relatively easy to derive the gradient-descent steps [6] for this optimization problem (without worrying about neural networks at all), our goal here is to capture this optimization problem within a neural architecture.Going through this exercise helps us show that SVD is a special case of an autoencoder architecture, which sets the stage for understanding the gains obtained with more complex autoencoders.This neural architecture for SVD is illustrated in Figure 2.7, where the hidden layer contains k units.The rows of D are input into the autoencoder, whereas the k-dimensional rows of U are the activations of the hidden layer.The k × d matrix of weights in the decoder is V T .As we discussed in the introduction to the multilayer neural network in Chapter 1, the vector of values in a particular layer of the network can be obtained by multiplying the vector of values in the previous layer with the matrix of weights connecting the two layers (with linear activation).Since the activations of the hidden layer are U and the decoder weights contain the matrix V T , it follows that the reconstructed output contains the rows of UV T .The autoencoder minimizes the sum-of-squared differences between the input and the output, which is equivalent to minimizing ||D − UV T || 2 .Therefore, the same problem is being solved as singular value decomposition.Note that one can use this approach to provide the reduced representation of out-ofsample instances that were not included in the original matrix D. One simply has to feed these out-of-sample rows as the input, and the activations of the hidden layer will provide the reduced representation.Reducing out-of-sample instances is particularly useful for nonlinear dimensionality-reduction methods, as it is more difficult for traditional machine learning methods to fold in new instances.As shown in Figure 2.7, the encoder weights are contained in the k × d matrix denoted by W . How is this matrix related to U and V ?Note that the autoencoder creates the reconstructed representation DW T V T of the original data matrix.Therefore, it tries to optimize the problem of minimizing ||DW T V T − D|| 2 .The optimal solution to this problem is obtained when the matrix W contains the pseudo-inverse of V , which is defined as follows:This result is easy to show at least for non-degenerate cases in which the rows of matrix D span the full rank of d dimensions (see Exercise 14).Of course, the final solution found by the training algorithm of the autoencoder might deviate from this condition because it might not solve the problem precisely or because the matrix D might be of smaller rank.By the definition of the pseudo-inverse, it follows that W V = I and V T W T = I, where I is a k × k identity matrix.Post-multiplying Equation 2.38 with W T we obtain the following:In other words, multiplying each row of the matrix D with the d × k matrix W T yields the reduced representation of that instance, which is the corresponding row in U .Furthermore, multiplying that row of U again with V T yields the reconstructed version of the original data matrix D.Note that there are many alternate optima for W and V , but in order for reconstruction to occur (i.e., minimization of loss function), the learned matrix W will always be (approximately) related to V as its pseudo-inverse and the columns of V will always span 3 a particular k-dimensional subspace defined by the SVD optimization problem.The single-layer autoencoder architecture is closely connected with singular value decomposition (SVD).Singular value decomposition finds a factorization UV T in which the columns of V are orthonormal.The loss function of this neural network is identical to that of singular value decomposition, and a solution V in which the columns of V are orthonormal will always be one of the possible optima obtained by training the neural network.However, since this loss function allows alternative optima, it is possible to find an optimal solution in which the columns of V are not necessarily mutually orthogonal or scaled to unit norm.SVD is defined by an orthonormal basis system.Nevertheless, the subspace spanned by the k columns of V will be the same as that spanned by the top-k basis vectors of SVD.Principal component analysis is identical to singular value decomposition, except that it is applied to a mean-centered matrix D. Therefore, the approach can also be used to find the subspace spanned by the top-k principal components.However, each column of D needs to be mean-centered up front by subtracting its mean.One can achieve an orthonormal basis system, which is even closer to SVD and PCA by sharing some of the weights in the encoder and decoder.This approach is discussed in the next section.There are many possible alternate solutions for W and V in the above discussion, in which W is the pseudo-inverse of V .One can, therefore, reduce the parameter footprint further without significant 4 loss in reconstruction accuracy.A common practice that is used in the autoencoder construction is to share some of the weights between the encoder and the decoder.This is also referred to as tying the weights.In particular, the autoencoder has an inherently symmetric structure, in which the weights of the encoder and decoder are forced to be the same in symmetrically matching layers.In the shallow case, the encoder and decoder weights are shared by using the following relationship: This architecture is shown in Figure 2.8, and it is identical to the architecture of Figure 2.7 except for the presence of tied weights.In other words, the d × k matrix V of weights is first used to transform the d-dimensional data point X into a k-dimensional representation.Then, the matrix V T of weights is used to reconstruct the data to its original representation.The tying of the weights effectively means that V T is the pseudo-inverse of V (see Exercise 14).In other words, we have V T V = I, and therefore the columns of V are mutually orthogonal.As a result, by tying the weights, it is now possible to exactly simulate SVD, in which the different basis vectors need to be mutually orthogonal.In this particular example of an architecture with a single hidden layer, the tying of weights is done only for a pair of weight matrices.In general, one would have an odd number of hidden layers and an even number of weight matrices.It is a common practice to match up the weight matrices in a symmetric way about the middle.In such a case, the symmetrically arranged hidden layers would need to have the same numbers of units.Even though it is not necessary to share weights between the encoder and decoder portions of the architecture, it reduces the number of parameters by a factor of 2. This is beneficial from the point of view of reducing overfitting.In other words, the approach would better reconstruct out-of-sample data.Another benefit of tying the weight matrices in the encoder and the decoder is that it automatically normalizes the columns of V to similar values.For example, if we do not tie the weight matrices in the encoder and the decoder, it is possible for the different columns of V to have very different norms.At least in the case of linear activations, tying the weight matrices forces all columns of V to have similar norms.This is also useful from the perspective of providing better normalization of the embedded representation.The normalization and orthogonality properties no longer hold exactly when nonlinear activations are used in the computational layers.However, there are considerable benefits in tying the weights even in these cases in terms of better conditioning of the solution.The sharing of weights does require some changes to the backpropagation algorithm during training.However, these modifications are not very difficult.All that one has to do is to perform normal backpropagation by pretending that the weights are not tied in order to compute the gradients.Then, the gradients across different copies of the same weight are added in order to compute the gradient-descent steps.The logic for handing shared weights in this way is discussed in Section 3.2.9 of Chapter 3.It is possible to modify the simple three-layer autoencoder to simulate other types of matrix factorization methods such as non-negative matrix factorization, probabilistic latent semantic analysis, and logistic matrix factorization methods.Different methods for logistic matrix factorization will be discussed in the next section, in Section 2.6.3, and in Exercise 8. Methods for non-negative matrix factorization and probabilistic latent semantic analysis are discussed in Exercises 9 and 10.It is instructive to examine the relationships between these different variations, because it shows how one can vary on simple neural architectures in order to get results with vastly different properties.So far, the discussion has focussed on simulating singular value decomposition using a neural architecture.Clearly, this does not seem to achieve much because many off-the-shelf tools exist for singular value decomposition.However, the real power of autoencoders is realized when one starts using nonlinear activations and multiple layers.For example, consider a situation in which the matrix D is binary.In such a case, one can use the same neural architecture as shown in Figure 2.7, but one can also use a sigmoid function in the final layer to predict the output.This sigmoid layer is combined with negative log loss.Therefore, for a binary matrix B = [b ij ], the model assumes the following:Here, the sigmoid function is applied in element-wise fashion.Note the use of ∼ instead of ≈ in the above expression, which indicates that the binary matrix B is an instantiation of random draws from Bernoulli distributions with corresponding parameters contained in sigmoid(UV T ).The resulting factorization can be shown to be equivalent to logistic matrix factorization.The basic idea is that the (i, j)th element of UV T is the parameter of a Bernoulli distribution, and the binary entry b ij is generated from a Bernoulli distribution with these parameters.Therefore, U and V are learned using the log-likelihood loss of this generative model.The log-likelihood loss implicitly tries to find parameter matrices U and V so that the probability of the matrix B being generated by these parameters is maximized.Logistic matrix factorization has only recently been proposed [224] as a sophisticated matrix factorization method for binary data, which is useful for recommender systems with implicit feedback ratings.Implicit feedback refers to the binary actions of users such as buying or not buying specific items.The solution methodology of this recent work on logistic matrix factorization [224] seems to be vastly different from SVD, and it is not based on a neural network approach.However, for a neural network practitioner, the change from the SVD model to that of logistic matrix factorization is a relatively small one, where only the final layer of the neural network needs to be changed.It is this modular nature of neural networks that makes them so attractive to engineers and encourages all types of experimentation.In fact, one of the variants of the popular word2vec neural approach [325,327] for text feature engineering is a logistic matrix factorization method, when one examines it more closely.Interestingly, word2vec was proposed earlier than logistic matrix factorization in traditional machine learning [224], although the equivalence of the two methods was not shown in the original work.The equivalence was first shown in [6], and a proof of this result is also provided later in this chapter.Indeed, for multilayer variants of the autoencoder, an exact counterpart does not even exist in traditional machine learning.All this seems to suggest that it is often more natural to discover sophisticated machine learning algorithms when working with the modular approach of constructing multilayer neural networks.Note that one can even use this approach to factorize real-valued matrix entries drawn from [0, 1], as long as the log-loss is suitably modified to handle fractional values (see Exercise 8).Logistic matrix factorization is a type of kernel matrix factorization.One can also use non-linear activations in the hidden layer rather than (or in addition to) the output layer.By using the non-linearity in the hidden layer to impose non-negativity, one can simulate non-negative matrix factorization (cf.Exercises 9 and 10).Furthermore, consider an autoencoder with a single hidden layer in which sigmoid units are used in the hidden layer, and the output layer is linear.Furthermore, the input-to-hidden and the hidden-to-output matrices are denoted by W T and V T , respectively.In this case, the matrix W will no longer be the pseudo-inverse of V because of the non-linear activation in the hidden layer.If U is the output of the hidden layer in which the nonlinear activation Φ(•) is applied, we have:If the output layer is linear, the overall factorization is still of the following form:Note, however, that we can write U = DW T , which is a linear projection of the original matrix D.Then, the factorization can be written as follows:Here, U is a linear projection of D. This is a different type of nonlinear matrix factorization [521,558].Although the specific form of the nonlinearity (e.g., sigmoid) might seem simplistic compared to what is considered typical in kernel methods, in reality multiple hidden layers are used to learn more complex forms of nonlinear dimensionality reduction.Nonlinearity can also be combined in the hidden layers and in the output layer.Nonlinear dimensionality reduction methods can map the data into much lower dimensional spaces (with good reconstruction characteristics) than would be possible with methods like PCA.An example of a data set, which is distributed on a nonlinear spiral, is shown in Figure 2.9(a).This data set cannot be reduced to lower dimensionality using PCA (without causing significant reconstruction error).However, the use of nonlinear dimensionality reduction methods can flatten out the nonlinear spiral into a 2-dimensional representation.This representation is shown in Figure 2.9(b).Nonlinear dimensionality-reduction methods often require deeper networks due to the more complex transformations possible with the combination of nonlinear units.The benefits of depth will be discussed in the next section.The real power of autoencoders in the neural network domain is realized when deeper variants are used.For example, an autoencoder with three hidden layers is shown in Figure 2.10.One can increase the number of intermediate layers in order to further increase the representation power of the neural network.It is noteworthy that it is essential for some of the layers of the deep autoencoder to use a nonlinear activation function to increase its representation power.As shown in Lemma 1.5.1 of Chapter 1, no additional power is gained by a multilayer network when only linear activations are used.Although this result was shown in Chapter 1 for the classification problem, it is broadly true for any type of multilayer neural network (including an autoencoder).Deep networks with multiple layers provide an extraordinary amount of representation power.The multiple layers of this network provide hierarchically reduced representations of the data.For some data domains like images, hierarchically reduced representations are particularly natural.Note that there is no precise analog of this type of model in traditional machine learning, and the backpropagation approach rescues us from the challenges associated in computing the complicated gradient-descent steps.A nonlinear dimensionality reduction might map a manifold of arbitrary shape into a reduced representation.Although several methods for nonlinear dimensionality reduction are known in machine learning, neural networks have some advantages over these methods:1.Many nonlinear dimensionality reduction methods have a very hard time mapping out-of-sample data points to reduced representations, unless these points are included in the training data up front.On the other hand, it is a relatively simple matter toFigure 2.11: A depiction of the typical difference between the embeddings created by nonlinear autoencoders and principal component analysis (PCA).Nonlinear and deep autoencoders are often able to separate out the entangled class structures in the underlying data, which is not possible within the constraints of linear transformations like PCA.This occurs because individual classes are often populated on curved manifolds in the original space, which would appear mixed when looking at a data in any 2-dimensional cross-section unless one is willing to warp the space itself.The figure above is drawn for illustrative purposes only and does not represent a specific data set.compute the reduced representation of an out-of-sample point by passing it through the network.2. Neural networks allow more power and flexibility in the nonlinear data reduction by varying on the number and type of layers used in intermediate stages.Furthermore, by choosing specific types of activation functions in particular layers, one can engineer the nature of the reduction to the properties of the data.For example, it makes sense to use a logistic output layer with logarithmic loss for a binary data set.It is possible to achieve extraordinarily compact reductions by using this approach.For example, the work in [198] shows how one can convert a 784-dimensional representation of the pixels of an image into a 6-dimensional reduction with the use of deep autoencoders.Greater reduction is always achieved by using nonlinear units, which implicitly map warped manifolds into linear hyperplanes.The superior reduction in these cases is because it is easier to thread a warped surface (as opposed to a linear surface) through a larger number of points.This property of nonlinear autoencoders is often used for 2-dimensional visualizations of the data by creating a deep autoencoder in which the most compact hidden layer has only two dimensions.These two dimensions can then be mapped on a plane to visualize the points.In many cases, the class structure of the data is exposed in terms of well-separated clusters.An illustrative example of the typical behavior of real data distributions is shown in Figure 2.11, in which the 2-dimensional mapping created by a deep autoencoder seems to clearly separate out the different classes.On the other hand, the mapping created by PCA does not seem to separate the classes well.Figure 2.9, which provides a nonlinear spiral mapped to a linear hyperplane, clarifies the reason for this behavior.In many cases, the data may contain heavily entangled spirals (or other shapes) that belong to different classes.Linear dimensionality reduction methods cannot attain clear separation because nonlinearly entangled shapes are not linearly separable.On the other hand, deep autoencoders with nonlinearity are far more powerful and able to disentangle such shapes.Deep autoencoders can sometimes be used as alternatives to other robust visualization methods like t-distributed stochastic neighbor embedding (t-SNE) [305].Although t-SNE can often provide better performance 5 for visualization (because it is specifically designed for visualization rather than dimensionality reduction), the advantage of an autoencoder over t-SNE is that it is easier to generalize to out-of-sample data.When new data points are received, they can simply be passed through the encoder portion of the autoencoder in order to add them to the current set of visualized points.A specific example of a visualization of a high-dimensional document collection with an autoencoder is provided in [198].It is, however, possible to go too far and create representations that are not useful.For example, one can compress a very high-dimensional data point into a single dimension, which reconstructs a point from the training data very well but gives high reconstruction error for test data.In other words, the neural network has found a way to memorize the data set without sufficient ability to create reduced representations of unseen points.Therefore, even for unsupervised problems like dimensionality reduction, it is important to keep aside some points as a validation set.The points in the validation set are not used during training.One can then quantify the difference in reconstruction error between the training and validation data.Large differences in reconstruction error between the training and validation data are indicative of overfitting.Another issue is that deep networks are harder to train, and therefore tricks like pretraining are important.These tricks will be discussed in Chapters 3 and 4.Dimensionality reduction is closely related to outlier detection, because outlier points are hard to encode and decode without losing substantial information.It is a well-known fact that if a matrix D is factorized as D ≈ D = UV T , then the low-rank matrix D is a de-noised representative of the data.After all, the compressed representation U captures only the regularities in the data, and is unable to capture the unusual variations in specific points.As a result, reconstruction to D misses all these unusual variations.The absolute values of the entries of (D − D ) represent the outlier scores of the matrix entries.Therefore, one can use this approach to find outlier entries, or add the squared scores of the entries in each row of D to find the outlier score of that row.Therefore, one can identify outlier data points.Furthermore, by adding the squared scores in each column of D, one can find outlier features.This is useful for applications like feature selection in clustering, where a feature with a large outlier score can be removed because it adds noise to the clustering.Although we have provided the description above with the use of matrix factorization, any type of autoencoder can be used.In fact, the construction of de-noising autoencoders is a vibrant field in its own right.Refer to the bibliographic notes.So far, we have only discussed cases in which the hidden layer has fewer units than the input layer.It makes sense for the hidden layer to have fewer units than the input layer when one is looking for a compressed representation of the data.A constricted hidden layer forces dimensionality reduction, and the loss function is designed to avoid information loss.Such representations are referred to as undercomplete representations, and they correspond to the traditional use-case of autoencoders.What about the case when the number of hidden units is greater than the input dimensionality?This situation corresponds to the case of over-complete representations.Increasing the number of hidden units beyond the number of input units makes it possible for the hidden layer to simply learn the identity function (with zero loss).Simply copying the input across the layers does not seem to be particularly useful.However, this does not occur in practice (while learning weights), especially if certain types of regularization and sparsity constraints are imposed on the hidden layer.Even if no sparsity constraints are imposed, and stochastic gradient descent is used for learning, the probabilistic regularization caused by stochastic gradient descent is sufficient to ensure that the hidden representation will always scramble the input before reconstructing it at the output.This is because stochastic gradient descent is a type of noise addition to the learning process, and therefore it will not be possible to learn weights that simply copy input to output as identity functions across layers.Furthermore, because of some peculiarities of the training process, a neural network almost never uses its full modeling ability, which leads to dependencies among the weights [94].Rather, an over-complete representation may be created, although it may not have the property of sparsity (which needs to be explicitly encouraged).The next section will discuss ways of encouraging sparsity.When explicit sparsity constraints are imposed, the resulting autoencoder is referred to as a sparse autoencoder.A sparse representation of a d-dimensional point is a k-dimensional point in which k d and most of the values in the sparse representation are 0s.Sparse feature learning has tremendous applicability to many settings like image data, where the learned features are often intuitively more interpretable from an application-specific perspective.Furthermore, points with a variable amount of information will be naturally represented by having varying numbers of nonzero feature values.This type of property is naturally true in some input representations like documents; documents with more information will have more non-zero features (word frequencies) when represented in multidimensional format.However, if the available input is not sparse to begin with, there are often benefits in creating a sparse transformation where such a flexibility of representation exists.Sparse representations also enable the effective use of particular types of efficient algorithms that are highly dependent on sparsity.There are many ways in which constraints might be enforced on the hidden layer to create sparsity.One approach is to add biases to the hidden layer, so that many units are encouraged to be zeros.Some examples are as follows:1.One can impose an L 1 -penalty on the activations in the hidden layer to force sparse activations.The notion of L 1 -penalties for creating sparse solutions (in terms of either weights or hidden units) is discussed in Sections 4.4.2 and 4.4.4 of Chapter 4. In such a case, backpropagation must also propagate the gradient of this penalty in the backwards direction.Surprisingly, this natural alternative is rarely used.2. One can allow only the top-r activations in the hidden layer to be nonzero for r ≤ k.In such a case, backpropagation only backpropagates through the activated units.This approach is referred to as the r-sparse autoencoder [309].3. Another approach is the winner-take-all autoencoder [310], in which only a fraction f of the activations of each hidden unit are allowed over the whole training data.In this case, the top activations are computed across training examples, whereas in the previous case the top activations are computed across a hidden layer for a single training example.Therefore node-specific thresholds need to be estimated using the statistics of a minibatch.The backpropagation algorithm needs to propagate the gradient only through the activated units.Note that the implementations of the competitive mechanisms are almost like ReLU activations with adaptive thresholds.Refer to the bibliographic notes for pointers and more details of these algorithms.Autoencoders form the workhorse of unsupervised learning in the neural network domain.They are used for a host of applications, which will be discussed later in the book.After training an autoencoder, it is not necessary to use both the encoder and decoder portions.For example, when using the approach for dimensionality reduction, one can use the encoder portion in order to create the reduced representations of the data.The reconstructions of the decoder might not be required at all.Although an autoencoder naturally removes noise (like almost any dimensionality reduction method), one can enhance the ability of the autoencoder to remove specific types of noise.To perform the training of a de-noising autoencoder, a special type of training is used.First, some noise is added to the training data before passing it through the neural network.The distribution of the added noise reflects the analyst's understanding of the natural types of noise in that particular data domain.However, the loss is computed with respect to the original training data instances rather than their corrupted versions.The original training data are relatively clean, although one expects the test instances to be corrupted.Therefore, the autoencoder learns to recover clean representations from corrupted data.A common approach to add noise is to randomly set a fraction f of the inputs to zeros [506].This approach is especially effective when the inputs are binary.The value of f regulates the level of corruption in the inputs.One can either fix f or even allow f to randomly vary over different training instances.In some cases, when the input is real-valued, Gaussian noise is also used.More details of the de-noising autoencoder are provided in Section 4.10.2 of Chapter 4. A closely related autoencoder is the contractive autoencoder, which is discussed in Section 4.10.3.Another interesting application of the autoencoder is one in which we use only the decoder portion of the network to create artistic renderings.This idea is based on the notion of variational autoencoders [242,399], in which the loss function is modified to impose a specific structure on the hidden layer.For example, one might add a term to the loss function to enforce the fact that the hidden variables are drawn from a Gaussian distribution.Then, one might repeatedly draw samples from this Gaussian distribution and use only the decoder portion of the network in order to generate samples of the original data.The generated samples often represent realistic samples from the original data distribution.A closely related model is that of generative adversarial networks, which have become increasingly popular in recent years.These models pair the learning of a decoding network with that of an adversarial discriminator in order to create generative samples of a data set.Generative adversarial networks are used frequently with image, video, and text data, and they generate artistic renderings of images and videos, which often have the flavor of an AI that is "dreaming."These methods can be used for image-to-image translation as well.The variational autoencoder is discussed in detail in Section 4.10.4 of Chapter 4. Generative adversarial networks are discussed in Section 10.4 of Chapter 10.Multimodal data is essentially data in which the input features are heterogeneous.For example, an image with descriptive tags can be considered multimodal data.Multimodal data pose challenges to mining applications because different features require different types of processing and treatment.By embedding the heterogeneous attributes in a unified space, one is removing this source of difficulty in the mining process.An autoencoder can be used to embed the heterogeneous data into a joint space.An example of such a setting is shown in Figure 2.12.This figure shows an autoencoder with only a single layer, although one might have multiple layers in general [357,468].Such joint spaces can be very useful in a variety of applications.Finally, autoencoders are used to improve the learning process in neural networks.A specific example is that of pretraining in which an autoencoder is used to initialize the weights of a neural network.The basic idea is that learning the manifold structure of a data set is also useful for supervised learning applications like classification.This is because the features that define the manifold of a data set are often likely to be more informative in terms of their relationships to different classes.Pretraining methods are discussed in Section 4.7 of Chapter 4.One of the most interesting applications of matrix factorization is the design of neural architectures for recommender systems.Consider an n × d ratings matrix D with n users and d items.The (i, j)th entry of the matrix is the rating of user i for item j.However, most entries in the matrix are not specified, which creates difficulties in using a traditional autoencoder architecture.This is because traditional autoencoders are designed for fully specified matrices, in which a single row of the matrix is input at one time.On the other hand, recommender systems are inherently suited to elementwise learning, in which a very small subset of ratings from a row may be available.As a practical matter, one might consider the input to a recommender system as a set of triplets of the following form: RowId , ColumnId , Rating As in traditional forms of matrix factorization, the ratings matrix D is given by UV T .However, the difference is that one must learn U and V using triplet-centric input because all entries of D are not observed.Therefore, a natural approach is to create an architecture in which the inputs are not affected by the missing entries and can be uniquely specified.The input layer contains n input units, which is the same as the number of rows (users).However, the input is a one-hot encoded index of the row identifier.Therefore, only one entry of the input takes on the value of 1, with the remaining entries taking on values of 0. The hidden layer contains k units, where k is the rank of the factorization.Finally, the output layer contains d units, where d is the number of columns (items).The output is a vector containing the d ratings (even though only a small subset of them are observed).The goal is to train the neural network with an incomplete data matrix D so that the network outputs all the ratings corresponding to a one-hot encoded row index after it is input.The approach is to be able to reconstruct the data by learning the ratings associated with each row index.Consider a setting in which the n × k input-to-hidden matrix is U , and the k × d hiddento-output matrix is V T .The entries of the matrix U are denoted by u iq , and those of the matrix V are denoted by v jq .Assume that all activation functions are linear.Furthermore, let the one-hot encoded input (row) vector for the rth user be e r .This row vector contains n dimensions in which only the rth value is 1, and the remaining values are zeros.The loss function is the sum of the squares of the errors in the output layer.However, because of the missing entries, not all output nodes have an observed output value, and the updates are performed only with respect to entries that are known.The overall architecture of this neural network is illustrated in Figure 2.13.For any particular row-wise input we are really training on a neural network that is a subset of this base network, depending on which entries are specified.However, it is possible to give predictions for all outputs in the network (even though a loss function cannot be computed for missing entries).Since a neural network with linear activations performs matrix multiplications, it is easy to see that the vector of d outputs for the rth user is given by e r UV T .In essence, pre-multiplication with e r pulls out the rth row in the matrix UV T .These values appear at the output layer and represent the item-wise ratings predictions for the rth user.Therefore, all feature values are reconstructed in one shot.How is training performed?The main attraction of this architecture is that one can perform the training either in row-wise fashion or in element-wise fashion.When performing the training in row-wise fashion, the one-hot encoded index for that row is input, and all specified entries of that row are used to compute the loss.The backpropagation algorithm is done only starting at output nodes where the values are specified.From a theoretical point of view, each row is being trained on a slightly different neural network with a subset of the base output nodes (depending on which entries are observed), although the weights for the different neural networks are shared.This situation is shown in Figure 2.14, where the neural networks for the movie ratings of two different users, Bob and Sayani, are shown.For example, Bob is missing a rating for Shrek, as a result of which the corresponding output node is missing.However, since both users have specified a rating for E.T., the k-dimensional hidden factors for this movie in matrix V will be updated during backpropagation when either Bob or Sayani is processed.This ability to train using only a subset of the output nodes is sometimes used as an efficiency optimization to reduce training time even in cases where all outputs are specified.Such situations occur often in binary recommendation data sets (referred to as implicit feedback data sets), where the vast majority of outputs are zeros.In such cases, only a subset of zeros is sampled for training in matrix factorization methods [4].This technique is referred to as negative sampling.A specific example is that of neural models for natural language processing like word2vec.It is also possible to perform the training in element-wise fashion, where a single triplet is input.In such a case, the loss is computed only with respect to a single column index specified in the triplet.Consider the case where the row index is i, and the column index is j.In this specific case, and the single error computed at the output layer is y − ŷ = e ij .the backpropagation algorithm essentially updates the weights on all the k paths from node j in the output layer to the node i in the input layer.These k paths pass through the k nodes in the hidden layer.It is easy to show that the update along the qth such path is as follows:Here, α is the step-size, and λ is the regularization parameter.These updates are identical to those used in stochastic gradient descent for matrix factorization in recommender systems.However, an important advantage of the use of the neural architecture (over traditional matrix factorization) is that we can vary on it in so many different ways in order to enforce different properties.For example, for matrices with binary data, we can use a logistic layer in the output.This will result in logistic matrix factorization.We can incorporate multiple hidden layers to create more powerful models.For matrices with categorical entries (and count-centric weights attached to entries), one can use a softmax layer at the very end.This will result in multinomial matrix factorization.To date, we are not aware of a formal description of multinomial matrix factorization in traditional machine learning; yet, it is a simple modification of the neural architecture (implicitly) used by recommender systems.In general, it is often easy to stumble upon sophisticated models when working with neural architectures because of their modular structure.One does not need to relate the neural architecture to a conventional machine learning model, as long as empirical results establish its robustness.For example, two variations of the (highly successful) skip-gram model of word2vec [325,327] correspond to logistic and multinomial matrix factorizations of wordcontext matrices; yet, this fact does not seem to be pointed 6 out by either by the original authors of word2vec [325,327] or the broader community.In conventional machine learning, models like logistic matrix factorization are considered relatively esoteric techniques that have only recently been proposed [224]; yet, these sophisticated models represent relatively simple neural architectures.In general, the neural network abstraction brings practitioners (without too much mathematical training) much closer to sophisticated methods in machine learning, while being shielded from the details of optimization with the backpropagation framework.The main goal of this section was to show the benefits of the modular nature of neural networks in unsupervised learning.In our particular example, we started with a simple simulation of SVD, and then showed how minor changes to the neural architecture can achieve very different types of goals in intuitive settings.However, from an architectural point of view, the amount of effort required by the analyst to change from one architecture to the other is often a few lines of code.This is because modern softwares for building neural networks often provide templates for describing the architecture of the neural network, where each layer is specified independently.In a sense, the neural network is "built" with the wellknown types of machine-learning units much like a child puts together building blocks of a toy.Backpropagation takes care of the details of optimization, while shielding the user from the complexities of the steps.Consider the significant mathematical differences between the specific details of SVD and logistic matrix factorization.Changing the output layer from linear to sigmoid (along with a change of loss function) can literally be a matter of changing a trivially small number of lines of code without affecting most of the remaining code (which usually isn't large anyway).This type of modularity is tremendously useful in applicationcentric settings.Autoencoders are also related to another type of unsupervised learning method, known as a Restricted Boltzmann Machines (RBM) (cf.Chapter 6).These methods can also be used for recommender systems, as discussed in Section 6.5.2 of Chapter 6.Neural network methods have been used to learn word embeddings of text data.In general, one can create embeddings of both documents and words by using methods like SVD.In SVD, an n × d matrix of document-word counts is created.This matrix is then factorized as D ≈ UV .Here, U and V are n × k and k × d matrices, respectively.The rows of U contain embeddings of documents and the columns of V contain embeddings of words.Note that we have changed the notation slightly from the previous section (by using UV instead of UV T for factorization), because it is more convenient for this section.SVD is, however, a method that treats a document as a bag of words.Here, we are interested in factorizations that use the sequential orderings among words to create embeddings.The focus here is to create word embeddings rather than document embeddings.The family of word2vec methods is well suited to creating word embeddings.The two variants of word2vec are as follows:1. Predicting target words from contexts: This model tries to predict the ith word, w i , in a sentence using a window of width t around the word.Therefore, the words w i−t w i−t+1 . . .w i−1 w i+1 . . .w i+t−1 w i+t are used to predict the target word w i .This model is also referred to as the continuous bag-of-words (CBOW) model.This model tries to predict the context w i−t w i−t+1 . . .w i−1 w i+1 . . .w i+t−1 w i+t around word w i , given the ith word in the sentence, denoted by w i .This model is referred to as the skip-gram model.There are, however, two ways in which one can perform this prediction.The first technique is a multinomial model which predicts one word out of d outcomes.The second model is a Bernoulli model, which predicts whether or not each context is present for a particular word.The second model uses negative sampling of contexts for better efficiency and accuracy.Each of these methods will be discussed in this section.In the continuous bag-of-words (CBOW) model, the training pairs are all context-word pairs in which a window of context words is input, and a single target word is predicted.The context contains 2 • t words, corresponding to t words both before and after the target word.For notational ease, we will use the length m = 2 • t to define the length of the context.Therefore, the input to the system is a set of m words.Without loss of generality, let the subscripts of these words be numbered so that they are denoted by w 1 . . .w m , and let the target (output) word in the middle of the context window be denoted by w.Note that w can be viewed as a categorical variable with d possible values, where d is the size of the lexicon.The goal of the neural embedding is to compute the probability P (w|w 1 w 2 . . .w m ) and maximize the product of these probabilities over all training samples.The overall architecture of this model is illustrated in Figure 2.15.In the architecture, we have a single input layer with m × d nodes, a hidden layer with p nodes, and an output layer with d nodes.The nodes in the input layer are clustered into m different groups, each of which has d units.Each group with d input units is the one-hot encoded input vector of one of the m context words being modeled by CBOW.Only one of these d inputs will be 1 and the remaining inputs will be 0. Therefore, one can represent an input x ij with two indices corresponding to contextual position and word identifier.Specifically, the input x ij ∈ {0, 1} contains two indices i and j in the subscript, where i ∈ {1 . . .m} is the position of the context, and j ∈ {1 . . .d} is the identifier of the word.The hidden layer contains p units, where p is the dimensionality of the hidden layer in word2vec.Let h 1 , h 2 , . . .h p be the outputs of the hidden layer nodes.Note that each of the d words in the lexicon has m different representatives in the input layer corresponding to the m different context words, but the weight of each of these m connections is the same.Such weights are referred to as shared.Sharing weights is a common trick used for regularization in neural networks, when one has specific insight about the domain at hand.Let the shared weight of each connection from the jth word in the lexicon to the qth hidden layer node be denoted by u jq .Note that each of the m groups in the input layer has connections to the hidden layer that are defined by the same d × p weight matrix U .This situation is shown in Figure 2.15.It is noteworthy that u j = (u j1 , u j2 , . . .u jp ) can be viewed as the p-dimensional embedding of the jth input word over the entire corpus, and h = (h 1 . . .h p ) provides the embedding of a specific instantiation of an input context.Then, the output of the hidden layer is obtained by averaging the embeddings of the words present in the context.In other words, we have the following:Many expositions use an additional factor of m in the denominator on the right-hand side, although this type of multiplicative scaling (with a constant) is inconsequential.One can also write this relationship in vector form:In essence, the one-hot encodings of the input words are aggregated, which implies that the ordering of the words within the window of size m does not affect the output of the model.This is the reason that the model is referred to as the continuous bag-of-words model.However, sequential information is still used by virtue of restricting the prediction to a context window.The embedding (h 1 . . .h p ) is used to predict the probability that the target word is one of each of the d outputs with the use of the softmax function.The weights in the output layer are parameterized with a p × d matrix V = [v qj ].The jth column of V is denoted by v j .The output after applying softmax creates d output values ŷ1 . . .ŷd , which are real values in (0, 1).These real values sum to 1 because they can be interpreted as probabilities.The ground-truth value of only one of the outputs y 1 . . .y d is 1 and the remaining values are 0 for a given training instance.One can write this condition as follows:1 if the target word w is the jth word 0 otherwise (2.48)The softmax function computes the probability P (w|w 1 . . .w m ) of the one-hot encoded ground-truth outputs y j as follows:Note that this probabilistic form of the prediction is based on the softmax layer (cf.Section 1.2.1.4 of Chapter 1).For a particular target word w = r ∈ {1 . . .d}, the loss function is given by L = −log[P (y r = 1|w 1 . . .w m )] = −log(ŷ r ).The use of the negative logarithm turns the multiplicative likelihoods over different training instances into an additive loss function using log-likelihoods.The updates are defined by using the backpropagation algorithm, as training instances are passed through the neural network one by one.First, the derivative of the aforementioned loss function can be used to update the gradients of the weight matrix V in the output layer.Then, backpropagation can be used to update the weight matrix U between the input and hidden layer.The update equations with learning rate α are as follows:One can compute the partial derivatives of this expression easily [325,327,404].The probability of making a mistake in prediction on the jth word in the lexicon is defined by |y j − ŷj |.However, we use signed mistakes j , in which only the correct word with y j = 1 is given a positive mistake value, while all the other words in the lexicon receive negative mistake values.This is achieved by dropping the modulus:Note that j can also be shown to be equal to the negative of the derivative of the crossentropy loss with respect to jth input into the softmax layer (which is h • v j ).This result is shown in Section 3.2.5.1 of the next chapter and is useful in deriving the backpropagation updates.Then, the updates 7 for a particular input context and output word are as follows:Here, α > 0 is the learning rate.Repetitions of the same word i in the context window trigger multiple updates of u i .It is noteworthy that the input embeddings of the context words are aggregated in both updates, considering the fact that h aggregates input embeddings according to Equation 2.47.This type of aggregation has a smoothing effect on the CBOW model, which is particularly helpful with smaller data sets.The training examples of context-target pairs are presented one by one, and the weights are trained to convergence.It is noteworthy that the word2vec model provides not one but two different embeddings corresponding to the p-dimensional rows of the matrix U and the p-dimensional columns of the matrix V .The former type of embedding of words is referred to as the input embedding, whereas the latter is referred to as the output embedding.In the CBOW model, the input embedding represents context, and therefore it makes sense to use the output embedding.However, the input embedding (or the sum/concatenation of input and output embeddings) can also be helpful for many tasks.In the skip-gram model, the target words are used to predict the m context words.Therefore, we have one input word and m outputs.One issue with the CBOW model is that the averaging effect of the input words in the context window (which creates the hidden representation) has a (helpful) smoothing effect with less data, but fails to take full advantage of a larger amount of data.The skip-gram model is the technique of choice when a large amount of data is available.The skip-gram model uses a single target word w as the input and outputs the m context words denoted by w 1 . . .w m .Therefore, the goal is to estimate P (w 1 , w 2 . . . .w m |w), which is different from the quantity P (w|w 1 . . .w m ) estimated in the CBOW model.As in the case of the continuous bag-of-words model, we can use one-hot encoding of the (categorical) input and outputs in the skip-gram model.After such an encoding, the skip-gram model will have d binary inputs denoted by x 1 . . .x d corresponding to the d possible values of the single input word.Similarly, the output of each training instance is encoded as m × d values y ij ∈ {0, 1}, where i ranges from 1 to m (size of context window), and j ranges from 1 to d (lexicon size).Each y ij ∈ {0, 1} indicates whether the ith contextual word takes on the jth possible value for that training instance.However, the (i, j)th output node only computes a soft probability value ŷij = P (y ij = 1|w).Therefore, the probabilities ŷij in the output layer for fixed i and varying j sum to 1, since the ith contextual position takes on exactly one of the d words.The hidden layer contains p units, the outputs are denoted by h 1 . . .h p .Each input x j is connected to all the hidden nodes with a d × p matrix U .Furthermore, the p hidden nodes are connected to each of the m groups of d output nodes with the same set of shared weights.This set of shared weights between the p hidden nodes and the d output nodes of each of the context words is defined by the p × d matrix V .Note that the input-output structure of the skip-gram model is an inverted version of the input-output structure of the CBOW model.The neural architecture of the skip-gram model is illustrated in Figure 2.16(a).However, in the case of the skip-gram model, one can collapse the m identical outputs into a single output, and achieve the same results simply by using a particular type of mini-batching during stochastic gradient descent.In particular, all elements of a single context window are always forced to belong to the same mini-batch.This architecture is shown in Figure 2.16(b).Since the value of m is small, this specific type of mini-batching has a very limited effect, and the simplified architecture of Figure 2.16(b) is sufficient to describe the model whether or not any specific type of mini-batching is used.For the purpose of further discussion, we will use the architecture of Figure 2.16(a).The output of the hidden layer can be computed from the input layer using the d × p matrix of weights U = [u jq ] between the input and hidden layer as follows:(2.51)The above equation has a simple interpretation because of the one-hot encoding of the input word w in terms of x 1 . . .x d .If the input word w is the rth word, then one simply copies u rq to the qth node of the hidden layer for each q ∈ {1 . . .p}.In other words, the rth row u r of U is copied to the hidden layer.As discussed above, the hidden layer is connected to m groups of d output nodes, each of which is connected to the hidden layer with a p × d matrix V = [v qj ].Each of these m groups of d output nodes computes the probabilities of the various words for a particular context word.The jth column of V is denoted by v j and represents the output embedding of the jth word.The output ŷij is the probability that the word in the ith context position takes on the jth word of the lexicon.However, since the same matrix V is shared by all groups, the neural network predicts the same multinomial distribution for each of the context words.Therefore, we have the following:Note that the probability ŷij is the same for varying i and fixed j, since the right-hand side of the above equation does not depend on the exact location i in the context window.The loss function for the backpropagation algorithm is the negative of the log-likelihood values of the ground truth y ij ∈ {0, 1} of a training instance.This loss function L is given by the following:(2.53) Note that the value outside the logarithm is a ground-truth binary value, whereas the value inside the logarithm is a predicted (probability) value.Since y ij is one-hot encoded for fixed i and varying j, the objective function has only m non-zero terms.For each training instance, this loss function is used in combination with backpropagation to update the weights of the connections between the nodes.The update equations with learning rate α are as follows:We state the details of the updates below after introducing some additional notations.The probability of making a mistake in predicting the jth word in the lexicon for the ith context is defined by |y ij − ŷij |.However, we use signed mistakes ij in which only the predicted words (positive examples) have a positive probability.This is achieved by dropping the modulus:Then, the updates for a particular input word r and its output context are as follows:Here, α > 0 is the learning rate.The p-dimensional rows of the matrix U are used as the embeddings of the words.In other words, the convention is to use the input embeddings in the rows of U rather than the output embeddings in the columns of V .It is stated in [288] that adding the input and output embeddings can help in some tasks (but hurt in others).The concatenation of the two can also be useful.Several practical issues are associated with the accuracy and efficiency of the word2vec framework.The embedding dimensionality, defined by the number of nodes in the hidden layer, provides the trade-off between bias and variance.Increasing the embedding dimensionality improves discrimination, but it requires a greater amount of data.In general, the typical embedding dimensionality is of the order of several hundred, although it is possible to choose dimensionalities in the thousands for very large collections.The size of the context window typically varies between 5 and 10, with larger window sizes being used for the skip-gram model as compared to the CBOW model.Using a random window size is a variant that has the implicit effect of giving greater weight to words that are placed close together.The skip-gram model is slower but it works better for infrequent words and for larger data sets.Another issue is that the effect of frequent and less discriminative words (e.g., "the") can dominate the results.Therefore, a common approach is to downsample the frequent words, which improves both accuracy and efficiency.Note that downsampling frequent words has the implicit effect of increasing the context window size because dropping a word in the middle of two words brings the latter pair closer.The words that are very rare are misspellings, and it is hard to create a meaningful embedding for them without overfitting.Therefore, such words are ignored.From a computational point of view, the updates of output embeddings are expensive.This is caused by applying the softmax over a lexicon of d words, which requires an update of each v j .Therefore, the softmax function is implemented hierarchically with Huffman encoding for better efficiency.We refer the reader to [325,327,404] for details.An efficient alternative to the hierarchical softmax technique is a method known as skipgram with negative sampling (SGNS) [327], in which both presence or absence of wordcontext pairs are used for training.As the name implies, the negative contexts are artificially generated by sampling words in proportion to their frequencies in the corpus (i.e., unigram distribution).This approach optimizes a different objective function from the skip-gram model, which is related to ideas from noise contrastive estimation [166,333,334].The basic idea is that instead of directly predicting each of the m words in the context window, we try to predict whether or not each of the d words in the lexicon is present in the window.In other words, the final layer of Figure 2.16 is not a softmax prediction, but a Bernoulli layer of sigmoids.The output unit for each word at each context position in Figure 2.16 is a sigmoid providing a probability value that the position takes on that word.As the ground-truth values are also available, it is possible to use the logistic loss function over all the words.Therefore, in this point of view, even the prediction problem is defined differently.Of course, it is computationally inefficient to try to make binary predictions for all d words.Therefore, the SGNS approach uses all the positive words in a context window and a sample of negative words.The number of negative samples is k times the number of positive samples.Here, k is a parameter controlling the sampling rate.Negative sampling becomes essential in this modified prediction problem to avoid learning trivial weights that predict all examples to 1.In other words, we cannot choose to avoid negative samples entirely (i.e., we cannot set k = 0).How does one generate the negative samples?The vanilla unigram distribution samples words in proportion to their relative frequencies f 1 . . .f d in the corpus.Better results are obtained [327] by sampling words in proportion to f 3/4 j rather than f j .As in all word2vec models, let U be a d × p matrix representing the input embedding, and V be a p × d matrix representing the output embedding.Let u i be the p-dimensional row of U (input embedding of ith word) and v j be the p-dimensional column of V (output embedding of jth word).Let P be the set of positive target-context word pairs in a context window, and N be the set of negative target-context word pairs which are created by sampling.Therefore, the size of P is equal to the context window m, and that of N is m • k.Then, the (minimization) objective function for each context window is obtained by summing up the logistic loss over the m positive samples and m • k negative samples:This modified objective function is used in the skip-gram with negative sampling (SGNS) model in order to update the weights of U and V .SGNS is mathematically different from the basic skip-gram model discussed earlier.SGNS is not only efficient, but it also provides the best results among the different variants of skip-gram models.What Is the Actual Neural Architecture of SGNS?Even though the original word2vec paper seems to treat SGNS as an efficiency optimization of the skip-gram model, it is using a fundamentally different architecture in terms of the activation function used in the final layer.Unfortunately, the original word2vec paper does not explicitly point this out (and only provides the changed objective function), which causes confusion.The modified neural architecture of SGNS is as follows.The softmax layer is no longer used in the SGNS implementation.Rather, each observed value y ij in Figure 2.16 is independently treated as a binary outcome, rather than as a multinomial outcome in which the probabilistic predictions of different outcomes at a contextual position depend on one another.Instead of using softmax to create the prediction ŷij , it uses the sigmoid activation to create probabilistic predictions ŷij , whether each y ij is 0 or 1.Then, one can add up the log loss of ŷij with respect to observed y ij over all m • d possible values of (i, j) to create the full loss function of a context window.However, this is impractical because the number of zero values of y ij is too large and zero values are noisy anyway.Therefore, SGNS uses negative sampling to approximate this modified objective function.This means that for each context window, we are backpropagating from only a subset of the m • d outputs in Figure 2.16.The size of this subset is m + m • k.This is where efficiency is achieved.However, since the final layer uses binary predictions (with sigmoids), it makes the SGNS architecture fundamentally different from the vanilla skip-gram model even in terms of the basic neural network it uses (i.e., logistic instead of softmax activation).The difference between the SGNS model and the vanilla skip-gram model is analogous to the difference between the Bernoulli and multinomial models in naïve Bayes classification (with negative sampling applied only to the Bernoulli model).Obviously, one cannot be considered a direct efficiency optimization of the other.Even though the work in [287] shows an implicit relationship between word2vec and matrix factorization, we provide a more direct relationship here.The architectures of the skipgram models look suspiciously similar to those used in row index to value prediction in recommender systems (cf.Section 2.5.7).The use of a backpropagation from a subset of observed outputs is similar to the negative sampling idea, except that the dropping of outputs in negative sampling is performed for the purpose of efficiency.However, unlike the linear outputs of Figure 2.13 in Section 2.5.7, the SGNS model uses logistic outputs to model binary predictions.The SGNS model of word2vec can be simulated with logistic matrix factorization.To understand the similarity with the problem setting of Section 2.5.7, one can understand the predictions of a particular word-context window using the following triplets:WordId , Context WordId , 0/1 Each context window produces m • d such triplets, although negative sampling only uses m • k + m of them, and mini-batches them during training.This mini-batching is another source of the difference between the architectures between Figures 2.13 and 2.16, wherein the latter has m different groups of outputs to accommodate m positive samples.However, these differences are relatively superficial, and one can still use logistic matrix factorization to represent the underlying model.Let B = [b ij ] be a binary matrix in which the (i, j)th value is 1 if word j occurs at least once in the context of word i in the data set, and 0 otherwise.The weight c ij for any word (i, j) that occurs in the corpus is defined by the number of times word j occurs in the context of word i.The weights of the zero entries in B are defined as follows.For each row i in B we sample k j b ij different entries from row i, among the entries for which b ij = 0, and the frequency with which the jth word is sampled is proportional to f 3/4 j .These are the negative samples, and one sets the weights c ij for the negative samples (i.e., those for which b ij = 0) to the number of times that each entry is sampled.As in word2vec, the p-dimensional embeddings of the ith word and jth context are denoted by u i and v j , respectively.The simplest way of factorizing is to use weighted matrix factorization of B with the Frobenius norm:(2.57)Even though the matrix B is of size O(d 2 ), this matrix factorization only has a limited number of nonzero terms in the objective function, which have c ij > 0. These weights are dependent on co-occurrence counts, but some zero entries also have positive weight.Therefore, the stochastic gradient-descent steps only have to focus on entries with c ij > 0.Each cycle of stochastic gradient-descent is linear in the number of non-zero entries, as in the SGNS implementation of word2vec.However, this objective function also looks somewhat different from word2vec, which has a logistic form.Just as it is advisable to replace linear regression with logistic regression in supervised learning of binary targets, one can use the same trick in matrix factorization of binary matrices [224].We can change the squared error term to the familiar likelihood term L ij , which is used in logistic regression:The value of L ij always lies in the range (0, 1), and higher values indicate greater likelihood (which results in a maximization objective).The modulus in the above expression flips the sign only for the negative samples in which b ij = 0. Now, one can optimize the following objective function in minimization form:The main difference from the objective function (cf.Equation 2.56) of word2vec is that this is a global objective function over all matrix entries, rather than a local objective function over a particular context window.Using mini-batch stochastic gradient-descent in matrix factorization (with an appropriately chosen mini-batch) makes the approach almost identical to word2vec's backpropagation updates.How can one interpret this type of factorization?Instead of B ≈ UV , we have B ≈ f (UV ), where f (•) is the sigmoid function.More precisely, this is a probabilistic factorization in which one computes the product of matrices U and V , and then applies the sigmoid function to obtain the parameters of the Bernoulli distribution from which B is generated:It is also easy to verify from Equation 2.58 that L ij is P (b ij = 1) for positive samples and P (b ij = 0) for negative samples.Therefore, the objective function of the factorization is in the form of log-likelihood maximization.This type of logistic matrix factorization is commonly used [224] in recommender systems with binary data (e.g., user click-streams).It is also helpful to examine the gradient-descent steps of the factorization.One can compute the derivative of J with respect to the input and output embeddings:The optimization procedure uses gradient descent to convergence:It is noteworthy that the derivatives can be expressed in terms of the probabilities of making mistakes in predicting b ij .This is common in gradient descent with log-likelihood optimization.It is also noteworthy that the derivative of the SGNS objective in Equation 2.56 yields a similar form of the gradient.The only difference is that the derivative of the SGNS objective is expressed over a smaller batch of instances, defined by a context window.We can also solve the probabilistic matrix factorization with mini-batch stochastic gradient descent.With an appropriate choice of the mini-batch, the stochastic gradient descent of matrix factorization becomes identical to the backpropagation update of SGNS.The only difference is that SGNS samples negative entries for each set of updates on the fly, whereas matrix factorization fixes the negative samples up front.Of course, on-the-fly sampling can also be used with matrix factorization updates.The similarity of SGNS to matrix factorization can also be inferred by observing that the architecture of Figure 2.16(b) is almost identical to the matrix factorization architecture for recommender systems in Figure 2.13.As in the case of recommender systems, SGNS has missing (negative) entries.This is caused by the fact the negative sampling uses only a subset of the zero values.The only difference between the two cases is that the architecture of SGNS caps the output layer with sigmoid units, whereas a linear layer is used for recommender systems.However, recommender systems with implicit feedback use logistic matrix factorization [224], which is similar to the word2vec setting.Since we have already shown that the SGNS enhancement of the skip-gram model is logistic matrix factorization, a natural question arises as to whether we can also recast the original skip-gram model as a matrix factorization method.It turns out that one can also recast the vanilla skip-gram model as a multinomial matrix factorization model because of the use of the softmax layer at the very end.Let C = [c ij ] be a d × d word-context co-occurrence matrix in which the value of c ij is the number of times that word j occurs in the context of word i.Let U be a d × p matrix containing the input embedding in its rows, and V be a p × d matrix containing the output embedding in its columns.Then, the skip-gram model roughly creates a model in which the frequency vector of the rth row of C is an empirical instantiation of the probabilities obtained by applying the softmax to the rth row of UV .Let u i be the p-dimensional vector corresponding to the ith row of U and v j be the p-dimensional vector corresponding to the jth column of V .The loss function of the aforementioned factorization is as follows:This loss function is written in minimization form.Note that this loss function is identical to the one used in the vanilla skip-gram model, except that the latter uses a mini-batch stochastic gradient descent in which the m words in a given context are grouped together.This type of specific mini-batch does not make a significant difference.Large networks have become very common because of their ubiquity in many social-and Web-centric applications.Graphs are structural entries containing nodes and edges connecting them.For example, in a social network, each person is a node, and a friendship link between two people is an edge.In this particular exposition, we consider the case of very large networks like the Web, a social network, or a communication network.The goal is to embed the nodes into feature vectors, so that the graph captures the relationships between nodes.For simplicity we consider undirected graphs, although directed graphs with weights on the edges can be easily handled with very few changes to the exposition below.Consider an n×n adjacency matrix B = [b ij ] for a graph with n nodes.The entry b ij is 1 if an undirected edge exists between nodes i and j.Furthermore, the matrix B is symmetric, because we have b ij = b ji for an undirected graph.In order to determine the embedding, we would like to determine two n × p factor matrices U and V , so that B can be derived as a function of UV T .In the simplest case, one can set B to exactly UV T , which is no different than a traditional matrix factorization method for factoring graphs [4].However, for binary matrices, one can do better and use logistic matrix factorization instead.In other words, each entry of B is generated using the matrix of Bernoulli parameters in f (UV T ), where f (•) is the element-wise application of the sigmoid function to each entry of the matrix in its argument:(2.61) The main difference is that there are no missing values above, and the number of inputs is the same as the number of outputs for a square matrix.Both input and outputs are binary vectors.However, if negative sampling is used with sigmoid activation, most output nodes with zero values may be dropped.Therefore, if u i is the ith row of U and v j is the jth row of V , we have the following:This type of generative model is typically solved using a log-likelihood model.Furthermore, the problem formulation is identical to the logistic matrix factorization equivalent of the SGNS model in word2vec.Note that all word2vec models are logistic/multinomial variants of the model in Figure 2.13 that maps row indexes to values with linear activation.In order to explain this point, we show the neural architecture in Figure 2.17 for a toy graph containing 5 nodes.The input is the one-hot encoded index of a row in B (i.e., node), and the output is the list of all 0/1 values for all nodes in the network.In this particular case, we have shown the input for node 3 and its corresponding output.Since the node 3 has three neighbors, the output vector contains three 1s.Note that this architecture is not very different from Figure 2.13 except that it uses a sigmoid activations at the output (rather than linear activations).Furthermore, since the number of 0s is usually much greater 8 than the number of 1s in the output, it is possible to drop many of the 0s with the use of negative sampling.This type of negative sampling will create a situation similar to that of Figure 2.14.With this neural architecture, the gradient-descent steps will be identical to the SGNS model of word2vec.The main difference is that a node appears at most once as a neighbor of another node, whereas a word might appear more than once in the context of another word.Allowing arbitrary counts on the edges takes away this distinction.The aforementioned discussion assumes that the weight of each edge is binary.Consider a setting in which an arbitrary count c ij is associated with the edge (i, j).In such cases, both positive and negative sampling are required.The first step is to sample an edge (i, j) from the network with probability proportional to c ij .The input is, therefore, a one-hot encoded vector of the node at one end point (say, i) of this edge.The output is the onehot encoding of node j.By default, both the input and output are n-dimensional vectors.However, if negative sampling is used, then one can reduce the output vector to a (k + 1)dimensional vector.Here, k n is a parameter that defines the sampling rate.A total of k negative nodes are sampled with probabilities proportional to their (weighted) degrees 9 and the outputs of these nodes are 0s.One can compute the log-likelihood loss by treating each output as the outcome of a Bernoulli trial, where the parameter of the Bernoulli trial is the output of the sigmoid activation function.The gradient descent is performed with respect to this loss.This variant is an almost exact simulation of the SGNS variant of the word2vec model.The vanilla skip-gram model of word2vec is a multinomial model.It is also possible to use a multinomial model to create the embedding.The only difference is that the final layer of the neural network in Figure 2.17 needs to use softmax activation (instead of the sigmoid activation function).Furthermore, negative sampling is not used in the multinomial model, and both input and output layers contain exactly n nodes.As in the SGNS model, a single edge (i, j) is sampled with probability proportional to c ij to create each input-output pair.The input is the one-hot encoding of i and the output is the one-hot encoding of j.One can also use mini-batch sampling of edges to improve performance.The stochastic gradientdescent steps of this model are virtually similar to those used in the vanilla skip-gram model of word2vec.The recently proposed DeepWalk [372] and node2vec models [164] belong to the family of multinomial models discussed above (with specialized preprocessing steps).The main difference is that the DeepWalk and node2vec models use depth-first or breadth-first walks to (indirectly) generate c ij .DeepWalk is itself a precursor to (and special case of) node2vec in terms of how the random walks are performed.In this case, c ij can be interpreted as the number of times that node j appears in the neighborhood of node i because it was included in a breadth-first or depth-first walk starting at node i.One can view the value of c ij in the walk-based models as providing a more robust measure of the affinity between nodes i and j, as compared to the raw weights in the original graph.Of course, there is nothing sacrosanct about using a random walk to improve the robustness of c ij .The number of choices is almost unlimited in terms of how to generate this type of affinity value.All link prediction methods [295] generate such affinity values.For example, the Katz measure [295], which is closely related to the number of random walks between a pair of nodes, is a robust measure of the affinity between nodes i and j. 9 The weighted degree of node j is r c rj .This chapter discusses a number of neural models supervised and unsupervised learning.One of the goals was to show that many of the traditional models used in machine learning are instantiations of relatively simple neural models.Methods for binary/multiclass classification and matrix factorization were discussed.In addition, the applications of the approach to recommender systems and word embedding were introduced.When a traditional machine learning technique like singular value decomposition is generalized to a neural representation, it is often inefficient compared to its counterpart in traditional machine learning.However, the advantage of neural models is that they can usually be generalized to more powerful nonlinear models.Furthermore, it is relatively easy to experiment with nonlinear variants of traditional machine learning models with the use of neural networks.This chapter also discusses several practical applications like recommender systems, text, and graph embeddings.The perceptron algorithm was proposed by Rosenblatt [405], and a detailed discussion may be found in [405].The Widrow-Hoff algorithm was proposed in [531] and is closely related to Tikhonov-Arsenin's work [499].The Fisher discriminant was proposed by Ronald Fisher [120] in 1936, and is a specific case of the family of linear discriminant analysis methods [322].Even though the Fisher discriminant uses an objective function that appears o to be different from least-squares regression, it turns out to be a special case of least-squares regression in which the binary response variable is used as the regressand [40].A detailed discussion of generalized linear models is provided in [320].A variety of procedures such as generalized iterative scaling, iteratively reweighted least-squares, and gradient descent for multinomial logistic regression are discussed in [178].The support-vector machine is generally credited to Cortes and Vapnik [82], although the primal method for L 2 -loss SVMs was proposed several years earlier by Hinton [190]!This approach repairs the loss function in least-squares classification by keeping only one-half of the quadratic loss curve and setting the remaining to zero, so that it looks like a smooth version of hinge loss (try this on Figure 2.4).The specific significance of this contribution was lost within the broader literature on neural networks.Hinton's work also does not focus on the importance of regularization in SVMs, although the general notion of adding shrinkage to gradient-descent steps in neural networks was well known.The hinge-loss SVM [82] is heavily presented from the perspective of duality and the maximum-margin interpretation, making its relationship to regularized least-squares classification somewhat opaque.The relationship of SVMs to least-squares classification is more evident from other related works [400,442], where it becomes evident that quadratic and hinge-loss SVMs are natural variations of regularized L 2 -loss (i.e., Fisher discriminant) and L 1 -loss classification that use the binary class variables as the regression responses [139].The Weston-Watkins multiclass SVM was introduced in [529].It was shown in [401] that the one-against-all approach to generalizing multiple classes seems to be as effective as the tightly integrated multiclass variants.Many hierarchical softmax methods are discussed in [325,327,332,344].An excellent overview paper on methods for reducing the dimensionality of data with neural networks is available in [198], although this work focuses on the use of a related model known as the Restricted Boltzmann Machine (RBM).The earliest introduction of the autoencoder (in a more general form) is given in the backpropagation paper [408].This work discusses the problem of recoding between input and output patterns.Both classification and autoencoders can be considered special cases of this architecture by using an appropriate choice of input and output patterns.The paper on backpropagation [408] also discusses the special case in which the recoding of the input is the identity mapping, which is exactly the scenario of the autoencoder.More detailed discussions of the autoencoder during its early years were provided in [48,275].A discussion of single-layer unsupervised learning may be found in [77].The standard method for regularizing an autoencoder is to use weight decay, which corresponds to L 2 -regularization.Sparse autoencoders are discussed in [67,273,274,284,354]. Another way of regularizing the autoencoder is to penalize the derivatives during gradient descent.This ensures that the learned function does not change too much with change in input.This method is referred to as the contractive autoencoder [397].Variational autoencoders can encode complex probabilistic distributions, and are discussed in [106,242,399].The de-noising autoencoder is discussed in [506].Many of these methods are discussed in detail in Chapter 4. The use of autoencoders for outlier detection is explored in [64,181,564], and a survey on the use in clustering is provided in [8].The application of dimensionality reduction for recommender systems may be found in [414], although this approach uses a restricted Boltzmann machine, which is different from the matrix factorization method discussed in this chapter.An item-based autoencoder is discussed in [436], and this approach is a neural generalization of item-based neighborhood regression [253].The main difference is that the regression weights are regularized with a constricted hidden layer.Similar works with different types of item-to-item models with the use of de-noising autoencoders are discussed in [472,535].A more direct generalization of matrix factorization methods may be found in [186], although the approach in [186] is slightly different from the simpler approach presented in this chapter.The incorporation of content in building recommender systems for deep learning is discussed in [513].A multiview deep learning approach, which has also been extended to temporal recommender systems in a later work [465], is proposed in [110].A survey of deep learning methods for recommenders may be found in [560].The word2vec model is proposed in [325,327], and a detailed exposition may be found in [404].The basic idea has been extended to sentence-and paragraph-level embeddings, with a model, which is referred to as doc2vec [272].An alternative of word2vec that uses a different type of matrix factorization is GloVe [371].Multi-lingual word embeddings are presented in [9].The extension of word2vec to graphs with node-level embeddings is provided in the DeepWalk [372] and node2vec [164] models.Various types of network embeddings are discussed in [62,512,547,548].Machine learning models like linear regression, SVMs, and logistic regression are available from scikit-learn [587].The DISSECT (Distributional Semantics Composition Toolkit) [588] is a toolkit that uses word co-occurrence counts in order to create embeddings.The GloVe method is available from Stanford NLP [589] and the gensim library [394].The word2vec tool is available under the Apache license [591], and as a TensorFlow version [592].The gensim library has Python implementations of word2vec and doc2vec [394].Java versions of doc2vec, word2vec, and GloVe may be found in the DeepLearning4j repository [590].In several cases, one can simply download pre-trained versions of the representations (on a large corpus that is considered generally representative of text) and use them directly, as a convenient alternative to training for the specific corpus at hand.The node2vec software is available from the original author at [593].1. Consider the following loss function for training pair (X, y):The test instances are predicted as ŷ = sign{W • X}.A value of a = 0 corresponds to the perceptron criterion and a value of a = 1 corresponds to the SVM.Show that any value of a > 0 leads to the SVM with an unchanged optimal solution when no regularization is used.What happens when regularization is used? 2. Based on Exercise 1, formulate a generalized objective for the Weston-Watkins SVM.α. Show that using any value of α is inconsequential in the sense that it only scales up the weight vector by a factor of α.Show that these results also hold true for the multiclass case.Do the results hold true when regularization is used?4. Show that if the Weston-Watkins SVM is applied to a data set with k = 2 classes, the resulting updates are equivalent to the binary SVM updates discussed in this chapter.Show that if multinomial logistic regression is applied to a data set with k = 2 classes, the resulting updates are equivalent to logistic regression updates.Implement the softmax classifier using a deep-learning library of your choice.In linear-regression-based neighborhood models, the rating of an item is predicted as a weighted combination of the ratings of other items of the same user, where the item-specific weights are learned with linear regression.Show how you can construct an autoencoder architecture to create this type of model.Discuss the relationship of this architecture with the matrix factorization architecture.Consider an autoencoder which has an input layer, a single hidden layer containing the reduced representation, and an output layer with sigmoid units.The hidden layer has linear activation:(a) Set up a negative log-likelihood loss function for the case when the input data matrix is known to contain binary values from {0, 1}.(b) Set up a negative log-likelihood loss function for the case when the input data matrix contains real values from [0, 1].Let D be an n × d data matrix with non-negative entries.Show how you can approximately factorize D ≈ UV T into two non-negative matrices U and V , respectively, by using an autoencoder architecture with d inputs and outputs.[Hint: Choose an appropriate activation function in the hidden layer, and modify the gradient-descent updates.]10.Probabilistic latent semantic analysis: Refer to [99,206] for a definition of probabilistic latent semantic analysis.Propose a modification of the approach in Exercise 9 for probabilistic latent semantic analysis.[Hint: What is the relationship between non-negative matrix factorization and probabilistic latent semantic analysis?]11. Simulating a model combination ensemble: In machine learning, a model combination ensemble averages the scores of multiple models in order to create a more robust classification score.Discuss how you can approximate the averaging of an Adaline and logistic regression with a two-layer neural network.Discuss the similarities and differences of this architecture with an actual model combination ensemble when backpropagation is used to train it.Show how to modify the training process so that the final result is a fine-tuning of the model combination ensemble.12. Simulating a stacking ensemble: In machine learning, a stacking ensemble creates a higher-level classification model on top of features learned from first-level classifiers.Discuss how you can modify the architecture of Exercise 11, so that the first level of classifiers correspond to an Adaline and a logistic regression classifier and the higherlevel classifier corresponds to a support vector machine.Discuss the similarities and differences of this architecture with an actual stacking ensemble when backpropagation is used to train it.Show how you can modify the training process of the neural network so that the final result is a fine-tuning of the stacking ensemble.13. Show that the stochastic gradient-descent updates of the perceptron, Widrow-Hoff learning, SVM, and logistic regression are all of the form W ⇐ W (1 − αλ) + αy[δ(X, y)]X.Here, the mistake function δ(X, y) is 1 − y(W • X) for least-squares classification, an indicator variable for perceptron/SVMs, and a probability value for logistic regression.Assume that α is the learning rate, and y ∈ {−1, +1}.Write the specific forms of δ(X, y) in each case.(a) For a fixed value of V , show that the optimal matrix W must satisfy(d) Repeat exercise parts (a), (b), and (c), when the encoder-decoder weights are tied as W = V T .Show that the columns of V must be orthonormal.Training Deep Neural Networks "I hated every minute of training, but I said, 'Don't quit.Suffer now and live the rest of your life as a champion."-MuhammadAliThe procedure for training neural networks with backpropagation is briefly introduced in Chapter 1.This chapter will expand on the description on Chapter 1 in several ways:1.The backpropagation algorithm is presented in greater detail together with implementation details.Some details from Chapter 1 are repeated for completeness of the presentation, and so that readers do not have to frequently refer back to the earlier text.2. Important issues related to feature preprocessing and initialization will be studied in the chapter.3. The computational procedures that are paired with gradient descent will be introduced.The effect of network depth on training stability will be studied, and methods will be presented for addressing these issues.4. The efficiency issues associated with training will be discussed.Methods for compressing trained models of neural networks will be presented.Such methods are useful for deploying pretrained networks on mobile devices.In the early years, methods for training multilayer networks were not known.In their influential book, Minsky and Papert [330] strongly argued against the prospects of neural networks because of the inability to train multilayer networks.Therefore, neural networks stayed out of favor as a general area of research till the eighties.The first significant breakthrough in this respect was proposed 1 by Rumelhart et al. [408,409] in the form of the backpropagation algorithm.The proposal of this algorithm rekindled an interest in neural networks.However, several computational, stability, and overfitting challenges were found in the use of this algorithm.As a result, research in the field of neural networks again fell from favor.At the turn of the century, several advances again brought popularity to neural networks.Not all of these advances were algorithm-centric.For example, increased data availability and computational power have played the primary role in this resurrection.However, some changes to the basic backpropagation algorithm and clever methods for initialization, such as pretraining, have also helped.It has also become easier in recent years to perform the intensive experimentation required for making algorithmic adjustments due to the reduced testing cycle times (caused by improved computational hardware).Therefore, increased data, computational power, and reduced experimentation time (for algorithmic tweaking) went hand-in-hand.These so-called "tweaks" are, nevertheless, very important; this chapter and the next will discuss most of these important algorithmic advancements.One key point is that the backpropagation algorithm is rather unstable to minor changes in the algorithmic setting, such as the initialization point used by the approach.This instability is particularly significant when one is working with very deep networks.A point to note is that neural network optimization is a multivariable optimization problem.These variables correspond to the weights of the connections in various layers.Multivariable optimization problems often face stability challenges because one must perform the steps along each direction in the "right" proportion.This turns out to be particularly hard in the neural network domain, and the effect of a gradient-descent step might be somewhat unpredictable.One issue is that a gradient only provides a rate of change over an infinitesimal horizon in each direction, whereas an actual step has a finite length.One needs to choose steps of reasonable size in order to make any real progress in optimization.The problem is that the gradients do change over a step of finite length, and in some cases they change drastically.The complex optimization surfaces presented by neural network optimization are particularly treacherous in this respect, and the problem is exacerbated with poorly chosen settings (such as the initialization point or the normalization of the input features).As a result, the (easily computable) steepest-descent direction is often not the best direction to use for retaining the ability to use large steps.Small step sizes lead to slow progress, whereas the optimization surface might change in unpredictable ways with the use of large step sizes.All these issues make neural network optimization more difficult than would seem at first sight.However, many of these problems can be avoided by carefully tailoring the gradient-descent steps to be more robust to the nature of the optimization surface.This chapter will discuss algorithms that leverage some of this understanding.This chapter is organized as follows.The next section reviews the backpropagation algorithm initially discussed in Chapter 1.The discussion in this chapter is more detailed, and several variants of the algorithm are discussed.Some parts of the backpropagation algorithm that were already discussed in Chapter 1 are repeated so that this chapter is self-contained.Feature preprocessing and initialization issues are discussed in Section 3.3.The vanishing and exploding gradient problem, which is common in deep networks, is discussed in Section 3.4, with common solutions for dealing with this issue presented.Gradient-descent strategies for deep learning are discussed in Section 3.5.Batch normalization methods are introduced in Section 3.6.A discussion of accelerated implementations of neural networks is found in Section 3.7.The summary is presented in Section 3.8.In this section, the backpropagation algorithm from Chapter 1 is reviewed again in considerably more detail.The goal of this more-detailed review is to show that the chain rule can be used in multiple ways.To this end, we first explore the standard backpropagation update as it is commonly presented in most textbooks (and Chapter 1).Second, a simplified and decoupled view of backpropagation is examined in which the linear matrix multiplications are decoupled from the activation layers.This decoupled view of backpropagation is what most off-the-shelf systems implement.A neural network is a computational graph, in which a unit of computation is the neuron.Neural networks are fundamentally more powerful than their building blocks because the parameters of these models are learned jointly to create a highly optimized composition function of these models.Furthermore, the nonlinear activations between the different layers add to the expressive power of the network.A multilayer network evaluates compositions of functions computed at individual nodes.A path of length 2 in the neural network in which the function f (•) follows g(•) can be considered a composition function f (g(•)).Just to provide an idea, let us look at a trivial computational graph with two nodes, in which the sigmoid function is applied at each node to the input weight w.In such a case, the computed function appears as follows:We can already see how awkward it would be to compute the derivative of this function with respect to w.Furthermore, consider the case in which g 1 (•), g 2 (•) . . .g k (•) are the functions computed in layer m, and they feed into a particular layer-(m +1) node that computes f (•).In such a case, the composition function computed by the layer-(m + 1) node in terms of the layer-m inputs is f (g 1 (•), . . .g k (•)).As we can see, this is a multivariate composition function, which looks rather ugly.Since the loss function uses the output(s) as its argument(s), it may typically be expressed a recursively nested function in terms of the weights in earlier layers.For a neural network with 10 layers and only 2 nodes in each layer, a recursively nested function of depth 10 will result in a summation of 2 10 recursively nested terms, which appear forbidding from the perspective of computing partial derivatives.Therefore, we need some kind of iterative approach to compute these derivatives.The resulting iterative approach is dynamic programming, and the corresponding update is really the chain rule of differential calculus.In order to understand how the chain rule works in a computational graph, we will discuss the two basic variants of the rule that one needs to keep in mind.The simplest version of the chain rule works for a straightforward composition of the functions:  Only two paths between input and output exist in this simplified example.This variant is referred to as the univariate chain rule.Note that each term on the righthand side is a local gradient because it computes the derivative of a function with respect to its immediate argument rather than a recursively derived argument.The basic idea is that a composition of functions is applied on the weight w to yield the final output, and the gradient of the final output is given by the product of the local gradients along that path.Each local gradient only needs to worry about its specific input and output, which simplifies the computation.An example is shown in Figure 3.1 in which the function f (y) is cos(y) and g(w) = w 2 .Therefore, the composition function is cos(w 2 ).On using the univariate chain rule, we obtain the following:The computational graphs in neural networks are not paths, which is the main reason that backpropagation is needed.A hidden layer often gets its input from multiple units, which results in multiple paths from a variable w to an output.Consider the function-sin(y) f (g 1 (w), . . .g k (w)), in which a unit computing the multivariate function f (•) gets its inputs from k units computing g 1 (w) . . .g k (w).In such cases, the multivariable chain rule needs to be used.The multivariable chain rule is defined as follows:It is easy to see that the multivariable chain rule of Equation 3.3 is a simple generalization of that in Equation 3.2.An important consequence of the multivariable chain rule is as follows:Lemma 3.2.1 (Pathwise Aggregation Lemma) Consider a directed acyclic computational graph in which the ith node contains variable y(i).The local derivative z(i, j) of the directed edge (i, j) in the graph is defined as z(i, j) = ∂y(j) ∂y(i) .Let a non-null set of paths P exist from variable w in the graph to output node containing variable o.Then, the value of ∂o ∂w is given by computing the product of the local gradients along each path in P, and summing these products over all paths.This lemma can be easily shown by applying Equation 3.3 recursively.Although Lemma 3.2.1 is not used anywhere in the backpropagation algorithm, it helps us develop another exponential-time algorithm that computes the derivatives explicitly.This point of view helps us interpret the multivariable chain rule as a dynamic programming recursion to compute a quantity that would otherwise be computationally too expensive to evaluate.Consider the example shown in Figure 3.2.There are two paths in this particular case.The recursive application of the chain rule is also shown in this example.It is evident that the final result is obtained by computing the product of the local gradients along each of the two paths and then adding them.In Figure 3.3, we have shown a more concrete example of a function that is evaluated by the same computational graph.We have also shown in Figure 3.3 that the application of the chain rule on the computational graph correctly evaluates the derivative, which is −2w • sin(w 2 ) + 2w • cos(w 2 ).The fact that we can compute the composite derivative as an aggregation of the products of local derivatives along all paths in the computational graph leads to the following exponential-time algorithm:1. Use computational graph to compute the value y(i) of each nodes i in a forward phase.2. Compute the local partial derivatives z(i, j) = ∂y(j) ∂y(i) on each edge in the computational graph.3. Let P be the set of all paths from an input node with value w to the output.For each path P ∈ P compute the product of each local derivative z(i, j) on that path.In general, a computational graph will have an exponentially increasing number of paths with depth and one must add the product of the local derivatives over all paths.An example is shown in Figure 3.4, in which we have five layers, each of which has only two units.Therefore, the number of paths between the input and output is 2 5 = 32.The jth hidden unit of the ith layer is denoted by h(i, j).Each hidden unit is defined as the product of its inputs:In this case, the output is w 32 , which is expressible in closed form, and can be differentiated easily with respect to w.However, we will use the exponential time algorithm to elucidate the workings of the exponential time algorithm.The derivative of each h(i, j) with respect to each of its two inputs are the values of the complementary inputs:The pathwise aggregation lemma implies that the value of ∂o ∂w is the product of the local derivatives (which are the complementary input values in this particular case) along all 32 paths from the input to the output:h(5, j 5 )  2,1) h(3,1) h(4,1) h(5,1) Figure 3.4: The number of paths in a computational graph increases exponentially with depth.In this case, the chain rule will aggregate the product of local derivatives along 2 5 = 32 paths.This result is, of course, consistent with what one would obtain on differentiating w 32 directly with respect to w.However, an important observation is that it requires 2 5 aggregations to compute the derivative in this way for a relatively simple graph.More importantly, we repeatedly differentiate the same function computed in a node for aggregation.Obviously, this is an inefficient approach to compute gradients.For a network with 100 nodes in each layer and three layers, we will have a million paths.Nevertheless, this is exactly what we do in traditional machine learning when our prediction function is a complex composition function.This also explains why most of traditional machine learning is a shallow neural model (cf.Chapter 2).Manually working out the details of a complex composition function is tedious and impractical beyond a certain level of complexity.It is here that the beautiful dynamic programming idea of backpropagation brings order to chaos, and enables models that would otherwise have been impossible.Although the summation discussed above has an exponential number of components (paths), one can compute it efficiently using dynamic programming.In graph theory, computing all types of path-aggregative values over directed acyclic graphs is done using dynamic programming.Consider a directed acyclic graph in which the value z(i, j) (interpreted as local partial derivative of variable in node j with respect to variable in node i) is associated with edge (i, j).An example of such a computational graph is shown in Figure 3.5.We would like to compute the product of z(i, j) over each path P ∈ P from source node w to output o and then add them.Let A(i) be the set of nodes at the end points of outgoing edges from node i.We can compute the aggregated value S(i, o) for each intermediate node i (between w and o) using the following well-known dynamic programming update:This computation can be performed backwards starting from the nodes directly incident on o, since S(o, o) is already known to be 1.The algorithm discussed above is among the most widely used methods for computing all types of path-centric functions on directed acyclic graphs, which would otherwise require exponential time.For example, one can even .This generic dynamic programming approach is used extensively in directed acyclic graphs.In fact, the aforementioned dynamic programming update is exactly the multivariable chain rule of Equation 3.3, which is repeated in the backwards direction starting at the output node where the local gradient is known.This is because we derived the path-aggregative form of the loss gradient (Lemma 3.2.1)using this chain rule in the first place.The main difference is that we apply the rule in a particular order in order to minimize computations.We summarize this point below:Using dynamic programming to efficiently aggregate the product of local gradients along the exponentially many paths in a computational graph results in a dynamic programming update that is identical to the multivariable chain rule of differential calculus.The above discussion is for the case of generic computational graphs.How do we apply these ideas to neural networks?In the case of neural networks, one can easily compute ∂L ∂o in terms of the known value of o (by running the input through the network).This derivative is propagated backwards using the local partial derivatives z(i, j), depending on which variables in the neural network are used as intermediate variables.For example, when the post-activation values inside nodes are treated as nodes of the computational graph, the value of z(i, j) is the product of the weight of edge (i, j) and the local derivative of the activation at node j.On the other hand, if we use the pre-activation variables as the nodes of the computational graph, the value of z(i, j) is product of the local derivative of the activation at node i and the weight of the edge (i, j).We will discuss more about the notion of pre-activation variables and post-activation variables in a neural network with the use of an example slightly later (Figure 3.6).We can even create computational graphs containing both pre-activation and post-activation variables to decouple linear operations from activation functions.All these methods are equivalent, and will be discussed in the upcoming sections.In this section, we show how to instantiate the aforementioned approach by considering a computational graph in which the nodes contain the post-activation variables in a neural network.These are the same as the hidden variables of different layers.The backpropagation algorithm first uses a forward phase in order to compute the output and the loss.Therefore, the forward phase sets up the initialization for the dynamic programming recurrence, and also the intermediate variables that will be needed in the backwards phase.As discussed in the previous section, the backwards phase uses the dynamic programming recurrence based on the multivariable chain rule of differential calculus.We describe the forward and backward phases as follows:Forward phase: In the forward phase, a particular input vector is used to compute the values of each hidden layer based on the current values of the weights; the name "forward phase" is used because such computations naturally cascade forward across the layers.The goal of the forward phase is to compute all the intermediate hidden and output variables for a given input.These values will be required during the backward phase.At the point at which the computation is completed, the value of the output o is computed, as is the derivative of the loss function L with respect to this output.The loss is typically a function of all the outputs in the presence of multiple nodes; therefore, the derivatives with respect to all outputs are computed.For now, we will consider the case of a single output node o for simplicity, and then discuss the straightforward generalization to multiple outputs.The backward phase computes the gradient of the loss function with respect to various weights.The first step is to compute the derivative ∂L ∂o .If the network has multiple outputs, then this value is computed for each output.This sets up the initialization of the gradient computation.Subsequently, the derivatives are propagated in the backwards direction using the multivariable chain rule of Equation 3.3.Consider a path is denoted by the sequence of hidden units h 1 , h 2 , . . ., h k followed by output o.The weight of the connection from hidden unit h r to h r+1 is denoted by w (hr,hr+1) .If a single path exists in the network, it would be a simple matter to backpropagate the derivative of the loss function L with respect to the weights along this path.In most cases, an exponentially large number of paths will exist in the network from any node h r to the output node o.As shown in Lemma 3.2.1, the partial derivative can be computed by aggregating the products of partial derivatives over all paths from h r to o.When a set P of paths exist from h r to o, one can write the loss derivative as follows:The computation of ∂hr ∂w (h r−1 ,hr ) on the right-hand side is useful in converting a recursively computed partial derivative with respect to layer activations into a partial derivative with respect to the weights.The path-aggregated term above [annotated by Δ(h r , o) = ∂L ∂hr ] is very similar to the quantity S(i, o) = ∂o ∂yi discussed in Section 3.2.2.As in that section, the idea is to first compute Δ(h k , o) for nodes h k closest to o, and then recursively compute these values for nodes in earlier layers in terms of nodes in later layers.The value of Δ(o, o) = ∂L ∂o is computed as the initial point of the recursion.Subsequently, this computation is propagated in the backwards direction with dynamic programming updates (similar to Equation 3.8).The multivariable chain rule directly provides the recursion for Δ(h r , o):Since each h is in a later layer than h r , Δ(h, o) has already been computed while evaluating Δ(h r , o).However, we still need to evaluate ∂h ∂hr in order to compute Equation 3.10.Consider a situation in which the edge joining h r to h has weight w (hr,h) , and let a h be the value computed in hidden unit h just before applying the activation function Φ(•).In other words, we have h = Φ(a h ), where a h is a linear combination of its inputs from earlier-layer units incident on h.Then, by the univariate chain rule, the following expression for ∂h ∂hr can be derived:This value of ∂h ∂hr is used in Equation 3.10 to obtain the following:This recursion is repeated in the backwards direction, starting with the output node.The entire process is linear in the number of edges in the network.Note that one could also have derived Equation 3.12 by using the generic computational graph algorithm in Section 3.2.2 with respect to post-activation variables.One simply needs to set z(i, j) in Equation 3.8 to the product of the weight between nodes i and j, and the activation derivative at node j.Backpropagation can be summarized in the following steps:1. Use a forward-pass to compute the values of all hidden units, output o, and loss L for a particular input-output pattern (X, y).2. Initialize Δ(o, o) to ∂L ∂o .3. Use the recurrence of Equation 3.12 to compute each Δ(h r , o) in the backwards direction.After each such computation, compute the gradients with respect to incident weights as follows:The partial derivatives with respect to incident biases can be computed by using the fact that bias neurons are always activated at a value of +1.Therefore, to compute the partial derivative of the loss with respect to the bias of node h r , we simply set h r−1 to 1 in the right-hand side of Equation 3.13.4. Use the computed partial derivatives of loss function with respect to weights in order to perform stochastic gradient descent for input-output pattern (X, y).This description of backpropagation is greatly simplified, and actual implementations have to incorporate numerous changes for efficiency and stability.For example, the gradients are computed with respect to multiple training instances at one time, and these multiple instances are referred to as a mini-batch.These are all backpropagated simultaneously in order to add up their local gradients and execute mini-batch stochastic gradient descent.This enhancement will be discussed in Section 3.2.8.Another difference is that we have assumed a single output.However, in many types of neural networks (e.g., multiclass perceptrons), multiple outputs exist.The description of this section can easily be generalized to multiple outputs by adding the contributions of different outputs to the loss derivatives (see Section 3.2.7).A few observations are noteworthy.Equation 3.13 shows that the partial derivative of the loss with respect to an edge from h r−1 to h r always contains h r−1 as a multiplicative  3.13 is seen as a backpropagated "error."In a sense, the algorithm recursively backpropagates the errors and multiplies them with the values in the hidden layer just before the weight matrix to be updated.This is why backpropagation is sometimes understood as error propagation.In the aforementioned discussion, the values h 1 . . .h k along a path are used to compute the chain rule.However, one can also use the values before computing the activation function Φ(•) in order to define the chain rule.In other words, the gradients are computed with respect to the pre-activation values of the hidden variables, which are then propagated backwards.This alternative approach to backpropagation is how it is presented in most textbooks.The pre-activation value of the hidden variable h r is denoted by a hr , where:Figure 3.6 shows the distinction between pre-and post-activation values.In such a case, we can rewrite Equation 3.9 as follows: By substituting the computed expression for ∂a h ∂a hr in the right-hand side of Equation 3.16, we obtain the following:Equation 3.18 can also be derived by using pre-activation variables in the generic computational graph algorithm of Section 3.2.2.One simply needs to set z(i, j) in Equation 3.8 to the product of the weight between nodes i and j, and the activation derivative at node i.One advantage of this recurrence condition over the one obtained using post-activation variables is that the activation gradient is outside the summation, and therefore we can easily compute the specific form of the recurrence for each type of activation function at node h r .Furthermore, since the activation gradient is outside the summation, one can simplify the backpropagation computation by decoupling the effect of the activation function and that of the linear transformation in backpropagation updates.The simplified and decoupled view will be discussed in more detail in Section 3.2.6, and it uses both pre-activation and post-activation variables for the dynamic programming recursion.This simplified approach represents how backpropagation is actually implemented in real systems.From an implementation point of view, decoupling the linear transformation from the activation function is helpful, because the linear portion is a simple matrix multiplication and the activation portion is an elementwise multiplication.Both can be implemented efficiently on all types of matrix-friendly hardware (such as graphics processor units).The backpropagation process can now be described as follows:1. Use a forward-pass to compute the values of all hidden units, output o, and loss L for a particular input-output pattern (X, y).Use the recurrence of Equation 3.18 to compute each δ(h r , o) in the backwards direction.After each such computation, compute the gradients with respect to incident weights as follows:The partial derivatives with respect to incident biases can be computed by using the fact that bias neurons are always activated at a value of +1.Therefore, to compute the partial derivative of the loss with respect to the bias of node h r , we simply set h r−1 to 1 in the right-hand side of Equation 3.19.4. Use the computed partial derivatives of loss function with respect to weights in order to perform stochastic gradient descent for input-output pattern (X, y).The main difference of this (more common) variant of the backpropagation algorithm is in terms of the way in which the recursion is written, because pre-activation variables have been used for dynamic programming.Both the pre-and post-activation variants of backpropagation are mathematically equivalent (see Exercise 9).We have chosen to show both variations of backpropagation in order to emphasize the fact that one can use dynamic programming in a variety of ways to derive equivalent equations.An even more simplified view of backpropagation, in which both pre-activation and post-activation variables are used, is provided in Section 3.2.6.One advantage of Equation 3.18 is that we can compute the specific types of updates for various nodes.In the following, we provide the instantiation of Equation 3.18 for different types of nodes:Note that the derivative of the sigmoid can be written in terms of its output value h r as h r (1 − h r ).Similarly, the tanh derivative can be expressed as (1 − h 2 r ).The derivatives of different activation functions are discussed in Section 1.2.1.6 of Chapter 1.For the ReLU function, the value of δ(h r , o) can be computed in case-wise fashion:A similar recurrence can be shown for the hard tanh function except that the update condition is slightly different:It is noteworthy that the ReLU and tanh are non-differentiable at exactly the condition boundaries.However, this is rarely a problem in practical settings, in which one works with finite precision.Softmax activation is a special case because the function is not computed with respect to one input, but with respect to multiple inputs.Therefore, one cannot use exactly the same type of update, as with other activation functions.As discussed in Equation 1.12 of Chapter 1, the softmax converts k real-valued predictions v 1 . . .v k into output probabilities o 1 . . .o k using the following relationship: {1, . . . , k} (3.20)Note that if we try to use the chain rule to backpropagate the derivative of the loss L with respect to v 1 . . .v k , then one has to compute each ∂L ∂oi and also each ∂oi ∂vj .This backpropagation of the softmax is greatly simplified, when we take two facts into account:1.The softmax is almost always used in the output layer.2. The softmax is almost always paired with the cross-entropy loss.Let y 1 . . .y k ∈ {0, 1}be the one-hot encoded (observed) outputs for the k mutually exclusive classes.Then, the cross-entropy loss is defined as follows:The key point is that the value of ∂L ∂vi has a particularly simple form in the case of the softmax:The reader is encouraged to work out the detailed derivation of the result above; it is tedious, but relatively straightforward algebra.The derivation is enabled by the fact that the value of ∂oj ∂vi in Equation 3.22 can be shown to be equal to o i (1 − o i ) when i = j (which is the same as sigmoid), and otherwise can be shown to be equal to −o i o j (see Exercise 10).Therefore, in the case of the softmax, one first backpropagates the gradient from the output to the layer containing v 1 . . .v k .Further backpropagation can proceed according to the rules discussed earlier in this section.Note that in this case, we have decoupled the backpropagation update of the softmax activation from the backpropagation in the rest of the network, in which matrix multiplications are always included along with the activation function in the backpropagation update.In general, it is helpful to create a view of backpropagation in which the linear matrix multiplications and activation layers are decoupled because it greatly simplifies the updates.This view will be discussed in the next section.In the previous discussion, two equivalent ways of computing the updates based on Equations 3.12 and 3.18 were provided.In each case, one is really backpropagating through a linear matrix multiplication and an activation computation simultaneously.The way in which we order these two coupled computations affects whether we obtain Equation 3.12 or 3.18.Unfortunately, this unnecessarily complicated view of backpropagation has proliferated in papers and textbooks since the beginning.This is, in part, because layers are traditionally defined in a neural network by combining the two separate operations of linear transformation and activation function computation.However, in many real implementations, the linear computations and the activation computations are decoupled as separate "layers," and one separately backpropagates through the two layers.Furthermore, we use a vector-centric representation of the neural network, so that operations on vector representations of layers are vector-to-vector operations such as a matrix multiplication in a linear layer [cf. Figure 1.11(d) in Chapter 1].This view greatly simplifies the computations.Therefore, one can create a neural network in which activation layers are alternately arranged with linear layers, as shown in Figure 3.7.Note that the activation layers can use identity activation if needed.Activation layers (usually) perform one-to-one, elementwise computations on the vector components with the activation function Φ(•), whereas linear layers perform all-to-all computations by multiplying with the coefficient matrix W .Then, for each pair of matrix multiplication and activation function layers, the following forward and backward steps need to be performed:Many-One Maximum of inputs Set to 0 (non-maximal inputs) Copy (maximal input)Anything zJ is Jacobian (Equation 3.23)1. Let z i and z i+1 be the column vectors of activations in the forward direction when the matrix of linear transformations from the ith to the (i + 1)th layer is denoted by W . Furthermore, let g i and g i+1 be the backpropagated vectors of gradients in the two layers.Each element of g i is the partial derivative of the loss function with respect to a hidden variable in the ith layer.Then, we have the following:2. Now consider a situation where the activation function Φ(•) is applied to each node in layer (i + 1) to obtain the activations in layer (i + 2).Then, we have the following:Here, Φ(•) and its derivative Φ (•) are applied in element-wise fashion to vector arguments.The symbol indicates elementwise multiplication.Note the extraordinary simplicity once the activation is decoupled from the matrix multiplication in a layer.The forward and backward computations are shown in Figure 3.7.Furthermore, the derivatives of Φ(z i+1 ) can often be computed in terms of the outputs of the next layer.Based on Section 3.2.5, one can show the following for sigmoid activations:Examples of different types of backpropagation updates for various forward functions are shown in Table 3.1.In this case, we have used layer indices of i and (i + 1) for both linear transformations and activation functions (rather than using (i + 2) for activation function).Note that the second to last entry in the table corresponds to the maximization function.This type of function is useful for max-pooling operations in convolutional neural networks.Therefore, the backward propagation operation is just like forward propagation.Given the vector of gradients in a layer, one only has to apply the operations shown in the final column of Table 3.1 to obtain the gradients with respect to the previous layer.Some neural operations are more complex many-to-many functions than simple matrix multiplications.These cases can be handled by assuming that the kth activation in layer-(i + 1) is obtained by applying an arbitrary function f k (•) on the vector of activations in layer-i.Then, the elements of the Jacobian are defined as follows:Here, z (r) iis the rth element in z i .Let J be the matrix whose elements are J kr .Then, it is easy to see that the backpropagation update from layer to layer can be written as follows:Writing backpropagation equations as matrix multiplications is often beneficial from an implementation-centric point of view, such as acceleration with Graphics Processor Units (cf.Section 3.7.1).Note that the elements in g i represent gradients of the loss with respect to the activations in the ith layer, and therefore an additional step is needed to compute gradients with respect to the weights.The gradient of the loss with respect to a weight between the pth unit of the (i − 1)th layer and the qth unit of ith layer is obtained by multiplying the pth element of z i−1 with the qth element of g i .For simplicity, the discussion above has used only a single output node at which the loss function is computed.However, in most applications, the loss function is computed over multiple output nodes O.The only difference in this case is that the value of each ∂L ∂ao = δ(o, O) for o ∈ O is initialized to ∂L ∂o Φ (o).Backpropagation is then executed in order to compute ∂L ∂a h = δ(h, O) for each hidden node h.In some forms of sparse feature learning, even the outputs of the hidden nodes have loss functions associated with them.This occurs frequently in order to encourage solutions with specific properties, such as a hidden layer that is sparse (e.g., sparse autoencoder), or a hidden layer with a specific type of regularization penalty (e.g., contractive autoencoder).The case of sparsity penalties is discussed in Section 4.4.4 of Chapter 4, and the problem of contractive autoencoders is discussed in Section 4.10.3 of Chapter 4. In such cases, the backpropagation algorithm requires only minor modifications in which the gradient flow in the backwards direction is based on all the nodes at which the loss is computed.This can be achieved by simple aggregation of the gradient flows resulting from different losses.One can view this as a special type of network in which the hidden nodes are also output nodes, and the output nodes are not restricted to the final layer of the network.At a fundamental level, the backpropagation methodology remains the same.Consider the case in which the loss function L hr is associated with the hidden node h r , whereas the overall loss over all nodes is L. Furthermore, let ∂L ∂a hr = δ(h r , N(h r )) denote the gradient flow from all nodes N (h r ) reachable from node h r , with which some portion of the loss is associated.In this case, the node set N (h r ) might contain both nodes in the output layer as well as nodes in the hidden layer (with which a loss is associated), as long as these nodes are reachable from h r .Therefore, the set N (h r ) uses h r as an argument.Note that the set N (h r ) includes the node h r .Then, the update of Equation 3.18 is first applied as follows:δ(h r , N(h r )) ⇐ Φ (a hr )This is similar to the standard backpropagation update.However, the current value of δ(h r , N(h r ))does not yet include the contribution of h r .Therefore, an additional step is executed to adjust δ(h r , N(h r )) based on the contribution of h r to the loss function:It is important to keep in mind that the overall loss L is different from L hr , which is the loss specific to node h r .Furthermore, the addition to the gradient flow in Equation 3.26 has a similar algebraic form to the value of the initialization of the output nodes.In other words, the gradient flows caused by the hidden nodes are similar to those of the output nodes.The only difference is that the computed value is added to the existing gradient flow at the hidden nodes.Therefore, the overall framework of backpropagation remains almost identical, with the main difference being that the backpropagation algorithm picks up additional contributions from the losses at the hidden nodes.From the very first chapter of this book, all updates to the weights are performed in pointspecific fashion, which is referred to as stochastic gradient descent.Such an approach is common in machine learning algorithms.In this section, we provide a justification for this choice along with related variants like mini-batch stochastic gradient descent.We also provide an understanding of the advantages and disadvantages of various choices.Most machine learning problems can be recast as optimization problems over specific objective functions.For example, the objective function in neural networks can be defined in terms optimizing a loss function L, which is often a linearly separable sum of the loss functions on the individual training data points.For example, in a linear regression application, one minimizes the sum of the squared prediction errors over the training data points.In a dimensionality reduction application, one minimizes the sum of squared representation errors in the reconstructed training data points.One can write the loss function of a neural network in the following form:Here, L i is the loss contributed by the ith training point.For most of the algorithms in Chapter 2, we have worked with training point-specific loss rather than the aggregate loss.In gradient descent, one tries to minimize the loss function of the neural network by moving the parameters along the negative direction of the gradient.For example, in the case of the perceptron, the parameters correspond to W = (w 1 . . .w d ).Therefore, one would try to compute the loss of the underlying objective function over all points simultaneously and perform gradient descent.Therefore, in traditional gradient descent, one would try to perform gradient-descent steps such as the following:This type of derivative can also be written succinctly in vector notation (i.e., matrix calculus notation):For single-layer networks like the perceptron, gradient-descent is done only with respect to W , whereas for larger networks, all parameters in the network need to be updated with backpropagation.The number of parameters can easily be on the order of millions in large-scale applications, and one needs to simultaneously run all examples forwards and backwards through the network in order to compute the backpropagation updates.It is, however, impractical to simultaneously run all examples through the network to compute the gradient with respect to the entire data set in one shot.Note that even the memory requirements of all intermediate/final predictions for each training instance would need to be maintained by gradient descent.This can be exceedingly large in most practical settings.At the beginning of the learning process, the weights are often incorrect to such a degree that even a small sample of points can be used to create an excellent estimate of the gradient's direction.The additive effect of the updates created from such samples can often provide an accurate direction of movement.This observation provides a practical foundation for the success of the stochastic gradient-descent method and its variants.Since the loss function of most optimization problems can be expressed as a linear sum of the losses with respect to individual points (cf.Equation 3.27), it is easy to show the following:In this case, updating the full gradient with respect to all the points sums up the individual point-specific effects.Machine learning problems inherently have a high level of redundancy between the knowledge captured by different training points, and one can often more efficiently undertake the learning process with the point-specific updates of stochastic gradient descent:This type of gradient descent is referred to as stochastic because one cycles through the points in some random order.Note that the long-term effect of repeated updates is approximately the same, although each update in stochastic gradient descent can only be viewed as a probabilistic approximation.Each local gradient can be computed efficiently, which makes stochastic gradient descent fast, albeit at the expense of accuracy in gradient computation.However, one interesting property of stochastic gradient descent is that even though it might not perform as well on the training data (compared to gradient descent), it often performs comparably (and sometimes even better) on the test data [171].As you will learn in Chapter 4, stochastic gradient descent has the indirect effect of regularization.However, it can occasionally provide very poor results with certain orderings of training points.In mini-batch stochastic descent, one uses a batch B = {j 1 . . .j m } of training points for the update:Mini-batch stochastic gradient descent often provides the best trade-off between stability, speed, and memory requirements.When using mini-batch stochastic gradient descent, the outputs of a layer are matrices instead of vectors, and forward propagation requires the multiplication of the weight matrix with the activation matrix.The same is true for backward propagation in which matrices of gradients are maintained.Therefore, the implementation of mini-batch stochastic gradient descent increases the memory requirements, which is a key limiting factor on the size of the mini-batch.The size of the mini-batch is therefore regulated by the amount of memory available on the particular hardware architecture at hand.Keeping a batch size that is too small also results in constant overheads, which is inefficient even from a computational point of view.Beyond a certain batch size (which is typically of the order of a few hundred points), one does not gain much in terms of the accuracy of gradient computation.It is common to use powers of 2 as the size of the mini-batch, because this choice often provides the best efficiency on most hardware architectures; commonly used values are 32, 64, 128, or 256.Although the use of mini-batch stochastic gradient descent is ubiquitous in neural network learning, most of this book will use a single point for the update (i.e., pure stochastic gradient descent) for simplicity in presentation.A very common approach for regularizing neural networks is to use shared weights.The basic idea is that if one has some semantic insight that a similar function will be computed in different nodes of the network, then the weights associated with those nodes will be constrained to be the same value.Some examples are as follows:2. In a recurrent neural network for text (cf.Chapter 7), the weights in different temporal layers are shared, because it is assumed that the language model at each time-stamp is the same.3. In a convolutional neural network, the same grid of weights (corresponding to a visual field) is used over the entire spatial extent of the neurons (cf.Chapter 8).Sharing weights in a semantically insightful way is one of the key tricks to successful neural network design.When one can identify the insight that the function computed at two nodes ought to be similar, it makes sense to use the same set of weights in that pair of nodes.At first sight, it might seem to be an onerous task to compute the gradient of the loss with respect to the shared weights in these different regions of the network, because the different uses of the weights would also influence one another in an unpredictable way in the computational graph.However, backpropagation with respect to shared weights turns out to be mathematically simple.Let w be a weight, which is shared at T different nodes in the network, and the corresponding copies of the weights at these nodes be denoted by w 1 . . .w T .Let the loss function be L.Then, it is easy to use the chain rule to show the following:In other words, all we have to do is to pretend that these weights are independent, compute their derivatives, and add them!Therefore, we simply have to execute the backpropagation algorithm without any change and then sum up the gradients of different copies of the shared weight.This simple observation is used at many places in neural network learning.It also forms the basis of the learning algorithm in recurrent neural networks.The backpropagation algorithm is quite complex, and one might occasionally check the correctness of gradient computation.This can be performed easily with the use of numerical methods.Consider a particular weight w of a randomly selected edge in the network.Let L(w) be the current value of the loss.The weight of this edge is perturbed by adding a small amount > 0 to it.Then, the forward algorithm is executed with this perturbed weight and the loss L(w + ) is computed.Then, the partial derivative of the loss with respect to w can be shown to be the following:When the partial derivatives do not match closely enough, it is easy to detect that an error must have occurred in computation.One needs to perform the above estimation for only two or three checkpoints in the training process, which is quite efficient.However, it might be advisable to perform the checking over a large subset of the parameters at these checkpoints.One problem is in determining when the gradients are "close enough," especially when one has no idea about the absolute magnitudes of these values.This is achieved by using relative ratios.Let the backpropagation-determined derivative be denoted by G e , and the aforementioned estimation be denoted by G a .Then, the relative ratio ρ is defined as follows:Typically, the ratio should be less than 10 −6 , although for some activation functions like the ReLU in which sharp changes in derivatives occur at particular points, it is possible for the numerical gradient to be different from the computed gradient.In such cases, the ratio should still be less than 10 −3 .One can use this numerical approximation to test various edges and check the correctness of their gradients.If there are millions of parameters, then one can test a sample of the derivatives for a quick check of correctness.It is also advisable to perform this check at two or three points in the training because the checks at initialization might correspond to special cases that do not generalize to arbitrary points in the parameter space.There are several important issues associated with the setup of the neural network, preprocessing, and initialization.First, the hyperparameters of the neural network (such as the learning rates and regularization parameters) need to be selected.Feature preprocessing and initialization can also be rather important.Neural networks tend to have larger parameter spaces compared to other machine learning algorithms, which magnifies the effect of preprocessing and initialization in many ways.In the following, we will discuss the basic methods used for feature preprocessing and initialization.Strictly speaking, advanced methods like pretraining can also be considered initialization techniques.However, these techniques require a deeper understanding of the model generalization issues associated with neural network training.For this reason, discussion on this topic will be deferred to the next chapter.Neural networks have a large number of hyperparameters such as the learning rate, the weight of regularization, and so on.The term "hyperparameter" is used to specifically refer to the parameters regulating the design of the model (like learning rate and regularization), and they are different from the more fundamental parameters representing the weights of connections in the neural network.In Bayesian statistics, the notion of hyperparameter is used to control the prior distribution, although we use this definition in a somewhat loose sense here.In a sense, there is a two-tiered organization of parameters in the neural network, in which primary model parameters like weights are optimized with backpropagation only after fixing the hyperparameters either manually or with the use of a tuning phase.As we will discuss in Section 4.3 of Chapter 4, the hyperparameters should not be tuned using the same data used for gradient descent.Rather, a portion of the data is held out as validation data, and the performance of the model is tested on the validation set with various choices of hyperparameters.This type of approach ensures that the tuning process does not overfit to the training data set (while providing poor test data performance).How should the candidate hyperparameters be selected for testing?The most well-known technique is grid search, in which a set of values is selected for each hyperparameter.In the most straightforward implementation of grid search, all combinations of selected values of the hyperparameters are tested in order to determine the optimal choice.One issue with this procedure is that the number of hyperparameters might be large, and the number of points in the grid increases exponentially with the number of hyperparameters.For example, if we have 5 hyperparameters, and we test 10 values for each hyperparameter, the training procedure needs to be executed 10 5 = 100000 times to test its accuracy.Although one does not run such testing procedures to completion, the number of runs is still too large to be reasonably executed for most settings of even modest size.Therefore, a commonly used trick is to first work with coarse grids.Later, when one narrows down to a particular range of interest, finer grids are used.One must be careful when the optimal hyperparameter selected is at the edge of a grid range, because one would need to test beyond the range to see if better values exist.The testing approach may at times be too expensive even with the coarse-to-fine-grained process.It has been pointed out [37] that grid-based hyperparameter exploration is not necessarily the best choice.In some cases, it makes sense to randomly sample the hyperparameters uniformly within the grid range.As in the case of grid ranges, one can perform multi-resolution sampling, where one first samples in the full grid range.One then creates a new set of grid ranges that are geometrically smaller than the previous grid ranges and centered around the optimal parameters from the previously explored samples.Sampling is repeated on this smaller box and the entire process is iteratively repeated multiple times to refine the parameters.Another key point about sampling many types of hyperparameters is that the logarithms of the hyperparameters are sampled uniformly rather than the hyperparameters themselves.Two examples of such parameters include the regularization rate and the learning rate.For example, instead of sampling the learning rate α between 0.1 and 0.001, we first sample log(α) uniformly between −1 and −3, and then exponentiate it to the power of 10.It is more common to search for hyperparameters in the logarithmic space, although there are some hyperparameters that should be searched for on a uniform scale.Finally, a key point about large-scale settings is that it is sometimes impossible to run these algorithms to completion because of the large training times involved.For example, a single run of a convolutional neural network in image processing might take a couple of weeks.Trying to run the algorithm over many different choices of parameter combinations is impractical.However, one can often obtain a reasonable estimate of the broader behavior of the algorithm in a short time.Therefore, the algorithms are often run for a certain number of epochs to test the progress.Runs that are obviously poor or diverge from convergence can be quickly killed.In many cases, multiple threads of the process with different hyperparameters can be run, and one can successively terminate or add new sampled runs.In the end, only one winner is allowed to train to completion.Sometimes a few winners may be allowed to train to completion, and their predictions will be averaged as an ensemble.A mathematically justified way of choosing for hyperparameters is the use of Bayesian optimization [42,306].However, these methods are often too slow to practically use in largescale neural networks and remain an intellectual curiosity for researchers.For smaller networks, it is possible to use libraries such as Hyperopt [614], Spearmint [616], and SMAC [615].The feature processing methods used for neural network training are not very different from those in other machine learning algorithms.There are two forms of feature preprocessing used in machine learning algorithms:1. Additive preprocessing and mean-centering: It can be useful to mean-center the data in order to remove certain types of bias effects.Many algorithms in traditional machine learning (such as principal component analysis) also work with the assumption of mean-centered data.In such cases, a vector of column-wise means is subtracted from each data point.Mean-centering is often paired with standardization, which is discussed in the section of feature normalization.A second type of pre-processing is used when it is desired for all feature values to be non-negative.In such a case, the absolute value of the most negative entry of a feature is added to the corresponding feature value of each data point.The latter is typically combined with min-max normalization, which is discussed below.2. Feature normalization: A common type of normalization is to divide each feature value by its standard deviation.When this type of feature scaling is combined with mean-centering, the data is said to have been standardized.The basic idea is that each feature is presumed to have been drawn from a standard normal distribution with zero mean and unit variance.The other type of feature normalization is useful when the data needs to be scaled in the range (0, 1).Let min j and max j be the minimum and maximum values of the jth attribute.Then, each feature value x ij for the jth dimension of the ith point is scaled by min-max normalization as follows:Feature normalization often does ensure better performance, because it is common for the relative values of features to vary by more than an order of magnitude.In such cases, parameter learning faces the problem of ill-conditioning, in which the loss function has an inherent tendency to be more sensitive to some parameters than others.As we will see later in this chapter, this type of ill-conditioning affects the performance of gradient descent.Therefore, it is advisable to perform the feature scaling up front.Another form of feature pre-processing is referred to as whitening, in which the axis-system is rotated to create a new set of de-correlated features, each of which is scaled to unit variance.Typically, principal component analysis is used to achieve this goal.Principal component analysis can be viewed as the application of singular value decomposition after mean-centering a data matrix (i.e., subtracting the mean from each column).Let D be an n × d data matrix that has already been mean-centered.Let C be the d × d co-variance matrix of D in which the (i, j)th entry is the co-variance between the dimensions i and j.Because the matrix D is mean-centered, we have the following:The eigenvectors of the co-variance matrix provide the de-correlated directions in the data.Furthermore, the eigenvalues provide the variance along each of the directions.Therefore, if one uses the top-k eigenvectors (i.e., largest k eigenvalues) of the covariance matrix, most of the variance in the data will be retained and the noise will be removed.One can also choose k = d, but this will often result in the variances along the near-zero eigenvectors being dominated by numerical errors in the computation.It is a bad idea to include dimensions in which the variance is caused by computational errors, because such dimensions will contain little useful information for learning application-specific knowledge.Furthermore, the whitening process will scale each transformed feature to unit variance, which will blow up the errors along these directions.At the very least, it is advisable to use some threshold like 10 −5 on the magnitude of the eigenvalues.Therefore, as a practical matter, k will rarely be exactly equal to d.Alternatively, one can add 10 −5 to each eigenvalue for regularization before scaling each dimension.Let P be a d × k matrix in which each column contains one of the top-k eigenvectors.Then, the data matrix D can be transformed into the k-dimensional axis system by postmultiplying with the matrix P .The resulting n × k matrix U , whose rows contain the transformed k-dimensional data points, is given by the following:Note that the variances of the columns of U are the corresponding eigenvalues, because this is the property of the de-correlating transformation of principal component analysis.In whitening, each column of U is scaled to unit variance by dividing it with its standard deviation (i.e., the square root of the corresponding eigenvalue).The transformed features are fed into the neural network.Since whitening might reduce the number of features, this type of preprocessing might also affect the architecture of the network, because it reduces the number of inputs.One important aspect of whitening is that one might not want to make a pass through a large data set to estimate its covariance matrix.In such cases, the covariance matrix and columnwise means of the original data matrix can be estimated on a sample of the data.The d × k eigenvector matrix P is computed in which the columns contain the top-k eigenvectors.Subsequently, the following steps are used for each data point: (i) The mean of each column is subtracted from the corresponding feature; (ii) Each d-dimensional row vector representing a training data point (or test data point) is post-multiplied with P to create a k-dimensional row vector; (iii) Each feature of this k-dimensional representation is divided by the square-root of the corresponding eigenvalue.The basic idea behind whitening is that data is assumed to be generated from an independent Gaussian distribution along each principal component.By whitening, one assumes that each such distribution is a standard normal distribution, and provides equal importance to the different features.Note that after whitening, the scatter plot of the data will roughly have a spherical shape, even if the original data is elliptically elongated with an arbitrary orientation.The idea is that the uncorrelated concepts in the data have now been scaled to equal importance (on an a priori basis), and the neural network can decide which of them to emphasize in the learning process.Another issue is that when different features are scaled very differently, the activations and gradients will be dominated by the "large" features in the initial phase of learning (if the weights are initialized randomly to values of similar magnitude).This might hurt the relative learning rate of some of the important weights in the network.The practical advantages of using different types of feature preprocessing and normalization are discussed in [278,532].Initialization is particularly important in neural networks because of the stability issues associated with neural network training.As you will learn in Section 3.4, neural networks often exhibit stability problems in the sense that the activations of each layer either become successively weaker or successively stronger.The effect is exponentially related to the depth of the network, and is therefore particularly severe in deep networks.One way of ameliorating this effect to some extent is to choose good initialization points in such a way that the gradients are stable across the different layers.One possible approach to initialize the weights is to generate random values from a Gaussian distribution with zero mean and a small standard deviation, such as 10 −2 .Typically, this will result in small random values that are both positive and negative.One problem with this initialization is that it is not sensitive to the number of inputs to a specific neuron.For example, if one neuron has only 2 inputs and another has 100 inputs, the output of the former is far more sensitive to the average weight because of the additive effect of more inputs (which will show up as a much larger gradient).In general, it can be shown that theThe vanishing and exploding gradient problems variance of the outputs linearly scales with the number of inputs, and therefore the standard deviation scales with the square root of the number of inputs.To balance this fact, each weight is initialized to a value drawn from a Gaussian distribution with standard deviation 1/r, where r is the number of inputs to that neuron.Bias neurons are always initialized to zero weight.Alternatively, one can initialize the weight to a value that is uniformlyMore sophisticated rules for initialization consider the fact that the nodes in different layers interact with one another to contribute to output sensitivity.Let r in and r out respectively be the fan-in and fan-out for a particular neuron.One suggested initialization rule, referred to as Xavier initialization or Glorot initialization is to use a Gaussian distribution with standard deviation of 2/(r in + r out ).An important consideration in using randomized methods is that symmetry breaking is important.if all weights are initialized to the same value (such as 0), all updates will move in lock-step in a layer.As a result, identical features will be created by the neurons in a layer.It is important to have a source of asymmetry among the neurons to begin with.Deep neural networks have several stability issues associated with training.In particular, networks with many layers may be hard to train because of the way in which the gradients in earlier and later layers are related.In order to understand this point, let us consider a very deep network that has a single node in each layer.We assume that there are (m + 1) layers, including the noncomputational input layer.The weights of the edges between the various layers are denoted by w 1 , w 2 , . . .w m .Furthermore, assume that the sigmoid activation function Φ(•) is applied in each layer.Let x be the input, h 1 . . .h m−1 be the hidden values in the various layers, and o be the final output.Let Φ (h t ) be the derivative of the activation function in hidden layer t.Let ∂L ∂ht be the derivative of the loss function with respect to the hidden activation h t .The neural architecture is illustrated in Figure 3.8.It is relatively easy to use the backpropagation update to show the following relationship:Since the fan-in is 1 of each node, assume that the weights are initialized from a standard normal distribution.Therefore, each w t has an expected average magnitude of 1.Let us examine the specific behavior of this recurrence in the case where the sigmoid activation is used.The derivative with a sigmoid with output f ∈ (0, 1) is given by f (1 − f ).This value takes on its maximum at f = 0.5, and therefore the value of Φ (h t ) is no more than 0.25 even at its maximum.Since the absolute value of w t+1 is expected to be 1, it follows that each weight update will (typically) cause the value of ∂L ∂ht to be less than 0.25 that of ∂L ∂ht+1 .Therefore, after moving by about r layers, this value will typically be less than 0.25 r .Just to get an idea of the magnitude of this drop, if we set r = 10, then the gradient update magnitudes drop to 10 −6 of their original values!Therefore, when backpropagation is used, the earlier layers will receive very small updates compared to the later layers.This problem is referred to as the vanishing gradient problem.Note that we could try to solve this problem by using an activation function with larger gradients and also initializing the weights to be larger.However, if we go too far in doing this, it is easy to end up in the opposite situation where the gradient explodes in the backward direction instead of vanishing.In general, unless we initialize the weight of every edge so that the product of the weight and the derivative of each activation is exactly 1, there will be considerable instability in the magnitudes of the partial derivatives.In practice, this is impossible with most activation functions because the derivative of an activation function will vary from iteration to iteration.Although we have used an oversimplified example here with only one node in each layer, it is easy to generalize the argument to cases in which multiple nodes are available in each layer.In general, it is possible to show that the layer-to-layer backpropagation update includes a matrix multiplication (rather than a scalar multiplication).Just as repeated scalar multiplication is inherently unstable, so is repeated matrix multiplication.In particular, the loss derivatives in layer-(i + 1) are multiplied by a matrix referred to as the Jacobian (cf.Equation 3.23).The Jacobian contains the derivatives of the activations in layer-(i + 1) with respect to those in layer i.In certain cases like recurrent neural networks, the Jacobian is a square matrix and one can actually impose stability conditions with respect to the largest eigenvalue of the Jacobian.These stability conditions are rarely satisfied exactly, and therefore the model has an inherent tendency to exhibit the vanishing and exploding gradient problems.Furthermore, the effect of activation functions like the sigmoid tends to encourage the vanishing gradient problem.One can summarize this problem as follows:Observation 3.4.1The relative magnitudes of the partial derivatives with respect to the parameters in different parts of the network tend to be very different, which creates problems for gradient-descent methods.In the next section, we will provide a geometric understanding of why it is natural for unstable gradient ratios to cause problems in most multivariate optimization problems, even when working in relatively simple settings.The vanishing and exploding gradient problems are inherent to multivariable optimization, even in cases where there are no local optima.In fact, minor manifestations of this problem are encountered in almost any convex optimization problem.Therefore, in this section, we will consider the simplest possible case of a convex, quadratic objective function with a bowllike shape and a single global minimum.In a single-variable problem, the path of steepest descent (which is the only path of descent), will always pass through the minimum point of the bowl (i.e., optimum objective function value).However, the moment we increase the number of variables in the optimization problem from 1 to 2, this is no longer the case.The key point to understand is that with very few exceptions, the path of steepest descent in most loss functions is only an instantaneous direction of best movement, and is not the correct direction of descent in the longer term.In other words, small steps with "course corrections" are always needed.When an optimization problem exhibits the vanishing gradient problem, it means that the only way to reach the optimum with steepest-descent updates is by using an extremely large number of tiny updates and course corrections, which is obviously very inefficient.In order to understand this point, we look at two bivariate loss functions in Figure 3.9.In this figure, the contour plots of the loss function are shown, in which each line corresponds to points in the XY-plane where the loss function has the same value.The direction of steepest descent is always perpendicular to this line.The first loss function is of the form L = x 2 + y 2 , which takes the shape of a perfectly circular bowl, if one were to view the height as the objective function value.This loss function treats x and y in a symmetric way.The second loss function is of the form L = x 2 + 4y 2 , which is an elliptical bowl.Note that this loss function is more sensitive to changes in the value of y as compared to changes in the value of x, although the specific sensitivity depends on the position of the data point.In the case of the circular bowl of Figure 3.9(a), the gradient points directly at the optimum solution, and one can reach the optimum in a single step, as long as the correct step-size is used.This is not quite the case in the loss function of Figure 3.9(b), in which the gradients are often more significant in the y-direction compared to the x-direction.Furthermore, the gradient never points to the optimal solution, as a result of which many course corrections are needed over the descent.A salient observation is that the steps along the y-direction are large, but subsequent steps undo the effect of previous steps.On the other hand, the progress along the x-direction is consistent but tiny.Although the situation of Figure 3.9(b) occurs in almost any optimization problem using steepest descent, the case of the vanishing gradient is an extreme manifestation 2 of this behavior.The fact that a simple quadratic bowl (which is trivial compared to the typical loss function of a deep network) shows so much oscillation with the steepest-descent method is concerning.After all, the repeated composition of functions (as implied by the underlying computational graph) is highly unstable in terms of the sensitivity of the output to the parameters in different parts of the network.The problem of differing relative derivatives is extraordinarily large in real neural networks, in which we have millions of parameters and gradient ratios that vary by orders of magnitude.Furthermore, many activation functions have small derivatives, which tends to encourage the vanishing gradient problem during backpropagation.As a result, the parameters in later layers with large descent components are often oscillating with large updates, whereas those in earlier layers make tiny but consistent updates.Therefore, neither the earlier nor the later layers make much progress in getting closer to the optimal solution.As a result, it is possible to get into situations where very little progress is made even after training for a long time.2 A different type of manifestation occurs in cases where the parameters in earlier and later layers are shared.In such cases, the effect of an update can be highly unpredictable because of the combined effect of different layers.Such scenarios occur in recurrent neural networks in which the parameters in later temporal layers are tied to those of earlier temporal layers.In such cases, small changes in the parameters can cause large changes in the loss function in very localized regions without any gradient-based indication in nearby regions.Such topological characteristics of the loss function are referred to as cliffs (cf.Section 3.5.4),and they make the problem harder to optimize because the gradient descent tends to either overshoot or undershoot.The specific choice of activation function often has a considerable effect on the severity of the vanishing gradient problem.The derivatives of the sigmoid and the tanh activation functions are illustrated in Figure 3.10(a) and (b), respectively.The sigmoid activation function never has a gradient of more than 0.25, and therefore it is very prone to the vanishing gradient problem.Furthermore, it saturates at large absolute values of the argument, which refers to the fact that the gradient is almost 0. In such cases, the weights of the neuron change very slowly.Therefore, a few such activations within the network can significantly affect the gradient computations.The tanh function fares better than the sigmoid function because it has a gradient of 1 near the origin, but the gradient saturates rapidly at increasingly large absolute values of the argument.Therefore, the tanh function will also be susceptible to the vanishing gradient problem.In recent years, the use of the sigmoid and the tanh activation functions has been increasingly replaced with the ReLU and the hard tanh functions.The ReLU is also faster to train because its gradient is efficient to compute.The derivatives of the ReLU and the hard tanh functions are shown in Figure 3.10(c) and (d), respectively.It is evident that these functions take on the derivative of 1 in certain intervals, although they might have zero gradient in others.As a result, the vanishing gradient problem tends to occur less often, as long as most of these units operate within the intervals where the gradient is 1.In recent years, these piecewise linear variants have become far more popular than their smooth counterparts.Note that the replacement of the activation function is only a partial fix because the matrix multiplication across layers still causes a certain level of instability.Furthermore, the piecewise linear activations introduce the new problem of dead neurons.It is evident from Figure 3.10(c) and (d) that the gradient of the ReLU is zero for negative values of its argument.This can occur for a variety of reasons.For example, consider the case where the input into a neuron is always nonnegative, whereas all the weights have somehow been initialized to negative values.Therefore, the output will be 0. Another example is the case where a high learning rate is used.In such a case, the pre-activation values of the ReLU can jump to a range where the gradient is 0 irrespective of the input.In other words, high learning rates can "knock out" ReLU units.In such cases, the ReLU might not fire for any data instance.Once a neuron reaches this point, the gradient of the loss with respect to the weights just before the ReLU will always be zero.In other words, the weights of this neuron will never be updated further during training.Furthermore, its output will not vary across different choices of inputs and therefore will not play a role in discriminating between different instances.Such a neuron can be considered dead, which is considered a kind of permanent "brain damage" in biological parlance.The problem of dying neurons can be partially ameliorated by using learning rates that are somewhat modest.Another fix is to use the leaky ReLU, which allows the neurons outside the active interval to leak some gradient backwards.The leaky ReLU is defined using an additional parameter α ∈ (0, 1):Although α is a hyperparameter chosen by the user, it is also possible to learn it.Therefore, at negative values of v, the leaky ReLU can still propagate some gradient backwards, albeit at a reduced rate defined by α < 1.The gains with the leaky ReLU are not guaranteed, and therefore this fix is not completely reliable.A key point is that dead neurons are not always a problem, because they represent a kind of pruning to control the precise structure of the neural network.Therefore, a certain level of dropping of neurons can be viewed as a part of the learning process.After all, there are limitations to our ability to tune the number of neurons in each layer.Dying neurons do a part of this tuning for us.Indeed, the intentional pruning of connections is sometimes used as a strategy for regularization [282].Of course, if a very large fraction of the neurons in the network are dead, that can be a problem as well because much of the neural network will be inactive.Furthermore, it is undesirable for too many neurons to be knocked out during the early training phases, when the model is very poor.A recently proposed solution is the use of maxout networks [148].The idea in the maxout unit is to have two coefficient vectors W 1 and W 2 instead of a single one.Subsequently, the activation used is max{W 1 • X, W 2 • X}.In the event that bias neurons are used, the maxout activation is max{W 1One can view the maxout as a generalization of the ReLU, because the ReLU is obtained by setting one of the coefficient vectors to 0. Even the leaky ReLU can be shown to be a special case of maxout, in which we set W 2 = αW 1 for α ∈ (0, 1).Like the ReLU, the maxout function is piecewise linear.However, it does not saturate at all, and is linear almost everywhere.In spite of its linearity, it has been shown [148] that maxout networks are universal function approximators.Maxout has advantages over the ReLU, and it enhances the performance of ensemble methods like Dropout (cf.Section 4.5.4 of Chapter 4).The only drawback with the use of maxout is that it doubles the number of required parameters.The most common method for parameter learning in neural networks is the steepest-descent method, in which the gradient of the loss function is used to make parameter updates.In fact, all the discussions in previous chapters are based on this assumption.As discussed in the earlier section, the steepest-gradient method can sometimes behave unexpectedly because it does not always point in the best direction of improvement, when steps of finite size are considered.The steepest-descent direction is the optimal direction only from the perspective of infinitesimal steps.A steepest-descent direction can sometimes become an ascent direction after a small update in parameters.As a result, many course corrections are needed.A specific example of this phenomenon is discussed in Section 3.4.1 in which minor differences in sensitivity to different features can cause a steepest-descent algorithm to have oscillations.The problem of oscillation and zigzagging is quite ubiquitous whenever the steepest-descent direction moves along a direction of high curvature in the loss function.The most extreme manifestation of this problem occurs in the case of extreme ill-conditioning, for which the partial derivatives of the loss are wildly different with respect to the different optimization variables.In this section, we will discuss several clever learning strategies that work well in these ill-conditioned settings.A constant learning rate is not desirable because it poses a dilemma to the analyst.The dilemma is as follows.A lower learning rate used early on will cause the algorithm to take too long to come even close to an optimal solution.On the other hand, a large initial learning rate will allow the algorithm to come reasonably close to a good solution at first; however, the algorithm will then oscillate around the point for a very long time, or diverge in an unstable way, if the high rate of learning is maintained.In either case, maintaining a constant learning rate is not ideal.Allowing the learning rate to decay over time can naturally achieve the desired learning-rate adjustment to avoid these challenges.The two most common decay functions are exponential decay and inverse decay.The learning rate α t can be expressed in terms of the initial decay rate α 0 and epoch t asThe parameter k controls the rate of the decay.Another approach is to use step decay in which the learning rate is reduced by a particular factor every few epochs.For example, the learning rate might be multiplied by 0.5 every 5 epochs.A common approach is to track the loss on a held-out portion of the training data set, and reduce the learning rate whenever this loss stops improving.In some cases, the analyst might even babysit the learning process, and use an implementation in which the learning rate can be changed manually depending on the progress.This type of approach can be used with simple implementations of gradient descent, although it does not address many of the other problematic issues.Momentum-based techniques recognize that zigzagging is a result of highly contradictory steps that cancel out one another and reduce the effective size of the steps in the correct (long-term) direction.An example of this scenario is illustrated in Figure 3.9(b).Simply attempting to increase the size of the step in order to obtain greater movement in the correct direction might actually move the current solution even further away from the optimum solution.In this point of view, it makes a lot more sense to move in an "averaged" direction of the last few steps, so that the zigzagging is smoothed out.In order to understand this point, consider a setting in which one is performing gradientdescent with respect to the parameter vector W .The normal updates for gradient-descent with respect to loss function L (defined over a mini-batch of instances) are as follows:.12: Effect of momentum in navigating complex loss surfaces.The annotation "GD" indicates pure gradient descent without momentum.Momentum helps the optimization process retain speed in flat regions of the loss surface and avoid local optima.Here, α is the learning rate.In momentum-based descent, the vector V is modified with exponential smoothing, where β ∈ (0, 1) is a smoothing parameter:Larger values of β help the approach pick up a consistent velocity V in the correct direction.Setting β = 0 specializes to straightforward mini-batch gradient-descent.The parameter β is also referred to as the momentum parameter or the friction parameter.The word "friction" is derived from the fact that small values of β act as "brakes," much like friction.With momentum-based descent, the learning is accelerated, because one is generally moving in a direction that often points closer to the optimal solution and the useless "sideways" oscillations are muted.The basic idea is to give greater preference to consistent directions over multiple steps, which have greater importance in the descent.This allows the use of larger steps in the correct direction without causing overflows or "explosions" in the sideways direction.As a result, learning is accelerated.An example of the use of momentum is illustrated in Figure 3.11.It is evident from Figure 3.11(a) that momentum increases the relative component of the gradient in the correct direction.The corresponding effects on the updates are illustrated in Figure 3.11(b) and (c).It is evident that momentum-based updates can reach the optimal solution in fewer updates.The use of momentum will often cause the solution to slightly overshoot in the direction where velocity is picked up, just as a marble will overshoot when it is allowed to roll down a bowl.However, with the appropriate choice of β, it will still perform better than a situation in which momentum is not used.The momentum-based method will generally perform better because the marble gains speed as it rolls down the bowl; the quicker arrival at the optimal solution more than compensates for the overshooting of the target.Overshooting is desirable to the extent that it helps avoid local optima.Figure 3.12, which shows a marble rolling down a complex loss surface (picking up speed as it rolls down), illustrates this concept.The marble's gathering of speed helps it efficiently navigate flat regions of the loss surface.The parameter β controls the amount of friction that the marble encounters while rolling down the loss surface.While increased values of β help in avoiding local optima, it might also increase oscillation at the end.In this sense, the momentum-based method has a neat interpretation in terms of the physics of a marble rolling down a complex loss surface.The Nesterov momentum [353] is a modification of the traditional momentum method in which the gradients are computed at a point that would be reached after executing a βdiscounted version of the previous step again (i.e., the momentum portion of the current step).This point is obtained by multiplying the previous update vector V with the friction parameter β and then computing the gradient at W + βV .The idea is that this corrected gradient uses a better understanding of how the gradients will change because of the momentum portion of the update, and incorporates this information into the gradient portion of the update.Therefore, one is using a certain amount of lookahead in computing the updates.Let us denote the loss function by L(W ) at the current solution W .In this case, it is important to explicitly denote the argument of the loss function because of the way in which the gradient is computed.Therefore, the update may be computed as follows:Note that the only difference from the standard momentum method is in terms of where the gradient is computed.Using the value of the gradient a little further along the previous update can lead to faster convergence.In the previous analogy of the rolling marble, such an approach will start applying the "brakes" on the gradient-descent procedure when the marble starts reaching near the bottom of the bowl, because the lookahead will "warn" it about the reversal in gradient direction.The Nesterov method works only in mini-batch gradient descent with modest batch sizes; using very small batches is a bad idea.In such cases, it can be shown that the Nesterov method reduces the error to O(1/t 2 ) after t steps, as compared to an error of O(1/t) in the momentum method.The basic idea in the momentum methods of the previous section is to leverage the consistency in the gradient direction of certain parameters in order to speed up the updates.This goal can also be achieved more explicitly by having different learning rates for different parameters.The idea is that parameters with large partial derivatives are often oscillating and zigzagging, whereas parameters with small partial derivatives tend to be more consistent but move in the same direction.An early method, which was proposed in this direction, was the delta-bar-delta method [217].This approach tracks whether the sign of each partial derivative changes or stays the same.If the sign of a partial derivative stays consistent, then it is indicative of the fact that the direction is correct.In such a case, the partial derivative in that direction increases.On the other hand, if the sign of the partial derivative flips all the time, then the partial derivative decreases.However, this kind of approach is designed for gradient descent rather than stochastic gradient descent, because the errors in stochastic gradient descent can get magnified.Therefore, a number of methods have been proposed that can work well even when the mini-batch method is used.There are two key differences from the RMSProp algorithm.First, the gradient is replaced with its exponentially smoothed value in order to incorporate momentum.Second, the learning rate α t now depends on the iteration index t, and is defined as follows:Technically, the adjustment to the learning rate is actually a bias correction factor that is applied to account for the unrealistic initialization of the two exponential smoothing mechanisms, and it is particularly important in early iterations.Both F i and A i are initializedSo far, only the use of first-order derivatives has been discussed in this chapter.The progress with first-order derivatives can be slow with some error surfaces.Part of the problem is that the first-order derivatives provide a limited amount of information about the error surface, which can cause the updates to overshoot.The complexity of the loss surfaces of many neural networks can cause gradient-based updates to perform in an unanticipated way.An example of a loss surface is shown in Figure 3.13.In this case, there is a gently sloping surface that rapidly changes into a cliff.However, if one computed only the first-order partial derivative with respect to the variable x shown in the figure, one would only see a gentle slope.As a result, a small learning rate will lead to very slow learning, whereas increasing the learning rate can suddenly cause overshooting to a point far from the optimal solution.This problem is caused by the nature of the curvature (i.e., changing gradient), where the first-order gradient does not contain the information needed to control the size of the update.In many cases, the rate of change of gradient can be computed using the secondorder derivative, which provides useful (additional) information.In general, second-order methods approximate the local loss surface with a quadratic bowl, which is more accurate than the linear approximation.Some second-order methods like the Newton method require exactly one iteration in order to find the local optimal solution for a quadratic surface.Of course, the loss surface of neural models is typically not quadratic.Nevertheless, the approximation is often good enough that gradient-descent methods are greatly accelerated at least in cases where the change in the gradient is not too sudden or drastic.Cliffs are not desirable because they manifest a certain level of instability in the loss function.This implies that a small change in some of the weights can either change the loss in a tiny way or suddenly change the loss by such a large amount that the resulting solution is even further away from the true optimum.As you will learn in Chapter 7, all temporal layers of a recurrent neural network share the same parameters.In such a case, the vanishing and exploding gradient means that there is varying sensitivity of the loss function with respect to the parameters in earlier and later layers (which are tied anyway).Therefore, a small change in a well-chosen parameter can cascade in an unstable way through the layers and either blow up or have negligible effect on the value of the loss function.Furthermore, it is hard to control the step size in a way that prevents one of these two events.This is the typical behavior one would encounter near a cliff.As a result, it is easy to miss the optimum during a gradient-descent step.One way of understanding this behavior is that sharing parameters across layers naturally leads to higher-order effects of weight perturbations on the loss function.This is because the shared weights of different layers are multiplied during neural network prediction, and a first-order gradient is now insufficient to model the effect of the curvature in the loss function, which is a measure of the change in gradient along a particular direction.Such settings are often addressed with techniques that either clip the gradient, or explicitly use the curvature (i.e., second-order derivative) of the loss function.Gradient clipping is a technique that is used to deal with settings in which the partial derivatives along different directions have exceedingly different magnitudes.Some forms of gradient clipping use a similar principle to that used in adaptive learning rates by trying the make the different components of the partial derivatives more even.However, the clipping is done only on the basis of the current values of the gradients rather than their historical values.Two forms of gradient clipping are most common:1. Value-based clipping: In value-based clipping, a minimum and maximum threshold are set on the gradient values.All partial derivatives that are less than the minimum are set to the minimum threshold.All partial derivatives that are greater than the maximum are set to the maximum threshold.In this case, the entire gradient vector is normalized by the L 2 -norm of the entire vector.Note that this type of clipping does not change the relative magnitudes of the updates along different directions.However, for neural networks that share parameters across different layers (like recurrent neural networks), the effect of the two types of clipping is very similar.By clipping, one can achieve a better conditioning of the values, so that the updates from mini-batch to mini-batch are roughly similar.Therefore, it would prevent an anomalous gradient explosion in a particular mini-batch from affecting the solution too much.By and large, the effects of gradient clipping are quite limited compared to many other methods.However, it is particularly effective in avoiding the exploding gradient problem in recurrent neural networks.In recurrent neural networks (cf.Chapter 7), the parameters are shared across different layers, and a derivative is computed with respect to each copy of the shared parameter by treating it as a separate variable.These derivatives are the temporal components of the overall gradient, and the values are clipped before adding them in order to obtain the overall gradient.A geometric interpretation of the exploding gradient problem is provided in [369], and a detailed exploration of why gradient clipping works is provided in [368].A number of methods have been proposed in recent years for using second-order derivatives for optimization.Such methods can partially alleviate some of the problems caused by curvature of the loss function.Consider the parameter vector W = (w 1 . . .w d ) T , which is expressed 3 as a column vector.The second-order derivatives of the loss function L(W ) are of the following form:Note that the partial derivatives use all pairwise parameters in the denominator.Therefore, for a neural network with d parameters, we have a d × d Hessian matrix H, for which the (i, j)th entry is H ij .The second-order derivatives of the loss function can be computed with backpropagation [315], although this is rarely done in practice.The Hessian can be viewed as the Jacobian of the gradient.One can write a quadratic approximation of the loss function in the vicinity of parameter vector W 0 by using the following Taylor expansion:Note that the Hessian H is computed at W 0 .Here, the parameter vectors W and W 0 are d-dimensional column vectors, as is the gradient of the loss function.This is a quadratic approximation, and one can simply set the gradient to 0, which results in the following optimality condition for the quadratic approximation: One can rearrange the above optimality condition to obtain the following Newton update:One interesting characteristic of this update is that it is directly obtained from an optimality condition, and therefore there is no learning rate.In other words, this update is approximating the loss function with a quadratic bowl and moving exactly to the bottom of the bowl in a single step; the learning rate is already incorporated implicitly.Recall from Figure 3.9 that first-order methods bounce along directions of high curvature.Of course, the bottom of the quadratic approximation is not the bottom of the true loss function, and therefore multiple Newton updates will be needed.The main difference of Equation 3.48 from the update of steepest-gradient descent is premultiplication of the steepest direction (which is [∇L(W 0 )]) with the inverse of the Hessian.This multiplication with the inverse Hessian plays a key role in changing the direction of the steepest-gradient descent, so that one can take larger steps in that direction (resulting in better improvement of the objective function) even if the instantaneous rate of change in that direction is not as large as the steepest-descent direction.This is because the Hessian encodes how fast the gradient is changing in each direction.Changing gradients are bad for larger updates because one might inadvertently worsen the objective function, if the signs of many components of the gradient change during the step.It is profitable to move in directions where the ratio of the gradient to the rate of change of the gradient is large, so that one can take larger steps without causing harm to the optimization.Pre-multiplication with the inverse of the Hessian achieves this goal.The effect of the pre-multiplication of the steepest-descent direction with the inverse Hessian is shown in Figure 3.14.It is helpful to reconcile this figure with the example of the quadratic bowl in Figure 3.9.In a sense, pre-multiplication with the inverse Hessian biases the learning steps towards low-curvature directions.In one dimension, the Newton step is simply the ratio of the first derivative (rate of change) to the second derivative (curvature).In multiple dimensions, the low-curvature directions tend to win out because of multiplication by the inverse Hessian.The specific effect of curvature is particularly evident when one encounters loss functions in the shape of sloping or winding valleys.An example of a sloping valley is shown in Figure 3.15.A valley is a dangerous topography for a gradient-descent method, particularly if the bottom of the valley has a steep and rapidly changing surface (which creates a narrow valley).This is, of course, not the case in Figure 3.15, which is a relatively easier case.However, even in this case, the steepest-descent direction will often bounce along the sides of the valley, and move down the slope relatively slowly if the step-sizes are chosen inaccurately.In narrow valleys, the gradient-descent method will bounce along the steep sides of the valley even more violently without making much progress in the gently sloping direction, where the greatest long-term gains are present.In such cases, it is only by normalizing the gradient information with the curvature, that will provide the correct directions of longterm movement.This type of normalization tends to favor low-curvature directions like the ones shown in Figure 3.15.Multiplication of the steepest-descent direction with the inverse Hessian achieves precisely this goal.In most large-scale neural network settings, the Hessian is too large to store or compute explicitly.It is not uncommon to have neural networks with millions of parameters.Trying to compute the inverse of a 10 6 × 10 6 Hessian matrix is impractical with the computational power available today.In fact, it is difficult to even compute the Hessian, let alone invert it!Therefore, many approximations and variations of the Newton method have been developed.Examples of such methods include Hessian-free optimization [41,189,313,314] (or method of conjugate gradients) and quasi-Newton methods that approximate the Hessian.The basic goal of these methods to make second-order updates without exactly computing the Hessian.The conjugate gradient method [189] requires d steps to reach the optimal solution of a quadratic loss function (instead of a single Newton step).This approach is well known in the classical literature on neural networks [41,443], and a variant has recently been reborn under the title of "Hessian-free optimization."This name is motivated by the fact that the search direction can be computed without the explicit computation of the Hessian.A key problem in first-order methods is the zigzag movement of the optimization process, which undoes much of the work done in previous iterations.In the conjugate gradient method, the directions of movement are related to one another in such a way that the work done in previous iterations is never undone (for a quadratic loss function).This is because the change in gradient in a step, when projected along the vector of any other movement direction, is always 0. Furthermore, one uses line search to determine the optimal step size by searching over different step sizes.Since an optimal step is taken along each direction and the work along that direction is never undone by subsequent steps, d linearly independent steps are needed to reach the optimum of a d-dimensional function.Since it is possible to find such directions only for quadratic loss functions, we will first discuss the conjugate gradient method under the assumption that the loss function L(W ) is quadratic.A quadratic and convex loss function L(W ) has an ellipsoidal contour plot of the type shown in Figure 3.16.The orthonormal eigenvectors q 0 . . .q d−1 of the symmetric Hessian represent the axes directions of the ellipsoidal contour plot.One can rewrite the loss function in a new coordinate space corresponding to the eigenvectors.In the axis system corresponding the eigenvectors, the (transformed) variables do not have interactions with one another because of the alignment of ellipsoidal loss contour with the axis system.This is because the new Hessian H q = Q T HQ obtained by rewriting the loss function in terms of the transformed variables is diagonal, where Q is a d × d matrix with columns containing the eigenvectors.Therefore, each transformed variable can be optimized independently of (a) Eigenvectors of Hessian (b) Arbitrary conjugate pair Mutually Orthogonal: q T i q j = 0 Non-orthogonal: q T i Hq j = 0 the others.Alternatively, one can work with the original variables by successively making the best (projected) gradient-descent step along each eigenvector so as to minimize the loss function.The best movement along a particular direction is done using line search to select the step size.The nature of the movement is illustrated in Figure 3.16(a).Note that movement along the jth eigenvector does not disturb the work done along earlier eigenvectors and therefore d steps are sufficient to each the optimal solution.Although it is impractical to compute the eigenvectors of the Hessian, there are other efficiently computable directions satisfying similar properties; this key property is referred to as mutual conjugacy of vectors.Note that two eigenvectors q i and q j of the Hessian satisfy q T i q j = 0 because of orthogonality.Furthermore, since q j is an eigenvector of H, we have Hq j = λ j q j for some scalar eigenvalue λ j .Multiplying both sides with q T i , we can easily show that the eigenvectors of the Hessian satisfy q T i Hq j = 0 in pairwise fashion.This condition is referred to as the mutual conjugacy condition, and it is equivalent to saying that the Hessian H q = Q T HQ in the transformed axis-system of directions q 0 . . .q d−1 is diagonal.In fact, it turns out that if we select any set of (not necessarily orthogonal) vectors q 0 . . .q d−1 satisfying the mutual conjugacy condition, then movement along any of these directions does not disturb the projected gradient along other directions.Conjugate directions other than Hessian eigenvectors, such as those shown in Figure 3.16(b), may not be mutually orthogonal.If we re-write the quadratic loss function in terms of coordinates in a nonorthogonal axis system of conjugate directions, we will get nicely separated variables with a diagonal Hessian H q = Q T HQ.However, H q is not a true diagonalization of H because Q T Q = I.Nevertheless, such non-interacting directions are crucial to avoid zigzagging.Let W t and W t+1 represent the respective parameter vectors before and after movement along q t .The change in gradient ∇L(W t+1 ) − ∇L(W t ) caused by movement along the direction q t points in the same direction as Hq t .This is because the product of the second-derivative (Hessian) matrix with a direction is proportional to the change in the first-derivative (gradient) when moving along that direction.This relationship is a finitedifference approximation for non-quadratic functions and it is exact for quadratic functions.Therefore, the projection (or dot product) of this change vector with respect to any other step vector (W i+1 − W i ) ∝ q i is given by the following:This means that the only change to the gradient along a particular direction q i (during the entire learning) occurs during the step along that direction.Line search ensures that the final gradient along that direction is 0. Convex loss functions have linearly independent conjugate directions (see Exercise 7).By making the best step along each conjugate direction, the final gradient will have zero dot product with d linearly independent directions; this is possible only when the final gradient is the zero vector (see Exercise 8), which implies optimality for a convex function.In fact, one can often reach a near-optimal solution in far fewer than d updates.How can one generate conjugate directions iteratively?The obvious approach requires one needs to track O(d 2 ) vector components of all previous O(d) conjugate directions in order to enforce conjugacy of the next direction with respect to all these previous directions (see Exercise 11).Surprisingly, only the most recent conjugate direction is needed to generate the next direction [359,443], when steepest decent directions are used for iterative generation.This is not an obvious result (see Exercise 12).The direction q t+1 is, therefore, defined iteratively as a linear combination of only the previous conjugate direction q t and the current steepest descent direction ∇L(W t+1 ) with combination parameter β t : q t+1 = −∇L(W t+1 ) + β t q t (3.49)Premultiplying both sides with q T t H and using the conjugacy condition to set the left-hand side to 0, one can solve for β t :This leads to an iterative update process, which initializes q 0 = −∇L(W 0 ), and computes q t+1 iteratively for t = 0, 1, 2, . . .T :1. Update W t+1 ⇐ W t + α t q t .Here, the step size α t is computed using line search to minimize the loss function.2. Set q t+1 = −∇L(W t+1 ) + q T t H[∇L(W t+1 )] q T t Hq t q t .Increment t by 1.It can be shown [359,443] that q t+1 satisfies conjugacy with respect to all previous q i .A systematic road-map of this proof is provided in Exercise 12.The above updates do not seem to be Hessian-free, because the matrix H is included in the above updates.However, the underlying computations only need the projection of the Hessian along particular directions; we will see that these can be computed indirectly using the method of finite differences without explicitly computing the individual elements of the Hessian.Let v be the vector direction for which the projection Hv needs to be computed.The method of finite differences computes the loss gradient at the current parameter vector W and at W + δv for some small value of δ in order to perform the approximation:The right-hand side is free of the Hessian.The condition is exact for quadratic functions.Other alternatives for Hessian-free updates are discussed in [41].So far, we have discussed the simplified case of quadratic loss functions, in which the second-order derivative matrix (i.e., Hessian) is a constant matrix (i.e., independent of the current parameter vector).However, neural loss functions are not quadratic and, therefore, the Hessian matrix is dependent on the current value of W t .Do we first create a quadratic approximation at a point and then solve it for a few iterations with the Hessian (quadratic approximation) fixed at that point, or do we change the Hessian every iteration?The former is referred to as the linear conjugate gradient method, whereas the latter is referred to as the nonlinear conjugate gradient method.The two methods are equivalent for quadratic loss functions, which almost never occur in neural networks.Classical work in neural networks and machine learning has predominantly explored the use of the nonlinear conjugate gradient method [41], whereas recent work [313,314] advocates the use of linear conjugate methods.In the nonlinear conjugate gradient method, the mutual conjugacy of the directions will deteriorate over time, which can have an unpredictable effect on overall progress even after a large number of iterations.A part of the problem is that the process of computing conjugate directions needs to be restarted every few steps as the mutual conjugacy deteriorates.If the deterioration occurs too fast, one does not gain much from conjugacy.On the other hand, each quadratic approximation in the linear conjugate gradient method can be solved exactly, and will typically be (almost) solved in much fewer than d iterations.Although multiple such approximations will be needed, there is guaranteed progress within each approximation, and the required number of approximations is often not too large.The work in [313] experimentally shows the superiority of linear conjugate gradient methods.The acronym BFGS stands for the Broyden-Fletcher-Goldfarb-Shanno algorithm, and it is derived as an approximation of the Newton method.Let us revisit the updates of the Newton method.A typical update of the Newton method is as follows:In quasi-Newton methods, a sequence of approximations of the inverse Hessian matrix are used in various steps.Let the approximation of the inverse Hessian matrix in the tth step be denoted by G t .In the very first iteration, the value of G t is initialized to the identity matrix, which amounts to moving along the steepest-descent direction.This matrix is continuously updated from G t to G t+1 with low-rank updates.A direct restatement of the Newton update in terms of the inverse Hessian G t ≈ H −1 t is as follows:The above update can be improved with an optimized learning rate α t for non-quadratic loss functions working with (inverse) Hessian approximations like G t :The optimized learning rate α t is identified with line search.The line search does not need to be performed exactly (like the conjugate gradient method), because maintenance of conjugacy is no longer critical.Nevertheless, approximate conjugacy of the early set of directions is maintained by the method when starting with the identity matrix.One can (optionally) reset G t to the identity matrix every d iterations (although this is rarely done).It remains to be discussed how the matrix G t+1 is approximated from G t .For this purpose, the quasi-Newton condition, also referred to as the secant condition, is needed:First derivative change (3.55)The above formula is simply a finite-difference approximation.Intuitively, multiplication of the second-derivative matrix (i.e., Hessian) with the parameter change (vector) approximately provides the gradient change.Therefore, multiplication of the inverse Hessian approximation G t+1 with the gradient change provides the parameter change.The goal is to find a symmetric matrix G t+1 satisfying Equation 3.55, but it represents an underdetermined system of equations with an infinite number of solutions.Among these, BFGS chooses the closest symmetric G t+1 to the current G t , and achieves this goal by posing a minimization objective function ||G t+1 − G t || F in the form of a weighted Frobenius norm.The solution is as follows: G t+1 ⇐ (I − Δ t q t v T t )G t (I − Δ t v t q T t ) + Δ t q t q T t (3.56)Here, the (column) vectors q t and v t represent the parameter change and the gradient change; the scalar Δ t = 1/(q T t v t ) is the inverse of the dot product of these two vectors.The update in Equation 3.56 can be made more space efficient by expanding it, so that fewer temporary matrices need to be maintained.Interested readers are referred to [300,359,376] for implementation details and derivation of these updates.Even though BFGS benefits from approximating the inverse Hessian, it does need to carry over a matrix G t of size O(d 2 ) from one iteration to the next.The limited memory BFGS (L-BFGS) reduces the memory requirement drastically from O(d 2 ) to O(d) by not carrying over the matrix G t from the previous iteration.In the most basic version of the L-BFGS method, the matrix G t is replaced with the identity matrix in Equation 3.56 in order to derive G t+1 .A more refined choice is to store the m ≈ 30 most recent vectors q t and v t .Then, L-BFGS is equivalent to initializing G t−m+1 to the identity matrix and recursively applying Equation 3.56 m times to derive G t+1 .In practice, the implementation is optimized to directly compute the direction of movement from the vectors without explicitly storing large intermediate matrices from G t−m+1 to G t .The directions found by L-BFGS roughly satisfy mutual conjugacy even with approximate line search.Second-order methods are susceptible to the presence of saddle points.A saddle point is a stationary point of a gradient-descent method because its gradient is zero, but it is not a minimum (or maximum).A saddle point is an inflection point, which appears to be either a minimum or a maximum depending on which direction we approach it from.Therefore, the quadratic approximation of the Newton method will give vastly different shapes depending on the direction that one approaches the saddle point from.A 1-dimensional function with a saddle point is the following: f (x) = x 3 This function is shown in Figure 3.17(a), and it has an inflection point at x = 0. Note that a quadratic approximation at x > 0 will look like an upright bowl, whereas a quadratic approximation at x < 0 will look like an inverted bowl.Furthermore, even if one reaches x = 0 in the optimization process, both the second derivative and the first derivative will be zero.Therefore, a Newton update will take the 0/0 form and become indefinite.Such a point is a degenerate point from the perspective of numerical optimization.Not all saddle points are degenerate points and vice versa.For multivariate problems, such degenerate points are often wide and flat regions that are not minima of the objective function.They do present a significant problem for numerical optimization.An example of such a function is h(x, y) = x 3 + y 3 , which is degenerate at (0, 0).Furthermore, the region near (0, 0) will appear like a flat plateau.These types of plateaus create problems for learning algorithms, because first-order algorithms slow down in these regions and second-order algorithms also cannot recognize them as spurious regions.It is noteworthy that such saddle points arise only in higher-order algebraic functions (i.e., higher than second order), which are common in neural network optimization.It is also instructive to examine the case of a saddle point that is not a degenerate point.An example of a 2-dimensional function with a saddle point is as follows:This function is shown in Figure 3.17(b).The saddle point is (0, 0).It is easy to see that the shape of this function resembles a riding saddle.In this case, approaching from the x direction or from the y direction will result in very different quadratic approximations.In one case, the function will appear to be a minimum, and in another case the function will appear to be a maximum.Furthermore, the saddle point (0, 0) will be a stationary point from the perspective of a Newton update, even though it is not an extremum.Saddle points occur frequently in regions between two hills of the loss function, and they present a problematic topography for second-order methods.Interestingly, first-order methods are often able to escape from saddle points [146], because the trajectory of first-order methods is simply not attracted by such points.On the other hand, Newton's method will jump directly to the saddle point.Unfortunately, some neural-network loss functions seem to contain a large number of saddle points.Second-order methods therefore are not always preferable to first-order methods; the specific topography of a particular loss function may have an important role to play.Second-order methods are advantageous in situations with complex curvatures of the loss function or in the presence of cliffs.In other functions with saddle points, first-order methods are advantageous.Note that the pairing of computational algorithms (like Adam) with first-order gradient-descent methods already incorporates several advantages of secondorder methods in an implicit way.Therefore, real-world practitioners often prefer first-order methods in combination with computational algorithms like Adam.Recently, some methods have been proposed [88] to address saddle points in second-order methods.One of the motivations for second-order methods is to avoid the kind of bouncing behavior caused by high-curvature regions.The example of the bouncing behavior caused in valleys (cf. Figure 3.15) is another example of this setting.One way of achieving some stability with any learning algorithm is to create an exponentially decaying average of the parameters over time, so that the bouncing behavior is avoided.Let W 1 . . .W T , be the sequence of parameters found by any learning method over the full sequence of T steps.In the simplest version of Polyak averaging, one simply computes the average of all the parameters as the final set W For simple averaging, we only need to compute W f T once at the end of the process, and we do not need to compute the values at 1 . . .T − 1.However, for exponential averaging with decay parameter β < 1, it is helpful to compute these values iteratively and maintain a running average over the course of the algorithm:The two formulas above are approximately equivalent at large values of t.The second formula is convenient because it enables maintenance over the course of the algorithm, and one does not need to maintain the entire history of parameters.Exponentially decaying averages are more useful than simple averages to avoid the effect of stale points.In simple averaging, the final result may be too heavily influenced by the early points, which are poor approximations to the correct solution.The example of the quadratic bowl given in earlier sections is a relatively simple optimization problem that has a single global optimum.Such problems are referred to as convex optimization problems, and they represent the simplest case of optimization.In general, however, the objective function of a neural network is not convex, and it is likely to have many local minima.In such cases, it is possible for the learning to converge to a suboptimal solution.In spite of this fact, with reasonably good initialization, the problem of local minima in neural networks causes fewer problems than might be expected.Local minima are problematic only when their objective function values are significantly larger than that of the global minimum.In practice, however, this does not seem to be the case in neural networks.Many research results [88,426] have shown that the local minima of real-life networks have very similar objective function values to the global minimum.As a result, their presence does not seem to cause as strong a problem as usually thought.Local minima often cause problems in the context of model generalization with limited data.An important point to keep in mind is that the loss function is always defined on a limited sample of the training data, which is only a rough approximation of what the shape of the loss function looks like on the true distribution of the unseen test data.When the size of the training data is small, a number of spurious global or local minima are created by the paucity of training data.These minima are not seen in the (infinitely large) unseen distribution of test examples, but they appear as random artifacts of the particular choice of the training data set.Such spurious minima are often more prominent and attractive when the loss function is constructed on smaller training samples.In such cases, spurious minima can indeed create a problem, because they do not generalize well to unseen test instances.This problem is slightly different from the usual concept of local minima understood in traditional optimization; the local minima on the training data do not generalize well to the test data.In other words, the shape of the loss function is not even the same on the training and on the test data, and therefore the minima in the two cases do not match.Here, it is important to understand that there are fundamental differences between traditional optimization and machine learning methods that attempt to generalize a loss function on a limited data set to the universe of test examples.This is a notion referred to as empirical risk minimization, in which one computes the (approximate) empirical risk for a learning algorithm because the true distribution of the examples is unknown.When starting with random initialization points, it is often possible to fall into one of these spurious minima, unless one is careful to move the initialization point to a place closer to the basins of true optima (from a model generalization point of view).One such approach is that of unsupervised pretraining, which is discussed in Chapter 4.The specific problem of spurious minima (caused by the inability to generalize the results from a limited training data to unseen test data) is a much larger problem in neural network learning than the problem of local minima (from the perspective of traditional optimization).The nature of this problem is different enough from the normal understanding of local minima, so that it discussed in a separate chapter on model generalization (cf.Chapter 4).Batch normalization is a recent method to address the vanishing and exploding gradient problems, which cause activation gradients in successive layers to either reduce or increase in magnitude.Another important problem in training deep networks is that of internal covariate shift.The problem is that the parameters change during training, and therefore the hidden variable activations change as well.In other words, the hidden inputs from early layers to later layers keep changing.Changing inputs from early layers to later layers causes slower convergence during training because the training data for later layers is not stable.Batch normalization is able to reduce this effect.In batch normalization, the idea is to add additional "normalization layers" between hidden layers that resist this type of behavior by creating features with somewhat similar variance.Furthermore, each unit in the normalization layers contains two additional parameters β i and γ i that regulate the precise level of normalization in the ith unit; these parameters are learned in a data-driven manner.The basic idea is that the output of the ith unit will have a mean of β i and a standard deviation of γ i over each mini-batch of training instances.One might wonder whether it might make sense to simply set each β i to 0 and each γ i to 1, but doing so reduces the representation power of the network.For example, if we make this transformation, then the sigmoid units will be operating within their linear regions, especially if the normalization is performed just before activation (see below for discussion of Figure 3.18).Recall from the discussion in Chapter 1 that multilayer networks do not gain power from depth without nonlinear activations.Therefore, allowing some "wiggle" with these parameters and learning them in a data-driven manner makes sense.Furthermore, the parameter β i plays the role of a learned bias variable, and therefore we do not need additional bias units in these layers.We assume that the ith unit is connected to a special type of node BN i , where BN stands for batch normalization.This unit contains two parameters β i and γ i that need to be learned.Note that BN i has only one input, and its job is to perform the normalization and scaling.This node is then connected to the next layer of the network in the standard way in which a neural network is connected to future layers.Here, we mention that there are two choices for where the normalization layer can be connected:1.The normalization can be performed just after applying the activation function to the linearly transformed inputs.This solution is shown in Figure 3.18(a).Therefore, the normalization is performed on post-activation values.2. The normalization can be performed after the linear transformation of the inputs, but before applying the activation function.This situation is shown in Figure 3.18(b).Therefore, the normalization is performed on pre-activation values.It is argued in [214] that the second choice has more advantages.Therefore, we focus on this choice in this exposition.The BN node shown in Figure 3.18(b) is just like any other computational node (albeit with some special properties), and one can perform backpropagation through this node just like any other computational node.What transformations does BN i apply?Consider the case in which its input is v (r)i , corresponding to the rth element of the batch feeding into the ith unit.Each v (r) i is obtained by using the linear transformation defined by the coefficient vector W i (and biases if any).For a particular batch of m instances, let the values of the m activations be denoted by v .The first step is to compute the mean μ i and standard deviation σ i for the ith hidden unit.These are then scaled using the parameters β i and γ i to create the outputs for the next layer:A small value of is added to σ 2 i to regularize cases in which all activations are the same, which results in zero variance.Note that a (r) i is the pre-activation output of the ith node, when the rth batch instance passes through it.This value would otherwise have been set to v (r) i , if we had not applied batch normalization.We conceptually represent this node with a special node BN i that performs this additional processing.This node is shown in Figure 3.18(b).Therefore, the backpropagation algorithm has to account for this additional node and ensure that the loss derivative of layers earlier than the batch normalization layer accounts for the transformation implied by these new nodes.It is important to note that the function applied at each of these special BN nodes is specific to the batch at hand.This type of computation is unusual for a neural network in which the gradients are linearly separable sums of the gradients with respect to individual training examples.This is not quite true in this case because the batch normalization layer computes nonlinear metrics from the batch (such as its standard deviation).Therefore, the activations depend on how the examples in a batch are related to one another, which is not common in most neural computations.However, this special property of the BN node does not prevent us from backpropagating through the computations performed in it.The following will describe the changes in the backpropagation algorithm caused by the normalization layer.The main point of this change is to show how to backpropagate through the newly added layer of normalization nodes.Another point to be aware of is that we want to optimize the parameters β i and γ i .For the gradient-descent steps with respect to each β i and γ i , we need the gradients with respect to these parameters.Assume that we have already backpropagated up to the output of the BN node, and therefore we have each for all nodes j in the previous layer uses the straightforward backpropagation update introduced earlier in this chapter.Therefore, the dynamicIn the AdaGrad algorithm [108], one keeps track of the aggregated squared magnitude of the partial derivative with respect to each parameter over the course of the algorithm.The square-root of this value is proportional to the root-mean-square slope for that parameter (although the absolute value will increase with the number of epochs because of successive aggregation).Let A i be the aggregate value for the ith parameter.Therefore, in each iteration, the following update is performed:The update for the ith parameter w i is as follows:If desired, one can use √ A i + in the denominator instead of √ A i to avoid ill-conditioning.Here, is a small positive value such as 10 −8 .Scaling the derivative inversely with √ A i is a kind of "signal-to-noise" normalization because A i only measures the historical magnitude of the gradient rather than its sign; it encourages faster relative movements along gently sloping directions with consistent sign of the gradient.If the gradient component along the ith direction keeps wildly fluctuating between +100 and −100, this type of magnitude-centric normalization will penalize that component far more than another gradient component that consistently takes on the value in the vicinity of 0.1 (but with a consistent sign).For example, in Figure 3.11, the movements along the oscillating direction will be de-emphasized, and the movement along the consistent direction will be emphasized.However, absolute movements along all components will tend to slow down over time, which is the main problem with the approach.The slowing down is caused by the fact that A i is the aggregate value of the entire history of partial derivatives.This will lead to diminishing values of the scaled derivative.As a result, the progress of AdaGrad might prematurely become too slow, and it will eventually (almost) stop making progress.Another problem is that the aggregate scaling factors depend on ancient history, which can eventually become stale.The use of stale scaling factors can increase inaccuracy.As we will see later, most of the other methods use exponential averaging, which solves both problems.The RMSProp algorithm [194] uses a similar motivation as AdaGrad for performing the "signal-to-noise" normalization with the absolute magnitude √ A i of the gradients.However, instead of simply adding the squared gradients to estimate A i , it uses exponential averaging.Since one uses averaging to normalize rather than aggregate values, the progress is not slowed prematurely by a constantly increasing scaling factor A i .The basic idea is to use a decay factor ρ ∈ (0, 1), and weight the squared partial derivatives occurring t updates ago by ρ t .Note that this can be easily achieved by multiplying the current squared aggregate (i.e., running estimate) by ρ and then adding (1 − ρ) times the current (squared) partial derivative.The running estimate is initialized to 0. This causes some (undesirable) bias in early iterations, which disappears over the longer term.Therefore, if A i is the exponentially averaged value of the ith parameter w i , we have the following way of updating A i :The square-root of this value for each parameter is used to normalize its gradient.Then, the following update is used for (global) learning rate α:Here, is a small positive value such as 10 −8 .Another advantage of RMSProp over AdaGrad is that the importance of ancient (i.e., stale) gradients decays exponentially with time.Furthermore, it can benefit by incorporating concepts of momentum within the computational algorithm (cf.Sections 3.5.3.3 and 3.5.3.5).The drawback of RMSProp is that the running estimate A i of the second-order moment is biased in early iterations because it is initialized to 0.RMSProp can also be combined with Nesterov momentum.Let A i be the squared aggregate of the ith weight.In such cases, we introduce the additional parameter β ∈ (0, 1) and use the following updates:Note that the partial derivative of the loss function is computed at a shifted point, as is common in the Nesterov method.The weight W is shifted with βV while computing the partial derivative with respect to the loss function.The maintenance of A i is done using the shifted gradients as well:Although this approach benefits from adding momentum to RMSProp, it does not correct for the initialization bias.The AdaDelta algorithm [553] uses a similar update as RMSProp, except that it eliminates the need for a global learning parameter by computing it as a function of incremental updates in previous iterations.Consider the update of RMSProp, which is repeated below:We will show how α is replaced with a value that depends on the previous incremental updates.In each update, the value of Δw i is the increment in the value of w i .As with the exponentially smoothed gradients A i , we keep an exponentially smoothed value δ i of the values of Δw i in previous iterations with the same decay parameter ρ:For a given iteration, the value of δ i can be computed using only the iterations before it because the value of Δw i is not yet available.On the other hand, A i can be computed using the partial derivative in the current iteration as well.This is a subtle difference between how A i and δ i are computed.This results in the following AdaDelta update:It is noteworthy that a parameter α for the learning rate is completely missing from this update.The AdaDelta method shares some similarities with second-order methods because the ratio δi Ai in the update is a heuristic surrogate for the inverse of the second derivative of the loss with respect to w i [553].As discussed in subsequent sections, many second-order methods like the Newton method also do not use learning rates.The Adam algorithm uses a similar "signal-to-noise" normalization as AdaGrad and RM-SProp; however, it also exponentially smooths the first-order gradient in order to incorporate momentum into the update.It also directly addresses the bias inherent in exponential smoothing when the running estimate of a smoothed value is unrealistically initialized to 0.As in the case of RMSProp, let A i be the exponentially averaged value of the ith parameter w i .This value is updated in the same way as RMSProp with the decay parameter ρ ∈ (0, 1):At the same time, an exponentially smoothed value of the gradient is maintained for which the ith component is denoted by F i .This smoothing is performed with a different decay parameter ρ f :This type of exponentially smoothing of the gradient with ρ f is a variation of the momentum method discussed in Section 3.5.2(which is parameterized by a friction parameter β instead of ρ f ).Then, the following update is used at learning rate α t in the tth iteration:In the AdaGrad algorithm [108], one keeps track of the aggregated squared magnitude of the partial derivative with respect to each parameter over the course of the algorithm.The square-root of this value is proportional to the root-mean-square slope for that parameter (although the absolute value will increase with the number of epochs because of successive aggregation).Let A i be the aggregate value for the ith parameter.Therefore, in each iteration, the following update is performed:The update for the ith parameter w i is as follows:If desired, one can use √ A i + in the denominator instead of √ A i to avoid ill-conditioning.Here, is a small positive value such as 10 −8 .Scaling the derivative inversely with √ A i is a kind of "signal-to-noise" normalization because A i only measures the historical magnitude of the gradient rather than its sign; it encourages faster relative movements along gently sloping directions with consistent sign of the gradient.If the gradient component along the ith direction keeps wildly fluctuating between +100 and −100, this type of magnitude-centric normalization will penalize that component far more than another gradient component that consistently takes on the value in the vicinity of 0.1 (but with a consistent sign).For example, in Figure 3.11, the movements along the oscillating direction will be de-emphasized, and the movement along the consistent direction will be emphasized.However, absolute movements along all components will tend to slow down over time, which is the main problem with the approach.The slowing down is caused by the fact that A i is the aggregate value of the entire history of partial derivatives.This will lead to diminishing values of the scaled derivative.As a result, the progress of AdaGrad might prematurely become too slow, and it will eventually (almost) stop making progress.Another problem is that the aggregate scaling factors depend on ancient history, which can eventually become stale.The use of stale scaling factors can increase inaccuracy.As we will see later, most of the other methods use exponential averaging, which solves both problems.The RMSProp algorithm [194] uses a similar motivation as AdaGrad for performing the "signal-to-noise" normalization with the absolute magnitude √ A i of the gradients.However, instead of simply adding the squared gradients to estimate A i , it uses exponential averaging.Since one uses averaging to normalize rather than aggregate values, the progress is not slowed prematurely by a constantly increasing scaling factor A i .The basic idea is to use a decay factor ρ ∈ (0, 1), and weight the squared partial derivatives occurring t updates ago by ρ t .Note that this can be easily achieved by multiplying the current squared aggregate (i.e., running estimate) by ρ and then adding (1 − ρ) times the current (squared) partial derivative.The running estimate is initialized to 0. This causes some (undesirable) bias in early iterations, which disappears over the longer term.Therefore, if A i is the exponentially averaged value of the ith parameter w i , we have the following way of updating A i :The square-root of this value for each parameter is used to normalize its gradient.Then, the following update is used for (global) learning rate α:Here, is a small positive value such as 10 −8 .Another advantage of RMSProp over AdaGrad is that the importance of ancient (i.e., stale) gradients decays exponentially with time.Furthermore, it can benefit by incorporating concepts of momentum within the computational algorithm (cf.Sections 3.5.3.3 and 3.5.3.5).The drawback of RMSProp is that the running estimate A i of the second-order moment is biased in early iterations because it is initialized to 0.RMSProp can also be combined with Nesterov momentum.Let A i be the squared aggregate of the ith weight.In such cases, we introduce the additional parameter β ∈ (0, 1) and use the following updates:Note that the partial derivative of the loss function is computed at a shifted point, as is common in the Nesterov method.The weight W is shifted with βV while computing the partial derivative with respect to the loss function.The maintenance of A i is done using the shifted gradients as well:Although this approach benefits from adding momentum to RMSProp, it does not correct for the initialization bias.The AdaDelta algorithm [553] uses a similar update as RMSProp, except that it eliminates the need for a global learning parameter by computing it as a function of incremental updates in previous iterations.Consider the update of RMSProp, which is repeated below:We will show how α is replaced with a value that depends on the previous incremental updates.In each update, the value of Δw i is the increment in the value of w i .As with the exponentially smoothed gradients A i , we keep an exponentially smoothed value δ i of the values of Δw i in previous iterations with the same decay parameter ρ:For a given iteration, the value of δ i can be computed using only the iterations before it because the value of Δw i is not yet available.On the other hand, A i can be computed using the partial derivative in the current iteration as well.This is a subtle difference between how A i and δ i are computed.This results in the following AdaDelta update:It is noteworthy that a parameter α for the learning rate is completely missing from this update.The AdaDelta method shares some similarities with second-order methods because the ratio δi Ai in the update is a heuristic surrogate for the inverse of the second derivative of the loss with respect to w i [553].As discussed in subsequent sections, many second-order methods like the Newton method also do not use learning rates.The Adam algorithm uses a similar "signal-to-noise" normalization as AdaGrad and RM-SProp; however, it also exponentially smooths the first-order gradient in order to incorporate momentum into the update.It also directly addresses the bias inherent in exponential smoothing when the running estimate of a smoothed value is unrealistically initialized to 0.As in the case of RMSProp, let A i be the exponentially averaged value of the ith parameter w i .This value is updated in the same way as RMSProp with the decay parameter ρ ∈ (0, 1):At the same time, an exponentially smoothed value of the gradient is maintained for which the ith component is denoted by F i .This smoothing is performed with a different decay parameter ρ f :This type of exponentially smoothing of the gradient with ρ f is a variation of the momentum method discussed in Section 3.5.2(which is parameterized by a friction parameter β instead of ρ f ).Then, the following update is used at learning rate α t in the tth iteration: