Work in machine learning is now converging from several sources.These different traditions each bring different methods and different vocabulary which are now being assimilated into a more unified discipline.Here is a brief listing of some of the separate disciplines that have contributed to machine learning; more details will follow in the the appropriate chapters:• Statistics: A long-standing problem in statistics is how best to use samples drawn from unknown probability distributions to help decide from which distribution some new sample is drawn.A related problem is how to estimate the value of an unknown function at a new point given the values of this function at a set of sample points.Statistical methods for dealing with these problems can be considered instances of machine learning because the decision and estimation rules depend on a corpus of samples drawn from the problem environment.We will explore some of the statistical methods later in the book.Details about the statistical theory underlying these methods can be found in statistical textbooks such as [Anderson, 1958].• Brain Models: Non-linear elements with weighted inputs have been suggested as simple models of biological neurons.Networks of these elements have been studied by several researchers including [McCulloch & Pitts, 1943, Hebb, 1949, Rosenblatt, 1958 and, more recently by [Gluck & Rumelhart, 1989, Sejnowski, Koch, & Churchland, 1988.Brain modelers are interested in how closely these networks approximate the learning phenomena of living brains.We shall see that several important machine learning techniques are based on networks of nonlinear elements-often called neural networks.Work inspired by this school is sometimes called connectionism, brain-style computation, or sub-symbolic processing.• Adaptive Control Theory: Control theorists study the problem of controlling a process having unknown parameters which must be estimated during operation.Often, the parameters change during operation, and the control process must track these changes.Some aspects of controlling a robot based on sensory inputs represent instances of this sort of problem.For an introduction see [Bollinger & Duffie, 1988].• Psychological Models: Psychologists have studied the performance of humans in various learning tasks.An early example is the EPAM network for storing and retrieving one member of a pair of words when given another [Feigenbaum, 1961].Related work led to a number of early decision tree [Hunt, Marin, & Stone, 1966] and semantic network [Anderson & Bower, 1973] methods.More recent work of this sort has been influenced by activities in artificial intelligence which we will be presenting.Some of the work in reinforcement learning can be traced to efforts to model how reward stimuli influence the learning of goal-seeking behavior in animals [Sutton & Barto, 1987].Reinforcement learning is an important theme in machine learning research.• Artificial Intelligence: From the beginning, AI research has been concerned with machine learning.Samuel developed a prominent early program that learned parameters of a function for evaluating board positions in the game of checkers [Samuel, 1959].AI researchers have also explored the role of analogies in learning [Carbonell, 1983] and how future actions and decisions can be based on previous exemplary cases [Kolodner, 1993].Recent work has been directed at discovering rules for expert systems using decision-tree methods  and inductive logic programming [Muggleton, 1991, Lavrač & Džeroski, 1994.Another theme has been saving and generalizing the results of problem solving using explanation-based learning [DeJong & Mooney, 1986, Minton, 1988, Etzioni, 1993.• Evolutionary Models:In nature, not only do individual animals learn to perform better, but species evolve to be better fit in their individual niches.Since the distinction between evolving and learning can be blurred in computer systems, techniques that model certain aspects of biological evolution have been proposed as learning methods to improve the performance of computer programs.Genetic algorithms [Holland, 1975] and genetic programming [Koza, 1992, Koza, 1994 are the most prominent computational techniques for evolution.Orthogonal to the question of the historical source of any learning technique is the more important question of what is to be learned.In this book, we take it that the thing to be learned is a computational structure of some sort.We will consider a variety of different computational structures:• Functions• Logic programs and rule sets• Finite-state machines• Grammars• Problem solving systemsWe will present methods both for the synthesis of these structures from examples and for changing existing structures.In the latter case, the change to the existing structure might be simply to make it more computationally efficient rather than to increase the coverage of the situations it can handle.Much of the terminology that we shall be using throughout the book is best introduced by discussing the problem of learning functions, and we turn to that matter first.We use Fig. 1.2 to help define some of the terminology used in describing the problem of learning a function.Imagine that there is a function, f , and the task of the learner is to guess what it is.Our hypothesis about the function to be learned is denoted by h.Both f and h are functions of a vector-valued input X = (x 1 , x 2 , . . ., x i , . . ., x n ) which has n components.We think of h as being implemented by a device that has X as input and h(X) as output.Both f and h themselves may be vector-valued.We assume a priori that the hypothesized function, h, is selected from a class of functions H. Sometimes we know that f also belongs to this class or to a subset of this class.We select h based on a training set, Ξ, of m input vector examples.Many important details depend on the nature of the assumptions made about all of these entities.There are two major settings in which we wish to learn a function.In one, called supervised learning, we know (sometimes only approximately) the values of f for the m samples in the training set, Ξ.We assume that if we can find a hypothesis, h, that closely agrees with f for the members of Ξ, then this hypothesis will be a good guess for f -especially if Ξ is large.= {X 1 , X 2 , . . .X i , . .., X m } Training Set:Curve-fitting is a simple example of supervised learning of a function.Suppose we are given the values of a two-dimensional function, f , at the four sample points shown by the solid circles in Fig. 1.3.We want to fit these four points with a function, h, drawn from the set, H, of second-degree functions.We show there a two-dimensional parabolic surface above the x 1 , x 2 plane that fits the points.This parabolic function, h, is our hypothesis about the function, f , that produced the four samples.In this case, h = f at the four samples, but we need not have required exact matches.In the other setting, termed unsupervised learning, we simply have a training set of vectors without function values for them.The problem in this case, typically, is to partition the training set into subsets, Ξ 1 , . . ., Ξ R , in some appropriate way.(We can still regard the problem as one of learning a function; the value of the function is the name of the subset to which an input vector belongs.)Unsupervised learning methods have application in taxonomic problems in which it is desired to invent ways to classify data into meaningful categories.We shall also describe methods that are intermediate between supervised and unsupervised learning.We might either be trying to find a new function, h, or to modify an existing one.An interesting special case is that of changing an existing function into an equivalent one that is computationally more efficient.This type of learning is sometimes called speed-up learning.A very simple example of speed-up learning involves deduction processes.From the formulas A ⊃ B and B ⊃ C, we can deduce C if we are given A. From this deductive process, we can create the formula A ⊃ C-a new formula but one that does not sanction any more con- clusions than those that could be derived from the formulas that we previously had.But with this new formula we can derive C more quickly, given A, than we could have done before.We can contrast speed-up learning with methods that create genuinely new functions-ones that might give different results after learning than they did before.We say that the latter methods involve inductive learning.As opposed to deduction, there are no correct inductions-only useful ones.Because machine learning methods derive from so many different traditions, its terminology is rife with synonyms, and we will be using most of them in this book.For example, the input vector is called by a variety of names.Some of these are: input vector, pattern vector, feature vector, sample, example, and instance.The components, x i , of the input vector are variously called features, attributes, input variables, and components.The values of the components can be of three main types.They might be real-valued numbers, discrete-valued numbers, or categorical values.As an example illustrating categorical values, information about a student might be represented by the values of the attributes class, major, sex, adviser.A particular student would then be represented by a vector such as: (sophomore, history, male, higgins).Additionally, categorical values may be ordered (as in {small, medium, large}) or unordered (as in the example just given).Of course, mixtures of all these types of values are possible.In all cases, it is possible to represent the input in unordered form by listing the names of the attributes together with their values.The vector form assumes that the attributes are ordered and given implicitly by a form.As an example of an attribute-value representation, we might have: (major: history, sex: male, class: sophomore, adviser: higgins, age: 19).We will be using the vector form exclusively.An important specialization uses Boolean values, which can be regarded as a special case of either discrete numbers (1,0) or of categorical variables (True, False).The output may be a real number, in which case the process embodying the function, h, is called a function estimator, and the output is called an output value or estimate.Alternatively, the output may be a categorical value, in which case the process embodying h is variously called a classifier, a recognizer, or a categorizer, and the output itself is called a label, a class, a category, or a decision.Classifiers have application in a number of recognition problems, for example in the recognition of hand-printed characters.The input in that case is some suitable representation of the printed character, and the classifier maps this input into one of, say, 64 categories.Vector-valued outputs are also possible with components being real numbers or categorical values.An important special case is that of Boolean output values.In that case, a training pattern having value 1 is called a positive instance, and a training sample having value 0 is called a negative instance.When the input is also Boolean, the classifier implements a Boolean function.We study the Boolean case in some detail because it allows us to make important general points in a simplified setting.Learning a Boolean function is sometimes called concept learning, and the function is called a concept.There are several ways in which the training set, Ξ, can be used to produce a hypothesized function.In the batch method, the entire training set is available and used all at once to compute the function, h.A variation of this method uses the entire training set to modify a current hypothesis iteratively until an acceptable hypothesis is obtained.By contrast, in the incremental method, we select one member at a time from the training set and use this instance alone to modify a current hypothesis.Then another member of the training set is selected, and so on.The selection method can be random (with replacement) or it can cycle through the training set iteratively.If the entire training set becomes available one member at a time, then we might also use an incremental method-selecting and using training set members as they arrive.(Alternatively, at any stage all training set members so far available could be used in a "batch" process.)Using the training set members as they become available is called an online method.Online methods might be used, for example, when the next training instance is some function of the current hypothesis and the previous instance-as it would be when a classifier is used to decide on a robot's next action given its current set of sensory inputs.The next set of sensory inputs will depend on which action was selected.Sometimes the vectors in the training set are corrupted by noise.There are two kinds of noise.Class noise randomly alters the value of the function; attribute noise randomly alters the values of the components of the input vector.In either case, it would be inappropriate to insist that the hypothesized function agree precisely with the values of the samples in the training set.Even though there is no correct answer in inductive learning, it is important to have methods to evaluate the result of learning.We will discuss this matter in more detail later, but, briefly, in supervised learning the induced function is usually evaluated on a separate set of inputs and function values for them called the testing set .A hypothesized function is said to generalize when it guesses well on the testing set.Both mean-squared-error and the total number of errors are common measures.Long before now the reader has undoubtedly asked why is learning a function possible at all? Certainly, for example, there are an uncountable number of different functions having values that agree with the four samples shown in Fig. 1.3.Why would a learning procedure happen to select the quadratic one shown in that figure?In order to make that selection we had at least to limit a priori the set of hypotheses to quadratic functions and then to insist that the one we chose passed through all four sample points.This kind of a priori information is called bias, and useful learning without bias is impossible.We can gain more insight into the role of bias by considering the special case of learning a Boolean function of n dimensions.There are 2 n different Boolean inputs possible.Suppose we had no bias; that is H is the set of all 2 2 n Boolean functions, and we have no preference among those that fit the samples in the training set.In this case, after being presented with one member of the training set and its value we can rule out precisely one-half of the members of H-those Boolean functions that would misclassify this labeled sample.The remaining functions constitute what is called a "version space;" we'll explore that concept in more detail later.As we present more members of the training set, the graph of the number of hypotheses not yet ruled out as a function of the number of different patterns presented is as shown in But suppose we limited H to some subset, H c , of all Boolean functions.Depending on the subset and on the order of presentation of training patterns, a curve of hypotheses not yet ruled out might look something like the one shown in Fig. 1.5.In this case it is even possible that after seeing fewer than all 2 n labeled samples, there might be only one hypothesis that agrees with the training set.Certainly, even if there is more than one hypothesis remaining, most of them may have the same value for most of the patterns not yet seen!The theory of Probably Approximately Correct (PAC) learning makes this intuitive idea precise.We'll examine that theory later.Let's look at a specific example of how bias aids learning.A Boolean function can be represented by a hypercube each of whose vertices represents a different input pattern.We show a 3-dimensional version in Fig. 1.6.There, we show a training set of six sample patterns and have marked those having a value of 1 by a small square and those having a value of 0 by a small circle.If the hypothesis set consists of just the linearly separable functions-those for which the positive and negative instances can be separated by a linear surface, then there is only one function remaining in this hypothsis set that is consistent with the training set.So, in this case, even though the training set does not contain all possible patterns, we can already pin down what the function must be-given the bias.Machine learning researchers have identified two main varieties of bias, absolute and preference.In absolute bias (also called restricted hypothesis-space bias), one restricts H to a definite subset of functions.In our example of Fig. 1.6, the restriction was to linearly separable Boolean functions.In preference bias, one selects that hypothesis that is minimal according to some ordering scheme over all hypotheses.For example, if we had some way of measuring the complexity of a hypothesis, we might select the one that was simplest among those that performed satisfactorily on the training set.The principle of Occam's razor, used in science to prefer simple explanations to more complex ones, is a type of preference bias.(William of Occam, 1285-?1349, was an English philosopher who said: "non sunt multiplicanda entia praeter necessitatem," which means "entities should not be multiplied unnecessarily.")Our main emphasis in this book is on the concepts of machine learning-not on its applications.Nevertheless, if these concepts were irrelevant to real-world problems they would probably not be of much interest.As motivation, we give a short summary of some areas in which machine learning techniques have been successfully applied.[Langley, 1992] cites some of the following applications and others: a. Rule discovery using a variant of ID3 for a printing industry problem  [Evans & Fisher, 1992].b.Electric power load forecasting using a k-nearest-neighbor rule system [Jabbour, K., et al., 1987].c. Automatic "help desk" assistant using a nearest-neighbor system [Acorn & Walden, 1992].d. Planning and scheduling for a steel mill using ExpertEase, a marketed (ID3-like) system [Michie, 1992].e. Classification of stars and galaxies [Fayyad, et al., 1993].Many application-oriented papers are presented at the annual conferences on Neural Information Processing Systems.Among these are papers on: speech recognition, dolphin echo recognition, image processing, bio-engineering, diagnosis, commodity trading, face recognition, music composition, optical character recognition, and various control applications [Various Editors, 1989.As additional examples, [Hammerstrom, 1993]  In summary, it is rather easy nowadays to find applications of machine learning techniques.This fact should come as no surprise inasmuch as many machine learning techniques can be viewed as extensions of well known statistical methods which have been successfully applied for many years.Besides the rich literature in machine learning (a small part of which is referenced in the Bibliography), there are several textbooks that are worth mentioning [Hertz, Krogh, & Palmer, 1991, Weiss & Kulikowski, 1991, Natarjan, 1991, Fu, 1994, Langley, 1996.[Shavlik & Dietterich, 1990, Buchanan & Wilkins, 1993 are edited volumes containing some of the most important papers.A survey paper by  gives a good overview of many important topics.There are also well established conferences and publications where papers are given and appear including:• The Annual Conferences on Advances in Neural Information Processing Systems• The Annual Workshops on Computational Learning Theory• The Annual International Workshops on Machine Learning• The Annual International Conferences on Genetic Algorithms (The Proceedings of the above-listed four conferences are published by Morgan Kaufmann.)• The journal Machine Learning (published by Kluwer Academic Publishers).There is also much information, as well as programs and datasets, available over the Internet through the World Wide Web.To be added.Every chapter will contain a brief survey of the history of the material covered in that chapter.Chapter 22.1 RepresentationMany important ideas about learning of functions are most easily presented using the special case of Boolean functions.There are several important subclasses of Boolean functions that are used as hypothesis classes for function learning.Therefore, we digress in this chapter to present a review of Boolean functions and their properties.(For a more thorough treatment see, for example, [Unger, 1989].)A Boolean function, f (x 1 , x 2 , . . ., x n ) maps an n-tuple of (0,1) values to {0, 1}.Boolean algebra is a convenient notation for representing Boolean functions.Boolean algebra uses the connectives •, +, and .For example, the and function of two variables is written x 1 • x 2 .By convention, the connective, "•" is usually suppressed, and the and function is written x 1 x 2 .x 1 x 2 has value 1 if and only if both x 1 and x 2 have value 1; if either x 1 or x 2 has value 0, x 1 x 2 has value 0. The (inclusive) or function of two variables is written x 1 + x 2 .x 1 + x 2 has value 1 if and only if either or both of x 1 or x 2 has value 1; if both x 1 and x 2 have value 0, x 1 + x 2 has value 0. The complement or negation of a variable, x, is written x.x has value 1 if and only if x has value 0; if x has value 1, x has value 0.These definitions are compactly given by the following rules for Boolean algebra:Sometimes the arguments and values of Boolean functions are expressed in terms of the constants T (True) and F (False) instead of 1 and 0, respectively.The connectives • and + are each commutative and associative.Thus, for example, x 1 (x 2 x 3 ) = (x 1 x 2 )x 3 , and both can be written simply as x 1 x 2 x 3 .Similarly for +.A Boolean formula consisting of a single variable, such as x 1 is called an atom.One consisting of either a single variable or its complement, such as x 1 , is called a literal.The operators • and + do not commute between themselves.Instead, we have DeMorgan's laws (which can be verified by using the above definitions):x 1 x 2 = x 1 + x 2 , andWe saw in the last chapter that a Boolean function could be represented by labeling the vertices of a cube.For a function of n variables, we would need an n-dimensional hypercube.In Fig. 2.1 we show some 2-and 3-dimensional examples.Vertices having value 1 are labeled with a small square, and vertices having value 0 are labeled with a small circle.x 1x 2x 1x 2x 1x 2 and or xor (exclusive or) Using the hypercube representations, it is easy to see how many Boolean functions of n dimensions there are.A 3-dimensional cube has 2 3 = 8 vertices, and each may be labeled in two different ways; thus there are 2 (2 3 ) = 256 different Boolean functions of 3 variables.In general, there are 2 2 n Boolean functions of n variables.We will be using 2-and 3-dimensional cubes later to provide some intuition about the properties of certain Boolean functions.Of course, we cannot visualize hypercubes (for n > 3), and there are many surprising properties of higher dimensional spaces, so we must be careful in using intuitions gained in low dimensions.One diagrammatic technique for dimensions slightly higher than 3 is the Karnaugh map.A Karnaugh map is an array of values of a Boolean function in which the horizontal rows are indexed by the values of some of the variables and the vertical columns are indexed by the rest.The rows and columns are arranged in such a way that entries that are adjacent in the map correspond to vertices that are adjacent in the hypercube representation.We show an example of the 4-dimensional even parity function in Fig. 2.2.(An even parity function is a Boolean function that has value 1 if there are an even number of its arguments that have value 1; otherwise it has value 0.) Note that all adjacent cells in the table correspond to inputs differing in only one component.Also describe general logic diagrams, [Wnek, et al., 1990].To use absolute bias in machine learning, we limit the class of hypotheses.In learning Boolean functions, we frequently use some of the common sub-classes of those functions.Therefore, it will be important to know about these subclasses.One basic subclass is called terms.A term is any function written in the form l 1 l 2 • • • l k , where the l i are literals.Such a form is called a conjunction of literals.Some example terms are x 1 x 7 and x 1 x 2 x 4 .The size of a term is the number of literals it contains.The examples are of sizes 2 and 3, respectively.(Strictly speaking, the class of conjunctions of literals is called the monomials, and a conjunction of literals itself is called a term.This distinction is a fine one which we elect to blur here.)It is easy to show that there are exactly 3 n possible terms of n variables.The number of terms of size k or less is bounded from above byProbably I'll put in a simple term-learning algorithm here-so we can get started on learning!Also for DNF functions and decision lists-as they are defined in the next few pages.A clause is any function written in the form l 1 + l 2 + • • • + l k , where the l i are literals.Such a form is called a disjunction of literals.Some example clauses are x 3 + x 5 + x 6 and x 1 + x 4 .The size of a clause is the number of literals it contains.There are 3 n possible clauses and fewer than k i=0 C(2n, i) clauses of size k or less.If f is a term, then (by De Morgan's laws) f is a clause, and vice versa.Thus, terms and clauses are duals of each other.In psychological experiments, conjunctions of literals seem easier for humans to learn than disjunctions of literals.A Boolean function is said to be in disjunctive normal form (DNF) if it can be written as a disjunction of terms.Some examples in DNF are:The examples above are 2-term and 3-term expressions, respectively.Both expressions are in the class 3-DNF.Each term in a DNF expression for a function is called an implicant because it "implies" the function (if the term has value 1, so does the function).In general, a term, t, is an implicant of a function, f , if f has value 1 whenever t does.A term, t, is a prime implicant of f if the term, t , formed by taking any literal out of an implicant t is no longer an implicant of f .(The implicant cannot be "divided" by any term and remain an implicant.)Thus, both x 2 x 3 and x 1 x 3 are prime implicants of f = x 2 x 3 +x 1 x 3 +x 2 x 1 x 3 , but x 2 x 1 x 3 is not.The relationship between implicants and prime implicants can be geometrically illustrated using the cube representation for Boolean functions.Consider, for example, the function f = x 2 x 3 + x 1 x 3 + x 2 x 1 x 3 .We illustrate it in Fig. 2.3.Note that each of the three planes in the figure "cuts off" a group of vertices having value 1, but none cuts off any vertices having value 0. These planes are pictorial devices used to isolate certain lower dimensional subfaces of the cube.Two of them isolate one-dimensional edges, and the third isolates a zero-dimensional vertex.Each group of vertices on a subface corresponds to one of the implicants of the function, f , and thus each implicant corresponds to a subface of some dimension.A k-dimensional subface corresponds to an (n − k)-size implicant term.The function is written as the disjunction of the implicants-corresponding to the union of all the vertices cut off by all of the planes.Geometrically, an implicant is prime if and only if its corresponding subface is the largest dimensional subface that includes all of its vertices and no other vertices having value 0. Note that the term x 2 x 1 x 3 is not a prime implicant of f .(In this case, we don't even have to include this term in the function because the vertex cut off by the plane corresponding to x 2 x 1 x 3 is already cut off by the plane corresponding to x 2 x 3 .)The other two implicants are prime because their corresponding subfaces cannot be expanded without including vertices having value 0.x 2x 2 x 3 and x 1 x 3 are prime implicantsNote that all Boolean functions can be represented in DNF-trivially by disjunctions of terms of size n where each term corresponds to one of the vertices whose value is 1.Whereas there are 2 2 n functions of n dimensions in DNF (since any Boolean function can be written in DNF), there are just 2 O(n k ) functions in k-DNF.All Boolean functions can also be represented in DNF in which each term is a prime implicant, but that representation is not unique, as shown in Fig. 2.4.If we can express a function in DNF form, we can use the consensus method to find an expression for the function in which each term is a prime implicant.The consensus method relies on two results:We may replace this section with one describing the Quine-McCluskey method instead.• Consensus:All of the terms are prime implicants, but there is not a unique representationwhere f 1 and f 2 are terms such that no literal appearing inReaders familiar with the resolution rule of inference will note that consensus is the dual of resolution.Examples: x 1 is the consensus of x 1 x 2 and x 1 x 2 .The terms x 1 x 2 and x 1 x 2 have no consensus since each term has more than one literal appearing complemented in the other.• Subsumption:where f 1 is a term.We say that f 1 subsumesThe consensus method for finding a set of prime implicants for a function, f , iterates the following operations on the terms of a DNF expression for f until no more such operations can be applied: a. initialize the process with the set, T , of terms in the DNF expression of f , b. compute the consensus of a pair of terms in T and add the result to T , c. eliminate any terms in T that are subsumed by other terms in T .When this process halts, the terms remaining in T are all prime implicants of f .Example:We show a derivation of a set of prime implicants in the consensus tree of Fig. 2.5.The circled numbers adjoining the terms indicate the order in which the consensus and subsumption operations were performed.Shaded boxes surrounding a term indicate that it was subsumed.The final form of the function in which all terms are prime implicants is: f = x 1 x 2 + x 1 x 3 + x 1 x 4 x 5 .Its terms are all of the non-subsumed terms in the consensus tree.x 1 x 2x 1 x 2 x 3 x 1 x 2 x 3 x 4 x 5x 1 x 3x 1 x 2 x 4 x 5x 1 x 4 x 5 f = x 1 x 2 + + x 1 x 3x 1 x 4 x 5Disjunctive normal form has a dual: conjunctive normal form (CNF).A Boolean function is said to be in CNF if it can be written as a conjunction of clauses.An example in CNF is:The example is a 2-clause expression in 3-CNF.If f is written in DNF, an application of De Morgan's law renders f in CNF, and vice versa.Because CNF and DNF are duals, there are also 2 O(n k ) functions in k-CNF.Rivest has proposed a class of Boolean functions called decision lists [Rivest, 1987].A decision list is written as an ordered list of pairs:where the v i are either 0 or 1, the t i are terms in (x 1 , . . ., x n ), and T is a term whose value is 1 (regardless of the values of the x i ).The value of a decision list is the value of v i for the first t i in the list that has value 1. (At least one t i will have value 1, because the last one does; v 1 can be regarded as a default value of the decision list.)The decision list is of size k, if the size of the largest term in it is k.The class of decision lists of size k or less is called k-DL.An example decision list is:f has value 0 for x 1 = 0, x 2 = 0, and x 3 = 1.It has value 1 for x 1 = 1, x 2 = 0, and x 3 = 1.This function is in 3-DL.It has been shown that the class k-DL is a strict superset of the union of k-DNF and k-CNF.There are 2 O[n k k log(n)] functions in k-DL [Rivest, 1987].Interesting generalizations of decision lists use other Boolean functions in place of the terms, t i .For example we might use linearly separable functions in place of the t i (see below and [Marchand & Golea, 1993]).A Boolean function is called symmetric if it is invariant under permutations of the input variables.For example, any function that is dependent only on the number of input variables whose values are 1 is a symmetric function.The parity functions, which have value 1 depending on whether or not the number of input variables with value 1 is even or odd is a symmetric function.(The exclusive or function, illustrated in Fig. 2.1, is an odd-parity function of two dimensions.The or and and functions of two dimensions are also symmetric.)An important subclass of the symmetric functions is the class of voting functions (also called m-of-n functions).A k-voting function has value 1 if and only if k or more of its n inputs has value 1.If k = 1, a voting function is the same as an n-sized clause; if k = n, a voting function is the same as an n-sized term; if k = (n + 1)/2 for n odd or k = 1 + n/2 for n even, we have the majority function.The linearly separable functions are those that can be expressed as follows:where w i , i = 1, . . ., n, are real-valued numbers called weights, θ is a real-valued number called the threshold, and thresh(σ, θ) is 1 if σ ≥ θ and 0 otherwise.(Note that the concept of linearly separable functions can be extended to non-Boolean inputs.)The k-voting functions are all members of the class of linearly separable functions in which the weights all have unit value and the threshold depends on k.Thus, terms and clauses are special cases of linearly separable functions.A convenient way to write linearly separable functions uses vector notation:where X = (x 1 , . . ., x n ) is an n-dimensional vector of input variables, W = (w 1 , . . ., w n ) is an n-dimensional vector of weight values, and X • W is the dot (or inner) product of the two vectors.Input vectors for which f has value 1 lie in a half-space on one side of (and on) a hyperplane whose orientation is normal to W and whose position (with respect to the origin) is determined by θ.We saw an example of such a separating plane in Fig. 1.6.With this idea in mind, it is easy to see that two of the functions in Fig. 2.1 are linearly separable, while two are not.Also note that the terms in Figs.15,028,134 [Muroga, 1971] has shown that (for n > 1) there are no more than 2 n 2 linearly separable functions of n dimensions.(See also [Winder, 1961, Winder, 1962].)The diagram in Fig. 2.6 shows some of the set inclusions of the classes of Boolean functions that we have considered.We will be confronting these classes again in later chapters.The sizes of the various classes are given in the following table (adapted from [Dietterich, 1990, page 262]):Size of Class termsTo be added.Using Version Spaces for LearningThe first learning methods we present are based on the concepts of version spaces and version graphs.These ideas are most clearly explained for the case of Boolean function learning.Given an initial hypothesis set H (a subset of all Boolean functions) and the values of f (X) for each X in a training set, Ξ, the version space is that subset of hypotheses, H v , that is consistent with these values.A hypothesis, h, is consistent with the values of X in Ξ if and only if h(X) = f (X) for all X in Ξ.We say that the hypotheses in H that are not consistent with the values in the training set are ruled out by the training set.We could imagine (conceptually only!) that we have devices for implementing every function in H.An incremental training procedure could then be defined which presented each pattern in Ξ to each of these functions and then eliminated those functions whose values for that pattern did not agree with its given value.At any stage of the process we would then have left some subset of functions that are consistent with the patterns presented so far; this subset is the version space for the patterns already presented.This idea is illustrated in Fig. 3.1.Consider the following procedure for classifying an arbitrary input pattern, X: the pattern is put in the same class (0 or 1) as are the majority of the outputs of the functions in the version space.During the learning procedure, if this majority is not equal to the value of the pattern presented, we say a mistake is made, and we revise the version space accordingly-eliminating all those (majority of the) functions voting incorrectly.Thus, whenever a mistake is made, we rule out at least half of the functions remaining in the version space.How many mistakes can such a procedure make?Obviously, we can make no more than log 2 (|H|) mistakes, where |H| is the number of hypotheses in the This theoretical (and very impractical!)result (due to [Littlestone, 1988]) is an example of a mistake bound-an important concept in machine learning theory.It shows that there must exist a learning procedure that makes no more mistakes than this upper bound.Later, we'll derive other mistake bounds.As a special case, if our bias was to limit H to terms, we would make no more than log 2 (3 n ) = n log 2 (3) = 1.585n mistakes before exhausting the version space.This result means that if f were a term, we would make no more than 1.585n mistakes before learning f , and otherwise we would make no more than that number of mistakes before being able to decide that f is not a term.Even if we do not have sufficient training patterns to reduce the version space to a single function, it may be that there are enough training patterns to reduce the version space to a set of functions such that most of them assign the same values to most of the patterns we will see henceforth.We could select one of the remaining functions at random and be reasonably assured that it will generalize satisfactorily.We next discuss a computationally more feasible method for representing the version space.Boolean functions can be ordered by generality.A Boolean function, f 1 , is more general than a function, f 2 , (and f 2 is more specific than f 1 ), if f 1 has value 1 for all of the arguments for which f 2 has value 1, and f 1 = f 2 .For example, x 3 is more general than x 2 x 3 but is not more general than x 3 + x 2 .We can form a graph with the hypotheses, {h i }, in the version space as nodes.A node in the graph, h i , has an arc directed to node, h j , if and only if h j is more general than h i .We call such a graph a version graph.In Fig. 3.2, we show an example of a version graph over a 3-dimensional input space for hypotheses restricted to terms (with none of them yet ruled out).Version Graph for Termsx 1x 2x 3(for simplicity, only some arcs in the graph are shown)(none yet ruled out)x 1 x 3That function, denoted here by "1," which has value 1 for all inputs, corresponds to the node at the top of the graph.(It is more general than any other term.)Similarly, the function "0" is at the bottom of the graph.Just below "1" is a row of nodes corresponding to all terms having just one literal, and just below them is a row of nodes corresponding to terms having two literals, and so on.There are 3 3 = 27 functions altogether (the function "0," included in the graph, is technically not a term).To make our portrayal of the graph less cluttered only some of the arcs are shown; each node in the actual graph has an arc directed to all of the nodes above it that are more general.We use this same example to show how the version graph changes as we consider a set of labeled samples in a training set, Ξ. Suppose we first consider the training pattern (1, 0, 1) with value 0. Some of the functions in the version graph of Fig. 3.2 are inconsistent with this training pattern.These ruled out nodes are no longer in the version graph and are shown shaded in Fig. 3.3.We also show there the three-dimensional cube representation in which the vertex (1, 0, 1) has value 0.New Version Graph 1, 0, 1 has value 0x 1 x 3x 1 x 2 x 2 x 3x 1 x 2 x 3x 1x 2x 3x 1 x 3 (only some arcs in the graph are shown) ruled out nodes In a version graph, there are always a set of hypotheses that are maximally general and a set of hypotheses that are maximally specific.These are called the general boundary set (gbs) and the specific boundary set (sbs), respectively.In Fig. 3.4, we have the version graph as it exists after learning that (1,0,1) has value 0 and (1, 0, 0) has value 1.The gbs and sbs are shown.x 3x 2 x 3 1x 1 x 2 x 3x 1x 2 x 3 x 1 x 3 general boundary set (gbs) specific boundary set (sbs)x 1 x 2 more specific than gbs, more general than sbs 1, 0, 1 has value 0x 1 x 2x 3 1, 0, 0 has value 1Figure 3.4: The Version Graph Upon Seeing (1, 0, 1) and (1, 0, 0) Boundary sets are important because they provide an alternative to representing the entire version space explicitly, which would be impractical.Given only the boundary sets, it is possible to determine whether or not any hypothesis (in the prescribed class of Boolean functions we are using) is a member or not of the version space.This determination is possible because of the fact that any member of the version space (that is not a member of one of the boundary sets) is more specific than some member of the general boundary set and is more general than some member of the specific boundary set.If we limit our Boolean functions that can be in the version space to terms, it is a simple matter to determine maximally general and maximally specific functions (assuming that there is some term that is in the version space).A maximally specific one corresponds to a subface of minimal dimension that contains all the members of the training set labelled by a 1 and no members labelled by a 0. A maximally general one corresponds to a subface of maximal dimension that contains all the members of the training set labelled by a 1 and no members labelled by a 0. Looking at Fig. 3.4, we see that the subface of minimal dimension that contains (1, 0, 0) but does not contain (1, 0, 1) is just the vertex (1, 0, 0) itself-corresponding to the function x 1 x 2 x 3 .The subface of maximal dimension that contains (1, 0, 0) but does not contain (1, 0, 1) is the bottom face of the cube-corresponding to the function x 3 .In Figs.3.2 through 3.4 the sbs is always singular.Version spaces for terms always have singular specific boundary sets.As seen in Fig. 3.3, however, the gbs of a version space for terms need not be singular.[To be written.Relate to term learning algorithm presented in Chapter Two.Also discuss best-first search methods.See Pat Langley's example using "pseudo-cells" of how to generate and eliminate hypotheses.]Selecting a hypothesis from the version space can be thought of as a search problem.One can start with a very general function and specialize it through various specialization operators until one finds a function that is consistent (or adequately so) with a set of training patterns.Such procedures are usually called top-down methods.Or, one can start with a very special function and generalize it-resulting in bottom-up methods.We shall see instances of both styles of learning in this book.Compare this view of top-down versus bottom-up with the divide-and-conquer and the covering (or AQ) methods of decision-tree induction.The candidate elimination method, is an incremental method for computing the boundary sets.Quoting from [Hirsh, 1994, page 6]: "The candidate-elimination algorithm manipulates the boundary-set representation of a version space to create boundary sets that represent a new version space consistent with all the previous instances plus the new one.For a positive exmple the algorithm generalizes the elements of the [sbs] as little as possible so that they cover the new instance yet remain consistent with past data, and removes those elements of the [gbs] that do not cover the new instance.For a negative instance the algorithm specializes elements of the [gbs] so that they no longer cover the new instance yet remain consistent with past data, and removes from the [sbs] those elements that mistakenly cover the new, negative instance."The method uses the following definitions (adapted from [Genesereth & Nilsson, 1987]):• a hypothesis is called sufficient if and only if it has value 1 for all training samples labeled by a 1,• a hypothesis is called necessary if and only if it has value 0 for all training samples labeled by a 0.Here is how to think about these definitions: A hypothesis implements a sufficient condition that a training sample has value 1 if the hypothesis has value 1 for all of the positive instances; a hypothesis implements a necessary condition that a training sample has value 1 if the hypothesis has value 0 for all of the negative instances.A hypothesis is consistent with the training set (and thus is in the version space) if and only if it is both sufficient and necessary.We start (before receiving any members of the training set) with the function "0" as the singleton element of the specific boundary set and with the function "1" as the singleton element of the general boundary set.Upon receiving a new labeled input vector, the boundary sets are changed as follows:a.If the new vector is labelled with a 1:The new general boundary set is obtained from the previous one by excluding any elements in it that are not sufficient.(That is, we exclude any elements that have value 0 for the new vector.)The new specific boundary set is obtained from the previous one by replacing each element, h i , in it by all of its least generalizations.The hypothesis h g is a least generalization of h if and only if: a) h is more specific than h g , b) h g is sufficient, c) no function (including h) that is more specific than h g is sufficient, and d) h g is more specific than some member of the new general boundary set.It might be that h g = h.Also, least generalizations of two different functions in the specific boundary set may be identical.b.If the new vector is labelled with a 0:The new specific boundary set is obtained from the previous one by excluding any elements in it that are not necessary.(That is, we exclude any elements that have value 1 for the new vector.)The new general boundary set is obtained from the previous one by replacing each element, h i , in it by all of its least specializations.The hypothesis h s is a least specialization of h if and only if: a) h is more general than h s , b) h s is necessary, c) no function (including h) that is more general than h s is necessary, and d) h s is more general than some member of the new specific boundary set.Again, it might be that h s = h, and least specializations of two different functions in the general boundary set may be identical.As an example, suppose we present the vectors in the following order: vector label (1, 0, 1) 0 (1, 0, 0) 1 (1, 1, 1) 0 (0, 0, 1) 0We start with general boundary set, "1", and specific boundary set, "0."After seeing the first sample, (1, 0, 1), labeled with a 0, the specific boundary set stays at "0" (it is necessary), and we change the general boundary set to {x 1 , x 2 , x 3 }.Each of the functions, x 1 , x 2 , and x 3 , are least specializations of "1" (they are necessary, "1" is not, they are more general than "0", and there are no functions that are more general than they and also necessary).Then, after seeing (1, 0, 0), labeled with a 1, the general boundary set changes to {x 3 } (because x 1 and x 2 are not sufficient), and the specific boundary set is changed to {x 1 x 2 x 3 }.This single function is a least generalization of "0" (it is sufficient, "0" is more specific than it, no function (including "0") that is more specific than it is sufficient, and it is more specific than some member of the general boundary set.When we see (1, 1, 1), labeled with a 0, we do not change the specific boundary set because its function is still necessary.We do not change the general boundary set either because x 3 is still necessary.Finally, when we see (0, 0, 1), labeled with a 0, we do not change the specific boundary set because its function is still necessary.We do not change the general boundary set either because x 3 is still necessary.Maybe I'll put in an example of a version graph for non-Boolean functions.The concept of version spaces and their role in learning was first investigated by Tom Mitchell [Mitchell, 1982].Although these ideas are not used in practical machine learning procedures, they do provide insight into the nature of hypothesis selection.In order to accomodate noisy data, version spaces have been generalized by [Hirsh, 1994] to allow hypotheses that are not necessarily consistent with the training set.More to be added.In chapter two we defined several important subsets of Boolean functions.Suppose we decide to use one of these subsets as a hypothesis set for supervised function learning.We next have the question of how best to implement the function as a device that gives the outputs prescribed by the function for arbitrary inputs.In this chapter we describe how networks of non-linear elements can be used to implement various input-output functions and how they can be trained using supervised learning methods.Networks of non-linear elements, interconnected through adjustable weights, play a prominent role in machine learning.They are called neural networks because the non-linear elements have as their inputs a weighted sum of the outputs of other elements-much like networks of biological neurons do.These networks commonly use the threshold element which we encountered in chapter two in our study of linearly separable Boolean functions.We begin our treatment of neural nets by studying this threshold element and how it can be used in the simplest of all networks, namely ones composed of a single threshold element.Linearly separable (threshold) functions are implemented in a straightforward way by summing the weighted inputs and comparing this sum to a threshold value as shown in Fig. 4.1.This structure we call a threshold logic unit (TLU).Its output is 1 or 0 depending on whether or not the weighted sum of its inputs is greater than or equal to a threshold value, θ.It has also been called an Adaline (for adaptive linear element) [Widrow, 1962, Widrow & Lehr, 1990, an LTU (linear threshold unit), a perceptron, and a neuron.(Although the word "perceptron" is often used nowadays to refer to a single TLU, Rosenblatt originally defined it as a class of networks of threshold elements [Rosenblatt, 1958].)The n-dimensional feature or input vector is denoted by X = (x 1 , . . ., x n ).When we want to distinguish among different feature vectors, we will attach subscripts, such as X i .The components of X can be any real-valued numbers, but we often specialize to the binary numbers 0 and 1.The weights of a TLU are represented by an n-dimensional weight vector, W = (w 1 , . . ., w n ).Its components are real-valued numbers (but we sometimes specialize to integers).The TLU has output 1 if n i=1 x i w i ≥ θ; otherwise it has output 0. The weighted sum that is calculated by the TLU can be simply represented as a vector dot product, X•W.(If the pattern and weight vectors are thought of as "column" vectors, this dot product is then sometimes written as X t W, where the "row" vector X t is the transpose of X.) Often, the threshold, θ, of the TLU is fixed at 0; in that case, arbitrary thresholds are achieved by using (n + 1)dimensional "augmented" vectors, Y, and V, whose first n components are the same as those of X and W, respectively.The (n + 1)-st component, x n+1 , of the augmented feature vector, Y, always has value 1; the (n + 1)-st component, w n+1 , of the augmented weight vector, V, is set equal to the negative of the desired threshold value.(When we want to emphasize the use of augmented vectors, we'll use the Y,V notation; however when the context of the discussion makes it clear about what sort of vectors we are talking about, we'll lapse back into the more familiar X,W notation.)In the Y,V notation, the TLU has an output of 1 if Y•V ≥ 0. Otherwise, the output is 0.We can give an intuitively useful geometric description of a TLU.A TLU divides the input space by a hyperplane as sketched in Fig. 4.2.The hyperplane is the boundary between patterns for which X•W + w n+1 > 0 and patterns for which X•W + w n+1 < 0. Thus, the equation of the hyperplane itself is X•W + w n+1 = 0.The unit vector that is normal to the hyperplane is n = W |W| , where |W| = (w 2 1 + . . .+ w 2 n ) is the length of the vector W. (The normal form of the hyperplane equation is X•n + W |W| = 0.) The distance from the hyperplane to the origin is wn+1|W| , and the distance from an arbitrary point, X, to the hyperplane is X•W+wn+1. When the distance from the hyperplane to the origin is negative (that is, when w n+1 < 0), then the origin is on the negative side of the hyperplane (that is, the side for which X•W + w n+1 < 0).Adjusting the weight vector, W, changes the orientation of the hyperplane; adjusting w n+1 changes the position of the hyperplane (relative to the origin).Thus, training of a TLU can be achieved by adjusting the values of the weights.In this way the hyperplane can be moved so that the TLU implements different (linearly separable) functions of the input.Any term of size k can be implemented by a TLU with a weight from each of those inputs corresponding to variables occurring in the term.A weight of +1 is used from an input corresponding to a positive literal, and a weight of −1 is used from an input corresponding to a negative literal.(Literals not mentioned in the term have weights of zero-that is, no connection at all-from their inputs.)The threshold, θ, is set equal to k p − 1/2, where k p is the number of positive literals in the term.Such a TLU implements a hyperplane boundary that is parallel to a subface of dimension (n − k) of the unit hypercube.We show a three-dimensional example in Fig. 4.3.Thus, linearly separable functions are a superset of terms.(1,1,1)Equation of plane is:The negation of a clause is a term.For example, the negation of the clause f = x 1 + x 2 + x 3 is the term f = x 1 x 2 x 3 .A hyperplane can be used to implement this term.If we "invert" the hyperplane, it will implement the clause instead.Inverting a hyperplane is done by multiplying all of the TLU weights-even w n+1 -by −1.This process simply changes the orientation of the hyperplane-flipping it around by 180 degrees and thus changing its "positive side."Therefore, linearly separable functions are also a superset of clauses.We show an example in Fig. 4.4.There are several procedures that have been proposed for adjusting the weights of a TLU.We present next a family of incremental training procedures with parameter c.These methods make adjustments to the weight vector only when the TLU being trained makes an error on a training pattern; they are called error-correction procedures.We use augmented feature and weight vectors in describing them.a.We start with a finite training set, Ξ, of vectors, Y i , and their binary labels.f = x 1 + x 2 + x 3x 1Equation of plane is:x 2x 3 c.Repeat forever:Present the next vector, Y i , in Σ to the TLU and note its response.(a) If the TLU responds correctly, make no change in the weight vector.(b) If Y i is supposed to produce an output of 0 and produces an output of 1 instead, modify the weight vector as follows:where c i is a positive real number called the learning rate parameter (whose value is differerent in different instances of this family of procedures and may depend on i).Note that after this adjustment the new dot product will be (which is smaller than it was before the weight adjustment.(c) If Y i is supposed to produce an output of 1 and produces an output of 0 instead, modify the weight vector as follows:In this case, the new dot product will be (which is larger than it was before the weight adjustment.Note that all three of these cases can be combined in the following rule:where d i is the desired response (1 or 0) for Y i , and f i is the actual response (1 or 0) for Y i .]Note also that because the weight vector V now includes the w n+1 threshold component, the threshold of the TLU is also changed by these adjustments.We identify two versions of this procedure: 1) In the fixed-increment procedure, the learning rate parameter, c i , is the same fixed, positive constant for all i.Depending on the value of this constant, the weight adjustment may or may not correct the response to an erroneously classified feature vector.2) In the fractional-correction procedure, the parameter c i is set to, where V is the weight vector before it is changed.Note that if λ = 0, no correction takes place at all.If λ = 1, the correction is just sufficient to make Y i •V = 0.If λ > 1, the error will be corrected.It can be proved that if there is some weight vector, V, that produces a correct output for all of the feature vectors in Ξ, then after a finite number of feature vector presentations, the fixed-increment procedure will find such a weight vector and thus make no more weight changes.The same result holds for the fractional-correction procedure if 1 < λ ≤ 2.For additional background, proofs, and examples of error-correction procedures, see .See [Maass & Turán, 1994] for a hyperplane-finding procedure that makes no more than O(n 2 log n) mistakes.We can give an intuitive idea about how these procedures work by considering what happens to the augmented weight vector in "weight space" as corrections are made.We use augmented vectors in our discussion here so that the threshold function compares the dot product, Y i •V, against a threshold of 0. A particular weight vector, V, then corresponds to a point in (n + 1)-dimensional weight space.Now, for any pattern vector, Y i , consider the locus of all points in weight space corresponding to weight vectors yielding Y i •V = 0.This locus is a hyperplane passing through the origin of the (n + 1)-dimensional space.Each pattern vector will have such a hyperplane corresponding to it.Weight points in one of the half-spaces defined by this hyperplane will cause the corresponding pattern to yield a dot product less than 0, and weight points in the other halfspace will cause the corresponding pattern to yield a dot product greater than 0.We show a schematic representation of such a weight space in Fig.The question of whether or not there exists a weight vector that gives desired responses for a given set of patterns can be given a geometric interpretation.To do so involves reversing the "polarity" of those hyperplanes corresponding to patterns for which a negative response is desired.If we do that for our example above, we get the weight space diagram shown in Fig. 4.6.If a weight vector exists that correctly classifies a set of patterns, then the half-spaces defined by the correct responses for these patterns will have a nonempty intersection, called the solution region.The solution region will be a "hyper-wedge" region whose vertex is at the origin of weight space and whose cross-section increases with increasing distance from the origin.This region is shown shaded in Fig. 4.6.(The boxed numbers show, for later purposes, the number of errors made by weight vectors in each of the regions.)The fixed-increment error-correction procedure changes a weight vector by moving it normal to any pattern hyperplane for which that weight vector gives an incorrect response.Suppose in our example that we present the patterns in the sequence Y 1 , Y 2 , Y 3 , Y 4 , and start the process with a weight point V 1 , as shown in Fig. 4.7.Starting at V 1 , we see that it gives an incorrect response for pattern Y 1 , so we move) Y 2 gives an incorrect response for pattern Y 2 , and so on.Ultimately, the responses are only incorrect for planes bounding the solution region.Some of the subsequent corrections may overshoot the solution region, but eventually we work our way out far enough in the solution region that corrections (for a fixed increment size) take us within it.The proofs for convergence of the fixed-increment rule make this intuitive argument precise.The Widrow-Hoff procedure (also called the LMS or the delta procedure) attempts to find weights that minimize a squared-error function between the pattern labels and the dot product computed by a TLU.For this purpose, the pattern labels are assumed to be either +1 or −1 (instead of 1 or 0).The squared error for a pattern, X i , with label d i (for desired output) is:where x ij is the j-th component of X i .The total squared error (over all patterns in a training set, Ξ, containing m patterns) is then:We want to choose the weights w j to minimize this squared error.One way to find such a set of weights is to start with an arbitrary weight vector and move it along the negative gradient of ε as a function of the weights.Since ε is quadratic in the w j , we know that it has a global minimum, and thus this steepest descent procedure is guaranteed to find the minimum.Each component of the gradient is the partial derivative of ε with respect to one of the weights.One problem with taking the partial derivative of ε is that ε depends on all the input vectors in Ξ. Often, it is preferable to use an incremental procedure in which we try the TLU on just one element, X i , of Ξ at a time, compute the gradient of the singlepattern squared error, ε i , make the appropriate adjustment to the weights, and then try another member of Ξ.Of course, the results of the incremental version can only approximate those of the batch one, but the approximation is usually quite effective.We will be describing the incremental version here.The j-th component of the gradient of the single-pattern error is:An adjustment in the direction of the negative gradient would then change each weight as follows:x ij w j , and c i governs the size of the adjustment.The entire weight vector (in augmented, or V, notation) is thus adjusted according to the following rule:where, as before, Y i is the i-th augmented pattern vector.The Widrow-Hoff procedure makes adjustments to the weight vector whenever the dot product itself, Y i •V, does not equal the specified desired target value, d i (which is either 1 or −1).The learning-rate factor, c i , might decrease with time toward 0 to achieve asymptotic convergence.The Widrow-Hoff formula for changing the weight vector has the same form as the standard fixed-increment error-correction formula.The only difference is that f i is the thresholded response of the TLU in the error-correction case while it is the dot product itself for the Widrow-Hoff procedure.Finding weight values that give the desired dot products corresponds to solving a set of linear equalities, and the Widrow-Hoff procedure can be interpreted as a descent procedure that attempts to minimize the mean-squared-error between the actual and desired values of the dot product.(For more on Widrow-Hoff and other related procedures, see [Duda & Hart, 1973, pp. 151ff].)Examples of training curves for TLU's; performance on training set; performance on test set; cumulative number of corrections.When the training set is not linearly separable (perhaps because of noise or perhaps inherently), it may still be desired to find a "best" separating hyperplane.Typically, the error-correction procedures will not do well on nonlinearly-separable training sets because they will continue to attempt to correct inevitable errors, and the hyperplane will never settle into an acceptable place.Several methods have been proposed to deal with this case.First, we might use the Widrow-Hoff procedure, which (although it will not converge to zero error on non-linearly separable problems) will give us a weight vector that minimizes the mean-squared-error.A mean-squared-error criterion often gives unsatisfactory results, however, because it prefers many small errors to a few large ones.As an alternative, error correction with a continuous decrease toward zero of the value of the learning rate constant, c, will result in ever decreasing changes to the hyperplane.Duda [Duda, 1966] has suggested keeping track of the average value of the weight vector during error correction and using this average to give a separating hyperplane that performs reasonably well on non-linearly-separable problems.Gallant [Gallant, 1986] proposed what he called the "pocket algorithm."As described in [Hertz, Krogh, & Palmer, 1991, p. 160 ]: . . . the pocket algorithm . . .consists simply in storing (or "putting in your pocket") the set of weights which has had the longest unmodified run of successes so far.The algorithm is stopped after some chosen time t . . .After stopping, the weights in the pocket are used as a set that should give a small number of errors on the training set.Error-correction proceeds as usual with the ordinary set of weights.Also see methods proposed by [John, 1995] and by [Marchand & Golea, 1993].The latter is claimed to outperform the pocket algorithm.The natural generalization of a (two-category) TLU to an R-category classifier is the structure, shown in Fig. 4.8, called a linear machine.Here, to use more familiar notation, the Ws and X are meant to be augmented vectors (with an (n+1)-st component).Such a structure is also sometimes called a "competitive" net or a "winner-take-all" net.The output of the linear machine is one of the numbers, {1, . . ., R}, corresponding to which dot product is largest.Note that when R = 2, the linear machine reduces to a TLU with weight vectorThe diagram in Fig. 4.9 shows the character of the regions in a 2-dimensional space created by a linear machine for R = 5.In n dimensions, every pair of regions is either separated by a section of a hyperplane or is non-adjacent.In this region: b.If the machine mistakenly classifies a category u pattern, X i , in category v (u = v), then:and all other weight vectors are not changed.This correction increases the value of the u-th dot product and decreases the value of the v-th dot product.Just as in the 2-category fixed increment procedure, this procedure is guaranteed to terminate, for constant c i , if there exists weight vectors that make correct separations of the training set.Note that when R = 2, this procedure reduces to the ordinary TLU error-correction procedure.A proof that this procedure terminates is given in [Nilsson, 1990, pp. 88-90] and in [Duda & Hart, 1973, pp. 174-177].To classify correctly all of the patterns in non-linearly-separable training sets requires separating surfaces more complex than hyperplanes.One way to achieve more complex surfaces is with networks of TLUs.Consider, for example, the 2dimensional, even parity function, f = x 1 x 2 + x 1 x 2 .No single line through the 2-dimensional square can separate the vertices (1,1) and (0,0) from the vertices (1,0) and (0,1)-the function is not linearly separable and thus cannot be implemented by a single TLU.But, the network of three TLUs shown in Fig. 4.10 does implement this function.In the figure, we show the weight values along input lines to each TLU and the threshold value inside the circle representing the TLU.The function implemented by a network of TLUs depends on its topology as well as on the weights of the individual TLUs.Feedforward networks have no cycles; in a feedforward network no TLU's input depends (through zero or more intermediate TLUs) on that TLU's output.(Networks that are not feedforward are called recurrent networks).If the TLUs of a feedforward network are arranged in layers, with the elements of layer j receiving inputs only from TLUs in layer j − 1, then we say that the network is a layered, feedforward  To train a two-layer network that implements a k-term DNF function, we first note that the output unit implements a disjunction, so the weights in the final layer are fixed.The weights in the first layer (except for the "threshold weights") can all have values of 1, −1, or 0. Later, we will present a training procedure for this first layer of weights.Discuss half-space intersections, half-space unions, NP-hardness of optimal versions, single-side-error-hypeplane methods, relation to "AQ" methods.Adding additional layers cannot compensate for an inadequate first layer of TLUs.The first layer of TLUs partitions the feature space so that no two differently labeled vectors are in the same region (that is, so that no two such vectors yield the same set of outputs of the first-layer units).If the first layer does not partition the feature space in this way, then regardless of what subsequent layers do, the final outputs will not be consistent with the labeled training set.Add diagrams showing the non-linear transformation performed by a layered network.An interesting example of a layered, feedforward network is the two-layer one which has an odd number of hidden units, and a "vote-taking" TLU as the output unit.Such a network was called a "Madaline" (for many adalines by Widrow.Typically, the response of the vote taking unit is defined to be the response of the majority of the hidden units, although other output logics are possible.Ridgway [Ridgway, 1962] proposed the following error-correction rule for adjusting the weights of the hidden units of a Madaline:• If the Madaline correctly classifies a pattern, X i , no corrections are made to any of the hidden units' weight vectors,• If the Madaline incorrectly classifies a pattern, X i , then determine the minimum number of hidden units whose responses need to be changed (from 0 to 1 or from 1 to 0-depending on the type of error) in order that the Madaline would correctly classify X i .Suppose that minimum number is k i .Of those hidden units voting incorrectly, change the weight vectors of those k i of them whose dot products are closest to 0 by using the error correction rule:where d i is the desired response of the hidden unit (0 or 1) and f i is the actual response (0 or 1).(We assume augmented vectors here even though we are using X, W notation.)That is, we perform error-correction on just enough hidden units to correct the vote to a majority voting correctly, and we change those that are easiest to change.There are example problems in which even though a set of weight values exists for a given Madaline structure such that it could classify all members of a training set correctly, this procedure will fail to find them.Nevertheless, the procedure works effectively in most experiments with it.We leave it to the reader to think about how this training procedure could be modified if the output TLU implemented an or function (or an and function).If there are k hidden units (k > 1) in a two-layer network, their responses correspond to vertices of a k-dimensional hypercube.The ordinary two-category Madaline identifies two special points in this space, namely the vertex consisting of k 1's and the vertex consisting of k 0's.The Madaline's response is 1 if the point in "hidden-unit-space" is closer to the all 1's vertex than it is to the all 0's vertex.We could design an R-category Madaline by identifying R vertices in hidden-unit space and then classifying a pattern according to which of these vertices the hidden-unit response is closest to.A machine using that idea was implemented in the early 1960s at SRI [Brain, et al., 1962].It used the fact that the 2 p so-called maximal-length shift-register sequences [Peterson, 1961, pp. 147ff] in a (2 p − 1)-dimensional Boolean space are mutually equidistant (for any integer p).For similar, more recent work see [Dietterich & Bakiri, 1991].A two-category training set is linearly separable if there exists a threshold function that correctly classifies all members of the training set.Similarly, we can say that an R-category training set is linearly separable if there exists a linear machine that correctly classifies all members of the training set.When an Rcategory problem is not linearly separable, we need a more powerful classifier.A candidate is a structure called a piecewise linear (PWL) machine illustrated in Fig. 4.14.The PWL machine groups its weighted summing units into R banks corresponding to the R categories.An input vector X is assigned to that category corresponding to the bank with the largest weighted sum.We can use an errorcorrection training algorithm similar to that used for a linear machine.If a pattern is classified incorrectly, we subtract (a constant times) the pattern vector from the weight vector producing the largest dot product (it was incorrectly the largest) and add (a constant times) the pattern vector to that weight vector in the correct bank of weight vectors whose dot product is locally largest in that bank.(Again, we use augmented vectors here.)Unfortunately, there are example training sets that are separable by a given PWL machine structure but for which this error-correction training method fails to find a solution.The method does appear to work well in some situations [Duda & Fossum, 1966], although [Nilsson, 1965, page 89] observed that "it is probably not a very effective method for training PWL machines having more than three [weight vectors] in each bank."Another interesting class of feedforward networks is that in which all of the TLUs are ordered and each TLU receives inputs from all of the pattern components and from all TLUs lower in the ordering.Such a network is called a cascade network.An example is shown in Fig. 4.15 in which the TLUs are labeled by the linearly separable functions (of their inputs) that they implement.Each TLU in the network implements a set of 2 k parallel hyperplanes, where k is the number of TLUs from which it receives inputs.(Each of the k preceding TLUs can have an output of 1 or 0; that's 2 k different combinations-resulting in 2 k different positions for the parallel hyperplanes.)We show a 3-dimensional sketch for a network of two TLUs in Fig. 4.16.The reader might consider how the n-dimensional parity function might be implemented by a cascade network having log 2 n TLUs.Cascade networks might be trained by first training L 1 to do as good a job as possible at separating all the training patterns (perhaps by using the pocket algorithm, for example), then training L 2 (including the weight from L 1 to L 2 ) also to do as good a job as possible at separating all the training patterns, and so on until the resulting network classifies the patterns in the training set satisfactorily.Also mention the "cascade-correlation" method of [Fahlman & Lebiere, 1990].The general problem of training a network of TLUs is difficult.Consider, for example, the layered, feedforward network of Fig. 4.11.If such a network makes an error on a pattern, there are usually several different ways in which the error can be corrected.It is difficult to assign "blame" for the error to any particular TLU in the network.Intuitively, one looks for weight-adjusting procedures that move the network in the correct direction (relative to the error) by making minimal changes.In this spirit, the Widrow-Hoff method of gradient descent has been generalized to deal with multilayer networks.In explaining this generalization, we use Fig. 4.17 to introduce some notation.This network has only one output unit, but, of course, it is possible to have several TLUs in the output layer-each implementing a different function.Each of the layers of TLUs will have outputs that we take to be the components of vectors, just as the input features are components of an input vector.The j-th layer of TLUs (1 ≤ j < k) will have as their outputs the vector X (j) .The input feature vector is denoted by X (0) , and the final output (of the k-th layer TLU) is f .Each TLU in each layer has a weight vector (connecting it to its inputs) and a threshold; the i-th TLU in the j-th layer has a weight vector denoted by W (j) i .(We will assume that the "threshold weight" is the last component of the associated weight vector; we might have used V notation instead to include this threshold component, but we have chosen here to use the familiar X,W notation, assuming that these vectors are "augmented" as appropriate.)We denote the weighted sum input to the i-th threshold unit in the j-th layer by sThe number of TLUs in the j-th layer is given by m j .The vector W (j) i has components w (j) l,i for l = 1, . . ., m (j−1) + 1.. . .X (j) . . .A gradient descent method, similar to that used in the Widrow Hoff method, has been proposed by various authors for training a multi-layer, feedforward network.As before, we define an error function on the final output of the network and we adjust each weight in the network so as to minimize the error.If we have a desired response, d i , for the i-th input vector, X i , in the training set, Ξ, we can compute the squared error over the entire training set to be:where f i is the actual response of the network for input X i .To do gradient descent on this squared error, we adjust each weight in the network by an amount proportional to the negative of the partial derivative of ε with respect to that weight.Again, we use a single-pattern error function so that we can use an incremental weight adjustment procedure.The squared error for a single input vector, X, evoking an output of f when the desired output is d is:It is convenient to take the partial derivatives of ε with respect to the various weights in groups corresponding to the weight vectors.We define a partial derivative of a quantity φ, say, with respect to a weight vector, W where wi .This vector partial derivative of φ is called the gradient of φ with respect to W and is sometimes denoted by ∇ W φ.i , we can use the chain rule to write: 1) .Substituting yields:Note that ∂ε ∂sThe quantity (d−f ) ∂f ∂s (j) iplays an important role in our calculations; we shall denote it by δi 's tells us how sensitive the squared error of the network output is to changes in the input to each threshold function.Since we will be changing weight vectors in directions along their negative gradient, our fundamental rule for weight changes throughout the network will be:where c (j) i is the learning rate constant for this weight vector.(Usually, the learning rate constants for all weight vectors in the network are the same.)We see that this rule is quite similar to that used in the error correction procedure for a single TLU.A weight vector is changed by the addition of a constant times its vector of (unweighted) inputs.Now, we must turn our attention to the calculation of the δ (j) i 's.Using the definition, we have:We have a problem, however, in attempting to carry out the partial derivatives of f with respect to the s's.The network output, f , is not continuously differentiable with respect to the s's because of the presence of the threshold functions.Most small changes in these sums do not change f at all, and when f does change, it changes abruptly from 1 to 0 or vice versa.A way around this difficulty was proposed by Werbos [Werbos, 1974] and (perhaps independently) pursued by several other researchers, for example [Rumelhart, Hinton, & Williams, 1986].The trick involves replacing all the threshold functions by differentiable functions called sigmoids. 1 The output of a sigmoid function, superimposed on that of a threshold function, is shown in Fig. 4.18.Usually, the sigmoid function used is f (s) = 1 1+e −s , where s is the input and f is the output.The output of the i-th sigmoid unit in the j-th layer is denoted by f.). . .X (j) . . .First Layer j-th Layer (k-1)-th Layer k-th Layer . . .We first calculate δ (k) in order to compute the weight change for the final sigmoid unit:Given the sigmoid function that we are using, namely f (s) = 1 1+e −s , we have that ∂f ∂s = f (1 − f ).Substituting gives us:Rewriting our general rule for weight vector changes, the weight vector in the final layer is changed according to the rule:whereIt is interesting to compare backpropagation to the error-correction rule and to the Widrow-Hoff rule.The backpropagation weight adjustment for the single element in the final layer can be written as:Written in the same format, the error-correction rule is:and the Widrow-Hoff rule is:The only difference (except for the fact that f is not thresholded in Widrow-Hoff) is the f (1 − f ) term due to the presence of the sigmoid function.With the sigmoid function,that is, when the input to the sigmoid is 0).The sigmoid function can be thought of as implementing a "fuzzy" hyperplane.For a pattern far away from this fuzzy hyperplane, f (1 − f ) has value close to 0, and the backpropagation rule makes little or no change to the weight values regardless of the desired output.(Small changes in the weights will have little effect on the output for inputs far from the hyperplane.)Weight changes are only made within the region of "fuzz" surrounding the hyperplane, and these changes are in the direction of correcting the error, just as in the error-correction and Widrow-Hoff rules.Using our expression for the δ's, we can similarly compute how to change each of the weight vectors in the network.Recall:Again we use a chain rule.The final output, f , depends on s (j) i through each of the summed inputs to the sigmoids in the (j + 1)-th layer.So:It remains to compute the ∂s (j+1) l ∂s (j) i 's.To do that we first write:And then, since the weights do not depend on the s's:Now, we note thatWe use this result in our expression for δ (j) i to give:The above equation is recursive in the δ's.(It is interesting to note that this expression is independent of the error function; the error function explicitly affects only the computation of δ (k) .)Having computed the δ (j+1) i 's for layer j + 1, we can use this equation to compute the δ (j) i 's.The base case is δ (k) , which we have already computed:We use this expression for the δ's in our generic weight changing rule, namely:Although this rule appears complex, it has an intuitively reasonable explanation.The quantity δ (k) = (d − f )f (1 − f ) controls the overall amount and sign of all weight adjustments in the network.(Adjustments diminish as the final output, f , approaches either 0 or 1, because they have vanishing effect on f then.)As the recursion equation for the δ's shows, the adjustments for the weights going in to a sigmoid unit in the j-th layer are proportional to the effect that such adjustments have on that sigmoid unit's output (its f (j) (1 − f (j) ) factor).They are also proportional to a kind of "average" effect that any change in the output of that sigmoid unit will have on the final output.This average effect depends on the weights going out of the sigmoid unit in the j-th layer (small weights produce little downstream effect) and the effects that changes in the outputs of (j + 1)-th layer sigmoid units will have on the final output (as measured by the δ (j+1) 's).These calculations can be simply implemented by "backpropagating" the δ's through the weights in reverse direction (thus, the name backprop for this algorithm).[To be written: problem of local minima, simulated annealing, momemtum (Plaut, et al., 1986, see [Hertz, Krogh, & Palmer, 1991]), quickprop, regularization methods]To apply simulated annealing, the value of the learning rate constant is gradually decreased with time.If we fall early into an error-function valley that is not very deep (a local minimum), it typically will neither be very broad, and soon a subsequent large correction will jostle us out of it.It is less likely that we will move out of deep valleys, and at the end of the process (with very small values of the learning rate constant), we descend to its deepest point.The process gets its name by analogy with annealing in metallurgy, in which a material's temperature is gradually decreased allowing its crystalline structure to reach a minimal energy state.A neural network system called ALVINN (Autonomous Land Vehicle in a Neural Network) has been trained to steer a Chevy van successfully on ordinary roads and highways at speeds of 55 mph [Pomerleau, 1991, Pomerleau, 1993.The input to the network is derived from a low-resolution (30 x 32) television image.The TV camera is mounted on the van and looks at the road straight ahead.This image is sampled and produces a stream of 960-dimensional input vectors to the neural network.The network is shown in Fig. 4.20.The network has five hidden units in its first layer and 30 output units in the second layer; all are sigmoid units.The output units are arranged in a linear order and control the van's steering angle.If a unit near the top of the array of output units has a higher output than most of the other units, the van is steered to the left; if a unit near the bottom of the array has a high output, the van is steered to the right.The "centroid" of the responses of all of the outputunits is computed, and the van's steering angle is set at a corresponding value between hard left and hard right.The system is trained by a modified on-line training regime.A driver drives the van, and his actual steering angles are taken as the correct labels for the corresponding inputs.The network is trained incrementally by backprop to produce the driver-specified steering angles in response to each visual pattern as it occurs in real time while driving.This simple procedure has been augmented to avoid two potential problems.First, since the driver is usually driving well, the network would never get any experience with far-from-center vehicle positions and/or incorrect vehicle orientations.Also, on long, straight stretches of road, the network would be trained for a long time only to produce straight-ahead steering angles; this training would swamp out earlier training to follow a curved road.We wouldn't want to try to avoid these problems by instructing the driver to drive erratically occasionally, because the system would learn to mimic this erratic behavior.Instead, each original image is shifted and rotated in software to create 14 additional images in which the vehicle appears to be situated differently relative to the road.Using a model that tells the system what steering angle ought to be used for each of these shifted images, given the driver-specified steering angle for the original image, the system constructs an additional 14 labeled training patterns to add to those encountered during ordinary driver training.To be written; discuss rule-generating procedures (such as [Towell & Shavlik, 1992]) and how expert-provided rules can aid neural net training and vice-versa [Towell, Shavlik, & Noordweier, 1990].To be added.Chapter 5Suppose the pattern vector, X, is a random variable whose probability distribution for category 1 is different than it is for category 2. (The treatment given here can easily be generalized to R-category problems.)Specifically, suppose we have the two probability distributions (perhaps probability density functions), p(X | 1) and p(X | 2).Given a pattern, X, we want to use statistical techniques to determine its category-that is, to determine from which distribution it was drawn.These techniques are based on the idea of minimizing the expected value of a quantity similar to the error function we used in deriving the weight-changing rules for backprop.In developing a decision method, it is necessary to know the relative seriousness of the two kinds of mistakes that might be made.(We might decide that a pattern really in category 1 is in category 2, and vice versa.)We describe this information by a loss function, λ(i | j), for i, j = 1, 2. λ(i | j) represents the loss incurred when we decide a pattern is in category i when really it is in category j.We assume here that λ(1 | 1) and λ(2 | 2) are both 0. For any given pattern, X, we want to decide its category in such a way that minimizes the expected value of this loss.Given a pattern, X, if we decide category i, the expected value of the loss will be:where p(j | X) is the probability that given a pattern X, its category is j.Our decision rule will be to decide that X belongs to category 1 if L X (1) ≤ L X (2), and to decide on category 2 otherwise.We can use Bayes' Rule to get expressions for p(j | X) in terms of p(X | j), which we assume to be known (or estimatible):where p(j) is the (a priori) probability of category j (one category may be much more probable than the other); and p(X) is the (a priori) probability of pattern X being the pattern we are asked to classify.Performing the substitutions given by Bayes' Rule, our decision rule becomes:Decide category 1 iff:Using the fact that λ(i | i) = 0, and noticing that p(X) is common to both expressions, we obtain, Decide category 1 iff:) and if p(1) = p(2), then the decision becomes particularly simple:Decide category 1 iff:Since p(X | j) is called the likelihood of j with respect to X, this simple decision rule implements what is called a maximum-likelihood decision.More generally, if we define k(i | j) as λ(i | j)p(j), then our decision rule is simply, Decide category1 iff:In any case, we need to compare the (perhaps weighted) quantities p(X | i) for i = 1 and 2. The exact decision rule depends on the the probability distributions assumed.We will treat two interesting distributions.The multivariate (n-dimensional) Gaussian distribution is given by the probability density function:where n is the dimension of the column vector X, the column vector M is called the mean vector, (X − M) t is the transpose of the vector (X − M), Σ is the covariance matrix of the distribution (an n × n symmetric, positive definite matrix), Σ −1 is the inverse of the covariance matrix, and |Σ| is the determinant of the covariance matrix.The mean vector, M, with components (m 1 , . . ., m n ), is the expected value of X (using this distribution); that is, M = E[X].The components of the covariance matrix are given by:In particular, σ 2 ii is called the variance of x i .Although the formula appears complex, an intuitive idea for Gaussian distributions can be given when n = 2.We show a two-dimensional Gaussian distribution in Fig. 5.1.A three-dimensional plot of the distribution is shown at the top of the figure, and contours of equal probability are shown at the bottom.In this case, the covariance matrix, Σ, is such that the elliptical contours of equal probability are skewed.If the covariance matrix were diagonal, that is if all off-diagonal terms were 0, then the major axes of the elliptical contours would be aligned with the coordinate axes.In general the principal axes are given by the eigenvectors of Σ.In any case, the equi-probability contours are all centered on the mean vector, M, which in our figure happens to be at the origin.In general, the formula in the exponent in the Gaussian distribution is a positive definite quadratic form (that is, its value is always positive); thus equi-probability contours are hyper-ellipsoids in n-dimensional space.Suppose we now assume that the two classes of pattern vectors that we want to distinguish are each distributed according to a Gaussian distribution but with different means and covariance matrices.That is, one class tends to have patterns clustered around one point in the n-dimensional space, and the other class tends to have patterns clustered around another point.We show a two-dimensional instance of this problem in Fig. 5.2.(In that figure, we have plotted the sum of the two distributions.)What decision rule should we use to separate patterns into the two appropriate categories?Substituting the Gaussian distributions into our maximum likelihood formula yields:where the category 1 patterns are distributed with mean and covariance M 1 and Σ 1 , respectively, and the category 2 patterns are distributed with mean and covariance M 2 and Σ 2 .The result of the comparison isn't changed if we compare logarithms instead.After some manipulation, our decision rule is then:where B, a constant bias term, incorporates the logarithms of the fractions preceding the exponential, etc.When the quadratic forms are multiplied out and represented in terms of the components x i , the decision rule involves a quadric surface (a hyperquadric) in n-dimensional space.The exact shape and position of this hyperquadric is determined by the means and the covariance matrices.The surface separates the space into two parts, one of which contains points that will be assigned to category 1 and the other contains points that will be assigned to category 2.It is interesting to look at a special case of this surface.If the covariance matrices for each category are identical and diagonal, with all σ ii equal to each other, then the contours of equal probability for each of the two distributions are hyperspherical.The quadric forms then become (1/|Σ|)(X − M i ) t (X − M i ), and the decision rule is:Decide category 1 iff:Multiplying out yields:where the constant depends on the lengths of the mean vectors.We see that the optimal decision surface in this special case is a hyperplane.In fact, the hyperplane is perpendicular to the line joining the two means.The weights in a TLU implementation are equal to the difference in the mean vectors.If the parameters (M i , Σ i ) of the probability distributions of the categories are not known, there are various techniques for estimating them, and then using those estimates in the decision rule.For example, if there are sufficient training patterns, one can use sample means and sample covariance matrices.(Caution: the sample covariance matrix will be singular if the training patterns happen to lie on a subspace of the whole n-dimensional space-as they certainly will, for example, if the number of training patterns is less than n.)Suppose the vector X is a random variable having binary (0,1) components.We continue to denote the two probability distributions by p(X | 1) and p(X | 2).Further suppose that the components of these vectors are conditionally independent given the category.By conditional independence in this case, we mean that the formulas for the distribution can be expanded as follows:Recall the minimum-average-loss decision rule, Decide category 1 iff:Assuming conditional independence of the components and that λ(1 | 2) = λ(2 | 1), we obtain, Decide category 1 iff:or iff:or iff:Let us define values of the components of the distribution for specific values of their arguments, x i :Now, we note that since x i can only assume the values of 1 or 0:Substituting these expressions into our decision rule yields:Decide category 1 iff:We see that we can achieve this decision with a TLU with weight values as follows:for i = 1, . . ., n, andIf we do not know the p i , q i and p(1), we can use a sample of labeled training patterns to estimate these parameters.To be added.Another class of methods can be related to the statistical ones.These are called nearest-neighbor methods or, sometimes, memory-based methods.(A collection of papers on this subject is in [Dasarathy, 1991].)Given a training set Ξ of m labeled patterns, a nearest-neighbor procedure decides that some new pattern, X, belongs to the same category as do its closest neighbors in Ξ.More precisely, a k-nearest-neighbor method assigns a new pattern, X, to that category to which the plurality of its k closest neighbors belong.Using relatively large values of k decreases the chance that the decision will be unduly influenced by a noisy training pattern close to X.But large values of k also reduce the acuity of the method.The k-nearest-neighbor method can be thought of as estimating the values of the probabilities of the classes given X.Of course the denser are the points around X, and the larger the value of k, the better the estimate.The distance metric used in nearest-neighbor methods (for numerical attributes) can be simple Euclidean distance.That is, the distance between two patterns (x 11 , x 12 , . . ., x 1n ) and (x 21 , x 22 , . . ., x 2n ) is n j=1 (x 1j − x 2j ) 2 .This distance measure is often modified by scaling the features so that the spread of attribute values along each dimension is approximately the same.In that case, the distance between the two vectors would be, where a j is the scale factor for dimension j.An example of a nearest-neighbor decision problem is shown in Fig. 5.3.In the figure the class of a training pattern is indicated by the number next to it.See [Baum, 1994] for theoretical analysis of error rate as a function of the number of training patterns for the case in which points are randomly distributed on the surface of a unit sphere and underlying function is linearly separable.Nearest-neighbor methods are memory intensive because a large number of training patterns must be stored to achieve good generalization.Since memory cost is now reasonably low, the method and its derivatives have seen several practical applications.(See, for example, [Moore, 1992, Moore, et al., 1994.Also, the distance calculations required to find nearest neighbors can often be efficiently computed by kd-tree methods [Friedman, et al., 1977].A theorem by Cover and Hart [Cover & Hart, 1967] relates the performance of the 1-nearest-neighbor method to the performance of a minimum-probabilityof-error classifier.As mentioned earlier, the minimum-probability-of-error classifier would assign a new pattern X to that category that maximized p(i)p(X | i), where p(i) is the a priori probability of category i, and p(X | i) is the probability (or probability density function) of X given that X belongs to category i, for categories i = 1, . . ., R. Suppose the probability of error in classifying patterns of such a minimum-probability-of-error classifier is ε.The Cover-Hart theorem states that under very mild conditions (having to do with the smoothness of probability density functions) the probability of error, ε nn , of a 1-nearestneighbor classifier is bounded by:where R is the number of categories.Also see [Aha, 1991].To be added.Chapter 6Decision TreesA decision tree (generally defined) is a tree whose internal nodes are tests (on input patterns) and whose leaf nodes are categories (of patterns).We show an example in Fig. 6.1.A decision tree assigns a class number (or output) to an input pattern by filtering the pattern down through the tests in the tree.Each test has mutually exclusive and exhaustive outcomes.For example, test T 2 in the tree of Fig. 6.1 has three outcomes; the left-most one assigns the input pattern to class 3, the middle one sends the input pattern down to test T 4 , and the right-most one assigns the pattern to class 1.We follow the usual convention of depicting the leaf nodes by the class number. 1 Note that in discussing decision trees we are not limited to implementing Boolean functions-they are useful for general, categorically valued functions.There are several dimensions along which decision trees might differ:a.The tests might be multivariate (testing on several features of the input at once) or univariate (testing on only one of the features).b.The tests might have two outcomes or more than two.(If all of the tests have two outcomes, we have a binary decision tree.)c.The features or attributes might be categorical or numeric.(Binary-valued ones can be regarded as either.)1 One of the researchers who has done a lot of work on learning decision trees is Ross Quinlan.Quinlan distinguishes between classes and categories.He calls the subsets of patterns that filter down to each tip categories and subsets of patterns having the same label classes.In Quinlan's terminology, our example tree has nine categories and three classes.We will not make this distinction, however, but will use the words "category" and "class" interchangeably to refer to what Quinlan calls "class."It is straightforward to represent the function implemented by a univariate Boolean decision tree in DNF form.The DNF form implemented by such a tree can be obtained by tracing down each path leading to a tip node corresponding to an output value of 1, forming the conjunction of the tests along this path, and then taking the disjunction of these conjunctions.We show an example in Fig. 6.2.In drawing univariate decision trees, each non-leaf node is depicted by a single attribute.If the attribute has value 0 in the input pattern, we branch left; if it has value 1, we branch right.The k-DL class of Boolean functions can be implemented by a multivariate decision tree having the (highly unbalanced) form shown in Fig. 6.3.Each test, c i , is a term of size k or less.The v i all have values of 0 or 1.Several systems for learning decision trees have been proposed.Prominent among these are ID3 and its new version, C4.5 [Quinlan, 1986, Quinlan, 1993, and CART [Breiman, et al., 1984] We discuss here only batch methods, although incremental ones have also been proposed [Utgoff, 1989].x 3x 2 x 4xThe main problem in learning decision trees for the binary-attribute case is selecting the order of the tests.For categorical and numeric attributes, we must also decide what the tests should be (besides selecting the order).Several techniques have been tried; the most popular one is at each stage to select that test that maximally reduces an entropy-like measure.We show how this technique works for the simple case of tests with binary outcomes.Extension to multiple-outcome tests is straightforward computationally but gives poor results because entropy is always decreased by having more outcomes.The entropy or uncertainty still remaining about the class of a patternknowing that it is in some set, Ξ, of patterns is defined as:where p(i|Ξ) is the probability that a pattern drawn at random from Ξ belongs to class i, and the summation is over all of the classes.We want to select tests at each node such that as we travel down the decision tree, the uncertainty about the class of a pattern becomes less and less.Since we do not in general have the probabilities p(i|Ξ), we estimate them by sample statistics.Although these estimates might be errorful, they are nevertheless useful in estimating uncertainties.Let p(i|Ξ) be the number of patterns in Ξ belonging to class i divided by the total number of patterns in Ξ.Then an estimate of the uncertainty is:For simplicity, from now on we'll drop the "hats" and use sample statistics as if they were real probabilities.If we perform a test, T , having k possible outcomes on the patterns in Ξ, we will create k subsets, Ξ 1 , Ξ 2 , . . ., Ξ k .Suppose that n i of the patterns in Ξ are in Ξ i for i = 1, ..., k. (Some n i may be 0.) If we knew that T applied to a pattern in Ξ resulted in the j-th outcome (that is, we knew that the pattern was in Ξ j ), the uncertainty about its class would be:and the reduction in uncertainty (beyond knowing only that the pattern was in Ξ) would be: Of course we cannot say that the test T is guaranteed always to produce that amount of reduction in uncertainty because we don't know that the result of the test will be the j-th outcome.But we can estimate the average uncertainty over all the Ξ j , by:where by H T (Ξ) we mean the average uncertainty after performing test T on the patterns in Ξ, p(Ξ j ) is the probability that the test has outcome j, and the sum is taken from 1 to k.Again, we don't know the probabilities p(Ξ j ), but we can use sample values.The estimate p(Ξ j ) of p(Ξ j ) is just the number of those patterns in Ξ that have outcome j divided by the total number of patterns in Ξ.The average reduction in uncertainty achieved by test T (applied to patterns in Ξ) is then:An important family of decision tree learning algorithms selects for the root of the tree that test that gives maximum reduction of uncertainty, and then applies this criterion recursively until some termination condition is met (which we shall discuss in more detail later).The uncertainty calculations are particularly simple when the tests have binary outcomes and when the attributes have binary values.We'll give a simple example to illustrate how the test selection mechanism works in that case.Suppose we want to use the uncertainty-reduction method to build a decision tree to classify the following patterns:What single test, x 1 , x 2 , or x 3 , should be performed first?The illustration in Fig. 6.5 gives geometric intuition about the problem.x 1x 2x 3The test x 1 The initial uncertainty for the set, Ξ, containing all eight points is:Next, we calculate the uncertainty reduction if we perform x 1 first.The lefthand branch has only patterns belonging to class 0 (we call them the set Ξ l ), and the right-hand-branch (Ξ r ) has two patterns in each class.So, the uncertainty of the left-hand branch is:And the uncertainty of the right-hand branch is:Half of the patterns "go left" and half "go right" on test x 1 .Thus, the average uncertainty after performing the x 1 test is:Therefore the uncertainty reduction on Ξ achieved by x 1 is:By similar calculations, we see that the test x 3 achieves exactly the same uncertainty reduction, but x 2 achieves no reduction whatsoever.Thus, our "greedy" algorithm for selecting a first test would select either x 1 or x 3 .Suppose x 1 is selected.The uncertainty-reduction procedure would select x 3 as the next test.The decision tree that this procedure creates thus implements the Boolean function: f = x 1 x 3 .See [Quinlan, 1986, sect. 4] for another example.If the attributes are non-binary, we can still use the uncertainty-reduction technique to select tests.But now, in addition to selecting an attribute, we must select a test on that attribute.Suppose for example that the value of an attribute is a real number and that the test to be performed is to set a threshold and to test to see if the number is greater than or less than that threshold.In principle, given a set of labeled patterns, we can measure the uncertainty reduction for each test that is achieved by every possible threshold (there are only a finite number of thresholds that give different test results if there are only a finite number of training patterns).Similarly, if an attribute is categorical (with a finite number of categories), there are only a finite number of mutually exclusive and exhaustive subsets into which the values of the attribute can be split.We can calculate the uncertainty reduction for each split.Since univariate Boolean decision trees are implementations of DNF functions, they are also equivalent to two-layer, feedforward neural networks.We show an example in Fig. 6.6.The decision tree at the left of the figure implements the same function as the network at the right of the figure.Of course, when implemented as a network, all of the features are evaluated in parallel for any input pattern, whereas when implemented as a decision tree only those features on the branch traveled down by the input pattern need to be evaluated.The decision-tree induction methods discussed in this chapter can thus be thought of as particular ways to establish the structure and the weight values for networks.Multivariate decision trees with linearly separable functions at each node can also be implemented by feedforward networks-in this case three-layer ones.We show an example in Fig. 6.7 in which the linearly separable functions, each implemented by a TLU, are indicated by L 1 , L 2 , L 3 , and L 4 .Again, the final layer has fixed weights, but the weights in the first two layers must be trained.Different approaches to training procedures have been discussed by [Brent, 1990], by [John, 1995], and (for a special case) by [Marchand & Golea, 1993].In supervised learning, we must choose a function to fit the training set from among a set of hypotheses.We have already showed that generalization is impossible without bias.When we know a priori that the function we are trying to guess belongs to a small subset of all possible functions, then, even with an incomplete set of training samples, it is possible to reduce the subset of functions that are consistent with the training set sufficiently to make useful guesses about the value of the function for inputs not in the training set.And,.7: A Multivariate Decision Tree and its Equivalent Network the larger the training set, the more likely it is that even a randomly selected consistent function will have appropriate outputs for patterns not yet seen.However, even with bias, if the training set is not sufficiently large compared with the size of the hypothesis space, there will still be too many consistent functions for us to make useful guesses, and generalization performance will be poor.When there are too many hypotheses that are consistent with the training set, we say that we are overfitting the training data.Overfitting is a problem that we must address for all learning methods.Since a decision tree of sufficient size can implement any Boolean function there is a danger of overfitting-especially if the training set is small.That is, even if the decision tree is synthesized to classify all the members of the training set correctly, it might perform poorly on new patterns that were not used to build the decision tree.Several techniques have been proposed to avoid overfitting, and we shall examine some of them here.They make use of methods for estimating how well a given decision tree might generalize-methods we shall describe next.The most straightforward way to estimate how well a hypothesized function (such as a decision tree) performs on a test set is to test it on the test set!But, if we are comparing several learning systems (for example, if we are comparing different decision trees) so that we can select the one that performs the best on the test set, then such a comparison amounts to "training on the test data."True, training on the test data enlarges the training set, with a consequent expected improvement in generalization, but there is still the danger of overfitting if we are comparing several different learning systems.Another technique is to split the training set-using (say) two-thirds for training and the other third for estimating generalization performance.But splitting reduces the size of the training set and thereby increases the possibility of overfitting.We next describe some validation techniques that attempt to avoid these problems.In cross-validation, we divide the training set Ξ into K mutually exclusive and exhaustive equal-sized subsets: Ξ 1 , . . ., Ξ K .For each subset, Ξ i , train on the union of all of the other subsets, and empirically determine the error rate, ε i , on Ξ i .(The error rate is the number of classification errors made on Ξ i divided by the number of patterns in Ξ i .)An estimate of the error rate that can be expected on new patterns of a classifier trained on all the patterns in Ξ is then the average of the ε i .Leave-one-out validation is the same as cross validation for the special case in which K equals the number of patterns in Ξ, and each Ξ i consists of a single pattern.When testing on each Ξ i , we simply note whether or not a mistake was made.We count the total number of mistakes and divide by K to get the estimated error rate.This type of validation is, of course, more expensive computationally, but useful when a more accurate estimate of the error rate for a classifier is needed.Describe "bootstrapping" also [Efron, 1982].Near the tips of a decision tree there may be only a few patterns per node.For these nodes, we are selecting a test based on a very small sample, and thus we are likely to be overfitting.This problem can be dealt with by terminating the test-generating procedure before all patterns are perfectly split into their separate categories.That is, a leaf node may contain patterns of more than one class, but we can decide in favor of the most numerous class.This procedure will result in a few errors but often accepting a small number of errors on the training set results in fewer errors on a testing set.This behavior is illustrated in Fig. 6.8.One can use cross-validation techniques to determine when to stop splitting nodes.If the cross validation error increases as a consequence of a node split, then don't split.One has to be careful about when to stop, though, because underfitting usually leads to more errors on test sets than does overfitting.There is a general rule that the lowest error-rate attainable by a sub-tree of a fully expanded tree can be no less than 1/2 of the error rate of the fully expanded tree [Weiss & Kulikowski, 1991, page 126].Rather than stopping the growth of a decision tree, one might grow it to its full size and then prune away leaf nodes and their ancestors until crossvalidation accuracy no longer increases.This technique is called post-pruning.Various techniques for pruning are discussed in [Weiss & Kulikowski, 1991].An important tree-growing and pruning technique is based on the minimumdescription-length (MDL) principle.(MDL is an important idea that extends beyond decision-tree methods [Rissanen, 1978].)The idea is that the simplest decision tree that can predict the classes of the training patterns is the best one.Consider the problem of transmitting just the labels of a training set of patterns, assuming that the receiver of this information already has the ordered set of patterns.If there are m patterns, each labeled by one of R classes, one could transmit a list of m R-valued numbers.Assuming equally probable classes, this transmission would require m log 2 R bits.Or, one could transmit a decision tree that correctly labelled all of the patterns.The number of bits that this transmission would require depends on the technique for encoding decision trees and on the size of the tree.If the tree is small and accurately classifies all of the patterns, it might be more economical to transmit the tree than to transmit the labels directly.In between these extremes, we might transmit a tree plus a list of labels of all the patterns that the tree misclassifies.In general, the number of bits (or description length of the binary encoded message) is t + d, where t is the length of the message required to transmit the tree, and d is the length of the message required to transmit the labels of the patterns misclassified by the tree.In a sense, that tree associated with the smallest value of t + d is the best or most economical tree.The MDL method is one way of adhering to the Occam's razor principle.Quinlan and Rivest [Quinlan & Rivest, 1989] have proposed techniques for encoding decision trees and lists of exception labels and for calculating the description length (t+d) of these trees and labels.They then use the description length as a measure of quality of a tree in two ways: a.In growing a tree, they use the reduction in description length to select tests (instead of reduction in uncertainty).b.In pruning a tree after it has been grown to zero error, they prune away those nodes (starting at the tips) that achieve a decrease in the description length.These techniques compare favorably with the uncertainty-reduction method, although they are quite sensitive to the coding schemes used.Noise in the data means that one must inevitably accept some number of errors-depending on the noise level.Refusal to tolerate errors on the training set when there is noise leads to the problem of "fitting the noise."Dealing with noise, then, requires accepting some errors at the leaf nodes just as does the fact that there are a small number of patterns at leaf nodes.Decision trees are not the most economical means of implementing some Boolean functions.Consider, for example, the function f = x 1 x 2 + x 3 x 4 .A decision tree for this function is shown in Fig. 6.9.Notice the replicated subtrees shown circled.The DNF-form equivalent to the function implemented by this decision tree is f = x 1 x 2 + x 1 x 2 x 3 x 4 + x 1 x 3 x 4 .This DNF form is non-minimal (in the number of disjunctions) and is equivalent to f = x 1 x 2 + x 3 x 4 .The need for replication means that it takes longer to learn the tree and that subtrees replicated further down the tree must be learned using a smaller training subset.This problem is sometimes called the fragmentation problem.Several approaches might be suggested for dealing with fragmentation.One is to attempt to build a decision graph instead of a tree [Oliver, Dowe, & Wallace, 1992, Kohavi, 1994.A decision graph that implements the same decisions as that of the decision tree of Fig. 6.9 is shown in Fig.Another approach is to use multivariate (rather than univariate tests at each node).In our example of learning f = x 1 x 2 + x 3 x 4 , if we had a test for x 1 x 2 x 1 x 3x 2 1 0x 4 0 1x 3 0 x 4 0 1 Figure 6.9:A Decision Tree with Subtree Replication and a test for x 3 x 4 , the decision tree could be much simplified, as shown in Fig. 6.11.Several researchers have proposed techniques for learning decision trees in which the tests at each node are linearly separable functions.[John, 1995] gives a nice overview (with several citations) of learning such linear discriminant trees and presents a method based on "soft entropy."A third method for dealing with the replicated subtree problem involves extracting propositional "rules" from the decision tree.The rules will have as antecedents the conjunctions that lead down to the leaf nodes, and as consequents the name of the class at the corresponding leaf node.An example rule from the tree with the repeating subtree of our example would be:Quinlan [Quinlan, 1987] discusses methods for reducing a set of rules to a simpler set by 1) eliminating from the antecedent of each rule any "unnecessary" conjuncts, and then 2) eliminating "unnecessary" rules.A conjunct or rule is determined to be unnecessary if its elimination has little effect on classification accuracy-as determined by a chi-square test, for example.After a rule set is processed, it might be the case that more than one rule is "active" for any given pattern, and care must be taken that the active rules do not conflict in their decision about the class of a pattern.To be added.Several experimenters have compared decision-tree, neural-net, and nearestneighbor classifiers on a wide variety of problems.For a comparison of neural nets versus decision trees, for example, see , Shavlik, Mooney, & Towell, 1991, Quinlan, 1994.In their StatLog project, [Taylor, Michie, & Spiegalhalter, 1994] give thorough comparisons of several machine learning algorithms on several different types of problems.There seems to be no single type of classifier that is best for all problems.And, there do not seem to be any general conclusions that would enable one to say which classifier method is best for which sorts of classification problems, although [Quinlan, 1994] does provide some intuition about properties of problems that might render them ill suited for decision trees, on the one hand, or backpropagation, on the other.To be added.There are many different representational forms for functions of input variables.So far, we have seen (Boolean) algebraic expressions, decision trees, and neural networks, plus other computational mechanisms such as techniques for computing nearest neighbors.Of course, the representation most important in computer science is a computer program.For example, a Lisp predicate of binary-valued inputs computes a Boolean function of those inputs.Similarly, a logic program (whose ordinary application is to compute bindings for variables) can also be used simply to decide whether or not a predicate has value True (T) or False (F).For example, the Boolean exclusive-or (odd parity) function of two variables can be computed by the following logic program:Parity(x,y) :-True(x), ¬ True(y) :-True(y), ¬ True(x)We follow Prolog syntax (see, for example, [Mueller & Page, 1988]), except that our convention is to write variables as strings beginning with lower-case letters and predicates as strings beginning with upper-case letters.The unary function "True" returns T if and only if the value of its argument is T .(We now think of Boolean functions and arguments as having values of T and F instead of 0 and 1.) Programs will be written in "typewriter" font.In this chapter, we consider the matter of learning logic programs given a set of variable values for which the logic program should return T (the positive instances) and a set of variable values for which it should return F (the negative instances).The subspecialty of machine learning that deals with learning logic programs is called inductive logic programming (ILP) [Lavrač & Džeroski, 1994].As with any learning problem, this one can be quite complex and intractably difficult unless we constrain it with biases of some sort.In ILP, there are a variety of possible biases (called language biases).One might restrict the program to Horn clauses, not allow recursion, not allow functions, and so on.As an example of an ILP problem, suppose we are trying to induce a function Nonstop(x,y), that is to have value T for pairs of cities connected by a non-stop air flight and F for all other pairs of cities.We are given a training set consisting of positive and negative examples.As positive examples, we might have (A,B), (A, A1), and some other pairs; as negative examples, we might have (A1, A2), and some other pairs.In ILP, we usually have additional information about the examples, called "background knowledge."In our air-flight problem, the background information might be such ground facts as Hub(A), Hub(B), Satellite(A1,A), plus others.(Hub(A) is intended to mean that the city denoted by A is a hub city, and Satellite(A1,A) is intended to mean that the city denoted by A1 is a satellite of the city denoted by A.) From these training facts, we want to induce a program Nonstop(x,y), written in terms of the background relations Hub and Satellite, that has value T for all the positive instances and has value F for all the negative instances.Depending on the exact set of examples, we might induce the program: Nonstop(x,y) :-Hub(x), Hub(y) :-Satellite(x,y) :-Satellite(y,x) which would have value T if both of the two cities were hub cities or if one were a satellite of the other.As with other learning problems, we want the induced program to generalize well; that is, if presented with arguments not represented in the training set (but for which we have the needed background knowledge), we would like the function to guess well.In evaluating logic programs in ILP, we implicitly append the background facts to the program and adopt the usual convention that a program has value T for a set of inputs if and only if the program interpreter returns T when actually running the program (with background facts appended) on those inputs; otherwise it has value F .Using the given background facts, the program above would return T for input (A, A1), for example.If a logic program, π, returns T for a set of arguments X, we say that the program covers the arguments and write covers(π, X).Following our terminology introduced in connection with version spaces, we will say that a program is sufficient if it covers all of the positive instances and that it is necessary if it does not cover any of the negative instances.(That is, a program implements a sufficient condition that a training instance is positive if it covers all of the positive training instances; it implements a necessary condition if it covers none of the negative instances.)In the noiseless case, we want to induce a program that is both sufficient and necessary, in which case we will call it consistent.With imperfect (noisy) training sets, we might relax this criterion and settle for a program that covers all but some fraction of the positive instances while allowing it to cover some fraction of the negative instances.We illustrate these definitions schematically in Fig. 7.1.Suppose we are attempting to induce a logic program to compute the relation ρ.The most general logic program, which is certainly sufficient, is the one that has value T for all inputs, namely a single clause with an empty body, [ρ :-], which is called a fact in Prolog.The most special logic program, which is certainly necessary, is the one that has value F for all inputs, namely [ρ :-F ].Two of the many different ways to search for a consistent logic program are: 1) start with [ρ :-] and specialize until the program is consistent, or 2) start with [ρ :-F ] and generalize until the program is consistent.We will be discussing a method that starts with [ρ :-], specializes until the program is necessary (but might no longer be sufficient), then reachieves sufficiency in stages by generalizing-ensuring within each stage that the program remains necessary (by specializing).Since the primary operators in our search for a consistent program are specialization and generalization, we must next discuss those operations.There are three major ways in which a logic program might be generalized: a. Replace some terms in a program clause by variables.(Readers familiar with substitutions in the predicate calculus will note that this process is the inverse of substitution.)b. Remove literals from the body of a clause.b.Add literals to the body of a clause.We will be presenting an ILP learning method that adds clauses to a program when generalizing and that adds literals to the body of a clause when specializing.When we add a clause, we will always add the clause [ρ :-] and then specialize it by adding literals to the body.Thus, we need only describe the process for adding literals.Clauses can be partially ordered by the specialization relation.In general, clause c 1 is more special than clause c 2 if c 2 |= c 1 .A special case, which is what we use here, is that a clause c 1 is more special than a clause c 2 if the set of literals in the body of c 2 is a subset of those in c 1 .This ordering relation can be used in a structure of partially ordered clauses, called the refinement graph, that is similar to a version space.Clause c 1 is an immediate successor of clause c 2 in this graph if and only if clause c 1 can be obtained from clause c 2 by adding a literal to the body of c 2 .A refinement graph then tells us the ways in which we can specialize a clause by adding a literal to it.(This possibility is equivalent to forming a specialization by making a substitution.)e.A literal that is the same (except for its arguments) as that in the head of the clause.(This possibility admits recursive programs, which are disallowed in some systems.)We can illustrate these possibilities using our air-flight example.We start with the program [Nonstop(x,y) :-].The literals used in the background knowledge are Hub and Satellite.Thus the literals that we might consider adding are:(If recursive programs are allowed, we could also add the literals Nonstop(x,z) and Nonstop(z,y).)These possibilities are among those illustrated in the refinement graph shown in Fig. 7.2.Whatever restrictions on additional literals are imposed, they are all syntactic ones from which the successors in the refinement graph are easily computed.ILP programs that follow the approach we are discussing (of specializing clauses by adding a literal) thus have well defined methods of computing the possible literals to add to a clause.Now we are ready to write down a simple generic algorithm for inducing a logic program, π for inducing a relation ρ.We are given a training set, Ξ of argument sets some known to be in the relation ρ and some not in ρ; Ξ + are the positive instances, and Ξ − are the negative instances.The algorithm has an outer loop in which it successively adds clauses to make π more and more sufficient.It has an inner loop for constructing a clause, c, that is more and more necessary and in which it refers only to a subset, Ξ cur , of the training instances.(The positive instances in Ξ cur will be denoted by Ξ + cur , and the negative ones by Ξ − cur .)The algorithm is also given background relations and the means for adding literals to a clause.It uses a logic program interpreter to compute whether or not the program it is inducing covers training instances.The algorithm can be written as follows:Generic ILP Algorithm (Adapted from [Lavrač & Džeroski, 1994, p. 60  (The termination tests for the inner and outer loops can be relaxed as appropriate for the case of noisy instances.)We illustrate how the algorithm works by returning to our example of airline flights.Consider the portion of an airline route map, shown in Fig. 7.3.Cities A, B, and C are "hub" cities, and we know that there are nonstop flights between all hub cities (even those not shown on this portion of the route map).The other cities are "satellites" of one of the hubs, and we know that there are nonstop flights between each satellite city and its hub.The learning program is given a set of positive instances, Ξ + , of pairs of cities between which there are nonstop flights and a set of negative instances, Ξ − , of pairs of cities between which there are not nonstop flights.Ξ + contains just the pairs:For our example, we will assume that Ξ − contains all those pairs of cities shown in Fig. 7.3 that are not in Ξ + (a type of closed-world assumption).These are:There may be other cities not shown on this map, so the training set does not necessarily exhaust all the cities.We want the learning program to induce a program for computing the value of the relation Nonstop.The training set, Ξ, can be thought of as a partial description of this relation in extensional form-it explicitly names some pairs in the relation and some pairs not in the relation.We desire to learn the Nonstop relation as a logic program in terms of the background relations, Hub and Satellite, which are also given in extensional form.Doing so will give us a more compact, intensional, description of the relation, and this description could well generalize usefully to other cities not mentioned in the map.We assume the learning program has the following extensional definitions of the relations Hub and Satellite:All other cities mentioned in the map are assumed not in the relation Hub.We will use the notation Hub(x) to express that the city named x is in the relation Hub.{< A1, A, >, < A2, A >, < B1, B >, < B2, B >, < C1, C >, < C2, C >} All other pairs of cities mentioned in the map are not in the relation Satellite.We will use the notation Satellite(x,y) to express that the pair < x, y > is in the relation Satellite.Knowing that the predicate Nonstop is a two-place predicate, the inner loop of our algorithm initializes the first clause to Nonstop(x,y) :-.This clause is not necessary because it covers all the negative examples (since it covers all examples).So we must add a literal to its (empty) body.Suppose (selecting a literal from the refinement graph) the algorithm adds Hub(x).The following positive instances in Ξ are covered by Nonstop(x,y) :-Hub(x):To compute this covering, we interpret the logic program Nonstop(x,y) :-Hub(x) for all pairs of cities in Ξ, using the pairs given in the background relation Hub as ground facts.The following negative instances are also covered:Thus, the clause is not yet necessary and another literal must be added.Suppose we next add Hub(y).The following positive instances are covered by Nonstop(x,y) :-Hub(x), Hub(y):There are no longer any negative instances in Ξ covered so the clause Nonstop(x,y) :-Hub(x), Hub(y) is necessary, and we can terminate the first pass through the inner loop.But the program, π, consisting of just this clause is not sufficient.These positive instances are not covered by the clause:The positive instances that were covered by Nonstop(x,y) :-Hub(x), Hub(y) are removed from Ξ to form the Ξ cur to be used in the next pass through the inner loop.Ξ cur consists of all the negative instances in Ξ plus the positive instances (listed above) that are not yet covered.In order to attempt to cover them, the inner loop creates another clause c, initially set to Nonstop(x,y) :-.This clause covers all the negative instances, and so we must add literals to make it necessary.Suppose we add the literal Satellite(x,y).The clause Nonstop(x,y) :-Satellite(x,y) covers no negative instances, so it is necessary.It does cover the following positive instances in Ξ cur : Since each clause is necessary, and the whole program is sufficient, the program is also consistent with all instances of the training set.Note that this program can be applied (perhaps with good generalization) to other cities besides those in our partial map-so long as we can evaluate the relations Hub and Satellite for these other cities.In the next section, we show how the technique can be extended to use recursion on the relation we are inducing.With that extension, the method can be used to induce more general logic programs.To induce a recursive program, we allow the addition of a literal having the same predicate letter as that in the head of the clause.Various mechanisms must be used to ensure that such a program will terminate; one such is to make sure that the new literal has different variables than those in the head literal.The process is best illustrated with another example.Our example continues the one using the airline map, but we make the map somewhat simpler in order to reduce the size of the extensional relations used.Consider the map shown in Fig. 7.4.Again, B and C are hub cities, B1 and B2 are satellites of B, C1 and C2 are satellites of C. We have introduced two new cities, B3 and C3.No flights exist between these cities and any other cities-perhaps there are only bus routes as shown by the grey lines in the map.We now seek to learn a program for Canfly(x,y) that covers only those pairs of cities that can be reached by one or more nonstop flights.The relation Canfly is satisfied by the following pairs of postive instances:Using a closed-world assumption on our map, we take the negative instances of Canfly to be:We will induce Canfly(x,y) using the extensionally defined background relation Nonstop given earlier (modified as required for our reduced airline map) and Canfly itself (recursively).As before, we start with the empty program and proceed to the inner loop to construct a clause that is necessary.Suppose that the inner loop adds the background literal Nonstop(x,y).The clause Canfly(x,y) :-Nonstop(x,y) is necessary; it covers no negative instances.But it is not sufficient because it does not cover the following positive instances:Thus, we must add another clause to the program.In the inner loop, we first create the clause Canfly(x,y) :-Nonstop(x,z) which introduces the new variable z.We digress briefly to describe how a program containing a clause with unbound variables in its body is interpreted.Suppose we try to interpret it for the positive instance Canfly(B1,B2).The interpreter attempts to establish Nonstop(B1,z) for some z.Since Nonstop(B1, B), for example, is a background fact, the interpreter returns T -which means that the instance < B1, B2 > is covered.Suppose now, we attempt to interpret the clause for the negative instance Canfly(B3,B).The interpreter attempts to establish Nonstop(B3,z) for some z.There are no background facts that match, so the clause does not cover < B3, B >. Using the interpreter, we see that the clause Canfly(x,y) :-Nonstop(x,z) covers all of the positive instances not already covered by the first clause, but it also covers many negative instances such as < B2, B3 >, and < B, B3 >.So the inner loop must add another literal.This time, suppose it adds Canfly(z,y) to yield the clause Canfly(x,y) :-Nonstop(x,z), Canfly(z,y).This clause is necessary; no negative instances are covered.The program is now sufficient and consistent; it is: Canfly(x,y) :-Nonstop(x,y) :-Nonstop(x,z), Canfly(z,y)One of the first practical ILP systems was Quinlan's FOIL .A major problem involves deciding how to select a literal to add in the inner loop (from among the literals that are allowed).In FOIL, Quinlan suggested that candidate literals can be compared using an information-like measure-similar to the measures used in inducing decision trees.A measure that gives the same comparison as does Quinlan's is based on the amount by which adding a literal increases the odds that an instance drawn at random from those covered by the new clause is a positive instance beyond what these odds were before adding the literal.Let p be an estimate of the probability that an instance drawn at random from those covered by a clause before adding the literal is a positive instance.That is, p =(number of positive instances covered by the clause)/(total number of instances covered by the clause).It is convenient to express this probability in "odds form."The odds, o, that a covered instance is positive is defined to be o = p/(1 − p).Expressing the probability in terms of the odds, we obtain p = o/(1 + o).After selecting a literal, l, to add to a clause, some of the instances previously covered are still covered; some of these are positive and some are negative.Let p l denote the probability that an instance drawn at random from the instances covered by the new clause (with l added) is positive.The odds will be denoted by o l .We want to select a literal, l, that gives maximal increase in these odds.That is, if we define λ l = o l /o, we want a literal that gives a high value of λ l .Specializing the clause in such a way that it fails to cover many of the negative instances previously covered but still covers most of the positive instances previously covered will result in a high value of λ l .(It turns out that the value of Quinlan's information theoretic measure increases monotonically with λ l , so we could just as well use the latter instead.)Besides finding a literal with a high value of λ l , Quinlan's FOIL system also restricts the choice to literals that: a) contain at least one variable that has already been used, b) place further restrictions on the variables if the literal selected has the same predicate letter as the literal being induced (in order to prevent infinite recursion), and c) survive a pruning test based on the values of λ l for those literals selected so far.We refer the reader to Quinlan's paper for further discussion of these points.Quinlan also discusses post-processing pruning methods and presents experimental results of the method applied to learning recursive relations on lists, on learning rules for chess endgames and for the card game Eleusis, and for some other standard tasks mentioned in the machine learning literature.The reader should also refer to [Pazzani & Kibler, 1992, Lavrač & Džeroski, 1994, Muggleton, 1991, Muggleton, 1992.Discuss preprocessing, postprocessing, bottom-up methods, and LINUS.The generic ILP algorithm can also be understood as a type of decision tree induction.Recall the problem of inducing decision trees when the values of attributes are categorical.When splitting on a single variable, the split at each node involves asking to which of several mutually exclusive and exhaustive subsets the value of a variable belongs.For example, if a node tested the variable x i , and if x i could have values drawn from {A, B, C, D, E, F }, then one possible split (among many) might be according to whether the value of x i had as value one of {A, B, C} or one of {D, E, F }.It is also possible to make a multi-variate split-testing the values of two or more variables at a time.With categorical variables, an n-variable split would be based on which of several n-ary relations the values of the variables satisfied.For example, if a node tested the variables x i and x j , and if x i and x j both could have values drawn from {A, B, C, D, E, F }, then one possible binary split (among many) might be according to whether or not < x i , x j > satisfied the relation {< A, C >, < C, D >}. (Note that our subset method of forming singlevariable splits could equivalently have been framed using 1-ary relations-which are usually called properties.)In this framework, the ILP problem is as follows: We are given a training set, Ξ, of positively and negatively labeled patterns whose components are drawn from a set of variables {x, y, z, . ..}.The positively labeled patterns in Ξ form an extensional definition of a relation, R. We are also given background relations, R 1 , . . ., R k , on various subsets of these variables.(That is, we are given sets of tuples that are in these relations.)We desire to construct an intensional definition of R in terms of the R 1 , . . ., R k , such that all of the positively labeled patterns in Ξ are satisfied by R and none of the negatively labeled patterns are.The intensional definition will be in terms of a logic program in which the relation R is the head of a set of clauses whose bodies involve the background relations.The generic ILP algorithm can be understood as decision tree induction, where each node of the decision tree is itself a sub-decision tree, and each subdecision tree consists of nodes that make binary splits on several variables using the background relations, R i .Thus we will speak of a top-level decision tree and various sub-decision trees.(Actually, our decision trees will be decision lists-a special case of decision trees, but we will refer to them as trees in our discussions.)In broad outline, the method for inducing an intensional version of the relation R is illustrated by considering the decision tree shown in Fig. 7.5.In this diagram, the patterns in Ξ are first filtered through the decision tree in toplevel node 1.The background relation R 1 is satisfied by some of these patterns; these are filtered to the right (to relation R 2 ), and the rest are filtered to the left (more on what happens to these later).Right-going patterns are filtered through a sequence of relational tests until only positively labeled patterns satisfy the last relation-in this case R 3 .That is, the subset of patterns satisfying all the relations, R 1 , R 2 , and R 3 contains only positive instances from Ξ. (We might say that this combination of tests is necessary.They correspond to the clause created in the first pass through the inner loop of the generic ILP algorithm.)Let us call the subset of patterns satisfying these relations, Ξ 1 ; these satisfy Node 1 at the top level.All other patterns, that is {Ξ − Ξ 1 } = Ξ 2 are filtered to the left by Node 1.Ξ 2 is then filtered by top-level Node 2 in much the same manner, so that Node 2 is satisfied only by the positively labeled samples in Ξ 2 .We continue filtering through top-level nodes until only the negatively labeled patterns fail to satisfy a top node.In our example, Ξ 4 contains only negatively labeled patterns and the union of Ξ 1 and Ξ 3 contains all the positively labeled patterns.The relation, R, that distinguishes positive from negative patterns in Ξ is then given in terms of the following logic program: R :-R1, R2, R3 If we apply this sort of decision-tree induction procedure to the problem of generating a logic program for the relation Nonstop (refer to Fig. 7.3), we obtain the decision tree shown in Fig. 7.6.The logic program resulting from this decision tree is the same as that produced by the generic ILP algorithm.In setting up the problem, the training set, Ξ can be expressed as a set of 2dimensional vectors with components x and y.The values of these components range over the cities {A, B, C, A1, A2, B1, B2, C1, C2} except (for simplicity) we do not allow patterns in which x and y have the same value.As before, the relation, Nonstop, contains the following pairs of cities, which are the positive instances:All other pairs of cities named in the map of Fig. 7.3 (using the closed world assumption) are not in the relation Nonstop and thus are negative instances.Because the values of x and y are categorical, decision-tree induction would be a very difficult task-involving as it does the need to invent relations onx and y to be used as tests.But with the background relations, R i (in this case Hub and Satellite), the problem is made much easier.We select these relations in the same way that we select literals; from among the available tests, we make a selection based on which leads to the largest value of λ Ri .To be added.Node 1 (top level)In chapter one we posed the problem of guessing a function given a set of sample inputs and their values.We gave some intuitive arguments to support the claim that after seeing only a small fraction of the possible inputs (and their values) that we could guess almost correctly the values of most subsequent inputs-if we knew that the function we were trying to guess belonged to an appropriately restricted subset of functions.That is, a given training set of sample patterns might be adequate to allow us to select a function, consistent with the labeled samples, from among a restricted set of hypotheses such that with high probability the function we select will be approximately correct (small probability of error) on subsequent samples drawn at random according to the same distribution from which the labeled samples were drawn.This insight led to the theory of probably approximately correct (PAC) learning-initially developed by Leslie Valiant [Valiant, 1984].We present here a brief description of the theory for the case of Boolean functions., Haussler, 1988, Haussler, 1990 give nice surveys of the important results.Other overviews?We assume a training set Ξ of n-dimensional vectors, X i , i = 1, . . ., m, each labeled (by 1 or 0) according to a target function, f , which is unknown to the learner.The probability of any given vector X being in Ξ, or later being presented to the learner, is P (X).The probability distribution, P , can be arbitrary.(In the literature of PAC learning theory, the target function is usually called the target concept and is denoted by c, but to be consistent with our previous notation we will continue to denote it by f .)Our problem is to guess 107 a function, h(X), based on the labeled samples in Ξ.In PAC theory such a guessed function is called the hypothesis.We assume that the target function is some element of a set of functions, C. We also assume that the hypothesis, h, is an element of a set, H, of hypotheses, which includes the set, C, of target functions.H is called the hypothesis space.In general, h won't be identical to f , but we can strive to have the value of h(X) = the value of f (X) for most X's.That is, we want h to be approximately correct.To quantify this notion, we define the error of h, ε h , as the probability that an X drawn randomly according to P will be misclassified:Boldface symbols need to be smaller when they are subscripts in math environments.We say that h is approximately (except for ε ) correct if ε h ≤ ε, where ε is the accuracy parameter.Suppose we are able to find an h that classifies all m randomly drawn training samples correctly; that is, h is consistent with this randomly selected training set, Ξ.If m is large enough, will such an h be approximately correct (and for what value of ε)?On some training occasions, using m randomly drawn training samples, such an h might turn out to be approximately correct (for a given value of ε), and on others it might not.We say that h is probably (except for δ) approximately correct (PAC) if the probability that it is approximately correct is greater than 1 − δ, where δ is the confidence parameter.We shall show that if m is greater than some bound whose value depends on ε and δ, such an h is guaranteed to be probably approximately correct.In general, we say that a learning algorithm PAC-learns functions from C in terms of H iff for every function f C, it outputs a hypothesis h H, such that with probability at least (1 − δ), ε h ≤ ε.Such a hypothesis is called probably (except for δ) approximately (except for ε) correct.We want learning algorithms that are tractable, so we want an algorithm that PAC-learns functions in polynomial time.This can only be done for certain classes of functions.If there are a finite number of hypotheses in a hypothesis set (as there are for many of the hypothesis sets we have considered), we could always produce a consistent hypothesis from this set by testing all of them against the training data.But if there are an exponential number of hypotheses, that would take exponential time.We seek training methods that produce consistent hypotheses in less time.The time complexities for various hypothesis sets have been determined, and these are summarized in a table to be presented later.A class, C, is polynomially PAC learnable in terms of H provided there exists a polynomial-time learning algorithm (polynomial in the number of samples needed, m, in the dimension, n, in 1/ε, and in 1/δ) that PAC-learns functions in C in terms of H.Initial work on PAC assumed H = C, but it was later shown that some functions cannot be polynomially PAC-learned under such an assumption (assuming P = NP)-but can be polynomially PAC-learned if H is a strict superset of C! Also our definition does not specify the distribution, P , from which patterns are drawn nor does it say anything about the properties of the learning algorithm.Since C and H do not have to be identical, we have the further restrictive definition:A properly PAC-learnable class is a class C for which there exists an algorithm that polynomially PAC-learns functions from C in terms of C.Suppose our learning algorithm selects some h randomly from among those that are consistent with the values of f on the m training patterns.The probability that the error of this randomly selected h is greater than some ε, with h consistent with the values of f (X) for m instances of X (drawn according to arbitrary P ), is less than or equal to |H|e −εm , where |H| is the number of hypotheses in H.We state this result as a theorem [Blumer, et al., 1987]: Theorem 8.1 (Blumer, et al.) Let H be any set of hypotheses, Ξ be a set of m ≥ 1 training examples drawn independently according to some distribution P , f be any classification function in H, and ε > 0.Then, the probability that there exists a hypothesis h consistent with f for the members of Ξ but with error greater than ε is at most |H|e −εm .Consider the set of all hypotheses, {h 1 , h 2 , . . ., h i , . . ., h S }, in H, where S = |H|.The error for h i is ε hi = the probability that h i will classify a pattern in error (that is, differently than f would classify it).The probability that h i will classify a pattern correctly is (1−ε hi ).A subset, H B , of H will have error greater than ε.We will call the hypotheses in this subset bad.The probability that any particular one of these bad hypotheses, say h b , would classify a pattern correctly is (1 − ε h b ).Since ε h b > ε, the probability that h b (or any other bad hypothesis) would classify a pattern correctly is less than (1 − ε).The probability that it would classify all m independently drawn patterns correctly is then less thanThat is, prob[there is a bad hypothesis that classifies all m patterns correctly]Since K ≤ |H| and (1 − ε) m ≤ e −εm , we have:prob[there is a bad hypothesis that classifies all m patterns correctly]= prob[there is a hypothesis with error > ε and that classifies all m patterns correctly] ≤ |H|e −εm .QED A corollary of this theorem is:independent samples, the probability that there exists a hypothesis in H that is consistent with f on these samples and has error greater than ε is at most δ.Proof: We are to find a bound on m that guarantees that prob[there is a hypothesis with error > ε and that classifies all m patterns correctly] ≤ δ.Thus, using the result of the theorem, we must show that |H|e −εm ≤ δ.Taking the natural logarithm of both sides yields:This corollary is important for two reasons.First it clearly states that we can select any hypothesis consistent with the m samples and be assured that with probability (1 − δ) its error will be less than ε.Also, it shows that in order for m to increase no more than polynomially with n, |H| can be no larger than 2 O(n k ) .No class larger than that can be guaranteed to be properly PAC learnable.Here is a possible point of confusion: The bound given in the corollary is an upper bound on the value of m needed to guarantee polynomial probably approximately correct learning.Values of m greater than that bound are sufficient (but might not be necessary).We will present a lower (necessary) bound later in the chapter.Terms Let H be the set of terms (conjunctions of literals).Then, |H| = 3 n , andNote that the bound on m increases only polynomially with n, 1/ε, and 1/δ.For n = 50, ε = 0.01 and δ = 0.01, m ≥ 5, 961 guarantees PAC learnability.In order to show that terms are properly PAC learnable, we additionally have to show that one can find in time polynomial in m and n a hypothesis h consistent with a set of m patterns labeled by the value of a term.The following procedure for finding such a consistent hypothesis requires O(nm) steps (adapted from [Dietterich, 1990, page 268]):We are given a training sequence, Ξ, of m examples.Find the first pattern, say X 1 , in that list that is labeled with a 1. Initialize a Boolean function, h, to the conjunction of the n literals corresponding to the values of the n components of X 1 .(Components with value 1 will have corresponding positive literals; components with value 0 will have corresponding negative literals.)If there are no patterns labeled by a 1, we exit with the null concept (h ≡ 0 for all patterns).Then, for each additional pattern, X i , that is labeled with a 1, we delete from h any Boolean variables appearing in X i with a sign different from their sign in h.After processing all the patterns labeled with a 1, we check all of the patterns labeled with a 0 to make sure that none of them is assigned value 1 by h.If, at any stage of the algorithm, any patterns labeled with a 0 are assigned a 1 by h, then there exists no term that consistently classifies the patterns in Ξ, and we exit with failure.Otherwise, we exit with h.Change this paragraph if this algorithm was presented in Chapter Three.As an example, consider the following patterns, all labeled with a 1 (from ):After processing the first pattern, we have h = x 1 x 2 x 3 x 4 ; after processing the second pattern, we have h = x 2 x 3 x 4 ; finally, after the third pattern, we have h = x 2 x 4 .Let H be the set of all linearly separable functions.Then, |H| ≤ 2 n 2 , and m ≥ (1/ε) n 2 ln 2 + ln(1/δ) Again, note that the bound on m increases only polynomially with n, 1/ε, and 1/δ.For n = 50, ε = 0.01 and δ = 0.01, m ≥ 173, 748 guarantees PAC learnability.To show that linearly separable functions are properly PAC learnable, we would have additionally to show that one can find in time polynomial in m and n a hypothesis h consistent with a set of m labeled linearly separable patterns.Linear programming is polynomial.Some properly PAC-learnable classes of functions are given in the following table .(Adapted from [Dietterich, 1990, pages 262 and 268] which also gives references to proofs of some of the time complexities.)polynomial yes (decision lists with k-sized terms) lin.sep.2 O(n 2 ) polynomial yes lin.sep.with (0,1) weights ?NP-hard no k-2NN ?NP-hard no DNF 2 2 n polynomial no (all Boolean functions) (Members of the class k-2NN are two-layer, feedforward neural networks with exactly k hidden units and one output unit.)Summary: In order to show that a class of functions is Properly PAC-Learnable : a. Show that there is an algorithm that produces a consistent hypothesis on m n-dimensional samples in time polynomial in m and n.b. Show that the sample size, m, needed to ensure PAC learnability is polynomial (or better) in (1/ε), (1/δ), and n by showing that ln |H| is polynomial or better in the number of dimensions.As hinted earlier, sometimes enlarging the class of hypotheses makes learning easier.For example, the table above shows that k-CNF is PAC learnable, but k-term-DNF is not.And yet, k-term-DNF is a subclass of k-CNF!So, even if the target function were in k-term-DNF, one would be able to find a hypothesis in k-CNF that is probably approximately correct for the target function.Similarly, linearly separable functions implemented by TLUs whose weight values are restricted to 0 and 1 are not properly PAC learnable, whereas unrestricted linearly separable functions are.It is possible that enlarging the space of hypotheses makes finding one that is consistent with the training examples easier.An interesting question is whether or not the class of functions in k-2NN is polynomially PAC learnable if the hypotheses are drawn from k -2NN with k > k.(At the time of writing, this matter is still undecided.)Although PAC learning theory is a powerful analytic tool, it (like complexity theory) deals mainly with worst-case results.The fact that the class of twolayer, feedforward neural networks is not polynomially PAC learnable is more an attack on the theory than it is on the networks, which have had many successful applications.As [Baum, 1994, page 416-17] says: " . . .humans are capable of learning in the natural world.Therefore, a proof within some model of learning that learning is not feasible is an indictment of the model.We should examine the model to see what constraints can be relaxed and made more realistic."Consider a set, H, of functions, and a set, Ξ, of (unlabeled) patterns.One measure of the expressive power of a set of hypotheses, relative to Ξ, is its ability to make arbitrary classifications of the patterns in Ξ. 1 If there are m patterns in Ξ, there are 2 m different ways to divide these patterns into two disjoint and exhaustive subsets.We say there are 2 m different dichotomies of Ξ.If Ξ were to include all of the 2 n Boolean patterns, for example, there are 2 2 n ways to dichotomize them, and (of course) the set of all possible Boolean functions dichotomizes them in all of these ways.But a subset, H, of the Boolean functions might not be able to dichotomize an arbitrary set, Ξ, of m Boolean patterns in all 2 m ways.In general (that is, even in the non-Boolean case), we say that if a subset, H, of functions can dichotomize a set, Ξ, of m patterns in all 2 m ways, then H shatters Ξ.As an example, consider a set Ξ of m patterns in the n-dimensional space, R n .(That is, the n components of these patterns are real numbers.)We define a linear dichotomy as one implemented by an (n − 1)-dimensional hyperplane in the n-dimensional space.How many linear dichotomies of m patterns in n dimensions are there?For example, as shown in Fig. 8.1, there are 14 dichotomies of four points in two dimensions (each separating line yields two dichotomies depending on whether the points on one side of the line are classified as 1 or 0).(Note that even though there are an infinite number of hyperplanes, there are, nevertheless, only a finite number of ways in which hyperplanes can dichotomize a finite number of patterns.Small movements of a hyperplane typically do not change the classifications of any patterns.)The number of dichotomies achievable by hyperplanes depends on how the patterns are disposed.For the maximum number of linear dichotomies, the points must be in what is called general position.For m > n, we say that a set of m points is in general position in an n-dimensional space if and only if no subset of (n+1) points lies on an (n−1)-dimensional hyperplane.When m ≤ n, a set of m points is in general position if no (m − 2)-dimensional hyperplane contains the set.Thus, for example, a set of m ≥ 4 points is in general position in a three-dimensional space if no four of them lie on a (two-dimensional) plane.We will denote the number of linear dichotomies of m points in general position in an n-dimensional space by the expression Π L (m, n).It is not too difficult to verify that:for m > n, and= the probability that a randomly selected dichotomy (out of the 2 m possible dichotomies of m patterns in n dimensions) will be linearly separable.In Fig. 8.2 we plot P λ(n+1),n versus λ and n, where λ = m/(n + 1).Note that for large n (say n > 30) how quickly P m,n falls from 1 to 0 as m goes above 2(n + 1).For m < 2(n + 1), any dichotomy of the m points is almost certainly linearly separable.But for m > 2(n + 1), a randomly selected dichotomy of the m points is almost certainly not linearly separable.For this reason m = 2(n + 1) is called the capacity of a TLU [Cover, 1965].Unless the number of training patterns exceeds the capacity, the fact that a TLU separates those training patterns according to their labels means nothing in terms of how well that TLU will generalize to new patterns.There is nothing special about a separation found for m < 2(n + 1) patterns-almost any dichotomy of those patterns would have been linearly separable.To make sure that the separation found is forced by the training set and thus generalizes well, it has to be the case that there are very few linearly separable functions that would separate the m training patterns.Analogous results about the generalizing abilities of neural networks have been developed by [Baum & Haussler, 1989] and given intuitive and experimental justification in [Baum, 1994, page 438]: The capacity result just presented applies to linearly separable functions for nonbinary patterns.We can extend these ideas to general dichotomies of non-binary patterns.In general, let us denote the maximum number of dichotomies of any set of m n-dimensional patterns by hypotheses in H as Π H (m, n).The number of dichotomies will, of course, depend on the disposition of the m points in the n-dimensional space; we take Π H (m, n) to be the maximum over all possible arrangements of the m points.(In the case of the class of linearly separable functions, the maximum number is achieved when the m points are in general position.)For each class, H, there will be some maximum value of m for which Π H (m, n) = 2 m , that is, for which H shatters the m patterns.This maximum number is called the Vapnik-Chervonenkis (VC) dimension and is denoted by VCdim(H) [Vapnik & Chervonenkis, 1971].We saw that for the class of linear dichotomies, VCdim(Linear) = (n + 1).As another example, let us calculate the VC dimension of the hypothesis space of single intervals on the real line-used to classify points on the real line.We show an example of how points on the line might be dichotomized by a single interval in Fig. 8.3.The set Ξ could be, for example, {0.5, 2.5, -2.3, 3.14}, and one of the hypotheses in our set would be [1, 4.5].This hypothesis would label the points 2.5 and 3.14 with a 1 and the points -2.3 and 0.5 with a 0. This set of hypotheses (single intervals on the real line) can arbitrarily classify any two points.But no single interval can classify three points such that the outer two are classified as 1 and the inner one as 0. Therefore the VC dimension of single intervals on the real line is 2. As soon as we have many more than 2 training patterns on the real line and provided we know that the classification function we are trying to guess is a single interval, then we begin to have good generalization.The VC dimension is a useful measure of the expressive power of a hypothesis set.Since any dichotomy of VCdim(H) or fewer patterns in general position in n dimensions can be achieved by some hypothesis in H, we must have many more than VCdim(H) patterns in the training set in order that a hypothesis consistent with the training set is sufficiently constrained to imply good generalization.Our examples have shown that the concept of VC dimension is not restricted to Boolean functions.• If there are a finite number, |H|, of hypotheses in H, then:• The VC dimension of terms in n dimensions is n.• Suppose we generalize our example that used a hypothesis set of single intervals on the real line.Now let us consider an n-dimensional feature space and tests of the form L i ≤ x i ≤ H i .We allow only one such test per dimension.A hypothesis space consisting of conjunctions of these tests (called axis-parallel hyper-rectangles) has VC dimension bounded by: n ≤ VCdim ≤ 2n• As we have already seen, TLUs with n inputs have a VC dimension of n + 1.• [Baum, 1994, page 438] gives experimental evidence for the proposition that " . . .multilayer [neural] nets have a VC dimension roughly equal to their total number of [adjustable] weights."There are two theorems that connect the idea of VC dimension with PAC learning [Blumer, et al., 1990].We state these here without proof.Theorem 8.3 (Blumer, et  The second of these two theorems improves the bound on the number of training patterns needed for linearly separable functions to one that is linear in n.In our previous example of how many training patterns were needed to ensure PAC learnability of a linearly separable function if n = 50, ε = 0.01, and δ = 0.01, we obtained m ≥ 173, 748.Using the Blumer, et al. result we would get m ≥ 52, 756.As another example of the second theorem, let us take H to be the set of closed intervals on the real line.The VC dimension is 2 (as shown previously).With n = 50, ε = 0.01, and δ = 0.01, m ≥ 16, 551 ensures PAC learnability.There is also a theorem that gives a lower (necessary) bound on the number of training patterns required for PAC learning [Ehrenfeucht, et al., 1988]: Theorem 8.5 Any PAC learning algorithm must examine at least Ω(1/ε lg(1/δ) + VCdim(H)) training patterns.The difference between the lower and upper bounds is O(log(1/ε)VCdim(H)/ε).To be added.Unsupervised Learning 9.1 What is Unsupervised Learning?Consider the various sets of points in a two-dimensional space illustrated in Fig. 9.1.The first set (a) seems naturally partitionable into two classes, while the second (b) seems difficult to partition at all, and the third (c) is problematic.Unsupervised learning uses procedures that attempt to find natural partitions of patterns.There are two stages:• Form an R-way partition of a set Ξ of unlabeled training patterns (where the value of R, itself, may need to be induced from the patterns).The partition separates Ξ into R mutually exclusive and exhaustive subsets, Ξ 1 , . . ., Ξ R , called clusters.• Design a classifier based on the labels assigned to the training patterns by the partition.We will explain shortly various methods for deciding how many clusters there should be and for separating a set of patterns into that many clusters.We can base some of these methods, and their motivation, on minimum-descriptionlength (MDL) principles.In that setting, we assume that we want to encode a description of a set of points, Ξ, into a message of minimal length.One encoding involves a description of each point separately; other, perhaps shorter, encodings might involve a description of clusters of points together with how each point in a cluster can be described given the cluster it belongs to.The specific techniques described in this chapter do not explicitly make use of MDL principles, but the MDL method has been applied with success.One of the MDL-based methods, Autoclass II [Cheeseman, et al., 1988]   divided into mutually exclusive and exhaustive subsets, Ξ 1 , . . ., Ξ R ; each set, Ξ i , (i = 1, . . ., R) is divided into mutually exclusive and exhaustive subsets, and so on.We show an example of such a hierarchical partition in Fig. 9.2.The hierarchical form is best displayed as a tree, as shown in Fig. 9.3.The tip nodes of the tree can further be expanded into their individual pattern elements.One application of such hierarchical partitions is in organizing individuals into taxonomic hierarchies such as those used in botany and zoology.Most of the unsupervised learning methods use a measure of similarity between patterns in order to group them into clusters.The simplest of these involves defining a distance between patterns.For patterns whose features are numeric, the distance measure can be ordinary Euclidean distance between two points in an n-dimensional space.There is a simple, iterative clustering method based on distance.It can be described as follows.Suppose we have R randomly chosen cluster seekers, C 1 , . . ., C R .These are points in an n-dimensional space that we want to adjust so that they each move toward the center of one of the clusters of patterns.We present the (unlabeled) patterns in the training set, Ξ, to the algorithm one-by-one.For each pattern, X i , presented, we find that cluster seeker, C j , that is closest to X i and move it closer to X i :where α j is a learning rate parameter for the j-th cluster seeker; it determines how far C j is moved toward X i .Refinements on this procedure make the cluster seekers move less far as training proceeds.Suppose each cluster seeker, C j , has a mass, m j , equal to the number of times that it has moved.As a cluster seeker's mass increases it moves less far towards a pattern.For example, we might set α j = 1/(1 + m j ) and use the above rule together with m j ←− m j + 1.With this adjustment rule, a cluster seeker is always at the center of gravity (sample mean) of the set of patterns toward which it has so far moved.Intuitively, if a cluster seeker ever gets within some reasonably well clustered set of patterns (and if that cluster seeker is the only one so located), it will converge to the center of gravity of that cluster.Once the cluster seekers have converged, the classifier implied by the nowlabeled patterns in Ξ can be based on a Voronoi partitioning of the space (based on distances to the various cluster seekers).This kind of classification, an example of which is shown in Fig. 9.4, can be implemented by a linear machine.Georgy Fedoseevich Voronoi, was a Russian mathematician who lived from 1868 to 1909.When basing partitioning on distance, we seek clusters whose patterns are as close together as possible.We can measure the badness, V , of a cluster of patterns, {X i }, by computing its sample variance defined by:where M is the sample mean of the cluster, which is defined to be:and K is the number of points in the cluster.We would like to partition a set of patterns into clusters such that the sum of the sample variances (badnesses) of these clusters is small.Of course if we have one cluster for each pattern, the sample variances will all be zero, so we must arrange that our measure of the badness of a partition must increase with the number of clusters.In this way, we can seek a trade-off between the variances of Elaborations of our basic cluster-seeking procedure allow the number of cluster seekers to vary depending on the distances between them and depending on the sample variances of the clusters.For example, if the distance, d ij , between two cluster seekers, C i and C j , ever falls below some threshold ε, then we can replace them both by a single cluster seeker placed at their center of gravity (taking into account their respective masses).In this way we can decrease the overall badness of a partition by reducing the number of clusters for comparatively little penalty in increased variance.On the other hand, if any of the cluster seekers, say C i , defines a cluster whose sample variance is larger than some amount δ, then we can place a new cluster seeker, C j , at some random location somewhat adjacent to C i and reset the masses of both C i and C j to zero.In this way the badness of the partition might ultimately decrease by decreasing the total sample variance with comparatively little penalty for the additional cluster seeker.The values of the parameters ε and δ are set depending on the relative weights given to sample variances and numbers of clusters.In distance-based methods, it is important to scale the components of the pattern vectors.The variation of values along some dimensions of the pattern vector may be much different than that of other dimensions.One commonly used technique is to compute the standard deviation (i.e., the square root of the variance) of each of the components over the entire training set and normalize the values of the components so that their adjusted standard deviations are equal.Suppose we have a partition of the training set, Ξ, into R mutually exclusive and exhaustive clusters, C 1 , . . ., C R .We can decide to which of these clusters some arbitrary pattern, X, should be assigned by selecting the C i for which the probability, p(C i |X), is largest, providing p(C i |X) is larger than some fixed threshold, δ.As we saw earlier, we can use Bayes rule and base our decision on maximizing p(X|C i )p(C i ).Assuming conditional independence of the pattern components, x i , the quantity to be maximized is:The p(x j |C i ) can be estimated from the sample statistics of the patterns in the clusters and then used in the above expression.(Recall the linear form that this formula took in the case of binary-valued components.)We call S(X, C i ) the similarity of X to a cluster, C i , of patterns.Thus, we assign X to the cluster to which it is most similar, providing the similarity is larger than δ.Just as before, we can define the sample mean of a cluster, C i , to be:where K i is the number of patterns in C i .We can base an iterative clustering algorithm on this measure of similarity [Mahadevan & Connell, 1992].It can be described as follows:a. Begin with a set of unlabeled patterns Ξ and an empty list, L, of clusters.b.For the next pattern, X, in Ξ, compute S(X, C i ) for each cluster, C i .(Initially, these similarities are all zero.)Suppose the largest of these similarities is S(X, C max ).(a) If S(X, C max ) > δ, assign X to C max .That is, The value of the parameter δ controls the number of clusters.If δ is high, there will be a large number of clusters with few patterns in each cluster.For small values of δ, there will be a small number of clusters with many patterns in each cluster.Similarly, the larger the value of ε, the smaller the number clusters that will be found.Designing a classifier based on the patterns labeled by the partitioning is straightforward.We assign any pattern, X, to that category that maximizes S(X, C i ).Mention "k-means and "EM" methods.Suppose we have a set, Ξ, of unlabeled training patterns.We can form a hierarchical classification of the patterns in Ξ by a simple agglomerative method.(The description of this algorithm is based on an unpublished manuscript by Pat Langley.)Our description here gives the general idea; we leave it to the reader to generate a precise algorithm.We first compute the Euclidean distance between all pairs of patterns in Ξ. (Again, appropriate scaling of the dimensions is assumed.)Suppose the smallest distance is between patterns X i and X j .We collect X i and X j into a cluster, C, eliminate X i and X j from Ξ and replace them by a cluster vector, C, equal to the average of X i and X j .Next we compute the Euclidean distance again between all pairs of points in Ξ.If the smallest distance is between pairs of patterns, we form a new cluster, C, as before and replace the pair of patterns in Ξ by their average.If the shortest distance is between a pattern, X i , and a cluster vector, C j (representing a cluster, C j ), we form a new cluster, C, consisting of the union of C j and {X i }.In this case, we replace C j and X i in Ξ by their (appropriately weighted) average and continue.If the shortest distance is between two cluster vectors, C i and C j , we form a new cluster, C, consisting of the union of C i and C j .In this case, we replace C i and C j by their (appropriately weighted) average and continue.Since we reduce the number of points in Ξ by one each time, we ultimately terminate with a tree of clusters rooted in the cluster containing all of the points in the original training set.An example of how this method aggregates a set of two dimensional patterns is shown in Fig. 9.5.The numbers associated with each cluster indicate the order in which they were formed.These clusters can be organized hierarchically in a binary tree with cluster 9 as root, clusters 7 and 8 as the two descendants of the root, and so on.A ternary tree could be formed instead if one searches for the three points in Ξ whose triangle defined by those patterns has minimal area.A probabilistic quality measure for partitionsWe can develop a measure of the goodness of a partitioning based on how accurately we can guess a pattern given only what partition it is in.Suppose we are given a partitioning of Ξ into R classes, C 1 , . . ., C R .As before, we can compute the sample statistics p(x i |C k ) which give probability values for each component given the class assigned to it by the partitioning.Suppose each component x i of X can take on the values v ij , where the index j steps over the domain of that component.We use the notationSuppose we use the following probabilistic guessing rule about the values of the components of a vector X given only that it is in class k.Guess thatThen, the probability that we guess the i-th component correctly is:The average number of (the n) components whose values are guessed correctly by this method is then given by the sum of these probabilities over all of the components of X:Given our partitioning into R classes, the goodness measure, G, of this partitioning is the average of the above expression over all classes:where p(C k ) is the probability that a pattern is in class C k .In order to penalize this measure for having a large number of classes, we divide it by R to get an overall "quality" measure of a partitioning:We give an example of the use of this measure for a trivially simple clustering of the four three-dimensional patterns shown in Fig. 9.6.There are several different partitionings.Let's evaluate Z values for the following ones: P 1 = {a, b, c, d}, P 2 = {{a, b}, {c, d}}, P 3 = {{a, c}, {b, d}}, and P 4 = {{a}, {b}, {c}, {d}}.The first, P 1 , puts all of the patterns into a single cluster.The sample probabilities p i (v i1 = 1) and p i (v i0 = 0) are all equal to 1/2 for each of the three components.Summing over the values of the components (0 and 1) gives (1/2) 2 + (1/2) 2 = 1/2.Summing over the three components gives 3/2.Averaging over all of the clusters (there is just one) also gives 3/2.Finally, dividing by the number of clusters produces the final Z value of this partition, Z(P 1 ) = 3/2.The second partition, P 2 , gives the following sample probabilities:Summing over the values of the components (0 and 1) gives (1) 2 + (0) 2 = 1 for component 1, (1/2) 2 + (1/2) 2 = 1/2 for component 2, and (1) 2 + (0) 2 = 1 for component 3. Summing over the three components gives 2 1/2 for class 1.A similar calculation also gives 2 1/2 for class 2. Averaging over the two clusters also gives 2 1/2.Finally, dividing by the number of clusters produces the final Z value of this partition, Z(P 2 ) = 1 1/4, not quite as high as Z(P 1 ).Similar calculations yield Z(P 3 ) = 1 and Z(P 4 ) = 3/4, so this method of evaluating partitions would favor placing all patterns in a single cluster.Evaluating all partitionings of m patterns and then selecting the best would be computationally intractable.The following iterative method is based on a hierarchical clustering procedure called COBWEB [Fisher, 1987].The procedure grows a tree each node of which is labeled by a set of patterns.At the end of the process, the root node contains all of the patterns in Ξ.The successors of the root node will contain mutually exclusive and exhaustive subsets of Ξ.In general, the successors of a node, η, are labeled by mutually exclusive and exhaustive subsets of the pattern set labelling node η.The tips of the tree will contain singleton sets.The method uses Z values to place patterns at the various nodes; sample statistics are used to update the Z values whenever a pattern is placed at a node.The algorithm is as follows: a.We start with a tree whose root node contains all of the patterns in Ξ and a single empty successor node.We arrange that at all times during the process every non-empty node in the tree has (besides any other successors) exactly one empty successor.b. Select a pattern X i in Ξ (if there are no more patterns to select, terminate).c. Set µ to the root node.d.For each of the successors of µ (including the empty successor!),calculate the best host for X i .A best host is determined by tentatively placing X i in one of the successors and calculating the resulting Z value for each one of these ways of accomodating X i .The best host corresponds to the assignment with the highest Z value.e.If the best host is an empty node, η, we place X i in η, generate an empty successor node of η, generate an empty sibling node of η, and go to 2.f.If the best host is a non-empty, singleton (tip) node, η, we place X i in η, create one successor node of η containing the singleton pattern that was in η, create another successor node of η containing X i , create an empty successor node of η, create empty successor nodes of the new non-empty successors of η, and go to 2.g.If the best host is a non-empty, non-singleton node, η, we place X i in η, set µ to η, and go to 4.This process is rather sensitive to the order in which patterns are presented.To make the final classification tree less order dependent, the COBWEB procedure incorporates node merging and splitting.Node merging: It may happen that two nodes having the same parent could be merged with an overall increase in the quality of the resulting classification performed by the successors of that parent.Rather than try all pairs to merge, a good heuristic is to attempt to merge the two best hosts.When such a merging improves the Z value, a new node containing the union of the patterns in the merged nodes replaces the merged nodes, and the two nodes that were merged are installed as successors of the new node.Node splitting: A heuristic for node splitting is to consider replacing the best host among a group of siblings by that host's successors.This operation is performed only if it increases the Z value of the classification performed by a group of siblings.We mention two experiments with COBWEB.In the first, the program attempted to find two categories (we will call them Class 1 and Class 2) of United States Senators based on their votes (yes or no) on six issues.After the clusters were established, the majority vote in each class was computed.These are shown in the table below.Class In the second experiment, the program attempted to classify soybean diseases based on various characteristics.COBWEB grouped the diseases in the taxonomy shown in Fig. 9.7.To be added.Chapter 10Temporal-Difference LearningIn this chapter, we consider problems in which we wish to learn to predict the future value of some quantity, say z, from an n-dimensional input pattern, X.In many of these problems, the patterns occur in temporal sequence, X 1 , X 2 , . .., X i , X i+1 , . .., X m , and are generated by a dynamical process.The components of X i are features whose values are available at time, t = i.We distinguish two kinds of prediction problems.In one, we desire to predict the value of z at time t = i + 1 based on input X i for every i.For example, we might wish to predict some aspects of tomorrow's weather based on a set of measurements made today.In the other kind of prediction problem, we desire to make a sequence of predictions about the value of z at some fixed time, say t = m + 1, based on each of the X i , i = 1, . . ., m.For example, we might wish to make a series of predictions about some aspect of the weather on next New Year's Day, based on measurements taken every day before New Year's.Sutton [Sutton, 1988] has called this latter problem, multi-step prediction, and that is the problem we consider here.In multi-step prediction, we might expect that the prediction accuracy should get better and better as i increases toward m.A training method that naturally suggests itself is to use the actual value of z at time m + 1 (once it is known) in a supervised learning procedure using a sequence of training patterns, {X 1 , X 2 , . .., X i , X i+1 , . .., X m }.That is, we seek to learn a function, f , such that f (X i ) is as close as possible to z for each i.Typically, we would need a training set, Ξ, consisting of several such sequences.We will show that a method that is better than supervised learning for some important problems is to base learning on the difference between f (X i+1 ) and f (X i ) rather than on the difference between z and f (X i ).Such methods involve what is called temporal-difference (TD) learning.We assume that our prediction, f (X), depends on a vector of modifiable weights, W. To make that dependence explicit, we write f (X, W).For supervised learning, we consider procedures of the following type: For each X i , the prediction f (X i , W) is computed and compared to z, and the learning rule (whatever it is) computes the change, (∆W i ), to be made to W.Then, taking into account the weight changes for each pattern in a sequence all at once after having made all of the predictions with the old weight vector, we change W as follows:Whenever we are attempting to minimize the squared error between z and f (X i , W) by gradient descent, the weight-changing rule for each pattern is:where c is a learning rate parameter, f i is our prediction of z, f (X i , W), at time t = i, and ∂fi ∂W is, by definition, the vector of partial derivatives ( ∂fi ∂w1 , . . ., ∂fi ∂wi , . . ., ∂fi ∂wn ) in which the w i are the individual components of W. (The expression ∂fi ∂W is sometimes written ∇ W f i .)The reader will recall that we used an equivalent expression for (∆W) i in deriving the backpropagation formulas used in training multi-layer neural networks.The Widrow-Hoff rule results when f (X, W) = X • W. Then:An interesting form for (∆W) i can be developed if we note thatwhere we define f m+1 = z.Substituting in our formula for (∆W) i yields:In this form, instead of using the difference between a prediction and the value of z, we use the differences between successive predictions-thus the phrase temporal-difference (TD) learning.In the case when f (X, W) = X • W, the temporal difference form of the Widrow-Hoff rule is:One reason for writing (∆W) i in temporal-difference form is to permit an interesting generalization as follows:where 0 < λ ≤ 1.Here, the λ term gives exponentially decreasing weight to differences later in time than t = i.When λ = 1, we have the same rule with which we began-weighting all differences equally, but as λ → 0, we weight only the (f i+1 − f i ) difference.With the λ term, the method is called TD(λ).It is interesting to compare the two extreme cases: For TD(0):For TD(1):Both extremes can be handled by the same learning mechanism; only the error term is different.In TD(0), the error is the difference between successive predictions, and in TD(1), the error is the difference between the finally revealed value of z and the prediction.Intermediate values of λ take into account differently weighted differences between future pairs of successive predictions.Only TD(1) can be considered a pure supervised learning procedure, sensitive to the final value of z provided by the teacher.For λ < 1, we have various degrees of unsupervised learning, in which the prediction function strives to make each prediction more like successive ones (whatever they might be).We shall soon see that these unsupervised procedures result in better learning than do the supervised ones for an important class of problems.We can rewrite our formula for (∆W) i , namelyto allow a type of incremental computation.First we write the expression for the weight change rule that takes into account all of the (∆W) i :Interchanging the order of the summations yields:Interchanging the indices k and i finally yields:If, as earlier, we want to use an expression of the form W ←− W+ m i=1 (∆W) i , we see that we can write:we can develop a computationally efficient recurrence equation for e i+1 as follows:Rewriting (∆W) i in these terms, we obtain:where:Quoting Sutton [Sutton, 1988, page 15] (about a different equation, but the quote applies equally well to this one):". . .this equation can be computed incrementally, because each (∆W) i depends only on a pair of successive predictions and on the [weighted] sum of all past values for ∂fi ∂W .This saves substantially on memory, because it is no longer necessary to individually remember all past values of ∂fi ∂W ."TD prediction methods [especially TD(0)] are well suited to situations in which the patterns are generated by a dynamic process.In that case, sequences of temporally presented patterns contain important information that is ignored by a conventional supervised method such as the Widrow-Hoff rule.Sutton [Sutton, 1988, page 19] gives an interesting example involving a random walk, which we repeat here.In Fig. 10.1, sequences of vectors, X, are generated as follows: We start with vector X D ; the next vector in the sequence is equally likely to be one of the adjacent vectors in the diagram.If the next vector is X C (or X E ), the next one after that is equally likely to be one of the vectors adjacent to X C (or X E ).When X B is in the sequence, it is equally likely that the sequence terminates with z = 0 or that the next vector is X C .Similarly, when X F is in the sequence, it is equally likely that the sequence terminates with z = 1 or that the next vector is X E .Thus the sequences are random, but they always start with X D .Some sample sequences are shown in the figure.This random walk is an example of a Markov process; transitions from state i to state j occur with probabilities that depend only on i and j.Given a set of sequences generated by this process as a training set, we want to be able to predict the value of z for each X in a test sequence.We assume that the learning system does not know the transition probabilities.For his experiments with this process, Sutton used a linear predictor, that is f (X, W) = X • W. The learning problem is to find a weight vector, W, that minimizes the mean-squared error between z and the predicted value of z.Given the five different values that X can take on, we have the following predictions:, where w i is the i-th component of the weight vector.(Note that the values of the predictions are not limited to 1 or 0-even though z can only have one of those values-because we are minimizing mean-squared error.)After training, these predictions will be compared with the optimal ones-given the transition probabilities.The experimental setup was as follows: ten random sequences were generated using the transition probabilities.Each of these sequences was presented in turn to a TD(λ) method for various values of λ.Weight vector increments, (∆W) i , were computed after each pattern presentation but no weight changes were made until all ten sequences were presented.The weight vector increments were summed after all ten sequences were presented, and this sum was used to change the weight vector to be used for the next pass through the ten sequences.This process was repeated over and over (using the same training sequences) until (quoting Sutton) "the procedure no longer produced any significant changes in the weight vector.For small c, the weight vector always converged in this way, and always to the same final value [for 100 different training sets of ten random sequences], independent of its initial value."(Even though, for fixed, small c, the weight vector always converged to the same vector, it might converge to a somewhat different vector for different values of c.)After convergence, the predictions made by the final weight vector are compared with the optimal predictions made using the transition probabilities.These optimal predictions are simply p(z = 1|X).We can compute these probabilities to be 1/6, 1/3, 1/2, 2/3, and 5/6 for X B , X C , X D , X E , X F , respectively.The root-mean-squared differences between the best learned predictions (over all c) and these optimal ones are plotted in Fig. 10.2 for seven different values of λ. (For each data point, the standard error is approximately σ = 0.01.)Notice that the Widrow-Hoff procedure does not perform as well as other versions of TD(λ) for λ < 1! Quoting [Sutton, 1988, page 21]: "This result contradicts conventional wisdom.It is well known that, under repeated presentations, the Widrow-Hoff procedure minimizes the RMS error between its predictions and the actual outcomes in the training set ([Widrow & Stearns, 1985]).How can it be that this optimal method peformed worse than all the TD methods for λ < 1?The answer is that the Widrow-Hoff procedure only minimizes error on the training set; it does not necessarily minimize error for future experience.[Later] we prove that in fact it is linear TD(0) that converges to what can be considered the optimal estimates for matching future experience-those consistent with the maximumlikelihood estimate of the underlying Markov process."It is possible to analyze the performance of the linear-prediction TD(λ) methods on Markov processes.We state some theorems here without proof.Theorem 10.1 (Sutton, page 24, 1988) For any absorbing Markov chain, and for any linearly independent set of observation vectors {X i } for the nonterminal states, there exists an ε > 0 such that for all positive c < ε and for any initial weight vector, the predictions of linear TD(0) (with weight updates after each sequence) converge in expected value to the optimal (maximum likelihood) predictions of the true process.Even though the expected values of the predictions converge, the predictions themselves do not converge but vary around their expected values depending on their most recent experience.Sutton conjectures that if c is made to approach 0 as training progresses, the variance of the predictions will approach 0 also.Dayan [Dayan, 1992] has extended the result of Theorem 9.1 to TD(λ) for arbitrary λ between 0 and 1. (Also see [Dayan & Sejnowski, 1994].)10.6 Intra-Sequence Weight Updating Our standard weight updating rule for TD(λ) methods is:where the weight update occurs after an entire sequence is observed.To make the method truly incremental (in analogy with weight updating rules for neural nets), it would be desirable to change the weight vector after every pattern presentation.The obvious extension is:where f i+1 is computed before making the weight change; that is, f i+1 = f (X i+1 , W i ).But that would make f i = f (X i , W i−1 ), and such a rule would make the prediction difference, namely (f i+1 − f i ), sensitive both to changes in X and changes in W and could lead to instabilities.Instead, we modify the rule so that, for every pair of predictions,This version of the rule has been used in practice with excellent results.A program called TD-gammon [Tesauro, 1992] learns to play backgammon by training a neural network via temporal-difference methods.The structure of the neural net, and its coding is as shown in Fig. 10.3.The network is trained to minimize the error between actual payoff and estimated payoff, where the actual payoff is defined to be d f = p 1 + 2p 2 − p 3 − 2p 4 , and the p i are the actual probabilities of the various outcomes as defined in the figure.. . .TD-gammon learned by using the network to select that move that results in the best predicted payoff.That is, at any stage of the game some finite set of moves is possible and these lead to the set, {X}, of new board positions.Each member of this set is evaluated by the network, and the one with the largest predicted payoff is selected if it is white's move (and the smallest if it is black's).The move is made, and the network weights are adjusted to make the predicted payoff from the original position closer to that of the resulting position.The weight adjustment procedure combines temporal-difference (TD(λ)) learning with backpropagation.If d t is the network's estimate of the payoff at time t (before a move is made), and d t+1 is the estimate at time t + 1 (after a move is made), the weight adjustment rule is:where W t is a vector of all weights in the network at time t, and ∂d k ∂W is the gradient of d k in this weight space.(For a layered, feedforward network, such as that of TD-gammon, the weight changes for the weight vectors in each layer can be expressed in the usual manner.)To make the special cases clear, recall that for TD(0), the network would be trained so that, for all t, its output, d t , for input X t tended toward its expected output, d t+1 , for input X t+1 .For TD(1), the network would be trained so that, for all t, its output, d t , for input X t tended toward the expected final payoff, d f , given that input.The latter case is the same as the Widrow-Hoff rule.After about 200,000 games the following results were obtained.TD-gammon (with 40 hidden units, λ = 0.7, and c = 0.1) won 66.2% of 10,000 games against SUN Microsystems Gammontool and 55% of 10,000 games against a neural network trained using expert moves.Commenting on a later version of TDgammon, incorporating special features as inputs, Tesauro said: "It appears to be the strongest program ever seen by this author."To be added.A "grid world," such as the one shown in Fig. 11.2 is often used to illustrate reinforcement learning.Imagine a robot initially in cell (2,3).The robot receives input vector (x 1 , x 2 ) telling it what cell it is in; it is capable of four actions, n, e, s, w moving the robot one cell up, right, down, or left, respectively.It is rewarded one negative unit whenever it bumps into the wall or into the blocked cells.For example, if the input to the robot is (1,3), and the robot chooses action w, the next input to the robot is still (1,3) and it receives a reward of −1.If the robot lands in the cell marked G (for goal), it receives a reward of +10.Let's suppose that whenever the robot lands in the goal cell and gets its reward, it is immediately transported out to some random cell, and the quest for reward continues.A policy for our robot is a specification of what action to take for every one of its inputs, that is, for every one of the cells in the grid.For example, a component of such a policy would be "when in cell (3,1), move right."An optimal policy is a policy that maximizes long-term reward.One way of displaying a policy for our grid-world robot is by an arrow in each cell indicating the direction the robot should move when in that cell.In Fig. 11.3, we show an optimal policy displayed in this manner.In this chapter we will describe methods for learning optimal policies based on reward values received by the learner.In delayed reinforcement learning, one often assumes that rewards in the distant future are not as valuable as are more immediate rewards.This preference can be accomodated by a temporal discount factor, 0 ≤ γ < 1.The present value of a reward, r i , occuring i time units in the future, is taken to be γ i r i .Suppose we have a policy π(X) that maps input vectors into actions, and let r π(X) i be the reward that will be received on the i-th time step after one begins executing policy π starting in state X.Then the total reward accumulated over all time steps by policy π beginning in state X is:One reason for using a temporal discount factor is so that the above sum will be finite.An optimal policy is one that maximizes V π (X) for all inputs, X.In general, we want to consider the case in which the rewards, r i , are random variables and in which the effects of actions on environmental states are random.In Markovian environments, for example, the probability that action a in state X i will lead to state X j is given by a transition probability p[X j |X i , a].Then, we will want to maximize expected future reward and would define V π (X) as:In either case, we call V π (X) the value of policy π for input X.If the action prescribed by π taken in state X leads to state X (randomly according to the transition probabilities), then we can write V π (X) in terms of V π (X ) as follows:where (in summary): γ = the discount factor, V π (X) = the value of state X under policy π, r[X, π(X)] = the expected immediate reward received when we execute the action prescribed by π in state X, and p[X |X, π(X)] = the probability that the environment transitions to state X when we execute the action prescribed by π in state X.In other words, the value of state X under policy π is the expected value of the immediate reward received when executing the action recommended by π plus the average value (under π) of all of the states accessible from X.For an optimal policy, π * (and no others!),we have the famous "optimality equation:"The theory of dynamic programming (DP) [Bellman, 1957, Ross, 1983 assures us that there is at least one optimal policy, π * , that satisfies this equation.DP also provides methods for calculating V π * (X) and at least one π * , assuming that we know the average rewards and the transition probabilities.If we knew the transition probabilities, the average rewards, and V π * (X) for all X and a, then it would be easy to implement an optimal policy.We would simply select that a that maximizes r(X, a) + γBut, of course, we are assuming that we do not know these average rewards nor the transition probabilities, so we have to find a method that effectively learns them.If we had a model of actions, that is, if we knew for every state, X, and action a, which state, X resulted, then we could use a method called value iteration to find an optimal policy.Value iteration works as follows: We begin by assigning, randomly, an estimated value V (X) to every state, X.On the i-th step of the process, suppose we are at state X i (that is, our input on the i-th step is X i ), and that the estimated value of state X i on the i-th step is Vi (X i ).We then select that action a that maximizes the estimated value of the predicted subsequent state.Suppose this subsequent state having the highest estimated value is X i .Then we update the estimated value, Vi (X i ), of state X i as follows:We see that this adjustment moves the value of Vi (X i ) an increment (depending on c i ) closer to r i + γ Vi (X i ) .Assuming that Vi (X i ) is a good estimate for V i (X i ), then this adjustment helps to make the two estimates more consistent.Providing that 0 < c i < 1 and that we visit each state infinitely often, this process of value iteration will converge to the optimal values.Discuss synchronous dynamic programming, asynchronous dynamic programming, and policy iteration.Watkins [Watkins, 1989] has proposed a technique that he calls incremental dynamic programming.Let a; π stand for the policy that chooses action a once, and thereafter chooses actions according to policy π.We define:• adjusts its Q i−1 values using a learning factor c i , according to:is the best the agent thinks it can do from state X . . . .The initial Q values, Q 0 (X, a), for all states and actions are assumed given."Using the current Q values, Q i (X, a), the agent always selects that action that maximizes Q i (X, a).Note that only the Q value corresponding to the state just exited and the action just taken is adjusted.And that Q value is adjusted so that it is closer (by an amount determined by c i ) to the sum of the immediate reward plus the discounted maximum (over all actions) of the Q values of the state just entered.If we imagine the Q values to be predictions of ultimate (infinite horizon) total reward, then the learning procedure described above is exactly a TD(0) method of learning how to predict these Q values.Q learning strengthens the usual TD methods, however, because TD (applied to reinforcement problems using value iteration) requires a one-step lookahead, using a model of the effects of actions, whereas Q learning does not.A convenient notation (proposed by [Schwartz, 1993]) for representing the change in Q value is:where Q(X, a) is the new Q value for input X and action a, r is the immediate reward when action a is taken in response to input X, V (X ) is the maximum (over all actions) of the Q value of the state next reached when action a is taken from state X, and β is the fraction of the way toward which the new Q value, Q(X, a), is adjusted to equal r + γV (X ).Watkins and Dayan [Watkins & Dayan, 1992] prove that, under certain conditions, the Q values computed by this learning procedure converge to optimal ones (that is, to ones on which an optimal policy can be based).We define n i (X, a) as the index (episode number) of the i-th time that action a is tried in state X.Then, we have: Theorem 11.1 (Watkins and Dayan) For Markov problems with states {X} and actions {a}, and given bounded rewards |r n | ≤ R, learning rates 0 ≤ c n < 1, andfor all X and a, thenas n → ∞, for all X and a, with probability 1, where Q * n (X, a) corresponds to the Q values of an optimal policy.Again, we quote from [Watkins & Dayan, 1992, page 281]:"The most important condition implicit in the convergence theorem . . . is that the sequence of episodes that forms the basis of learning must include an infinite number of episodes for each starting state and action.This may be considered a strong condition on the way states and actions are selected-however, under the stochastic conditions of the theorem, no method could be guaranteed to find an optimal policy under weaker conditions.Note, however, that the episodes need not form a continuous sequence-that is the X of one episode need not be the X of the next episode."The relationships among Q learning, dynamic programming, and control are very well described in [Barto, Bradtke, & Singh, 1994].Q learning is best thought of as a stochastic approximation method for calculating the Q values.Although the definition of the optimal Q values for any state depends recursively on expected values of the Q values for subsequent states (and on the expected values of rewards), no expected values are explicitly computed by the procedure.Instead, these values are approximated by iterative sampling using the actual stochastic mechanism that produces successor states.11.5 Discussion, Limitations, and Extensions of Q-LearningThe Q-learning procedure requires that we maintain a table of Q(X, a) values for all state-action pairs.In the grid world that we described earlier, such a table would not be excessively large.We might start with random entries in the table; a portion of such an intial table might be as follows:Suppose the robot is in cell (2,3).The maximum Q value occurs for a = w, so the robot moves west to cell (1,3)-receiving no immediate reward.The maximum Q value in cell (1,3) is 5, and the learning mechanism attempts to make the value of Q((2, 3), w) closer to the discounted value of 5 plus the immediate reward (which was 0 in this case).With a learning rate parameter c = 0.5 and γ = 0.9, the Q value of Q((2, 3), w) is adjusted from 7 to 5.75.No other changes are made to the table at this episode.The reader might try this learning procedure on the grid world with a simple computer program.Notice that an optimal policy might not be discovered if some cells are not visited nor some actions not tried frequently enough.The learning problem faced by the agent is to associate specific actions with specific input patterns.Q learning gradually reinforces those actions that contribute to positive rewards by increasing the associated Q values.Typically, as in this example, rewards occur somewhat after the actions that lead to themhence the phrase delayed-reinforcement learning.One can imagine that better and better approximations to the optimal Q values gradually propagate back from states producing rewards toward all of the other states that the agent frequently visits.With random Q values to begin, the agent's actions amount to a random walk through its space of states.Only when this random walk happens to stumble into rewarding states does Q learning begin to produce Q values that are useful, and, even then, the Q values have to work their way outward from these rewarding states.The general problem of associating rewards with state-action pairs is called the temporal credit assignment problem-how should credit for a reward be apportioned to the actions leading up to it?Q learning is, to date, the most successful technique for temporal credit assignment, although a related method, called the bucket brigade algorithm, has been proposed by [Holland, 1986].Learning problems similar to that faced by the agent in our grid world have been thoroughly studied by Sutton who has proposed an architecture, called DYNA, for solving them [Sutton, 1990].DYNA combines reinforcement learning with planning.Sutton characterizes planning as learning in a simulated world that models the world that the agent inhabits.The agent's model of the world is obtained by Q learning in its actual world, and planning is accomplished by Q learning in its model of the world.We should note that the learning problem faced by our grid-world robot could be modified to have several places in the grid that give positive rewards.This possibility presents an interesting way to generalize the classical notion of a "goal" in AI planning systems-even in those that do no learning.Instead of representing a goal as a condition to be achieved, we represent a "goal structure" as a set of rewards to be given for achieving various conditions.Then, the generalized "goal" becomes maximizing discounted future reward instead of simply achieving some particular condition.This generalization can be made to encompass so-called goals of maintenance and goals of avoidance.The example presented above included avoiding bumping into the grid-world boundary.A goal of maintenance, of a particular state, could be expressed in terms of a reward that was earned whenever the agent was in that state and performed an action that transitioned back to that state in one step.When the next pattern presentation in a sequence of patterns is the one caused by the agent's own action in response to the last pattern, we have what is called an on-line learning method.In Watkins and Dayan's terminology, in on-line learning the episodes form a continous sequence.As already mentioned, the convergence theorem for Q learning does not require on-line learning; indeed, special precautions must be taken to ensure that on-line learning meets the conditions of the theorem.If on-line learning discovers some good paths to rewards, the agent may fixate on these and never discover a policy that leads to a possibly greater long-term reward.In reinforcement learning phraseology, this problem is referred to as the problem of exploitation (of already learned behavior) versus exploration (of possibly better behavior).One way to force exploration is to perform occasional random actions (instead of that single action prescribed by the current Q values).For example, in the grid-world problem, one could imagine selecting an action randomly according to a probability distribution over the actions (n, e, s, and w).This distribution, in turn, could depend on the Q values.For example, we might first find that action prescribed by the Q values and then choose that action with probability 1/2, choose the two orthogonal actions with probability 3/16 each, and choose the opposite action with probability 1/8.This policy might be modified by "simulated annealing" which would gradually increase the probability of the action prescribed by the Q values more and more as time goes on.This strategy would favor exploration at the beginning of learning and exploitation later.Other methods, also, have been proposed for dealing with exploration, including making unvisited states intrinsically rewarding and using an "interval estimate," which is related to the uncertainty in the estimate of a state's value [Kaelbling, 1993].For large problems it would be impractical to maintain a table like that used in our grid-world example.Various researchers have suggested mechanisms for computing Q values, given pattern inputs and actions.One method that suggests itself is to use a neural network.For example, consider the simple linear machine shown in Fig. 11.4.X . . .Such a neural net could be used by an agent that has R actions to select from.The Q values (as a function of the input pattern X and the action a i ) are computed as dot products of weight vectors (one for each action) and the input vector.Weight adjustments are made according to a TD(0) procedure to bring the Q value for the action last selected closer to the sum of the immediate reward (if any) and the (discounted) maximum Q value for the next input pattern.If the optimum Q values for the problem (whatever they might be) are more complex than those that can be computed by a linear machine, a layered neural network might be used.Sigmoid units in the final layer would compute Q values in the range 0 to 1.The TD(0) method for updating Q values would then have to be combined with a multi-layer weight-changing rule, such as backpropagation.Networks of this sort are able to aggregate different input vectors into regions for which the same action should be performed.This kind of aggregation is an example of what has been called structural credit assignment.Combining TD(λ) and backpropagation is a method for dealing with both the temporal and the structural credit assignment problems.Interesting examples of delayed-reinforcement training of simulated and actual robots requiring structural credit assignment have been reported by [Lin, 1992, Mahadevan & Connell, 1992.So far, we have identified the input vector, X, with the actual state of the environment.When the input vector results from an agent's perceptual apparatus (as we assume it does), there is no reason to suppose that it uniquely identifies the environmental state.Because of inevitable perceptual limitations, several different environmental states might give rise to the same input vector.This phenomenon has been referred to as perceptual aliasing.With perceptual aliasing, we can no longer guarantee that Q learning will result in even useful action policies, let alone optimal ones.Several researchers have attempted to deal with this problem using a variety of methods including attempting to model "hidden" states by using internal memory [Lin, 1993].That is, if some aspect of the environment cannot be sensed currently, perhaps it was sensed once and can be remembered by the agent.When such is the case, we no longer have a Markov problem; that is, the next X vector, given any action, may depend on a sequence of previous ones rather than just the immediately preceding one.It might be possible to reinstate a Markov framework (over the X's) if X includes not only current sensory precepts but information from the agent's memory.Several difficulties have so far prohibited wide application of reinforcement learning to large problems.(The TD-gammon program, mentioned in the last chapter, is probably unique in terms of success on a high-dimensional problem.)We have already touched on some difficulties; these and others are summarized below with references to attempts to overcome them.a. Exploration versus exploitation.• use random actions • use a hierarchy of actions; learn primitive actions first and freeze the useful sequences into macros and then learn how to use the macros• employ a teacher; use graded "lessons"-starting near the rewards and then backing away, and use examples of good behavior [Lin, 1992] • use more efficient computations; e.g.do several updates per episode [Moore & Atkeson, 1993] c.Large state spaces• use hand-coded features• use neural networks• use nearest-neighbor methods [Moore, 1990] d.Temporal discounting problems.Using small γ can make the learner too greedy for present rewards and indifferent to the future; but using large γ slows down learning.• use a learning method based on average rewards [Schwartz, 1993] e.No "transfer" of learning .What is learned depends on the reward structure; if the rewards change, learning has to start over.• Separate the learning into two parts: learn an "action model" which predicts how actions change states (and is constant over all problems), and then learn the "values" of states by reinforcement learning for each different set of rewards.Sometimes the reinforcement learning part can be replaced by a "planner" that uses the action model to produce plans to achieve goals.Also see other articles in the special issue on reinforcement learning: Machine Learning, 8, May, 1992.To be added.Explanation-Based LearningIn the learning methods studied so far, typically the training set does not exhaust the version space.Using logical terminology, we could say that the classifier's output does not logically follow from the training set.In this sense, these methods are inductive.In logic, a deductive system is one whose conclusions logically follow from a set of input facts, if the system is sound. 1 To contrast inductive with deductive systems in a logical setting, suppose we have a set of facts (the training set) that includes the following formulas: {Round(Obj1), Round(Obj2), Round(Obj3), Round(Obj4), Ball(Obj1), Ball(Obj2), Ball(Obj3), Ball(Obj4)} A learning system that forms the conclusion (∀x)[Ball(x) ⊃ Round(x)] is inductive.This conclusion may be useful (if there are no facts of the form Ball(σ) ∧ ¬Round(σ)), but it does not logically follow from the facts.On the other hand, if we had the facts Green(Obj5) and Green(Obj5) ⊃ Round(Obj5), then we could logically conclude Round(Obj5).Making this conclusion and saving it is an instance of deductive learning-a topic we study in this chapter.Suppose that some logical proposition, φ, logically follows from some set of facts, ∆.Under what circumstances might we say that the process of deducing φ from ∆ results in our learning φ?In a sense, we implicitly knew φ all along, since it was inherent in knowing ∆.Yet, φ might not be obvious given ∆, and the deduction process to establish φ might have been arduous.Rather than have to deduce φ again, we might want to save it, perhaps along with its deduction, in case it is needed later.Shouldn't that process count as learning?Dietterich  has called this type of learning speed-up learning.Strictly speaking, speed-up learning does not result in a system being able to make decisions that, in principle, could not have been made before the learning took place.Speed-up learning simply makes it possible to make those decisions more efficiently.But, in practice, this type of learning might make possible certain decisions that might otherwise have been infeasible.To take an extreme case, a chess player can be said to learn chess even though optimal play is inherent in the rules of chess.On the surface, there seems to be no real difference between the experience-based hypotheses that a chess player makes about what constitutes good play and the kind of learning we have been studying so far.As another example, suppose we are given some theorems about geometry and are asked to prove that the sum of the angles of a right triangle is 180 degrees.Let us further suppose that the proof we constructed did not depend on the given triangle being a right triangle; in that case we can learn a more general fact.The learning technique that we are going to study next is related to this example.It is called explanation-based learning (EBL).EBL can be thought of as a process in which implicit knowledge is converted into explicit knowledge.In EBL, we specialize parts of a domain theory to explain a particular example, then we generalize the explanation to produce another element of the domain theory that will be useful on similar examples.This process is illustrated in Fig. 12.1.Two types of information were present in the inductive methods we have studied: the information inherent in the training samples and the information about the domain that is implied by the "bias" (for example, the hypothesis set from which we choose functions).The learning methods are successful only if the hypothesis set is appropriate for the problem.Typically, the smaller the hypothesis set (that is, the more a priori information we have about the function being sought), the less dependent we are on information being supplied by a training set (that is, fewer samples).A priori information about a problem can be expressed in several ways.The methods we have studied so far restrict the hypotheses in a rather direct way.A less direct method involves making assertions in a logical language about the property we are trying to learn.A set of such assertions is usually called a "domain theory."Suppose, for example, that we wanted to classify people according to whether or not they were good credit risks.We might represent a person by a set of properties (income, marital status, type of employment, etc.), assemble such data about people who are known to be good and bad credit risks and train a classifier to make decisions.Or, we might go to a loan officer of a bank, ask him or her what sorts of things s/he looks for in making a decision about a loan, encode this knowledge into a set of rules for an expert system, and then use the expert system to make decisions.The knowledge used by the loan officer might have originated as a set of "policies" (the domain theory), but perhaps the application of these policies were specialized and made more efficient through experience with the special cases of loans made in his or her district.To make our discussion more concrete, let's consider the following fanciful example.We want to find a way to classify robots as "robust" or not.The attributes that we use to represent a robot might include some that are relevant to this decision and some that are not.We are also told that Robust(N um5) is true, but we nevertheless attempt to find a proof of that assertion using these facts about Num5 and the domain theory.The facts about Num5 correspond to the features that we might use to represent Num5.In this example, not all of them are relevant to a decision about Robust(N um5).The relevant ones are those used or needed in proving Robust(N um5) using the domain theory.The proof tree in Fig. 12.2 is one that a typical theorem-proving system might produce.In the language of EBL, this proof is an explanation for the fact Robust(N um5).We see from this explanation that the only facts about Num5 that were used were Robot(N um5) and R2D2(N um5).In fact, we could construct the following rule from this explanation: Robot(N um5) ∧ R2D2(N um5) ⊃ Robust(N um5)The explanation has allowed us to prune some attributes about Num5 that are irrelevant (at least for deciding Robust(N um5)).This type of pruning is the first sense in which an explanation is used to generalize the classification problem.([DeJong & Mooney, 1986] call this aspect of explanation-based learning feature elimination.)But the rule we extracted from the explanation applies only to Num5.There might be little value in learning that rule since it is so specific.Can it be generalized so that it can be applied to other individuals as well?Examination of the proof shows that the same proof structure, using the same sentences from the domain theory, could be used independently of whether we are talking about Num5 or some other individual.We can generalize the proof by a process that replaces constants in the tip nodes of the proof tree with variables and works upward-using unification to constrain the values of variables as needed to obtain a proof.In this example, we replace Robot(N um5) by Robot(r) and R2D2(N um5) by R2D2(s) and redo the proof-using the explanation proof as a template.Note that we use different values for the two different occurrences of N um5 at the tip nodes.Doing so sometimes results in more general, but nevertheless valid rules.We now apply the rules used in the proof in the forward direction, keeping track of the substitutions imposed by the most general unifiers used in the proof.(Note that we always substitute terms that are already in the tree for variables in rules.)This process results in the generalized proof tree shown in Fig. 12.3.Note that the occurrence of Sees(r, r) as a node in the tree forces the unification of x with y in the domain rule, Sees(x, y) ∧ Habile(y) ⊃ F ixes(x, y).The substitutions are then applied to the variables in the tip nodes and the root node to yield the general rule: Robot(r) ∧ R2D2(r) ⊃ Robust(r).This rule is the end result of EBL for this example.The process by which N um5 in this example was generalized to a variable is what [DeJong & Mooney, 1986] call identity elimination (the precise identity of Num5 turned out to be irrelevant).(The generalization process described in this example is based on that of [DeJong & Mooney, 1986] and differs from that of [Mitchell, et al., 1986].It is also similar to that used in [Fikes, et al., 1972].)Clearly, under certain assumptions, this general rule is more easily used to conclude Robust about an individual than the original proof process was.It is important to note that we could have derived the general rule from the domain theory without using the example.(In the literature, doing so is called static analysis [Etzioni, 1991].)In fact, the example told us nothing new other than what it told us about Num5.The sole role of the example in this instance of EBL was to provide a template for a proof to help guide the generalization process.Basing the generalization process on examples helps to insure that we learn rules matched to the distribution of problems that occur.There are a number of qualifications and elaborations about EBL that need to be mentioned.The domain theory includes a number of predicates other than the one occuring in the formula we are trying to prove and other than those that might customarily be used to describe an individual.One might note, for example, that if we used Habile(N um5) to describe Num5, the proof would have been shorter.Why didn't we?The situation is analogous to that of using a data base augmented by logical rules.In the latter application, the formulas in the actual data base are "extensional," and those in the logical rules are "intensional."This usage reflects the fact that the predicates in the data base part are defined by their extension-we explicitly list all the tuples sastisfying a relation.The logical rules serve to connect the data base predicates with higher level abstractions that are described (if not defined) by the rules.We typically cannot look up the truth values of formulas containing these intensional predicates; they have to be derived using the rules and the database.The EBL process assumes something similar.The domain theory is useful for connecting formulas that we might want to prove with those whose truth values can be "looked up" or otherwise evaluated.In the EBL literature, such formulas satisfy what is called the operationality criterion.Perhaps another analogy might be to neural networks.The evaluable predicates correspond to the components of the input pattern vector; the predicates in the domain theory correspond to the hidden units.Finding the new rule corresponds to finding a simpler expression for the formula to be proved in terms only of the evaluable predicates.Examining the domain theory for our example reveals that an alternative rule might have been: Robot(u) ∧ C3P O(u) ⊃ Robust(u).Such a rule might have resulted if we were given {C3P O(N um6), Robot(N um6), . ..} and proved Robust(N um6).After considering these two examples (Num5 and Num6), the question arises, do we want to generalize the two rules to something like: Robot(u) ∧ [C3P O(u) ∨ R2D2(u)] ⊃ Robust(u)?Doing so is an example of what [DeJong & Mooney, 1986] call structural generalization (via disjunctive augmentation ).Adding disjunctions for every alternative proof can soon become cumbersome and destroy any efficiency advantage of EBL.In our example, the efficiency might be retrieved if there were another evaluable predicate, say, Bionic(u) such that the domain theory also contained R2D2(x) ⊃ Bionic(x) and C3P O(x) ⊃ Bionic(x).After seeing a number of similar examples, we might be willing to induce the formula Bionic(u) ⊃ [C3P O(u) ∨ R2D2(u)] in which case the rule with the disjunction could be replaced with Robot(u) ∧ Bionic(u) ⊃ Robust(u).It is well known in theorem proving that the complexity of finding a proof depends both on the number of formulas in the domain theory and on the depth of the shortest proof.Adding a new rule decreases the depth of the shortest proof but it also increases the number of formulas in the domain theory.In realistic applications, the added rules will be relevant for some tasks and not for others.Thus, it is unclear whether the overall utility of the new rules will turn out to be positive.EBL methods have been applied in several settings, usually with positive utility.(See [Minton, 1990] for an analysis).There have been several applications of EBL methods.We mention two here, namely the formation of macro-operators in automatic plan generation and learning how to control search.In automatic planning systems, efficiency can sometimes be enhanced by chaining together a sequence of operators into macro-operators.We show an example of a process for creating macro-operators based on techniques explored by [Fikes, et al., 1972].Referring to Fig. 12.4, consider the problem of finding a plan for a robot in room R1 to fetch a box, B1, by going to an adjacent room, R2, and pushing it back to R1.The goal for the robot is IN ROOM (B1, R1), and the facts that are true in the initial state are listed in the figure.For TD(0) and linear predictors, the rule is:The rule is implemented as follows: a. Initialize the weight vector, W, arbitrarily.b.For i = 1, ..., m, do:(We compute f i anew each time through rather than use the value of f i+1 the previous time through.)(b)(If f i were computed again with this changed weight vector, its value would be closer to f i+1 as desired.)The linear TD(0) method can be regarded as a technique for training a very simple network consisting of a single dot product unit (and no threshold or sigmoid function).TD methods can also be used in combination with backpropagation to train neural networks.For TD(0) we change the network weights according to the expression:∂f i ∂W The only change that must be made to the standard backpropagation weightchanging rule is that the difference term between the desired output and the output of the unit in the final (k-th) layer, namely (d − f (k) ), must be replaced by a difference term between successive outputs, (f i+1 − f i ).This change has a direct effect only on the expression for δ (k) which becomes:where f (k) and f (k) are two successive outputs of the network.The weight changing rule for the i-th weight vector in the j-th layer of weights has the same form as before, namely:where the δ (j) i are given recursively by:is the l-th component of the i-th weight vector in the (j + 1)-th layer of weights.Of course, here also it is assumed that f (k) and f (k) are computed using the same weights and then the weights are changed.In the next section we shall see an interesting example of this application of TD learning.Delayed-Reinforcement LearningImagine a robot that exists in an environment in which it can sense and act.Suppose (as an extreme case) that it has no idea about the effects of its actions.That is, it doesn't know how acting will change its sensory inputs.Along with its sensory inputs are "rewards," which it occasionally receives.How should it choose its actions so as to maximize its rewards over the long run?To maximize rewards, it will need to be able to predict how actions change inputs, and in particular, how actions lead to rewards.We formalize the problem in the following way: The robot exists in an environment consisting of a set, S, of states.We assume that the robot's sensory apparatus constructs an input vector, X, from the environment, which informs the robot about which state the environment is in.For the moment, we will assume that the mapping from states to vectors is one-to-one, and, in fact, will use the notation X to refer to the state of the environment as well as to the input vector.When presented with an input vector, the robot decides which action from a set, A, of actions to perform.Performing the action produces an effect on the environment-moving it to a new state.The new state results in the robot perceiving a new input vector, and the cycle repeats.We assume a discrete time model; the input vector at time t = i is X i , the action taken at that time is a i , and the expected reward, r i , received at t = i depends on the action taken and on the state, that is r i = r(X i , a i ).The learner's goal is to find a policy, π(X), that maps input vectors to actions in such a way that maximizes rewards accumulated over time.This type of learning is called reinforcement learning.The learner must find the policy by trial and error; it has no initial knowledge of the effects of its actions.The situation is as shown in Fig. 11.1.Then the optimal value from state X is given by:This equation holds only for an optimal policy, π * .The optimal policy is given by:Note that if an action a makes Q π (X, a) larger than V π (X), then we can improve π by changing it so that π(X) = a.Making such a change is the basis for a powerful learning rule that we shall describe shortly.Suppose action a in state X leads to state X .Then using the definitions of Q and V , it is easy to show that:where r(X, a) is the average value of the immediate reward received when we execute action a in state X.For an optimal policy (and no others), we have another version of the optimality equation in terms of Q values:for all actions, a, and states, X.Now, if we had the optimal Q values (for all a and X), then we could implement an optimal policy simply by selecting that action that maximized r(X, a)Watkins' proposal amounts to a TD(0) method of learning the Q values.We quote (with minor notational changes) from [Watkins & Dayan, 1992, page 281]: "In Q-Learning, the agent's experience consists of a sequence of distinct stages or episodes.In the i-th episode, the agent:• observes its current state X i ,• selects [using the method described below] and performs an action a i ,• observes the subsequent state X i ,• receives an immediate reward r i , and Suppose we have a domain theory of logical sentences that taken together, help to define whether or not a robot can be classified as robust.(The same domain theory may be useful for several other purposes also, but among other things, it describes the concept "robust.")In this example, let's suppose that our domain theory includes the sentences: . . .(By convention, variables are assumed to be universally quantified.)We could use theorem-proving methods operating on this domain theory to conclude whether certain robots are robust.These methods might be computationally quite expensive because extensive search may have to be performed to derive a conclusion.But after having found a proof for some particular robot, we might be able to derive some new sentence whose use allows a much faster conclusion.We next show how such a new rule might be derived in this example.Suppose we are given a number of facts about Num5, such as: Saving this specific plan, valid only for the specific constants it mentions, would not be as useful as would be saving a more general one.We first generalize these preconditions by substituting variables for constants.We then follow the structure of the specific plan to produce the generalized plan shown in Fig. 12.6 that achieves IN ROOM (b1, r4).Note that the generalized plan does not require pushing the box back to the place where the robot started.The preconditions for the generalized plan are: PRODIGY keeps statistics on how often these learned rules are used, their savings (in time to find plans), and their cost of application.It saves only the rules whose utility, thus measured, is judged to be high.Minton [Minton, 1990] has shown that there is an overall advantage of using these rules (as against not having any rules and as against hand-coded search control rules).To be added.Delayed-Reinforcement LearningImagine a robot that exists in an environment in which it can sense and act.Suppose (as an extreme case) that it has no idea about the effects of its actions.That is, it doesn't know how acting will change its sensory inputs.Along with its sensory inputs are "rewards," which it occasionally receives.How should it choose its actions so as to maximize its rewards over the long run?To maximize rewards, it will need to be able to predict how actions change inputs, and in particular, how actions lead to rewards.We formalize the problem in the following way: The robot exists in an environment consisting of a set, S, of states.We assume that the robot's sensory apparatus constructs an input vector, X, from the environment, which informs the robot about which state the environment is in.For the moment, we will assume that the mapping from states to vectors is one-to-one, and, in fact, will use the notation X to refer to the state of the environment as well as to the input vector.When presented with an input vector, the robot decides which action from a set, A, of actions to perform.Performing the action produces an effect on the environment-moving it to a new state.The new state results in the robot perceiving a new input vector, and the cycle repeats.We assume a discrete time model; the input vector at time t = i is X i , the action taken at that time is a i , and the expected reward, r i , received at t = i depends on the action taken and on the state, that is r i = r(X i , a i ).The learner's goal is to find a policy, π(X), that maps input vectors to actions in such a way that maximizes rewards accumulated over time.This type of learning is called reinforcement learning.The learner must find the policy by trial and error; it has no initial knowledge of the effects of its actions.The situation is as shown in Fig. 11.1.Then the optimal value from state X is given by:This equation holds only for an optimal policy, π * .The optimal policy is given by:Note that if an action a makes Q π (X, a) larger than V π (X), then we can improve π by changing it so that π(X) = a.Making such a change is the basis for a powerful learning rule that we shall describe shortly.Suppose action a in state X leads to state X .Then using the definitions of Q and V , it is easy to show that:where r(X, a) is the average value of the immediate reward received when we execute action a in state X.For an optimal policy (and no others), we have another version of the optimality equation in terms of Q values:for all actions, a, and states, X.Now, if we had the optimal Q values (for all a and X), then we could implement an optimal policy simply by selecting that action that maximized r(X, a)Watkins' proposal amounts to a TD(0) method of learning the Q values.We quote (with minor notational changes) from [Watkins & Dayan, 1992, page 281]: "In Q-Learning, the agent's experience consists of a sequence of distinct stages or episodes.In the i-th episode, the agent:• observes its current state X i ,• selects [using the method described below] and performs an action a i ,• observes the subsequent state X i ,• receives an immediate reward r i , and Suppose we have a domain theory of logical sentences that taken together, help to define whether or not a robot can be classified as robust.(The same domain theory may be useful for several other purposes also, but among other things, it describes the concept "robust.")In this example, let's suppose that our domain theory includes the sentences: . . .(By convention, variables are assumed to be universally quantified.)We could use theorem-proving methods operating on this domain theory to conclude whether certain robots are robust.These methods might be computationally quite expensive because extensive search may have to be performed to derive a conclusion.But after having found a proof for some particular robot, we might be able to derive some new sentence whose use allows a much faster conclusion.We next show how such a new rule might be derived in this example.Suppose we are given a number of facts about Num5, such as: Saving this specific plan, valid only for the specific constants it mentions, would not be as useful as would be saving a more general one.We first generalize these preconditions by substituting variables for constants.We then follow the structure of the specific plan to produce the generalized plan shown in Fig. 12.6 that achieves IN ROOM (b1, r4).Note that the generalized plan does not require pushing the box back to the place where the robot started.The preconditions for the generalized plan are: PRODIGY keeps statistics on how often these learned rules are used, their savings (in time to find plans), and their cost of application.It saves only the rules whose utility, thus measured, is judged to be high.Minton [Minton, 1990] has shown that there is an overall advantage of using these rules (as against not having any rules and as against hand-coded search control rules).To be added.