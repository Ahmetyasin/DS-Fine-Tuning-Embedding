. I reply to everything eventually.4. Buy a copy.I took 18 months completely off work to write this book and ideally I'd like to make minimum wage (or better) for this time.Also, I'd like to write a second edition, but I need to sell enough copies to do this.Thanks!The most recent version of this document can be found at http://udlbook.com.The history of deep learning is unusual in science.The perseverance of a small cabal of scientists, working over twenty-five years in a seemingly unpromising area, has revolutionized a field and dramatically impacted society.Usually, when researchers investigate an esoteric and apparently impractical corner of science or engineering, it remains just that -esoteric and impractical.However, this was a notable exception.Despite widespread skepticism, the systematic efforts of Yoshua Bengio, Geoffrey Hinton, Yann LeCun, and others eventually paid off.The title of this book is "Understanding Deep Learning" to distinguish it from volumes that cover coding and other practical aspects.This text is primarily about the ideas that underlie deep learning.The first part of the book introduces deep learning models and discusses how to train them, measure their performance, and improve this performance.The next part considers architectures that are specialized to images, text, and graph data.These chapters require only introductory linear algebra, calculus, and probability and should be accessible to any second-year undergraduate in a quantitative discipline.Subsequent parts of the book tackle generative models and reinforcement learning.These chapters require more knowledge of probability and calculus and target more advanced students.The title is also partly a joke -no-one really understands deep learning at the time of writing.Modern deep networks learn piecewise linear functions with more regions than there are atoms in the universe and can be trained with fewer data examples than model parameters.It is neither obvious that we should be able to fit these functions reliably nor that they should generalize well to new data.The penultimate chapter addresses these and other aspects that are not yet fully understood.Regardless, deep learning will change the world for better or worse.The final chapter discusses AI ethics and concludes with an appeal for practitioners to consider the moral implications of their work.Your time is precious, and I have striven to curate and present the material so you can understand it as efficiently as possible.The main body of each chapter comprises a succinct description of only the most essential ideas, together with accompanying illustrations.The appendices review all mathematical prerequisites, and there should be no need to refer to external material.For readers wishing to delve deeper, each chapter has associated problems, Python notebooks, and extensive background notes.Writing a book is a lonely, grinding, multiple-year process and is only worthwhile if the volume is widely adopted.If you enjoy reading this or have suggestions for improving it, please contact me via the accompanying website.I would love to hear your thoughts, which will inform and motivate subsequent editions.Artificial intelligence, or AI, is concerned with building systems that simulate intelligent behavior.It encompasses a wide range of approaches, including those based on logic, search, and probabilistic reasoning.Machine learning is a subset of AI that learns to make decisions by fitting mathematical models to observed data.This area has seen explosive growth and is now (incorrectly) almost synonymous with the term AI.A deep neural network is a type of machine learning model, and when it is fitted to data, this is referred to as deep learning.At the time of writing, deep networks are the most powerful and practical machine learning models and are often encountered in day-to-day life.It is commonplace to translate text from another language using a natural language processing algorithm, to search the internet for images of a particular object using a computer vision system, or to converse with a digital assistant via a speech recognition interface.All of these applications are powered by deep learning.As the title suggests, this book aims to help a reader new to this field understand the principles behind deep learning.The book is neither terribly theoretical (there are no proofs) nor extremely practical (there is almost no code).The goal is to explain the underlying ideas; after consuming this volume, the reader will be able to apply deep learning to novel situations where there is no existing recipe for success.Machine learning methods can coarsely be divided into three areas: supervised, unsupervised, and reinforcement learning.At the time of writing, the cutting-edge methods in all three areas rely on deep learning (figure 1.1).This introductory chapter describes these three areas at a high level, and this taxonomy is also loosely reflected in the book's organization.Whether we like it or not, deep learning is poised to change our world, and this change will not all be positive.Hence, this chapter also contains brief primer on AI ethics.We conclude with advice on how to make the most of this book.Figure 1.2 depicts several regression and classification problems.In each case, there is a meaningful real-world input (a sentence, a sound file, an image, etc.), and this is encoded as a vector of numbers.This vector forms the model input.The model maps the input to an output vector which is then "translated" back to a meaningful real-world prediction.For now, we focus on the inputs and outputs and treat the model as a black box that ingests a vector of numbers and returns another vector of numbers.The model in figure 1.2a predicts the price of a house based on input characteristics such as the square footage and the number of bedrooms.This is a regression problem because the model returns a continuous number (rather than a category assignment).In contrast, the model in 1.2b takes the chemical structure of a molecule as an input and predicts both the melting and boiling points.This is a multivariate regression problem since it predicts more than one number.The model in figure 1.2c receives a text string containing a restaurant review as input and predicts whether the review is positive or negative.This is a binary classification problem because the model attempts to assign the input to one of two categories.The output vector contains the probabilities that the input belongs to each category.Figures 1.2d and 1.2e depict multiclass classification problems.Here, the model assigns the input to one of N > 2 categories.In the first case, the input is an audio file, and the model predicts which genre of music it contains.In the second case, the input is an image, and the model predicts which object it contains.In each case, the model returns a vector of size N that contains the probabilities of the N categories.The input data in figure 1.2 varies widely.In the house pricing example, the input is a fixed-length vector containing values that characterize the property.This is an example of tabular data because it has no internal structure; if we change the order of the inputs and build a new model, then we expect the model prediction to remain the same.Conversely, the input in the restaurant review example is a body of text.This may be of variable length depending on the number of words in the review, and here input 1 Introduction Figure 1.3 Machine learning model.The model represents a family of relationships that relate the input (age of child) to the output (height of child).The particular relationship is chosen using training data, which consists of input/output pairs (orange points).When we train the model, we search through the possible relationships for one that describes the data well.Here, the trained model is the cyan curve and can be used to compute the height for any age.order is important; my wife ate the chicken is not the same as the chicken ate my wife.The text must be encoded into numerical form before passing it to the model.Here, we use a fixed vocabulary of size 10,000 and simply concatenate the word indices.For the music classification example, the input vector might be of fixed size (perhaps a 10-second clip) but is very high-dimensional.Digital audio is usually sampled at 44.1 kHz and represented by 16-bit integers, so a ten-second clip consists of 441, 000 integers.Clearly, supervised learning models will have to be able to process sizeable inputs.The input in the image classification example (which consists of the concatenated RGB values at every pixel) is also enormous.Moreover, its structure is naturally two-dimensional; two pixels above and below one another are closely related, even if they are not adjacent in the input vector.Finally, consider the input for the model that predicts the melting and boiling points of the molecule.A molecule may contain varying numbers of atoms that can be connected in different ways.In this case, the model must ingest both the geometric structure of the molecule and the constituent atoms to the model.Until now, we have treated the machine learning model as a black box that takes an input vector and returns an output vector.But what exactly is in this black box?Consider a model to predict the height of a child from their age (figure 1.3).The machine learning model is a mathematical equation that describes how the average height varies as a function of age (cyan curve in figure 1.3).When we run the age through this equation, it returns the height.For example, if the age is 10 years, then we predict that the height will be 139 cm.More precisely, the model represents a family of equations mapping the input to the output (i.e., a family of different cyan curves).The particular equation (curve) is chosen using training data (examples of input/output pairs).In figure 1.3, these pairs are represented by the orange points, and we can see that the model (cyan line) describes these data reasonably.When we talk about training or fitting a model, we mean that we search through the family of possible equations (possible cyan curves) relating input to output to find the one that describes the training data most accurately.It follows that the models in figure 1.2 require labeled input/output pairs for training.For example, the music classification model would require a large number of audio clips where a human expert had identified the genre of each.These input/output pairs take the role of a teacher or supervisor for the training process, and this gives rise to the term supervised learning.This book concerns deep neural networks, which are a particularly useful type of machine learning model.They are equations that can represent an extremely broad family of relationships between input and output, and where it is particularly easy to search through this family to find the relationship that describes the training data.Deep neural networks can process inputs that are very large, of variable length, and contain various kinds of internal structures.They can output single real numbers (regression), multiple numbers (multivariate regression), or probabilities over two or more classes (binary and multiclass classification, respectively).As we shall see in the next section, their outputs may also be very large, of variable length, and contain internal structure.It is probably hard to imagine equations with these properties, and the reader should endeavor to suspend disbelief for now.Here, every pixel of an input image is assigned a binary label that indicates whether it belongs to a cow or the background.Figure 1.4b shows a multivariate regression model where the input is an image of a street scene and the output is the depth at each pixel.In both cases, the output is high-dimensional and structured.However, this structure is closely tied to the input, and this can be exploited; if a pixel is labeled as "cow," then a neighbor with a similar RGB value probably has the same label.Figures 1.4c-e depict three models where the output has a complex structure that is not so closely tied to the input.Figure 1.4c shows a model where the input is an audio file and the output is the transcribed words from that file.Figure 1.4d is a translation  Noh et al., 2015).b) This monocular depth estimation model maps an RGB image to an output image where each pixel represents the depth (adapted from Cordts et al., 2016).c) This audio transcription model maps an audio sample to a transcription of the spoken words in the audio.d) This translation model maps an English text string to its French translation.e) This image synthesis model maps a caption to an image (example from https://openai.com/dall-e-2/).In each case, the output has a complex internal structure or grammar.In some cases, many outputs are compatible with the input.model in which the input is a body of text in English, and the output contains the French translation.Figure 1.4e depicts a very challenging task in which the input is descriptive text, and the model must produce an image that matches this description.In principle, the latter three tasks can be tackled in the standard supervised learning framework, but they are more difficult for two reasons.First, the output may genuinely be ambiguous; there are multiple valid translations from an English sentence to a French one and multiple images that are compatible with any caption.Second, the output contains considerable structure; not all strings of words make valid English and French sentences, and not all collections of RGB values make plausible images.In addition to learning the mapping, we also have to respect the "grammar" of the output.Fortunately, this "grammar" can be learned without the need for output labels.For example, we can learn how to form valid English sentences by learning the statistics of a large corpus of text data.This provides a connection with the next section of the book, which considers unsupervised learning models.Constructing a model from input data without corresponding output labels is termed unsupervised learning; the absence of output labels means there can be no "supervision."Rather than learning a mapping from input to output, the goal is to describe or understand the structure of the data.As was the case for supervised learning, the data may have very different characteristics; it may be discrete or continuous, low-dimensional or high-dimensional, and of constant or variable length.This book focuses on generative unsupervised models, which learn to synthesize new data examples that are statistically indistinguishable from the training data.Some generative models explicitly describe the probability distribution over the input data and here new examples are generated by sampling from this distribution.Others merely learn a mechanism to generate new examples without explicitly describing their distribution.State-of-the-art generative models can synthesize examples that are extremely plausible but distinct from the training examples.They have been particularly successful at generating images (figure 1.5) and text (figure 1.6).They can also synthesize data under the constraint that some outputs are predetermined (termed conditional generation).Examples include image inpainting (figure 1.7) and text completion (figure 1.8).Indeed, modern generative models for text are so powerful that they can appear intelligent.Given a body of text followed by a question, the model can often "fill in" the missing answer by generating the most likely completion of the document.However, in reality, the model only knows about the statistics of language and does not understand the significance of its answers.The moon had risen by the time I reached the edge of the forest, and the light that filtered through the trees was silver and cold.I shivered, though I was not cold, and quickened my pace.I had never been so far from the village before, and I was not sure what to expect.I had been walking for hours, and I was tired and hungry.I had left in such a hurry that I had not thought to pack any food, and I had not thought to bring a weapon.I was unarmed and alone in a strange place, and I did not know what I was doing.I had been walking for so long that I had lost all sense of time, and I had no idea how far I had come.I only knew that I had to keep going.I had to find her.I was getting close.I could feel it.She was nearby, and she was in trouble.I had to find her and help her, before it was too late.I was a little nervous before my first lecture at the University of Bath.It seemed like there were hundreds of students and they looked intimidating.I stepped up to the lectern and was about to speak when something bizarre happened.Suddenly, the room was filled with a deafening noise, like a giant roar.It was so loud that I couldn't hear anything else and I had to cover my ears.I could see the students looking around, confused and frightened.Then, as quickly as it had started, the noise stopped and the room was silent again.I stood there for a few moments, trying to make sense of what had just happened.Then I realized that the students were all staring at me, waiting for me to say something.I tried to think of something witty or clever to say, but my mind was blank.So I just said, "Well, that was strange,' and then I started my lecture.Figure 1.8 Conditional text synthesis.Given an initial body of text (in black), generative models of text can continue the string plausibly by synthesizing the "missing" remaining part of the string.Generated by GPT3 (Brown et al., 2020).Figure 1.9 Variation of the human face.The human face contains roughly 42 muscles, so it's possible to describe most of the variation in images of the same person in the same lighting with just 42 numbers.In general, datasets of images, music, and text can be described by a relatively small number of underlying variables although it is typically more difficult to tie these to particular physical mechanisms.Images from Dynamic FACES database (Holland et al., 2019).Some (but not all) generative models exploit the observation that data can be lower dimensional than the raw number of observed variables suggests.For example, the number of valid and meaningful English sentences is considerably smaller than the number of strings created by drawing words at random.Similarly, real-world images are a tiny subset of the images that can be created by drawing random RGB values for every pixel.This is because images are generated by physical processes (see figure 1.9).This leads to the idea that we can describe each data example using a smaller number of underlying latent variables.Here, the role of deep learning is to describe the mapping between these latent variables and the data.The latent variables typically have a simple   probability distribution by design.By sampling from this distribution and passing the result through the deep learning model, we can create new samples (figure 1.10).These models lead to new methods for manipulating real data.For example, consider finding the latent variables that underpin two real examples.We can interpolate between these examples by interpolating between their latent representations and mapping the intermediate positions back into the data space (figure 1.11).Generative models with latent variables can also benefit supervised learning models where the outputs have structure (figure 1.4).For example, consider learning to predict the images corresponding to a caption.Rather than directly map the text input to an image, we can learn a relation between latent variables that explain the text and the latent variables that explain the image.This has three advantages.First, we may need fewer text/image pairs to learn this mapping now that the inputs and outputs are lower dimensional.Second, we are more likely to generate a plausible-looking image; any sensible values of the latent variables should produce something that looks like a plausible example.Third, if we introduce randomness to either the mapping between the two sets of latent variables or the mapping from the latent variables to the image, then we can generate multiple images that are all described well by the caption (figure 1.12).The final area of machine learning is reinforcement learning.This paradigm introduces the idea of an agent which lives in a world and can perform certain actions at each time step.The actions change the state of the system but not necessarily in a deterministic way.Taking an action can also produce rewards, and the goal of reinforcement learning 1 Introduction is for the agent to learn to choose actions that lead to high rewards on average.One complication is that the reward may occur some time after the action is taken, so associating a reward with an action is not straightforward.This is known as the temporal credit assignment problem.As the agent learns, it must trade off exploration and exploitation of what it already knows; perhaps the agent has already learned how to receive modest rewards; should it follow this strategy (exploit what it knows), or should it try different actions to see if it can improve (explore other opportunities)?Consider teaching a humanoid robot to locomote.The robot can perform a limited number of actions at a given time (moving various joints), and these change the state of the world (its pose).We might reward the robot for reaching checkpoints in an obstacle course.To reach each checkpoint, it must perform many actions, and it's unclear which ones contributed to the reward when it is received and which were irrelevant.This is an example of the temporal credit assignment problem.A second example is learning to play chess.Again, the agent has a set of valid actions (chess moves) at any given time.However, these actions change the state of the system in a non-deterministic way; for any choice of action, the opposing player might respond with many different moves.Here, we might set up a reward structure based on capturing pieces or just have a single reward at the end of the game for winning.In the latter case, the temporal credit assignment problem is extreme; the system must learn which of the many moves it made were instrumental to success or failure.The exploration-exploitation trade-off is also apparent in these two examples.The robot may have discovered that it can make progress by lying on its side and pushing with one leg.This strategy will move the robot and yields rewards, but much more slowly than the optimal solution: to balance on its legs and walk.So, it faces a choice between exploiting what it already knows (how to slide along the floor awkwardly) and exploring the space of actions (which might result in much faster locomotion).Similarly, in the chess example, the agent may learn a reasonable sequence of opening moves.Should it exploit this knowledge or explore different opening sequences?It is perhaps not obvious how deep learning fits into the reinforcement learning framework.There are several possible approaches, but one technique is to use deep networks to build a mapping from the observed world state to an action.This is known as a policy network.In the robot example, the policy network would learn a mapping from its sensor measurements to joint movements.In the chess example, the network would learn a mapping from the current state of the board to the choice of move (figure 1.13).It would be irresponsible to write this book without discussing the ethical implications of artificial intelligence.This potent technology will change the world to at least theFigure 1.13 Policy networks for reinforcement learning.One way to incorporate deep neural networks into reinforcement learning is to use them to define a mapping from the state (here position on chessboard) to the actions (possible moves).This mapping is known as a policy.same extent as electricity, the internal combustion engine, the transistor, or the internet.The potential benefits in healthcare, design, entertainment, transport, education, and almost every area of commerce are enormous.However, scientists and engineers are often unrealistically optimistic about the outcomes of their work, and the potential for harm is just as great.The following paragraphs highlight five concerns.If we train a system to predict salary levels for individuals based on historical data, then this system will reproduce historical biases; for example, it will probably predict that women should be paid less than men.Several such cases have already become international news stories: an AI system for super-resolving face images made non-white people look more white; a system for generating images produced only pictures of men when asked to synthesize pictures of lawyers.Careless application of algorithmic decision-making using AI has the potential to entrench or aggravate existing biases.See Binns (2018) for further discussion.Explainability: Deep learning systems make decisions, but we do not usually know exactly how or based on what information.They may contain billions of parameters, and there is no way we can understand how they work based on examination.This has led to the sub-field of explainable AI.One moderately successful area is producing local explanations; we cannot explain the entire system, but we can produce an interpretable description of why a particular decision was made.However, it remains unknown whether it is possible to build complex decision-making systems that are fully transparent to their users or even their creators.See Grennan et al. (2022) for further information.Weaponizing AI: All significant technologies have been applied directly or indirectly toward war.Sadly, violent conflict seems to be an inevitable feature of human behavior.AI is arguably the most powerful technology ever built and will doubtless be deployed extensively in a military context.Indeed, this is already happening (Heikkilä, 2022).Concentrating power: It is not from a benevolent interest in improving the lot of the human race that the world's most powerful companies are investing heavily in artificial intelligence.They know that these technologies will allow them to reap enormous profits.Like any advanced technology, deep learning is likely to concentrate power in the hands of the few organizations that control it.Automating jobs that are currently done by humans will change the economic environment and disproportionately affect the livelihoods of lower-paid workers with fewer skills.Optimists argue similar disruptions happened during the industrial revolution and resulted in shorter working hours.The truth is that we simply do not know what effects the large-scale adoption of AI will have on society (see David, 2015).The major existential risks to the human race all result from technology.Climate change has been driven by industrialization.Nuclear weapons derive from the study of physics.Pandemics are more probable and spread faster because innovations in transport, agriculture, and construction have allowed a larger, denser, and more interconnected population.Artificial intelligence brings new existential risks.We should be very cautious about building systems that are more capable and extensible than human beings.In the most optimistic case, it will put vast power in the hands of the owners.In the most pessimistic case, we will be unable to control it or even understand its motives (see Tegmark, 2018).This list is far from exhaustive.AI could also enable surveillance, disinformation, violations of privacy, fraud, and manipulation of financial markets, and the energy required to train AI systems contributes to climate change.Moreover, these concerns are not speculative; there are already many examples of ethically dubious applications of AI (consult Dao, 2021, for a partial list).In addition, the recent history of the internet has shown how new technology can cause harm in unexpected ways.The online community of the eighties and early nineties could hardly have predicted the proliferation of fake news, spam, online harassment, fraud, cyberbullying, incel culture, political manipulation, doxxing, online radicalization, and revenge porn.Everyone studying or researching (or writing books about) AI should contemplate to what degree scientists are accountable for the uses of their technology.We should consider that capitalism primarily drives the development of AI and that legal advances and deployment for social good are likely to lag significantly behind.We should reflect on whether it's possible, as scientists and engineers, to control progress in this field and to reduce the potential for harm.We should consider what kind of organizations we are prepared to work for.How serious are they in their commitment to reducing the potential harms of AI? Are they simply "ethics-washing" to reduce reputational risk, or do they actually implement mechanisms to halt ethically suspect projects?All readers are encouraged to investigate these issues further.The online course at https://ethics-of-ai.mooc.fi/ is a useful introductory resource.If you are a professor teaching from this book, you are encouraged to raise these issues with your students.If you are a student taking a course where this is not done, then lobby your professor to make this happen.If you are deploying or researching AI in a corporate environment, you are encouraged to scrutinize your employer's values and to help change them (or leave) if they are wanting.The structure of the book follows the structure of this introduction.Chapters 2-9 walk through the supervised learning pipeline.We describe shallow and deep neural networks and discuss how to train them and measure and improve their performance.Chapters 10-13 describe common architectural variations of deep neural networks, including convolutional networks, residual connections, and transformers.These architectures are used across supervised, unsupervised, and reinforcement learning.Chapters 14-18 tackle unsupervised learning using deep neural networks.We devote a chapter each to four modern deep generative models: generative adversarial networks, variational autoencoders, normalizing flows, and diffusion models.Chapter 19 is a brief introduction to deep reinforcement learning.This is a topic that easily justifies its own book, so the treatment is necessarily superficial.However, this treatment is intended to be a good starting point for readers unfamiliar with this area.Despite the title of this book, some aspects of deep learning remain poorly understood.Chapter 20 poses some fundamental questions.Why are deep networks so easy to train?Why do they generalize so well?Why do they need so many parameters?Do they need to be deep?Along the way, we explore unexpected phenomena such as the structure of the loss function, double descent, grokking, and lottery tickets.The book concludes with chapter 21, which discusses ethics and deep learning.This book is self-contained but is limited to coverage of deep learning.It is intended to be the spiritual successor to Deep Learning (Goodfellow et al., 2016) which is a fantastic resource but does not cover recent advances.For a broader look at machine learning, the most up-to-date and encyclopedic resource is Probabilistic Machine Learning (Murphy, 2022(Murphy, , 2023.However, Pattern Recognition and Machine Learning (Bishop, 2006) is still an excellent and relevant book.If you enjoy this book, then my previous volume, Computer Vision: Models, Learning, and Inference (Prince, 2012), is still worth reading.Some parts have dated badly, but it contains a thorough introduction to probability, including Bayesian methods, and good introductory coverage of latent variable models, geometry for computer vision, Gaussian processes, and graphical models.It uses identical notation to this book and can be found online.A detailed treatment of graphical models can be found in Probabilistic Graphical Models: Principles and Techniques (Koller & Friedman, 2009), and Gaussian processes are covered by Gaussian Processes for Machine Learning (Williams & Rasmussen, 2006).For background mathematics, consult Mathematics for Machine Learning (Deisenroth et al., 2020).For a more coding-oriented approach, consult Dive into Deep Learning (Zhang et al., 2023).The best overview for computer vision is Szeliski (2022), and there is also the impending book Foundations of Computer Vision (Torralba et al., 2024).A good starting point to learn about graph neural networks is Graph Representation Learning (Hamilton, 2020).The definitive work on reinforcement learning is Reinforce-1 Introduction ment Learning: An Introduction (Sutton & Barto, 2018).A good initial resource is Foundations of Deep Reinforcement Learning (Graesser & Keng, 2019).Most remaining chapters in this book contain a main body of text, a notes section, and a set of problems.The main body of the text is intended to be self-contained and can be read without recourse to the other parts of the chapter.As much as possible, background mathematics is incorporated into the main body of the text.However, for larger topics that would be a distraction to the main thread of the argument, the background material is appendicized, and a reference is provided in the margin.Most notation in this book is Appendix A Notation standard.However, some conventions are less widely used, and the reader is encouraged to consult appendix A before proceeding.The main body of text includes many novel illustrations and visualizations of deep learning models and results.I've worked hard to provide new explanations of existing ideas rather than merely curate the work of others.Deep learning is a new field, and sometimes phenomena are poorly understood.I try to make it clear where this is the case and when my explanations should be treated with caution.References are included in the main body of the chapter only where results are depicted.Instead, they can be found in the notes section at the end of the chapter.I do not generally respect historical precedent in the main text; if an ancestor of a current technique is no longer useful, then I will not mention it.However, the historical development of the field is described in the notes section, and hopefully, credit is fairly assigned.The notes are organized into paragraphs and provide pointers for further reading.They should help the reader orient themselves within the sub-area and understand how it relates to other parts of machine learning.The notes are less self-contained than the main text.Depending on your level of background knowledge and interest, you may find these sections more or less useful.Each chapter has a number of associated problems.They are referenced in the margin of the main text at the point that they should be attempted.As George Pólya noted, "Mathematics, you see, is not a spectator sport."He was correct, and I highly recommend that you attempt the problems as you go.In some cases, they provide insights that will help you understand the main text.Problems for which the answers are provided on the associated website are indicated with an asterisk.Additionally, Python notebooks that will help you understand the ideas in this book are also available via the website, and these are also referenced in the margins of the text.Indeed, if you are feeling rusty, it Notebook 1.1 Background mathematics might be worth working through the notebook on background mathematics right now.Unfortunately, the pace of research in AI makes it inevitable that this book will be a constant work in progress.If there are parts you find hard to understand, notable omissions, or sections that seem extraneous, please get in touch via the associated website.Together, we can make the next edition better.A supervised learning model defines a mapping from one or more inputs to one or more outputs.For example, the input might be the age and mileage of a secondhand Toyota Prius, and the output might be the estimated value of the car in dollars.The model is just a mathematical equation; when the inputs are passed through this equation, it computes the output, and this is termed inference.The model equation also contains parameters.Different parameter values change the outcome of the computation; the model equation describes a family of possible relationships between inputs and outputs, and the parameters specify the particular relationship.When we train or learn a model, we find parameters that describe the true relationship between inputs and outputs.A learning algorithm takes a training set of input/output pairs and manipulates the parameters until the inputs predict their corresponding outputs as closely as possible.If the model works well for these training pairs, then we hope it will make good predictions for new inputs where the true output is unknown.The goal of this chapter is to expand on these ideas.First, we describe this framework more formally and introduce some notation.Then we work through a simple example in which we use a straight line to describe the relationship between input and output.This linear model is both familiar and easy to visualize, but nevertheless illustrates all the main ideas of supervised learning.In supervised learning, we aim to build a model that takes an input x and outputs a prediction y.For simplicity, we assume that both the input x and output y are vectors of a predetermined and fixed size and that the elements of each vector are always ordered in the same way; in the Prius example above, the input x would always contain the age of the car and then the mileage, in that order.This is termed structured or tabular data.To make the prediction, we need a model f[•] that takes input x and returns y, so:(2.1)When we compute the prediction y from the input x, we call this inference.The model is just a mathematical equation with a fixed form.It represents a family of different relations between the input and the output.The model also contains parameters ϕ.The choice of parameters determines the particular relation between input and output, so we should really write:(2.2)When we talk about learning or training a model, we mean that we attempt to find parameters ϕ that make sensible output predictions from the input.We learn these parameters using a training dataset of I pairs of input and output examples {x i , y i }.We aim to select parameters that map each training input to its associated output as closely as possible.We quantify the degree of mismatch in this mapping with the loss L. This is a scalar value that summarizes how poorly the model predicts the training outputs from their corresponding inputs for parameters ϕ.We can treat the loss as a function L[ϕ] of these parameters.When we train the model, we are seeking parameters φ that minimize this loss function:(2.3)If the loss is small after this minimization, we have found model parameters that accurately predict the training outputs y i from the training inputs x i .After training a model, we must now assess its performance; we run the model on separate test data to see how well it generalizes to examples that it didn't observe during training.If the performance is adequate, then we are ready to deploy the model.Let's now make these ideas concrete with a simple example.We consider a model y = f[x, ϕ] that predicts a single output y from a single input x.Then we develop a loss function, and finally, we discuss model training.A 1D linear regression model describes the relationship between input x and output y as a straight line:(2.4)This model has two parameters ϕ = [ϕ 0 , ϕ 1 ] T , where ϕ 0 is the y-intercept of the line and ϕ 1 is the slope.Different choices for the y-intercept and slope result in different relations between input and output (figure 2.1).Hence, equation 2.4 defines a family of possible input-output relations (all possible lines), and the choice of parameters determines the member of this family (the particular line).For this model, the training dataset (figure 2.2a) consists of I input/output pairs {x i , y i }.Figures 2.2b-d show three lines defined by three sets of parameters.The green line in figure 2.2d describes the data more accurately than the other two since it is much closer to the data points.However, we need a principled approach for deciding which parameters ϕ are better than others.To this end, we assign a numerical value to each choice of parameters that quantifies the degree of mismatch between the model and the data.We term this value the loss; a lower loss means a better fit.The mismatch is captured by the deviation between the model predictions f[x i , ϕ] (height of the line at x i ) and the ground truth outputs y i .These deviations are depicted as orange dashed lines in figures 2.2b-d.We quantify the total mismatch, training error, or loss as the sum of the squares of these deviations for all I training pairs:(2.5)Since the best parameters minimize this expression, we call this a least-squares loss.The squaring operation means that the direction of the deviation (i.e., whether the line is  above or below the data) is unimportant.There are also theoretical reasons for this choice which we return to in chapter 5.The loss L is a function of the parameters ϕ; it will be larger when the model fit is poor (figure 2.2b,c) and smaller when it is good (figure 2.2d).Considered in this light, we term L[ϕ] the loss function or cost function.The goal is to find the parameters φ that minimize this quantity:(2.6)There are only two parameters (the y-intercept ϕ 0 and slope ϕ 1 ), so we can calculate the loss for every combination of values and visualize the loss function as a surfaceProblems 2.1-2.2(figure 2.3).The "best" parameters are at the minimum of this surface.The process of finding parameters that minimize the loss is termed model fitting, training, or learning.The basic method is to choose the initial parameters randomly and then improve them by "walking down" the loss function until we reach the bottom (figure 2.4).One way to do this is to measure the gradient of the surface at the current position and take a step in the direction that is most steeply downhill.Then we repeat this process until the gradient is flat and we can improve no further. 2Having trained the model, we want to know how it will perform in the real world.We do this by computing the loss on a separate set of test data.The degree to which the prediction accuracy generalizes to the test data depends in part on how representative and complete the training data is.However, it also depends on how expressive the model is.A simple model like a line might not be able to capture the true relationship between input and output.This is known as underfitting.Conversely, a very expressive model may describe statistical peculiarities of the training data that are atypical and lead to unusual predictions.This is known as overfitting.A supervised learning model is a function y = f[x, ϕ] that relates inputs x to outputs y.The particular relationship is determined by parameters ϕ.To train the model, we define a loss function L[ϕ] over a training dataset {x i , y i }.This quantifies the mismatch between the model predictions f[x i , ϕ] and observed outputs y i as a function of the parameters ϕ.Then we search for the parameters that minimize the loss.We evaluate the model on a different set of test data to see how well it generalizes to new inputs.Chapters 3-9 expand on these ideas.First, we tackle the model itself; 1D linear regression has the obvious drawback that it can only describe the relationship between the input and output as a straight line.Shallow neural networks (chapter 3) are only slightly more complex than linear regression but describe a much larger family of input/output relationships.Deep neural networks (chapter 4) are just as expressive but can describe complex functions with fewer parameters and work better in practice.Chapter 5 investigates loss functions for different tasks and reveals the theoretical underpinnings of the least-squares loss.Chapters 6 and 7 discuss the training process.Chapter 8 discusses how to measure model performance.Chapter 9 considers regularization techniques, which aim to improve that performance.Loss functions vs. cost functions: In much of machine learning and in this book, the terms loss function and cost function are used interchangeably.However, more properly, a loss function is the individual term associated with a data point (i.e., each of the squared terms on the righthand side of equation 2.5), and the cost function is the overall quantity that is minimized (i.e., the entire right-hand side of equation 2.5).A cost function can contain additional terms that are not associated with individual data points (see section 9.1).More generally, an objective function is any function that is to be maximized or minimized.The models y = f[x, ϕ] in this chapter are discriminative models.These make an output prediction y from real-world measurements x.Another Problem 2.3 approach is to build a generative model x = g[y, ϕ], in which the real-world measurements x are computed as a function of the output y.The generative approach has the disadvantage that it doesn't directly predict y.To perform inference, we must invert the generative equation as y = g −1 [x, ϕ], and this may be difficult.However, generative models have the advantage that we can build in prior knowledge about how the data were created.For example, if we wanted to predict the 3D position and orientation y of a car in an image x, then we could build knowledge about car shape, 3D geometry, and light transport into the function x = g[y, ϕ].This seems like a good idea, but in fact, discriminative models dominate modern machine learning; the advantage gained from exploiting prior knowledge in generative models is usually trumped by learning very flexible discriminative models with large amounts of training data.Problem 2.1 To walk "downhill" on the loss function (equation 2.5), we measure its gradient with respect to the parameters ϕ0 and ϕ1.Calculate expressions for the slopes ∂L/∂ϕ0 and ∂L/∂ϕ1.Problem 2.2 Show that we can find the minimum of the loss function in closed form by setting the expression for the derivatives from problem 2.1 to zero and solving for ϕ0 and ϕ1.Note that this works for linear regression but not for more complex models; this is why we use iterative model fitting methods like gradient descent (figure 2.4).Problem 2.3 * Consider reformulating linear regression as a generative model, so we have x = g[y, ϕ] = ϕ0 + ϕ1y.What is the new loss function?Find an expression for the inverse function y = g −1 [x, ϕ] that we would use to perform inference.Will this model make the same predictions as the discriminative version for a given training dataset {xi, yi}?One way to establish this is to write code that fits a line to three data points using both methods and see if the result is the same.Chapter 2 introduced supervised learning using 1D linear regression.However, this model can only describe the input/output relationship as a line.This chapter introduces shallow neural networks.These describe piecewise linear functions and are expressive enough to approximate arbitrarily complex relationships between multi-dimensional inputs and outputs.Shallow neural networks are functions y = f[x, ϕ] with parameters ϕ that map multivariate inputs x to multivariate outputs y.We defer a full definition until section 3.4 and introduce the main ideas using an example network f[x, ϕ] that maps a scalar input x to a scalar output y and has ten parameters ϕ = {ϕ 0 , ϕ 1 , ϕ 2 , ϕ 3 , θ 10 , θ 11 , θ 20 , θ 21 , θ 30 , θ 31 }:We can break down this calculation into three parts: first, we compute three linear functions of the input data (θ 10 + θ 11 x, θ 20 + θ 21 x, and θ 30 + θ 31 x).Second, we pass the three results through an activation function a [•].Finally, we weight the three resulting activations with ϕ 1 , ϕ 2 , and ϕ 3 , sum them, and add an offset ϕ 0 .To complete the description, we must define the activation function a [•].There are many possibilities, but the most common choice is the rectified linear unit or ReLU:This returns the input when it is positive and zero otherwise (figure 3.1).It is probably not obvious which family of input/output relations is represented by equation 3.1.Nonetheless, the ideas from the previous chapter are all applicable.Equation 3.1 represents a family of functions where the particular member of the family This activation function returns zero if the input is less than zero and returns the input unchanged otherwise.In other words, it clips negative values to zero.Note that there are many other possible choices for the activation function (see figure 3.13), but the ReLU is the most commonly used and the easiest to understand.depends on the ten parameters in ϕ.If we know these parameters, we can perform inference (predict y) by evaluating the equation for a given input x.Given a training dataset {x i , y i } I i=1 , we can define a least squares loss function L [ϕ] and use this to measure how effectively the model describes this dataset for any given parameter values ϕ.To train the model, we search for the values φ that minimize this loss.In fact, equation 3.1 represents a family of continuous piecewise linear functions (figure 3.2) with up to four linear regions.We now break down equation 3.1 and show why it describes this family.To make this easier to understand, we split the function into two parts.First, we introduce the intermediate quantities:(3.3)where we refer to h 1 , h 2 , and h 3 as hidden units.Second, we compute the output by combining these hidden units with a linear function:(3.4) Figure 3.3 shows the flow of computation that creates the function in figure 3.2a.Each hidden unit contains a linear function θ •0 + θ •1 x of the input, and that line is clipped by the ReLU function a[•] below zero.The positions where the three lines cross zero become the three "joints" in the final output.The three clipped lines are then weighted by ϕ 1 , ϕ 2 , and ϕ 3 , respectively.Finally, the offset ϕ 0 is added, which controls the overall height of the final function.Each linear region in figure 3.3j corresponds to a different activation pattern in the hidden units.When a unit is clipped, we refer to it as inactive, and when it is not clipped, we refer to it as active.For example, the shaded region receives contributions from h 1 and h 3 (which are active) but not from h 2 (which is inactive).The slope of each linear region is determined by (i) the original slopes θ •1 of the active inputs for this region and (ii) the weights ϕ • that were subsequently applied.For example, the slope in the shaded region (see problem 3.3) is θ 11 ϕ 1 + θ 31 ϕ 3 , where the first term is the slope in panel (g) and the second term is the slope in panel (i).Each hidden unit contributes one "joint" to the function, so with three hidden units, Notebook 3.1 Shallow networks I there can be four linear regions.However, only three of the slopes of these regions are independent; the fourth is either zero (if all the hidden units are inactive in this region) Problem 3.9or is a sum of slopes from the other regions.We have been discussing a neural network with one input, one output, and three hidden units.We visualize this network in figure 3.4a.The input is on the left, the hidden units are in the middle, and the output is on the right.Each connection represents one of the ten parameters.To simplify this representation, we do not typically draw the intercept parameters, so this network is usually depicted as in figure 3.4b.1 For the purposes of this book, a linear function has the form z ′ = ϕ 0 + ∑ i ϕ i z i .Any other type of function is nonlinear.For instance, the ReLU function (equation 3.2) and the example neural network that contains it (equation 3.1) are both nonlinear.See notes at end of chapter for further clarification.d-f) Each line is passed through the ReLU activation function, which clips negative values to zero.g-i) The three clipped lines are then weighted (scaled) by ϕ1, ϕ2, and ϕ3, respectively.j) Finally, the clipped and weighted functions are summed, and an offset ϕ0 that controls the height is added.Each of the four linear regions corresponds to a different activation pattern in the hidden units.In the shaded region, h2 is inactive (clipped), but h1 and h3 are both active.Figure 3.4 Depicting neural networks.a) The input x is on the left, the hidden units h1, h2, and h3 in the center, and the output y on the right.Computation flows from left to right.The input is used to compute the hidden units, which are combined to create the output.Each of the ten arrows represents a parameter (intercepts in orange and slopes in black).Each parameter multiplies its source and adds the result to its target.For example, we multiply the parameter ϕ1 by source h1 and add it to y.We introduce additional nodes containing ones (orange circles) to incorporate the offsets into this scheme, so we multiply ϕ0 by one (with no effect) and add it to y. ReLU functions are applied at the hidden units.b) More typically, the intercepts, ReLU functions, and parameter names are omitted; this simpler depiction represents the same network.In the previous section, we introduced an example neural network with one input, one output, ReLU activation functions, and three hidden units.Let's now generalize this slightly and consider the case with D hidden units where the d th hidden unit is:(3.5) and these are combined linearly to create the output:The number of hidden units in a shallow network is a measure of the network capacity.With ReLU activation functions, the output of a network with D hidden units has at Problem 3.10 most D joints and so is a piecewise linear function with at most D + 1 linear regions.As we add more hidden units, the model can approximate more complex functions.Indeed, with enough capacity (hidden units), a shallow network can describe any continuous 1D function defined on a compact subset of the real line to arbitrary precision.To see this, consider that every time we add a hidden unit, we add another linear region to the function.As these regions become more numerous, they represent smaller sections of the function, which are increasingly well approximated by a line (figure 3.5).The universal approximation theorem proves that for any continuous function, there exists a shallow network that can approximate this function to any specified precision.A neural network with a scalar input creates one extra linear region per hidden unit.The universal approximation theorem proves that, with enough hidden units, there exists a shallow neural network that can describe any given continuous function defined on a compact subset of R D i to arbitrary precision.In the above example, the network has a single scalar input x and a single scalar output y.However, the universal approximation theorem also holds for the more general case where the network maps multivariate inputs x = [x 1 , x 2 , . . ., x Di ] T to multivariate output predictions y = [y 1 , y 2 , . . ., y Do ] T .We first explore how to extend the model to predict multivariate outputs.Then we consider multivariate inputs.Finally, in section 3.4, we present a general definition of a shallow neural network.To extend the network to multivariate outputs y, we simply use a different linear function of the hidden units for each output.So, a network with a scalar input x, four hidden units h 1 , h 2 , h 3 , and h 4 , and a 2D multivariate output y = [y 1 , y 2 ] T would be defined as:(3.7) and The four "joints" of these functions (at vertical dotted lines) are constrained to be in the same places since they share the same hidden units, but the slopes and overall height may differ.The two outputs are two different linear functions of the hidden units.As we saw in figure 3.3, the "joints" in the piecewise functions depend on where the initial linear functions θ •0 + θ •1 x are clipped by the ReLU functions a[•] at the hidden units.Since both outputs y 1 and y 2 are different linear functions of the same four hidden Problem 3.11 units, the four "joints" in each must be in the same places.However, the slopes of the linear regions and the overall vertical offset can differ (figure 3.6).To cope with multivariate inputs x, we extend the linear relations between the input and the hidden units.So a network with two inputs x = [x 1 , x 2 ] T and a scalar output y (figure 3.7) might have three hidden units defined by:(3.9)where there is now one slope parameter for each input.The hidden units are combined to form the output in the usual way:(3.10) Figure 3.8 illustrates the processing of this network.Each hidden unit receives a linear Problems 3.12-3.13combination of the two inputs, which forms an oriented plane in the 3D input/output Notebook 3.2 Shallow networks II space.The activation function clips the negative values of these planes to zero.The clipped planes are then recombined in a second linear function (equation 3.10) to create a continuous piecewise linear surface consisting of convex polygonal regions (figure 3.8j).Each region corresponds to a different activation pattern.For example, in the central triangular region, the first and third hidden units are active, and the second is inactive.When there are more than two inputs to the model, it becomes difficult to visualize.However, the interpretation is similar.The output will be a continuous piecewise linear function of the input, where the linear regions are now convex polytopes in the multidimensional input space.Note that as the input dimensions grow, the number of linear regions increases rapidly (figure 3.9).To get a feeling for how rapidly, consider that each hidden unit defines a hyperplane that delineates the part of space where this unit is active from the part Notebook 3.3 Shallow network regions where it is not (cyan lines in 3.8d-f).If we had the same number of hidden units as input dimensions D i , we could align each hyperplane with one of the coordinate axes (figure 3.10).For two input dimensions, this would divide the space into four quadrants.For three dimensions, this would create eight octants, and for D i dimensions, this would create 2 Di orthants.Shallow neural networks usually have more hidden units than input dimensions, so they typically create more than 2 Di linear regions.We have described several example shallow networks to help develop intuition about how they work.We now define a general equation for a shallow neural network y = f[x, ϕ] that maps a multi-dimensional input x ∈ R Di to a multi-dimensional output y ∈ R Do using h ∈ R D hidden units.Each hidden unit is computed as: (3.11) and these are combined linearly to create the output:  ) With two input dimensions, a model with two hidden units can divide the input space using two lines (here aligned with axes) to create four regions.c) With three input dimensions, a model with three hidden units can divide the input space using three planes (again aligned with axes) to create eight regions.Continuing this argument, it follows that a model with Di input dimensions and Di hidden units can divide the input space with Di hyperplanes to create 2 D i linear regions.Problems 3.14-3.17The activation function permits the model to describe nonlinear relations between input and the output, and as such, it must be nonlinear itself; with no activation function, or a linear activation function, the overall mapping from input to output would be restricted to be linear.Many different activation functions have been tried (see figure 3.13), but the most common choice is the ReLU (figure 3.1), which has the merit Notebook 3.4Activation functions of being easily interpretable.With ReLU activations, the network divides the input space into convex polytopes defined by the intersections of hyperplanes computed by the "joints" in the ReLU functions.Each convex polytope contains a different linear function.The polytopes are the same for each output, but the linear functions they contain can differ.We conclude this chapter by introducing some terminology.Regrettably, neural networks have a lot of associated jargon.They are often referred to in terms of layers.The left of figure 3.12 is the input layer, the center is the hidden layer, and to the right is the output layer.We would say that the network in figure 3.12 has one hidden layer containing four hidden units.The hidden units themselves are sometimes referred to as neurons.When we pass data through the network, the values of the inputs to the hidden layer (i.e., before the ReLU functions are applied) are termed pre-activations.The values at the hidden layer (i.e., after the ReLU functions) are termed activations.For historical reasons, any neural network with at least one hidden layer is also called a multi-layer perceptron, or MLP for short.Networks with one hidden layer (as described in this chapter) are sometimes referred to as shallow neural networks.Networks with multiple hidden layers (as described in the next chapter) are referred to as deep neural networks.Neural networks in which the connections form an acyclic graph (i.e., a graph with no loops, as in all the examples in this chapter) are referred to as feed-forward networks.If every element in one layer connects to every element in the next (as in all the examples in this chapter), the network is fully connected.These connections Figure 3.12 Terminology.A shallow network consists of an input layer, a hidden layer, and an output layer.Each layer is connected to the next by forward connections (arrows).For this reason, these models are referred to as feed-forward networks.When every variable in one layer connects to every variable in the next, we call this a fully connected network.Each connection represents a slope parameter in the underlying equation, and these parameters are termed weights.The variables in the hidden layer are termed neurons or hidden units.The values feeding into the hidden units are termed pre-activations, and the values at the hidden units (i.e., after the ReLU function is applied) are termed activations.represent slope parameters in the underlying equations and are referred to as network weights.The offset parameters (not shown in figure 3.12) are called biases.Shallow neural networks have one hidden layer.They (i) compute several linear functions of the input, (ii) pass each result through an activation function, and then (iii) take a linear combination of these activations to form the outputs.Shallow neural networks make predictions y based on inputs x by dividing the input space into a continuous surface of piecewise linear regions.With enough hidden units (neurons), shallow neural networks can approximate any continuous function to arbitrary precision.Chapter 4 discusses deep neural networks, which extend the models from this chapter by adding more hidden layers.Chapters 5-7 describe how to train these models."Neural" networks: If the models in this chapter are just functions, why are they called "neural networks"?The connection is, unfortunately, tenuous.Visualizations like figure 3.12 consist of nodes (inputs, hidden units, and outputs) that are densely connected to one another.This bears a superficial similarity to neurons in the mammalian brain, which also have dense connections.However, there is scant evidence that brain computation works in the same way as neural networks, and it is unhelpful to think about biology going forward.McCulloch & Pitts (1943) first came up with the notion of an artificial neuron that combined inputs to produce an output, but this model did not have a practical learning algorithm.Rosenblatt (1958) developed the perceptron, which linearly combined inputs and then thresholded them to make a yes/no decision.He also provided an algorithm to learn the weights from data.Minsky & Papert (1969) argued that the linear function was inadequate for general classification problems but that adding hidden layers with nonlinear activation functions (hence the term multi-layer perceptron) could allow the learning of more general input/output relations.However, they concluded that Rosenblatt's algorithm could not learn the parameters of such models.It was not until the 1980s that a practical algorithm (backpropagation, see chapter 7) was developed, and significant work on neural networks resumed.The history of neural networks is chronicled by Kurenkov (2020), Sejnowski (2018), and Schmidhuber (2022.The ReLU function has been used as far back as Fukushima (1969).However, in the early days of neural networks, it was more common to use the logistic sigmoid or tanh activation functions (figure 3.13a).The ReLU was re-popularized by Jarrett et al. (2009), Nair & Hinton (2010), and Glorot et al. (2011 and is an important part of the success story of modern neural networks.It has the nice property that the derivative of the output with respect to the input is always one for inputs greater than zero.This contributes to the stability and efficiency of training (see chapter 7) and contrasts with the derivatives of sigmoid activation functions, which saturate (become close to zero) for large positive and large negative inputs.However, the ReLU function has the disadvantage that its derivative is zero for negative inputs.If all the training examples produce negative inputs to a given ReLU function, then we cannot improve the parameters feeding into this ReLU during training.The gradient with respect to the incoming weights is locally flat, so we cannot "walk downhill."This is known as the dying ReLU problem.Many variations on the ReLU have been proposed to resolve this problem (figure 3.13b), including (i) the leaky ReLU (Maas et al., 2013), which also has a linear output for negative values with a smaller slope of 0.1, (ii) the parametric ReLU (He et al., 2015), which treats the slope of the negative portion as an unknown parameter, and (iii) the concatenated ReLU (Shang et al., 2016), which produces two outputs, one of which clips below zero (i.e., like a typical ReLU) and one of which clips above zero.A variety of smooth functions have also been investigated (figure 3.13c-d), including the softplus function (Glorot et al., 2011), Gaussian error linear unit (Hendrycks & Gimpel, 2016), sigmoid linear unit (Hendrycks & Gimpel, 2016), and exponential linear unit (Clevert et al., 2015).Most of these are attempts to avoid the dying ReLU problem while limiting the gradient for negative values.Klambauer et al. (2017) introduced the scaled exponential linear unit (figure 3.13e), which is particularly interesting as it helps stabilize the variance of the activations when the input variance has a limited range (see section 7.5).Ramachandran et al. (2017) adopted an empirical approach to choosing an activation function.They searched the space of possible functions to find the one that performed best over a variety of supervised learning tasks.The optimal function was found to be a[x] = x/(1 + exp[−βx]), where β is a learned parameter (figure 3.13f).They termed this function Swish.Interestingly, this was a rediscovery of activation functions previously proposed by Hendrycks & Gimpel (2016) and Elfwing et al. (2018). Howard et al. (2019 approximated Swish by the HardSwish function, which has a very similar shape but is faster to compute:There is no definitive answer as to which of these activations functions is empirically superior.However, the leaky ReLU, parameterized ReLU, and many of the continuous functions can be shown to provide minor performance gains over the ReLU in particular situations.We restrict attention to neural networks with the basic ReLU function for the rest of this book because it's easy to characterize the functions they create in terms of the number of linear regions.The width version of this theorem states that there exists a network with one hidden layer containing a finite number of hidden units that can approximate any specified continuous function on a compact subset of R n to arbitrary accuracy.This was proved by Cybenko (1989) for a class of sigmoid activations and was later shown to be true for a larger class of nonlinear activation functions (Hornik, 1991).Consider a shallow network with Di ≥ 2-dimensional inputs and D hidden units.The number of linear regions is determined by the intersections of the D hyperplanes created by the "joints" in the ReLU functions (e.g., figure 3.8d-f).Each region is created by a different combination of the ReLU functions clipping or not clipping the input.The number of regions created by D hyperplanes in the Di ≤ D-dimensional input space was Problem 3.18shown by Zaslavsky (1975) to be at most D i j=0 D j (i.e., a sum of binomial coefficients).As a rule of thumb, shallow neural networks almost always have a larger number D of hidden units than input dimensions Di and create between 2 D i and 2 D linear regions.Linear, affine, and nonlinear functions: Technically, a linear transformation f[•] is any function that obeys the principle of superposition, so f[a.The weighted sum f[h1, h2, h3] = ϕ1h1 + ϕ2h2 + ϕ3h3 is linear, but once the offset (bias) is added so f[h1, h2, h3] = ϕ0 + ϕ1h1 + ϕ2h2 + ϕ3h3, this is no longer true.To see this, consider that the output is doubled when we double the arguments of the former function.This is not the case for the latter function, which is more properly termed an affine function.However, it is common in machine learning to conflate these terms.We follow this convention in this book and refer to both as linear.All other functions we will encounter are nonlinear.Problem 3.1 What kind of mapping from input to output would be created if the activation function in equation 3.1 was linear so that a[z] = ψ0 + ψ1z?What kind of mapping would be created if the activation function was removed, so a[z] = z?Problem 3.2 For each of the four linear regions in figure 3.3j, indicate which hidden units are inactive and which are active (i.e., which do and do not clip their inputs).This is known as the non-negative homogeneity property of the ReLU function.Draft: please send errata to udlbookmail@gmail.com.Redraw a version of figure 3.3 for each of these functions.The original parameters were: ϕ1, ϕ2, ϕ3, θ10, θ11, θ20, θ21, θ30, θ31} = {−0.23, −1.3, 1.3, 0.66, −0.2, 0.4, −0.9, 0.9, 1.1, −0.7}.Provide an informal description of the family of functions that can be created by neural networks with one input, three hidden units, and one output for each activation function.Problem 3.9 * Show that the third linear region in figure 3.3 has a slope that is the sum of the slopes of the first and fourth linear regions.Chapter 4The last chapter described shallow neural networks, which have a single hidden layer.This chapter introduces deep neural networks, which have more than one hidden layer.With ReLU activation functions, both shallow and deep networks describe piecewise linear mappings from input to output.As the number of hidden units increases, shallow neural networks improve their descriptive power.Indeed, with enough hidden units, shallow networks can describe arbitrarily complex functions in high dimensions.However, it turns out that for some functions, the required number of hidden units is impractically large.Deep networks can produce many more linear regions than shallow networks for a given number of parameters.Hence, from a practical standpoint, they can be used to describe a broader family of functions.To gain insight into the behavior of deep neural networks, we first consider composing two shallow networks so the output of the first becomes the input of the second.Consider two shallow networks with three hidden units each (figure 4.1a).The first network takes an input x and returns output y and is defined by:(4.1)andThe second network takes y as input and returns y ′ and is defined by: The first network maps inputs x ∈ [−1, 1] to outputs y ∈ [−1, 1] using a function comprising three linear regions that are chosen so that they alternate the sign of their slope (fourth linear region is outside range of graph).Multiple inputs x (gray circles) now map to the same output y (cyan circle).c) The second network defines a function comprising three linear regions that takes y and returns y ′ (i.e., the cyan circle is mapped to the brown circle).d) The combined effect of these two functions when composed is that (i) three different inputs x are mapped to any given value of y by the first network and (ii) are processed in the same way by the second network; the result is that the function defined by the second network in panel (c) is duplicated three times, variously flipped and rescaled according to the slope of the regions of panel (b).andWith ReLU activations, this model also describes a family of piecewise linear functions.However, the number of linear regions is potentially greater than for a shallow network with six hidden units.To see this, consider choosing the first network to produce three A different way to think about composing networks is that the first network "folds" the input space x back onto itself so that multiple inputs generate the same output.Then the second network applies a function, which is replicated at all points that were folded on top of one another (figure 4.3).The previous section showed that we could create complex functions by passing the output of one shallow neural network into a second network.We now show that this is a special case of a deep network with two hidden layers.The output of the first network (y) is a linear combination of the activations at the hidden units.The first operations of the second network (equation 4.3 in which we calculate θ ′ 10 + θ ′ 11 y, θ ′ 20 + θ ′ 21 y, and θ ′ 30 + θ ′ 31 y) are linear in the output of the first network.Applying one linear function to another yields another linear function.Substituting the expression for y into equation 4.3 gives:4.5) which we can rewrite as:(4.6)   where11 ϕ 2 and so on.The result is a network with two hidden layers (figure 4.4).It follows that a network with two layers can represent the family of functions created by passing the output of one single-layer network into another.In fact, it represents a broader family because in equation 4.6, the nine slope parameters ψ 11 , ψ 21 , . . ., ψ 33 can take arbitrary values, whereas, in equation 4.5, these parameters are constrained to be the outer productIn the previous section, we showed that composing two shallow networks yields a special case of a deep network with two hidden layers.Now we consider the general case of a deep network with two hidden layers, each containing three hidden units (figure 4.4).The first layer is defined by:the second layer by:and the output by:Considering these equations leads to another way to think about how the network con- 1.The three hidden units h 1 , h 2 , and h 3 in the first layer are computed as usual by forming linear functions of the input and passing these through ReLU activation functions (equation 4.7).2. The pre-activations at the second layer are computed by taking three new linear functions of these hidden units (arguments of the activation functions in equation 4.8).At this point, we effectively have a shallow network with three outputs; we have computed three piecewise linear functions with the "joints" between linear regions in the same places (see figure 3.6).3.At the second hidden layer, another ReLU function a[•] is applied to each function (equation 4.8), which clips them and adds new "joints" to each.4. The final output is a linear combination of these hidden units (equation 4.9).In conclusion, we can either think of each layer as "folding" the input space or as creating new functions, which are clipped (creating new regions) and then recombined.The former view emphasizes the dependencies in the output function but not how clipping creates new joints, and the latter has the opposite emphasis.Ultimately, both descriptions provide only partial insight into how deep neural networks operate.Regardless, it's important not to lose sight of the fact that this is still merely an equation relating input x to output y ′ .Indeed, we can combine equations 4.7-4.9 to get one expression:although this is admittedly rather difficult to understand.We can extend the deep network construction to more than two hidden layers; modern networks might have more than a hundred layers with thousands of hidden units at each layer.The number of hidden units in each layer is referred to as the width of the network, and the number of hidden layers as the depth.The total number of hidden units is a measure of the network's capacity.We denote the number of layers as K and the number of hidden units in each layer as D 1 , D 2 , . . ., D K .These are examples of hyperparameters.They are quantities chosen Problem 4.2 before we learn the model parameters (i.e., the slope and intercept terms).For fixed hyperparameters (e.g., K = 2 layers with D k = 3 hidden units in each), the model describes a family of functions, and the parameters determine the particular function.Hence, when we also consider the hyperparameters, we can think of neural networks as representing a family of families of functions relating input to output.c) The inputs to the second hidden layer (i.e., the pre-activations) are three piecewise linear functions where the "joints" between the linear regions are at the same places (see figure 3.6).d-f) Each piecewise linear function is clipped to zero by the ReLU activation function.g-i) These clipped functions are then weighted with parameters ϕ ′ 1 , ϕ ′ 2 , and ϕ ′ 3 , respectively.j) Finally, the clipped and weighted functions are summed and an offset ϕ ′ 0 that controls the overall height is added.The weights are stored in matrices Ω k that pre-multiply the activations from the preceding layer to create the preactivations at the subsequent layer.For example, the weight matrix Ω1 that computes the pre-activations at h2 from the activations at h1 has dimension 2 × 4. It is applied to the four hidden units in layer one and creates the inputs to the two hidden units at layer two.The biases are stored in vectors β k and have the dimension of the layer into which they feed.For example, the bias vector β 2 is length three because layer h3 contains three hidden units.We have seen that a deep neural network consists of linear transformations alternatingwith activation functions.We could equivalently describe equations 4.7-4.9 in matrix notation as:andor even more compactly in matrix notation as:where, in each case, the function a[•] applies the activation function separately to every element of its vector input.This notation becomes cumbersome for networks with many layers.Hence, from now on, we will describe the vector of hidden units at layer k as h k , the vector of biases (intercepts) that contribute to hidden layer k +1 as β k , and the weights (slopes) that are applied to the k th layer and contribute to the (k+1) th layer as Ω k .A general deep network y = f[x, ϕ] with K layers can now be written as:The parameters ϕ of this model comprise all of these weight matrices and bias vectors We can equivalently write the network as a single function:Chapter 3 discussed shallow networks (with a single hidden layer), and here we have described deep networks (with multiple hidden layers).We now compare these models.In section 3.2, we argued that shallow neural networks with enough capacity (hidden units) could model any continuous function arbitrarily closely.In this chapter, we saw that a deep network with two hidden layers could represent the composition of two shallow networks.If the second of these networks computes the identity function, then this deep network replicates a single shallow network.Hence, it can also approximate any continuous function arbitrarily closely given sufficient capacity.Problem 4.7A shallow network with one input, one output, and D > 2 hidden units can create up to D + 1 linear regions and is defined by 3D + 1 parameters.A deep network with one Problems 4.8-4.11input, one output, and K layers of D > 2 hidden units can create a function with up to (D + 1) K linear regions using 3D + 1 + (K − 1)D(D + 1) parameters.Figure 4.7a shows how the maximum number of linear regions increases as a function of the number of parameters for networks mapping scalar input x to scalar output y.Deep neural networks create much more complex functions for a fixed parameter budget.This effect is magnified as the number of input dimensions D i increases (figure 4.7b), although computing the maximum number of regions is less straightforward.This seems attractive, but the flexibility of the functions is still limited by the number of parameters.Deep networks can create extremely large numbers of linear regions, but these contain complex dependencies and symmetries.We saw some of these when we considered deep networks as "folding" the input space (figure 4.3).So, it's not clear that the greater number of regions is an advantage unless (i) there are similar symmetries in the real-world functions that we wish to approximate or (ii) we have reason to believe that the mapping from input to output really does involve a composition of simpler functions.Both deep and shallow networks can model arbitrary functions, but some functions can be approximated much more efficiently with deep networks.Functions have been identified that require a shallow network with exponentially more hidden units to achieve an equivalent approximation to that of a deep network.This phenomenon is referred to as the depth efficiency of neural networks.This property is also attractive, but it's not clear that the real-world functions that we want to approximate fall into this category.We have discussed fully connected networks where every element of each layer contributes to every element of the subsequent one.However, these are not practical for large, structured inputs like images, where the input might comprise ∼ 10 6 pixels.The number of parameters would be prohibitive, and moreover, we want different parts of the image to be processed similarly; there is no point in independently learning to recognize the same object at every possible position in the image.The solution is to process local image regions in parallel and then gradually integrate information from increasingly large regions.This kind of local-to-global processing is difficult to specify without using multiple layers (see chapter 10).A further possible advantage of deep networks over shallow networks is their ease of fitting; it is usually easier to train moderately deep networks than to train shallow ones (see figure 20.2).It may be that over-parameterized deep models (i.e., those with more parameters than training examples) have a large family of roughly equivalent solutions that are easy to find.However, as we add more hidden layers, training becomes more difficult again.Many methods have been developed to mitigate this problem (see chapter 11).Deep neural networks also seem to generalize to new data better than shallow ones.In practice, the best results for most tasks have been achieved using networks with tens or hundreds of layers.Neither of these phenomena are well understood, and we return to them in chapter 20.In this chapter, we first considered what happens when we compose two shallow networks.We argued that the first network "folds" the input space, and the second network then applies a piecewise linear function.The effects of the second network are duplicated where the input space is folded onto itself.We then showed that this composition of shallow networks is a special case of a deep network with two layers.We interpreted the ReLU functions in each layer as clipping the input functions in multiple places and creating more "joints" in the output function.We introduced the idea of hyperparameters, which for the networks we've seen so far, comprise the number of hidden layers and the number of hidden units in each.Finally, we compared shallow and deep networks.We saw that (i) both networks can approximate any function given enough capacity, (ii) deep networks produce many more linear regions per parameter, (iii) some functions can be approximated much more efficiently by deep networks, (iv) large, structured inputs like images are best processed in multiple stages, and (v) in practice, the best results for most tasks are achieved using deep networks with many layers.Now that we understand deep and shallow network models, we turn our attention to training them.In the next chapter, we discuss loss functions.For any given parameter values ϕ, the loss function returns a single number that indicates the mismatch between the model outputs and the ground truth predictions for a training dataset.In chapters 6 and 7, we deal with the training process itself, in which we seek the parameter values that minimize this loss.Deep learning: It has long been understood that it is possible to build more complex functions by composing shallow neural networks or developing networks with more than one hidden layer.Indeed, the term "deep learning" was first used by Dechter (1986).However, interest was limited due to practical concerns; it was not possible to train such networks well.The modern era of deep learning was kick-started by startling improvements in image classification reported by Krizhevsky et al. (2012).This sudden progress was arguably due to the confluence of four factors: larger training datasets, improved processing power for training, the use of the ReLU activation function, and the use of stochastic gradient descent (see chapter 6).LeCun et al. (2015) present an overview of early advances in the modern era of deep learning.2018) provide an algorithm that counts the number of linear regions in a neural network, although it is only practical for very small networks.If the number of hidden units D in each of the K layers is the same, and D is an integer multiple of the input dimensionality Di, then the maximum number of linear regions Nr can be computed exactly and is:(4.17)The first term in this expression corresponds to the first K − 1 layers of the network, which can be thought of as repeatedly folding the input space.However, we now need to devote D/Di hidden units to each input dimension to create these folds.The last term in this equation (a sum of binomial coefficients) is the number of regions that a shallow network can create and isWe argued in section 4.5.1 that if the layers of a deep network have enough hidden units, then the width version of the universal approximation theorem applies: there exists a network that can approximate any given continuous function on a compact subset of R D i to arbitrary accuracy.Lu et al. (2017) proved that there exists a network with ReLU activation functions and at least Di + 4 hidden units in each layer can approximate any specified Di-dimensional Lebesgue integrable function to arbitrary accuracy given enough layers.This is known as the depth version of the universal approximation theorem.Several results show that there are functions that can be realized by deep networks but not by any shallow network whose capacity is bounded above exponentially.In other words, it would take an exponentially larger number of units in a shallow network to describe these functions accurately.This is known as the depth efficiency of neural networks.Telgarsky (2016) shows that for any integer k, it is possible to construct networks with one input, one output, and O[k 3 ] layers of constant width, which cannot be realized with O[k] layers and less than 2 k width.Perhaps surprisingly, Eldan & Shamir (2016) showed that when there are multivariate inputs, there is a three-layer network that cannot be realized by any two-layer network if the capacity is sub-exponential in the input dimension.Cohen et al. (2016), Safran & Shamir (2017), and Poggio et al. (2017 also demonstrate functions that deep networks can approximate efficiently, but shallow ones cannot.Liang & Srikant (2016) show that for a broad class of functions, including univariate functions, shallow networks require exponentially more hidden units than deep networks for a given upper bound on the approximation error.Width efficiency: Lu et al. (2017) investigate whether there are wide shallow networks (i.e., shallow networks with lots of hidden units) that cannot be realized by narrow networks whose depth is not substantially larger.They show that there exist classes of wide, shallow networks that can only be expressed by narrow networks with polynomial depth.This is known as the width efficiency of neural networks.This polynomial lower bound on width is less restrictive than the exponential lower bound on depth, suggesting that depth is more important.Vardi et al. ( 2022) subsequently showed that the price for making the width small is only a linear increase in the network depth for networks with ReLU activations.ReLUwhere λ0 and λ1 are non-negative scalars.From this, we see that the weight matrices can be rescaled by any magnitude as long as the biases are also adjusted, and the scale factors can be re-applied at the end of the network.Problem 4.8 * Figure 4.9 shows the activations in the three hidden units of a shallow network (as in figure 3.3).The slopes in the hidden units are 1.0, 1.0, and -1.0, respectively, and the "joints" in the hidden units are at positions 1/6, 2/6, and 4/6.Find values of ϕ0, ϕ1, ϕ2, and ϕ3 that will combine the hidden unit activations as ϕ0 + ϕ1h1 + ϕ2h2 + ϕ3h3 to create a function with four linear regions that oscillate between output values of zero and one.The slope of the leftmost region should be positive, the next one negative, and so on.How many linear regions will we create if we compose this network with itself?How many will we create if we compose it with itself K times?Problem 4.9 * Following problem 4.8, is it possible to create a function with three linear regions that oscillates back and forth between output values of zero and one using a shallow network with two hidden units?Is it possible to create a function with five linear regions that oscillates in the same way using a shallow network with four hidden units?Draft: please send errata to udlbookmail@gmail.com.Chapter 5The last three chapters described linear regression, shallow neural networks, and deep neural networks.Each represents a family of functions that map input to output, where the particular member of the family is determined by the model parameters ϕ.When we train these models, we seek the parameters that produce the best possible mapping from input to output for the task we are considering.This chapter defines what is meant by the "best possible" mapping.That definition requires a training dataset {x i , y i } of input/output pairs.A loss function or cost function L[ϕ] returns a single number that describes the mismatch between the model predictions f[x i , ϕ] and their corresponding ground-truth outputs y i .During training, we seek parameter values ϕ that minimize the loss and hence map the training inputs to the outputs as closely as possible.We saw one example of a loss function in chapter 2; the least squares loss function is suitable for univariate regression problems for which the target is a real number y ∈ R. It computes the sum of the squares This chapter provides a framework that both justifies the choice of the least squares criterion for real-valued outputs and allows us to build loss functions for other prediction types.We consider binary classification, where the prediction y ∈ {0, 1} is one of two categories, multiclass classification, where the prediction y ∈ {1, 2, . . ., K} is one of K categories, and more complex cases.In the following two chapters, we address model training, where the goal is to find the parameter values that minimize these loss functions.In this section, we develop a recipe for constructing loss functions.Consider a model f[x, ϕ] with parameters ϕ that computes an output from input x.Until now, we haveThis raises the question of exactly how a model f[x, ϕ] can be adapted to compute a probability distribution.The solution is simple.First, we choose a parametric distribution P r(y|θ) defined on the output domain y.Then we use the network to compute one or more of the parameters θ of this distribution.For example, suppose the prediction domain is the set of real numbers, so y ∈ R. Here, we might choose the univariate normal distribution, which is defined on R.This distribution is defined by the mean µ and variance σ 2 , so θ = {µ, σ 2 }.The machine learning model might predict the mean µ, and the variance σ 2 could be treated as an unknown constant.The model now computes different distribution parameters θ i = f[x i , ϕ] for each training input x i .Each observed training output y i should have high probability under its corresponding distribution P r(y i |θ i ).Hence, we choose the model parameters ϕ so that they maximize the combined probability across all I training examples:(5.1)The combined probability term is the likelihood of the parameters, and hence equation 5.1 is known as the maximum likelihood criterion. 1Here we are implicitly making two assumptions.First, we assume that the data are identically distributed (the form of the probability distribution over the outputs y i is the same for each data point).Second, we assume that the conditional distributions P r(y i |x i ) of the output given the input are independent, so the total likelihood of the training data decomposes as:(5.2)In other words, we assume the data are independent and identically distributed (i.i.d.).].All positions on g[z] with a positive slope retain a positive slope after the log transform, and those with a negative slope retain a negative slope.The position of the maximum remains the same.The maximum likelihood criterion (equation 5.1) is not very practical.Each term P r(y i |f[x i , ϕ]) can be small, so the product of many of these terms can be tiny.It may be difficult to represent this quantity with finite precision arithmetic.Fortunately, we can equivalently maximize the logarithm of the likelihood:] and vice versa (figure 5.2).It follows that when we change the model parameters ϕ to improve the log-likelihood criterion, we also improve the original maximum likelihood criterion.It also follows that the overall maxima of the two criteria must be in the same place, so the best model parameters φ are the same in both cases.However, the log-likelihood criterion has the practical advantage of using a sum of terms, not a product, so representing it with finite precision isn't problematic.Finally, we note that, by convention, model fitting problems are framed in terms of minimizing a loss.To convert the maximum log-likelihood criterion to a minimization problem, we multiply by minus one, which gives us the negative log-likelihood criterion:(5.4)which is what forms the final loss function L[ϕ].The network no longer directly predicts the outputs y but instead determines a probability distribution over y.When we perform inference, we often want a point estimate rather than a distribution, so we return the maximum of the distribution:It is usually possible to find an expression for this in terms of the distribution parameters θ predicted by the model.For example, in the univariate normal distribution, the maximum occurs at the mean µ.The recipe for constructing loss functions for training data {x i , y i } using the maximum likelihood approach is hence:1. Choose a suitable probability distribution P r(y|θ) defined over the domain of the predictions y with distribution parameters θ. 2. Set the machine learning model f[x, ϕ] to predict one or more of these parameters, so θ = f[x, ϕ] and P r(y|θ) = P r(y|f[x, ϕ]). 3. To train the model, find the network parameters φ that minimize the negative log-likelihood loss function over the training dataset pairs {x i , y i }:(5.6) 4. To perform inference for a new test example x, return either the full distribution P r (y|f[x, φ]) or the maximum of this distribution.We devote most of the rest of this chapter to constructing loss functions for common prediction types using this recipe.Since the total probability density sums to one, the peak becomes higher as the variance decreases and the distribution becomes narrower.We start by considering univariate regression models.Here the goal is to predict a single scalar output y ∈ R from input x using a model f[x, ϕ] with parameters ϕ.Following the recipe, we choose a probability distribution over the output domain y.We select the univariate normal (figure 5.3), which is defined over y ∈ R.This distribution has two parameters (mean µ and variance σ 2 ) and has a probability density function:(5.7)Second, we set the machine learning model f[x, ϕ] to compute one or more of the parameters of this distribution.Here, we just compute the mean so µ = f[x, ϕ]:(5.8)We aim to find the parameters ϕ that make the training data {x i , y i } most probable under this distribution (figure 5.4).To accomplish this, we choose a loss function L[ϕ] based on the negative log-likelihood:(5.9)When we train the model, we seek parameters φ that minimize this loss.Draft: please send errata to udlbookmail@gmail.com.Now let's perform some algebraic manipulations on the loss function.We seek:where we have removed the first term between the second and third lines because it does not depend on ϕ.We have removed the denominator between the third and fourth lines, as this is just a constant scaling factor that does not affect the position of the minimum.The result of these manipulations is the least squares loss function that we originally introduced when we discussed linear regression in chapter 2:(5.11)We see that the least squares loss function follows naturally from the assumptions that Notebook 5.1 Least squares loss the prediction errors are (i) independent and (ii) drawn from a normal distribution with mean µ = f[x i , ϕ] (figure 5.4).The network no longer directly predicts y but instead predicts the mean µ = f[x, ϕ] of the normal distribution over y.When we perform inference, we usually want a single "best" point estimate ŷ, so we take the maximum of the predicted distribution:(5.12)For the univariate normal, the maximum position is determined by the mean parameter µ (figure 5.3).This is precisely what the model computed, so ŷ = f[x, φ].To formulate the least squares loss function, we assumed that the network predicted the mean of a normal distribution.The final expression in equation 5.11 (perhaps surpris- Here the fit is good, so these deviations are small (e.g., for the two highlighted points).b) For these parameters, the fit is bad, and the squared deviations are large.c) The least squares criterion follows from the assumption that the model predicts the mean of a normal distribution over the outputs and that we maximize the probability.For the first case, the model fits well, so the probability P r(yi|xi) of the data (horizontal orange dashed lines) is large (and the negative log probability is small).d) For the second case, the model fits badly, so the probability is small and the negative log probability is large.ingly) does not depend on the variance σ 2 .However, there is nothing to stop us from treating σ 2 as a parameter of the model and minimizing equation 5.9 with respect to both the model parameters ϕ and the distribution variance σ 2 :(5.13)In inference, the model predicts the mean µ = f[x, φ] from the input, and we learned the variance σ2 during the training process.The former is the best prediction.The latter tells us about the uncertainty of the prediction.The model above assumes that the variance of the data is constant everywhere.However, this might be unrealistic.When the uncertainty of the model varies as a function of the input data, we refer to this as heteroscedastic (as opposed to homoscedastic, where the uncertainty is constant).A simple way to model this is to train a neural network f[x, ϕ] that computes both the mean and the variance.For example, consider a shallow network with two outputs.We denote the first output as f 1 [x, ϕ] and use this to predict the mean, and we denote the second output as f 2 [x, ϕ] and use it to predict the variance.There is one complication; the variance must be positive, but we can't guarantee that the network will always produce a positive output.To ensure that the computed variance is positive, we pass the second network output through a function that maps an arbitrary value to a positive one.A suitable choice is the squaring function, giving: (5.14) which results in the loss function:(5.15)Homoscedastic and heteroscedastic models are compared in figure 5.5.In binary classification, the goal is to assign the data x to one of two discrete classes y ∈ {0, 1}.In this context, we refer to y as a label.Examples of binary classification include (i) predicting whether a restaurant review is positive (y = 1) or negative (y = 0) from text data x and (ii) predicting whether a tumor is present (y = 1) or absent (y = 0) from an MRI scan x.Figure 5.5 Homoscedastic vs. heteroscedastic regression.a) A shallow neural network for homoscedastic regression predicts just the mean µ of the output distribution from the input x. b) The result is that while the mean (blue line) is a piecewise linear function of the input x, the variance is constant everywhere (arrows and gray region show ±2 standard deviations).c) A shallow neural network for heteroscedastic regression also predicts the variance σ 2 (or, more precisely, computes its square root, which we then square).d) The standard deviation now also becomes a piecewise linear function of the input x.Once again, we follow the recipe from section 5.2 to construct the loss function.First, we choose a probability distribution over the output space y ∈ {0, 1}.A suitable choice is the Bernoulli distribution, which is defined on the domain {0, 1}.This has a single parameter λ ∈ [0, 1] that represents the probability that y takes the value one (figure 5.6):which can equivalently be written as:(5.17)Second, we set the machine learning model f[x, ϕ] to predict the single distribution parameter λ.However, λ can only take values in the range [0, 1], and we cannot guarantee that the network output will lie in this range.Consequently, we pass the network output through a function that maps the real numbers R to [0, 1].A suitable function is the logistic sigmoid (figure 5.7):.(5.18)Hence, we predict the distribution parameter asThe likelihood is now:This is depicted in figure 5.8 for a shallow neural network model.The loss function is the negative log-likelihood of the training set:(5.20)For reasons to be explained in section 5.7, this is known as the binary cross-entropy loss.The transformed model output sig[f[x, ϕ]] predicts the parameter λ of the Bernoulli Notebook 5.2 Binary cross-entropy loss distribution.This represents the probability that y = 1, and it follows that 1 − λ represents the probability that y = 0.When we perform inference, we may want a point Problem 5.2 estimate of y, so we set y = 1 if λ > 0.5 and y = 0 otherwise.Figure 5.9 Categorical distribution.The categorical distribution assigns probabilities to K > 2 categories, with associated probabilities λ1, λ2, . . ., λK .Here, there are five categories, so K = 5.To ensure that this is a valid probability distribution, each parameter λ k must lie in the range [0, 1], and all K parameters must sum to one.The goal of multiclass classification is to assign an input data example x to one of K > 2 classes, so y ∈ {1, 2, . . ., K}. Real-world examples include (i) predicting which of K = 10 digits y is present in an image x of a handwritten number and (ii) predicting which of K possible words y follows an incomplete sentence x.We once more follow the recipe from section 5.2.We first choose a distribution over the prediction space y.In this case, we have y ∈ {1, 2, . . ., K}, so we choose the categorical distribution (figure 5.9), which is defined on this domain.This has K parameters λ 1 , λ 2 , . . ., λ K , which determine the probability of each category: (5.21)The parameters are constrained to take values between zero and one, and they must collectively sum to one to ensure a valid probability distribution.Then we use a network f[x, ϕ] with K outputs to compute these K parameters from the input x.Unfortunately, the network outputs will not necessarily obey the aforementioned constraints.Consequently, we pass the K outputs of the network through a function that ensures these constraints are respected.A suitable choice is the softmax function (figure 5.10).This takes an arbitrary vector of length K and returns a vector of the same length but where the elements are now in the range [0, 1] and sum to one.The k th output of the softmax function is:where the exponential functions ensure positivity, and the sum in the denominator en-sures that the K numbers sum to one.The likelihood that input x has label y = k (figure 5.10) is hence:(5.23)The loss function is the negative log-likelihood of the training data:(5.24)where f k [x, ϕ] denotes the k th output of the neural network.For reasons that will be explained in section 5.7, this is known as the multiclass cross-entropy loss.The transformed model output represents a categorical distribution over possible Notebook 5.3 Multiclass cross-entropy loss classes y ∈ {1, 2, . . ., K}.For a point estimate, we take the most probable category ŷ = argmax k [P r(y = k|f[x, φ])].This corresponds to whichever curve is highest for that value of x in figure 5.10.In this chapter, we have focused on regression and classification because these problems are widespread.However, to make different types of predictions, we simply choose an appropriate distribution over that domain and apply the recipe in section 5.2. Figure 5.11 enumerates a series of probability distributions and their prediction domains.Some of Problems 5.3-5.6 these are explored in the problems at the end of the chapter.Often, we wish to make more than one prediction with the same model, so the target output y is a vector.For example, we might want to predict a molecule's melting and boiling point (a multivariate regression problem, figure 1.2b) or the object class at every point in an image (a multivariate classification problem, figure 1.4a).While it is possible to define multivariate probability distributions and use a neural network to model their parameters as a function of the input, it is more usual to treat each prediction as independent.Independence implies that we treat the probability P r(y|f[x, ϕ]) as a product of When we minimize the negative log probability, this product becomes a sum of terms:(5.26)where y id is the d th output from the i th training example.To make two or more prediction types simultaneously, we similarly assume the errors in each are independent.For example, to predict wind direction and strength, we might Problems 5.7-5.10 choose the von Mises distribution (defined on circular domains) for the direction and the exponential distribution (defined on positive real numbers) for the strength.The independence assumption implies that the joint likelihood of the two predictions is the product of individual likelihoods.These terms will become additive when we compute the negative log-likelihood.Figure 5.12 Cross-entropy method.a) Empirical distribution of training samples (arrows denote Dirac delta functions).b) Model distribution (a normal distribution with parameters θ = µ, σ 2 ).In the cross-entropy approach, we minimize the distance (KL divergence) between these two distributions as a function of the model parameters θ.In this chapter, we developed loss functions that minimize negative log-likelihood.However, the term cross-entropy loss is also commonplace.In this section, we describe the cross-entropy loss and show that it is equivalent to using negative log-likelihood.The cross-entropy loss is based on the idea of finding parameters θ that minimize the distance between the empirical distribution q(y) of the observed data y and a model distribution P r(y|θ) (figure 5.12).The distance between two probability distributions q(z)where the first term disappears, as it has no dependence on θ.The remaining second term is known as the cross-entropy.It can be interpreted as the amount of uncertainty that remains in one distribution after taking into account what we already know from the other.Now, we substitute in the definition of q(y) from equation 5.28: (5.30)The product of the two terms in the first line corresponds to pointwise multiplying the point masses in figure 5.12a with the logarithm of the distribution in figure 5.12b.We are left with a finite set of weighted probability masses centered on the data points.In the last line, we have eliminated the constant scaling factor 1/I, as this does not affect the position of the minimum.In machine learning, the distribution parameters θ are computed by the model f[x i , ϕ], so we have:(5.31)This is precisely the negative log-likelihood criterion from the recipe in section 5.2.It follows that the negative log-likelihood criterion (from maximizing the data likelihood) and the cross-entropy criterion (from minimizing the distance between the model and empirical data distributions) are equivalent.We previously considered neural networks as directly predicting outputs y from data x.In this chapter, we shifted perspective to think about neural networks as computing the parameters θ of probability distributions P r(y|θ) over the output space.This led to a principled approach to building loss functions.We selected model parameters ϕ that maximized the likelihood of the observed data under these distributions.We saw that this is equivalent to minimizing the negative log-likelihood.The least squares criterion for regression is a natural consequence of this approach; it follows from the assumption that y is normally distributed and that we are predicting the mean.We also saw how the regression model could be (i) extended to estimate the uncertainty over the prediction and (ii) extended to make that uncertainty dependent on the input (the heteroscedastic model).We applied the same approach to both binary and multiclass classification and derived loss functions for each.We discussed how to tackle more complex data types and how to deal with multiple outputs.Finally, we argued that cross-entropy is an equivalent way to think about fitting models.In previous chapters, we developed neural network models.In this chapter, we developed loss functions for deciding how well a model describes the training data for a given set of parameters.The next chapter considers model training, in which we aim to find the model parameters that minimize this loss.Losses based on the normal distribution: Nix & Weigend (1994) and Williams (1996) investigated heteroscedastic nonlinear regression in which both the mean and the variance of the output are functions of the input.In the context of unsupervised learning, Burda et al.Qi et al. (2020) investigate the properties of regression models that minimize mean absolute error rather than mean squared error.This loss function follows from assuming a Laplace distribution over the outputs and estimates the median output for a given input rather than the mean.Barron (2019) presents a loss function that parameterizes the degree of robustness.When interpreted in a probabilistic context, it yields a family of univariate probability distributions that includes the normal and Cauchy distributions as special cases.Estimating quantiles: Sometimes, we may not want to estimate the mean or median in a regression task but may instead want to predict a quantile.For example, this is useful for risk models, where we want to know that the true value will be less than the predicted value 90% of the time.This is known as quantile regression (Koenker & Hallock, 2001).This could be done by fitting a heteroscedastic regression model and then estimating the quantile based on the predicted normal distribution.Alternatively, the quantiles can be estimated directly using quantile loss (also known as pinball loss).In practice, this minimizes the absolute deviations of the data from the model but weights the deviations in one direction more than the other.Recent work has investigated simultaneously predicting multiple quantiles to get an idea of the overall distribution shape (Rodrigues & Pereira, 2020).2009) all used the Plackett-Luce model in loss functions for learning to rank data.This is the listwise approach to learning to rank as the model ingests an entire list of objects to be ranked at once.Alternative approaches are the pointwise approach, in which the model ingests a single object, and the pairwise approach, where the model ingests pairs of objects.Chen et al. (2009) summarize different approaches for learning to rank.It is not strictly necessary to adopt the probabilistic approach discussed in this chapter, but this has become the default in recent years; any loss function that aims to reduce the distance between the model output and the training outputs will suffice, and distance can be defined in any way that seems sensible.There are several well-known non-probabilistic machine learning models for classification, including support vector machines (Vapnik, 1995;Cristianini & Shawe-Taylor, 2000), which use hinge loss, and AdaBoost (Freund & Schapire, 1997), which uses exponential loss.Problem 5.1 Show that the logistic sigmoid function sig[z] becomes 0 as z → −∞, is 0.5 when z = 0, and becomes 1 when z → ∞, where:.(5.32)Problem 5.2 The loss L for binary classification for a single training pair {x, y} is: where µ is a measure of the mean direction and κ is a measure of concentration (i.e., the inverse of the variance).The term Bessel0[κ] is a modified Bessel function of the first kind of order 0. Use the recipe from section 5.2 to develop a loss function for learning the parameter µ of a model f[x, ϕ] to predict the most likely wind direction.Your solution should treat the concentration κ as constant.How would you perform inference?Problem 5.4 * Sometimes, the outputs y for input x are multimodal (figure 5.14a); there is more than one valid prediction for a given input.Here, we might use a weighted sum of normal components as the distribution over the output.This is known as a mixture of Gaussians model.For example, a mixture of two Gaussians has parameters θ = {λ, µ1, σ 2 1 , µ2, σ 2 2 }:where λ ∈ [0, 1] controls the relative weight of the two components, which have means µ1, µ2 and variances σ 2 1 , σ 2 2 , respectively.This model can represent a distribution with two peaks (figure 5.14b) or a distribution with one peak but a more complex shape (figure 5.14c).Use the recipe from section 5.2 to construct a loss function for training a model f[x, ϕ] that takes input x, has parameters ϕ, and predicts a mixture of two Gaussians.The loss should be based on I training data pairs {xi, yi}.What problems do you foresee when performing inference?Problem 5.5 Consider extending the model from problem 5.3 to predict the wind direction using a mixture of two von Mises distributions.Write an expression for the likelihood P r(y|θ) for this model.How many outputs will the network need to produce? Figure 5.15 Poisson distribution.This discrete distribution is defined over nonnegative integers z ∈ {0, 1, 2, . ..}.It has a single parameter λ ∈ R + , which is known as the rate and is the mean of the distribution.a-c) Poisson distributions with rates of 1.4, 2.8, and 6.0, respectively.Problem 5.6 Consider building a model to predict the number of pedestrians y ∈ {0, 1, 2, . ..} that will pass a given point in the city in the next minute, based on data x that contains information about the time of day, the longitude and latitude, and the type of neighborhood.A suitable distribution for modeling counts is the Poisson distribution (figure 5.15).This has a single parameter λ > 0 called the rate that represents the mean of the distribution.The distribution has probability density function: Problem 5.9 * Consider a multivariate regression problem in which we predict the height of a person in meters and their weight in kilos from data x.Here, the units take quite different ranges.What problems do you see this causing?Propose two solutions to these problems.Problem 5.10 Extend the model from problem 5.3 to predict both the wind direction and the wind speed and define the associated loss function.Chapter 6Chapters 3 and 4 described shallow and deep neural networks.These represent families of piecewise linear functions, where the parameters determine the particular function.Chapter 5 introduced the loss -a single number representing the mismatch between the network predictions and the ground truth for a training set.The loss depends on the network parameters, and this chapter considers how to find the parameter values that minimize this loss.This is known as learning the network's parameters or simply as training or fitting the model.The process is to choose initial parameter values and then iterate the following two steps: (i) compute the derivatives (gradients) of the loss with respect to the parameters, and (ii) adjust the parameters based on the gradients to decrease the loss.After many iterations, we hope to reach the overall minimum of the loss function.This chapter tackles the second of these steps; we consider algorithms that adjust the parameters to decrease the loss.Chapter 7 discusses how to initialize the parameters and compute the gradients for neural networks.To fit a model, we need a training set {x i , y i } of input/output pairs.We seek parameters ϕ for the model f[x i , ϕ] that map the inputs x i to the outputs y i as well as possible.To this end, we define a loss function L[ϕ] that returns a single number that quantifies the mismatch in this mapping.The goal of an optimization algorithm is to find parameters φ that minimize the loss:There are many families of optimization algorithms, but the standard methods for training neural networks are iterative.These algorithms initialize the parameters heuristically and then adjust them repeatedly in such a way that the loss decreases.The simplest method in this class is gradient descent.This starts with initial parameters ϕ = [ϕ 0 , ϕ 1 , . . ., ϕ N ] T and iterates two steps:Step 1. Compute the derivatives of the loss with respect to the parameters:. . .Step 2. Update the parameters according to the rule:where the positive scalar α determines the magnitude of the change.The first step computes the gradient of the loss function at the current position.This determines the uphill direction of the loss function.The second step moves a small distance α downhill (hence the negative sign).The parameter α may be fixed (in which Notebook 6.1 Line search case, we call it a learning rate), or we may perform a line search where we try several values of α to find the one that most decreases the loss.At the minimum of the loss function, the surface must be flat (or we could improve further by going downhill).Hence, the gradient will be zero, and the parameters will stop changing.In practice, we monitor the gradient magnitude and terminate the algorithm when it becomes too small.Consider applying gradient descent to the 1D linear regression model from chapter 2. The model f[x, ϕ] maps a scalar input x to a scalar output y and has parameters ϕ = [ϕ 0 , ϕ 1 ] T , which represent the y-intercept and the slope:Given a dataset {x i , y i } containing I input/output pairs, we choose the least squares loss function:2 , (6.5) where the term ℓ i = (ϕ 0 + ϕ 1 x i − y i ) 2 is the individual contribution to the loss from the i th training example.The derivative of the loss function with respect to the parameters can be decomposed into the sum of the derivatives of the individual contributions:where these are given by: Problem 6.1.(6.7) Figure 6.1 shows the progression of this algorithm as we iteratively compute the Notebook 6.2 Gradient descent derivatives according to equations 6.6 and 6.7 and then update the parameters using the rule in equation 6.3.In this case, we have used a line search procedure to find the value of α that decreases the loss the most at each iteration.Loss functions for linear regression problems (figure 6.1c) always have a single welldefined global minimum.More formally, they are convex, which means that no chord Problem 6.2 (line segment between two points on the surface) intersects the function.Convexity implies that wherever we initialize the parameters, we are bound to reach the minimum if we keep walking downhill; the training procedure can't fail.Unfortunately, loss functions for most nonlinear models, including both shallow and deep networks, are non-convex.Visualizing neural network loss functions is challenging due to the number of parameters.Hence, we first explore a simpler nonlinear model with two parameters to gain insight into the properties of non-convex loss functions:2 .(6.9)Once more, the goal is to find the parameters φ that minimize this loss.increases if we move in any direction, but we are not at the overall minimum of the function.The point with the lowest loss is known as the global minimum and is depicted by the gray circle.If we start in a random position and use gradient descent to go downhill, there is Problems 6.7-6.8no guarantee that we will wind up at the global minimum and find the best parameters (figure 6.5a).It's equally or even more likely that the algorithm will terminate in one of the local minima.Furthermore, there is no way of knowing whether there is a better solution elsewhere.In addition, the loss function contains saddle points (e.g., the blue cross in figure 6.4).Here, the gradient is zero, but the function increases in some directions and decreases in others.If the current parameters are not exactly at the saddle point, then gradient descent can escape by moving downhill.However, the surface near the saddle point is flat, so it's hard to be sure that training hasn't converged; if we terminate the algorithm when the gradient is small, we may erroneously stop near a saddle point.The Gabor model has two parameters, so we could find the global minimum by either (i) exhaustively searching the parameter space or (ii) repeatedly starting gradient descent from different positions and choosing the result with the lowest loss.However, neural network models can have millions of parameters, so neither approach is practical.In short, using gradient descent to find the global optimum of a high-dimensional loss function is challenging.We can find a minimum, but there is no way to tell whether this is the global minimum or even a good one.One of the main problems is that the final destination of a gradient descent algorithm Notebook 6.3 Stochastic gradient descent is entirely determined by the starting point.Stochastic gradient descent (SGD) attempts to remedy this problem by adding some noise to the gradient at each step.The solution still moves downhill on average, but at any given iteration, the direction chosen is not necessarily in the steepest downhill direction.Indeed, it might not be downhill at all.The SGD algorithm has the possibility of moving temporarily uphill and hence jumping from one "valley" of the loss function to another (figure 6.5b).The mechanism for introducing randomness is simple.At each iteration, the algorithm chooses a random subset of the training data and computes the gradient from these examples alone.This subset is known as a minibatch or batch for short.The update rule for the model parameters ϕ t at iteration t is hence:where B t is a set containing the indices of the input/output pairs in the current batch and, as before, ℓ i is the loss due to the i th pair.The term α is the learning rate, and together with the gradient magnitude, determines the distance moved at each iteration.The learning rate is chosen at the start of the procedure and does not depend on the local properties of the function.The batches are usually drawn from the dataset without replacement.The algorithm works through the training examples until it has used all the data, at which point it Problem 6.9 starts sampling from the full training dataset again.A single pass through the entire training dataset is referred to as an epoch.A batch may be as small as a single example or as large as the whole dataset.The latter case is called full-batch gradient descent and is identical to regular (non-stochastic) gradient descent.An alternative interpretation of SGD is that it computes the gradient of a different loss function at each iteration; the loss function depends on both the model and the training data and hence will differ for each randomly selected batch.In this view, SGD performs deterministic gradient descent on a constantly changing loss function (figure 6.6).However, despite this variability, the expected loss and expected gradients at any point remain the same as for gradient descent.SGD has several attractive features.First, although it adds noise to the trajectory, it still improves the fit to a subset of the data at each iteration.Hence, the updates tend to be sensible even if they are not optimal.Second, because it draws training examples without replacement and iterates through the dataset, the training examples all still contribute equally.Third, it is less computationally expensive to compute the gradient from just a subset of the training data.Fourth, it can (in principle) escape local minima.Fifth, it reduces the chances of getting stuck near saddle points; it is likely that at least some of the possible batches will have a significant gradient at any point on the loss function.Finally, there is some evidence that SGD finds parameters for neural networks that cause them to generalize well to new data in practice (see section 9.2).SGD does not necessarily "converge" in the traditional sense.However, the hope is that when we are close to the global minimum, all the data points will be well described by the model.Consequently, the gradient will be small, whichever batch is chosen, and the parameters will cease to change much.In practice, SGD is often applied with a learning rate schedule.The learning rate α starts at a high value and is decreased by a constant factor every N epochs.The logic is that in the early stages of training, we want the algorithm to explore the parameter space, jumping from valley to valley to find a sensible region.In later stages, we are roughly in the right place and are more concerned with fine-tuning the parameters, so we decrease α to make smaller changes.A common modification to stochastic gradient descent is to add a momentum term.We update the parameters with a weighted combination of the gradient computed from the current batch and the direction moved in the previous step: (6.11)where m t is the momentum (which drives the update at iteration t), β ∈ [0, 1) controls the degree to which the gradient is smoothed over time, and α is the learning rate.The recursive formulation of the momentum calculation means that the gradient step is an infinite weighted sum of all the previous gradients, where the weights get smaller as we move back in time.The effective learning rate increases if all these gradients Problem 6.10 are aligned over multiple iterations but decreases if the gradient direction repeatedly changes as the terms in the sum cancel out.The overall effect is a smoother trajectory and reduced oscillatory behavior in valleys (figure 6.7).The momentum term can be considered a coarse prediction of where the SGD algorithm Notebook 6.4 Momentum will move next.Nesterov accelerated momentum (figure 6.8) computes the gradients at this predicted point rather than at the current point:  A traditional momentum update measures the gradient at point 1, moves some distance in this direction to point 2, and then adds the momentum term from the previous iteration (i.e., in the same direction as the dashed line), arriving at point 3.The Nesterov momentum update first applies the momentum term (moving from point 1 to point 4) and then measures the gradient and applies an update to arrive at point 5. (6.12)where now the gradients are evaluated at ϕ t − αβ • m t .One way to think about this is that the gradient term now corrects the path provided by momentum alone.Gradient descent with a fixed step size has the following undesirable property: it makes large adjustments to parameters associated with large gradients (where perhaps we should be more cautious) and small adjustments to parameters associated with small gradients (where perhaps we should explore further).When the gradient of the loss surface is much steeper in one direction than another, it is difficult to choose a learning rate that (i) makes good progress in both directions and (ii) is stable (figures 6.9a-b).A straightforward approach is to normalize the gradients so that we move a fixed distance (governed by the learning rate) in each direction.To do this, we first measure the gradient m t+1 and the pointwise squared gradient v t+1 :Then we apply the update rule:where the square root and division are both pointwise, α is the learning rate, and ϵ is a small constant that prevents division by zero when the gradient magnitude is zero.The term v t+1 is the squared gradient, and the positive root of this is used to normalize the gradient itself, so all that remains is the sign in each coordinate direction.The result is that the algorithm moves a fixed distance α along each coordinate, where the direction is determined by whichever way is downhill (figure 6.9c).This simple algorithm makes good progress in both directions but will not converge unless it happens to land exactly at the minimum.Instead, it will bounce back and forth around the minimum.Adaptive moment estimation, or Adam, takes this idea and adds momentum to both the estimate of the gradient and the squared gradient: If we run full-batch gradient descent with a learning rate that makes good progress in the vertical direction, then the algorithm takes a long time to reach the final horizontal position.b) If the learning rate is chosen so that the algorithm makes good progress in the horizontal direction, it overshoots in the vertical direction and becomes unstable.c) A straightforward approach is to move a fixed distance along each axis at each step so that we move downhill in both directions.This is accomplished by normalizing the gradient magnitude and retaining only the sign.However, this does not usually converge to the exact minimum but instead oscillates back and forth around it (here between the last two points).d) The Adam algorithm uses momentum in both the estimated gradient and the normalization term, which creates a smoother path.where β and γ are the momentum coefficients for the two statistics.Using momentum is equivalent to taking a weighted average over the history of each of these statistics.At the start of the procedure, all the previous measurements are effectively zero, resulting in unrealistically small estimates.Consequently, we modify these statistics using the rule:(6.16)Since β and γ are in the range [0, 1), the terms with exponents t + 1 become smaller with each time step, the denominators become closer to one, and this modification has a diminishing effect.Finally, we update the parameters as before, but with the modified terms:The result is an algorithm that can converge to the overall minimum and makes good Notebook 6.5 Adam progress in every direction in the parameter space.Note that Adam is usually used in a stochastic setting where the gradients and their squares are computed from mini-batches:and so the trajectory is noisy in practice.As we shall see in chapter 7, the gradient magnitudes of neural network parameters can depend on their depth in the network.Adam helps compensate for this tendency and balances out changes across the different layers.In practice, Adam also has the advantage of being less sensitive to the initial learning rate because it avoids situations like those in figures 6.9a-b, so it doesn't need complex learning rate schedules.The choices of learning algorithm, batch size, learning rate schedule, and momentum coefficients are all considered hyperparameters of the training algorithm; these directly affect the final model performance but are distinct from the model parameters.Choosing these can be more art than science, and it's common to train many models with different hyperparameters and choose the best one.This is known as hyperparameter search.We return to this issue in chapter 8.This chapter discussed model training.This problem was framed as finding parameters ϕ that corresponded to the minimum of a loss function L [ϕ].The gradient descent method measures the gradient of the loss function for the current parameters (i.e., how the loss changes when we make a small change to the parameters).Then it moves the parameters in the direction that decreases the loss fastest.This is repeated until convergence.For nonlinear functions, the loss function may have both local minima (where gradient descent gets trapped) and saddle points (where gradient descent may appear to have converged but has not).Stochastic gradient descent helps mitigate these problems. 1At each iteration, we use a different random subset of the data (a batch) to compute the gradient.This adds noise to the process and helps prevent the algorithm from getting trapped in a sub-optimal region of parameter space.Each iteration is also computationally cheaper since it only uses a subset of the data.We saw that adding a momentum term makes convergence more efficient.Finally, we introduced the Adam algorithm.The ideas in this chapter apply to optimizing any model.The next chapter tackles two aspects of training specific to neural networks.First, we address how to compute the gradients of the loss with respect to the parameters of a neural network.This is accomplished using the famous backpropagation algorithm.Second, we discuss how to initialize the network parameters before optimization begins.Without careful initialization, the gradients used by the optimization can become extremely large or extremely small, which can hinder the training process.Optimization algorithms: Optimization algorithms are used extensively throughout engineering, and it is generally more typical to use the term objective function rather than loss function or cost function.Gradient descent was invented by Cauchy (1847), and stochastic gradient descent dates back to at least Robbins & Monro (1951).A modern compromise between the two is stochastic variance-reduced descent (Johnson & Zhang, 2013), in which the full gradient is computed periodically, with stochastic updates interspersed.Reviews of optimization algorithms for neural networks can be found in Ruder (2016), Bottou et al. (2018), andSun (2020).Bottou (2012) discusses best practice for SGD, including shuffling without replacement.A function is convex if no chord (line segment between two points on the surface) intersects the function.This can be tested algebraically by considering the Hessian matrix (the matrix of second derivatives): values, then the function is convex; the loss function will look like a smooth bowl (as in figure 6.1c), so training will be relatively easy.There will be a single global minimum and no local minima or saddle points.For any loss function, the eigenvalues of the Hessian matrix at places where the gradient is zero allow us to classify this position as (i) a minimum (the eigenvalues are all positive), (ii) a maximum (the eigenvalues are all negative), or (iii) a saddle point (positive eigenvalues are associated with directions in which we are at a minimum and negative ones with directions where we are at a maximum).Line search: Gradient descent with a fixed step size is inefficient because the distance moved depends entirely on the magnitude of the gradient.It moves a long distance when the function is changing fast (where perhaps it should be more cautious) but a short distance when the function is changing slowly (where perhaps it should explore further).For this reason, gradient descent methods are usually combined with a line search procedure in which we sample the function along the desired direction to try to find the optimal step size.One such approach is bracketing (figure 6.10).Another problem with gradient descent is that it tends to lead to inefficient oscillatory behavior when descending valleys (e.g., path 1 in figure 6.5a).Beyond gradient descent: Numerous algorithms have been developed that remedy the problems of gradient descent.Most notable is the Newton method, which takes the curvature of the surface into account using the inverse of the Hessian matrix; if the gradient of the function is changing quickly, then it applies a more cautious update.This method eliminates the need for line search and does not suffer from oscillatory behavior.However, it has its own problems; in its simplest form, it moves toward the nearest extremum, but this may be a maximum if we are closer to the top of a hill than we are to the bottom of a valley.Moreover, computing the Problem 6.11inverse Hessian is intractable when the number of parameters is large, as in neural networks.The limit of SGD as the learning rate tends to zero is a stochastic differential equation.Jastrzębski et al. (2018) showed that this equation relies on the learningrate to batch size ratio and that there is a relation between the learning rate to batch size ratio and the width of the minimum found.The idea of using momentum to speed up optimization dates to Polyak (1964).Goh ( 2017) presents an in-depth discussion of the properties of momentum.The Nesterov Adaptive training algorithms: AdaGrad (Duchi et al., 2011) is an optimization algorithm that addresses the possibility that some parameters may have to move further than others by assigning a different learning rate to each parameter.AdaGrad uses the cumulative squared gradient for each parameter to attenuate its learning rate.This has the disadvantage that the learning rates decrease over time, and learning can halt before the minimum is found.RMSProp (Hinton et al., 2012a) and AdaDelta (Zeiler, 2012) modified this algorithm to help prevent these problems by recursively updating the squared gradient term.By far the most widely used adaptive training algorithm is adaptive moment optimization or Adam (Kingma & Ba, 2015).This combines the ideas of momentum (in which the gradient vector is averaged over time) and AdaGrad, AdaDelta, and RMSProp (in which a smoothed squared gradient term is used to modify the learning rate for each parameter  2017) proposed a method called SWATS that starts using Adam (to make rapid initial progress) and then switches to SGD (to get better final generalization performance).Exhaustive search: All the algorithms discussed in this chapter are iterative.A completely different approach is to quantize the network parameters and exhaustively search the resulting discretized parameter space using SAT solvers (Mézard & Mora, 2009).This approach has the potential to find the global minimum and provide a guarantee that there is no lower loss elsewhere but is only practical for very small models.Problem 6.1 Show that the derivatives of the least squares loss function in equation 6.5 are given by the expressions in equation 6.7.Problem 6.2 A surface is convex if the eigenvalues of the Hessian H[ϕ] are positive everywhere.In this case, the surface has a unique minimum, and optimization is easy.Find an algebraic expression for the Hessian matrix, (6.20) for the linear regression model (equation 6.5).Prove that this function is convex by showing Problem 6.7 * The gradient descent trajectory for path 1 in figure 6.5a oscillates back and forth inefficiently as it moves down the valley toward the minimum.It's also notable that it turns at right angles to the previous direction at each step.Provide a qualitative explanation for these phenomena.Propose a solution that might help prevent this behavior.Problem 6.8 * Can (non-stochastic) gradient descent with a fixed learning rate escape local minima?Problem 6.9 We run the stochastic gradient descent algorithm for 1,000 iterations on a dataset of size 100 with a batch size of 20.For how many epochs did we train the model?Problem 6.10 Show that the momentum term mt (equation 6.11) is an infinite weighted sum of the gradients at the previous iterations and derive an expression for the coefficients (weights) of that sum.Problem 6.11 What dimensions will the Hessian have if the model has one million parameters?Draft: please send errata to udlbookmail@gmail.com.Chapter 7Chapter 6 introduced iterative optimization algorithms.These are general-purpose methods for finding the minimum of a function.In the context of neural networks, they find parameters that minimize the loss so that the model accurately predicts the training outputs from the inputs.The basic approach is to choose initial parameters randomly and then make a series of small changes that decrease the loss on average.Each change is based on the gradient of the loss with respect to the parameters at the current position.This chapter discusses two issues that are specific to neural networks.First, we consider how to calculate the gradients efficiently.This is a serious challenge since the largest models at the time of writing have ∼10 12 parameters, and the gradient needs to be computed for every parameter at every iteration of the training algorithm.Second, we consider how to initialize the parameters.If this is not done carefully, the initial losses and their gradients can be extremely large or small.In either case, this impedes the training process.Consider a network f[x, ϕ] with multivariate input x, parameters ϕ, and three hidden layers h 1 , h 2 , and h 3 :where the function a[•] applies the activation function separately to every element of the input.The model parameters ϕ = {β 0 , Ω 0 , β 1 , Ω 1 , β 2 , Ω 2 , β 3 , Ω 3 } consist of the bias vectors β k and weight matrices Ω k between every layer (figure 7.1).We also have individual loss terms ℓ i , which return the negative log-likelihood of the ground truth label y i given the model prediction f[x i , ϕ] for training input x i .For example, this might be the least squares loss ℓ i = (f[x i , ϕ] − y i ) 2 .The total loss is the sum of these terms over the training data:The most commonly used optimization algorithm for training neural networks is stochastic gradient descent (SGD), which updates the parameters as:where α is the learning rate, and B t contains the batch indices at iteration t.To compute this update, we need to calculate the derivatives: the batch.The first part of this chapter describes the backpropagation algorithm, which computes these derivatives efficiently.In the second part of the chapter, we consider how to initialize the network parameters before we commence training.We describe methods to choose the initial weights Ω k and biases β k so that training is stable.The derivatives of the loss tell us how the loss changes when we make a small change to the parameters.Optimization algorithms exploit this information to manipulate the parameters so that the loss becomes smaller.The backpropagation algorithm computes these derivatives.The mathematical details are somewhat involved, so we first make two observations that provide some intuition.Observation 1: Each weight (element of Ω k ) multiplies the activation at a source hidden unit and adds the result to a destination hidden unit in the next layer.It follows that the effect of any small change to the weight is amplified or attenuated by the activation at the source hidden unit.Hence, we run the network for each data example in the batch and store the activations of all the hidden units.This is known as the forward pass (figure 7.1).The stored activations will subsequently be used to compute the gradients.Observation 2: A small change in a bias or weight causes a ripple effect of changes through the subsequent network.The change modifies the value of its destination hidden In other words, we want to know how a small change to each parameter will affect the loss.Each weight multiplies the hidden unit at its source and contributes the result to the hidden unit at its destination.Consequently, the effects of any small change to the weight will be scaled by the activation of the source hidden unit.For example, the blue weight is applied to the second hidden unit at layer 1; if the activation of this unit doubles, then the effect of a small change to the blue weight will double too.Hence, to compute the derivatives of the weights, we need to calculate and store the activations at the hidden layers.This is known as the forward pass since it involves running the network equations sequentially.unit.This, in turn, changes the values of the hidden units in the subsequent layer, which will change the hidden units in the layer after that, and so on, until a change is made to the model output and, finally, the loss.Hence, to know how changing a parameter modifies the loss, we also need to know how changes to every subsequent hidden layer will, in turn, modify their successor.These same quantities are required when considering other parameters in the same or earlier layers.It follows that we can calculate them once and reuse them.For example, consider computing the effect of a small change in weights that feed into hidden layers h 3 , h 2 , and h 1 , respectively:• To calculate how a small change in a weight or bias feeding into hidden layer h 3 modifies the loss, we need to know (i) how a change in layer h 3 changes the model output f , and (ii) how a change in this output changes the loss ℓ (figure 7.2a).• To calculate how a small change in a weight or bias feeding into hidden layer h 2 modifies the loss, we need to know (i) how a change in layer h 2 affects h 3 , (ii) how h 3 changes the model output, and (iii) how this output changes the loss (figure 7.2b).• To calculate how a small change in a weight or bias feeding into hidden layer h 1 modifies the loss, we need to know (i) how a change in layer h 1 affects layer h 2 , (ii) how a change in layer h 2 affects layer h 3 , (iii) how layer h 3 changes the model output, and (iv) how the model output changes the loss (figure 7.2c).To compute how a small change to a weight feeding into h2 (blue arrow) changes the loss, we need to know (i) how the hidden unit in h2 changes h3, (ii) how h3 changes f , and (iii) how f changes the loss (orange arrows).c) Similarly, to compute how a small change to a weight feeding into h1 (blue arrow) changes the loss, we need to know how h1 changes h2 and how these changes propagate through to the loss (orange arrows).The backward pass first computes derivatives at the end of the network and then works backward to exploit the inherent redundancy of these computations.As we move backward through the network, we see that most of the terms we need were already calculated in the previous step, so we do not need to re-compute them.Proceeding backward through the network in this way to compute the derivatives is known as the backward pass.The ideas behind backpropagation are relatively easy to understand.However, the derivation requires matrix calculus because the bias and weight terms are vectors and matrices, respectively.To help grasp the underlying mechanics, the following section derives backpropagation for a simpler toy model with scalar parameters.We then apply the same approach to a deep neural network in section 7.4.(7.5) and a least squares loss function L[ϕ] = i ℓ i with individual terms: (7.6) where, as usual, x i is the i th training input, and y i is the i th training output.You can think of this as a simple neural network with one input, one output, one hidden unit at each layer, and different activation functions sin[•], exp[•], and cos[•] between each layer.We aim to compute the derivatives:, and ∂ℓ i ∂ω 3 .Of course, we could find expressions for these derivatives by hand and compute them directly.However, some of these expressions are quite complex.For example:Such expressions are awkward to derive and code without mistakes and do not exploit the inherent redundancy; notice that the three exponential terms are the same.The backpropagation algorithm is an efficient method for computing all of these derivatives at once.It consists of (i) a forward pass, in which we compute and store a series of intermediate values and the network output, and (ii) a backward pass, in which we calculate the derivatives of each parameter, starting at the end of the network, and reusing previous calculations as we move toward the start.We treat the computation of the loss as a series of calculations:(7.8)We compute and store the values of the intermediate variables f k and h k (figure 7.3).We now compute the derivatives of ℓ i with respect to these intermediate variables, but in reverse order:, and ∂ℓ i ∂f 0 .(7.9)The first of these derivatives is straightforward:(7.10)The next derivative can be calculated using the chain rule:The left-hand side asks how ℓ i changes when h 3 changes.The right-hand side says we can decompose this into (i) how f 3 changes when h 3 changes and (ii) how ℓ i changes when f 3 changes.In the original equations, h 3 changes f 3 , which changes ℓ i , and the derivatives represent the effects of this chain.Notice that we already computed the second of these derivatives, and the other is the derivative of β 3 + ω 3 • h 3 with respect to h 3 , which is ω 3 .We continue in this way, computing the derivatives of the output with respect to these intermediate quantities (figure 7.4):In each case, we have already computed the quantities in the brackets in the previous Problem 7.2step, and the last term has a simple expression.These equations embody Observation 2 from the previous section (figure 7.2); we can reuse the previously computed derivatives if we calculate them in reverse order.Backward pass #2: Finally, we consider how the loss ℓ i changes when we change the parameters {β k } and {ω k }.Once more, we apply the chain rule (figure 7.5):In each case, the second term on the right-hand side was computed in equation 7.12.When k > 0, we have This is consistent with Observation 1 from the previous section; the effect of a change in the weight ω k is proportional to the value of the source variable h k (which was stored in the forward pass).The final derivatives from the term f 0 = β 0 + ω 0 • x i are:Backpropagation is both simpler and more efficient than computing the derivatives individually, as in equation 7.7. 1Now we repeat this process for a three-layer network (figure 7.1).The intuition and much of the algebra are identical.The main differences are that intermediate variables f k , h k are vectors, the biases β k are vectors, the weights Ω k are matrices, and we are using ReLU functions rather than simple algebraic functions like cos [•].We write the network as a series of sequential calculations:(7.16)Figure 7.6 Derivative of rectified linear unit.The rectified linear unit (orange curve) returns zero when the input is less than zero and returns the input otherwise.Its derivative (cyan curve) returns zero when the input is less than zero (since the slope here is zero) and one when the input is greater than zero (since the slope here is one).where f k−1 represents the pre-activations at the k th hidden layer (i.e., the values before the ReLU function a[•]) and h k contains the activations at the k th hidden layer (i.e., after the ReLU function).The term l[f 3 , y i ] represents the loss function (e.g., least squares or binary cross-entropy loss).In the forward pass, we work through these calculations and store all the intermediate quantities.Backward pass #1: Now let's consider how the loss changes when we modify the preactivations f 0 , f 1 , f 2 .Applying the chain rule, the expression for the derivative of the Appendix B.5 Matrix calculus loss ℓ i with respect to f 2 is:(7.17)The three terms on the right-hand side have sizes D 3 × D 3 , D 3 × D f , and D f × 1, respectively, where D 3 is the number of hidden units in the third layer, and D f is the dimensionality of the model output f 3 .Similarly, we can compute how the loss changes when we change f 1 and f 0 :Note that in each case, the term in brackets was computed in the previous step.By hand side of equation 7.17, we have:• The derivative ∂ℓ i /∂f 3 of the loss ℓ i with respect to the network output f 3 will depend on the loss function but usually has a simple form.• The derivative ∂f 3 /∂h 3 of the network output with respect to hidden layer h 3 is:If you are unfamiliar with matrix calculus, this result is not obvious.It is explored Problem 7.6 in problem 7.6.• The derivative ∂h 3 /∂f 2 of the output h 3 of the activation function with respect to its input f 2 will depend on the activation function.It will be a diagonal matrix since each activation only depends on the corresponding pre-activation.For ReLU functions, the diagonal terms are zero everywhere f 2 is less than zero and oneProblems 7.7-7.8otherwise (figure 7.6).Rather than multiply by this matrix, we extract the diagonal terms as a vector I[f 2 > 0] and pointwise multiply, which is more efficient.The terms on the right-hand side of equations 7.18 and 7.19 have similar forms.As we progress back through the network, we alternately (i) multiply by the transpose of the weight matrices Ω T k and (ii) threshold based on the inputs f k−1 to the hidden layer.These inputs were stored during the forward pass.Backward pass #2: Now that we know how to compute ∂ℓ i /∂f k , we can focus on calculating the derivatives of the loss with respect to the weights and biases.To calculate the derivatives of the loss with respect to the biases β k , we again use the chain rule:which we already calculated in equations 7.17 and 7.18.Similarly, the derivative for the weights matrix Ω k , is given by:Again, the progression from line two to line three is not obvious and is explored in Problem 7.9 problem 7.9.However, the result makes sense.The final line is a matrix of the same size as Ω k .It depends linearly on h k , which was multiplied by Ω k in the original expression.This is also consistent with the initial intuition that the derivative of the weights in Ω k will be proportional to the values of the hidden units h k that they multiply.Recall that we already computed these during the forward pass.We now briefly summarize the final backpropagation algorithm.Consider a deep neural network f[x i , ϕ] that takes input x i , has K hidden layers with ReLU activations, and individual loss termThe goal of backpropagation is to compute the derivatives ∂ℓ i /∂β k and ∂ℓ i /∂Ω k with respect to the biases β k and weights Ω k .We compute and store the following quantities:Backward pass: We start with the derivative ∂ℓ i /∂f K of the loss function ℓ i with respect to the network output f K and work backward through the network: 7.24) where ⊙ denotes pointwise multiplication, and I[f k−1 > 0] is a vector containing ones where f k−1 is greater than zero and zeros elsewhere.Finally, we compute the derivatives with respect to the first set of biases and weights:We calculate these derivatives for every training example in the batch and sum them Problem 7.10 together to retrieve the gradient for the SGD update.Note that the backpropagation algorithm is extremely efficient; the most demanding computational step in both the forward and backward pass is matrix multiplication (by Ω and Ω T , respectively) which only requires additions and multiplications.However, it is not memory efficient; the intermediate values in the forward pass must all be stored, and this can limit the size of the model we can train.Although it's important to understand the backpropagation algorithm, it's unlikely that you will need to code it in practice.Modern deep learning frameworks such as PyTorch and TensorFlow calculate the derivatives automatically, given the model specification.This is known as algorithmic differentiation.Each functional component (linear transform, ReLU activation, loss function) in the framework knows how to compute its own derivative.For example, the PyTorch ReLU function z out = relu[z in ] knows how to compute the derivative of its output z out with respect to its input z in .Similarly, a linear function z out = β + Ωz in knows how to compute the derivatives of the output z out with respect to the input z in and with respect to the parameters β and Ω.The algorithmic differentiation framework also knows the sequence of operations in the network and thus has all the information required to perform the forward and backward passes.These frameworks exploit the massive parallelism of modern graphics processing units (GPUs).Computations such as matrix multiplication (which features in both the forward and backward pass) are naturally amenable to parallelization.Moreover, it's possible to Problem 7.11 perform the forward and backward passes for the entire batch in parallel if the model and intermediate results in the forward pass do not exceed the available memory.Since the training algorithm now processes the entire batch in parallel, the input becomes a multi-dimensional tensor.In this context, a tensor can be considered the generalization of a matrix to arbitrary dimensions.Hence, a vector is a 1D tensor, a matrix is a 2D tensor, and a 3D tensor is a 3D grid of numbers.Until now, the training data have been 1D, so the input for backpropagation would be a 2D tensor where the first dimension indexes the batch element and the second indexes the data dimension.In subsequent chapters, we will encounter more complex structured input data.For example, in models where the input is an RGB image, the original data examples are 3D (height × width × channel).Here, the input to the learning framework would be a 4D tensor, where the extra dimension indexes the batch element.We have described backpropagation in a deep neural network that is naturally sequential; we calculate the intermediate quantities f 0 , h 1 , f 1 , h 2 . . ., f k in turn.However, models need not be restricted to sequential computation.Later in this book, we will meet models with branching structures.For example, we might take the values in a hidden layer and process them through two different sub-networks before recombining.Problems 7.12-7.13Fortunately, the ideas of backpropagation still hold if the computational graph is acyclic.Modern algorithmic differentiation frameworks such as PyTorch and TensorFlow can handle arbitrary acyclic computational graphs.The backpropagation algorithm computes the derivatives that are used by stochastic gradient descent and Adam to train the model.We now address how to initialize the parameters before we start training.To see why this is crucial, consider that during the forward pass, each set of pre-activations f k is computed as:where a[•] applies the ReLU functions and Ω k and β k are the weights and biases, respectively.Imagine that we initialize all the biases to zero and the elements of Ω k according to a normal distribution with mean zero and variance σ 2 .Consider two scenarios:• If the variance σ 2 is very small (e.g., 10 −5 ), then each element of β k + Ω k h k will be a weighted sum of h k where the weights are very small; the result will likely have a smaller magnitude than the input.In addition, the ReLU function clips values less than zero, so the range of h k will be half that of f k−1 .Consequently, the magnitudes of the pre-activations at the hidden layers will get smaller and smaller as we progress through the network.• If the variance σ 2 is very large (e.g., 10 5 ), then each element of β k + Ω k h k will be a weighted sum of h k where the weights are very large; the result is likely to have a much larger magnitude than the input.The ReLU function halves the range of the inputs, but if σ 2 is large enough, the magnitudes of the pre-activations will still get larger as we progress through the network.In these two situations, the values at the pre-activations can become so small or so large that they cannot be represented with finite precision floating point arithmetic.Even if the forward pass is tractable, the same logic applies to the backward pass.Each gradient update (equation 7.24) consists of multiplying by Ω T .If the values of Ω are not initialized sensibly, then the gradient magnitudes may decrease or increase uncontrollably during the backward pass.These cases are known as the vanishing gradient problem and the exploding gradient problem, respectively.In the former case, updates to the model become vanishingly small.In the latter case, they become unstable.We now present a mathematical version of the same argument.Consider the computation between adjacent pre-activations f and f ′ with dimensions D h and D h ′ , respectively:where f represents the pre-activations, Ω, and β represent the weights and biases, and a[•] is the activation function.Assume the pre-activations f j in the input layer f have variance σ 2 f .Consider initializing the biases β i to zero and the weights Ω ij as normally distributed with mean zero and variance σ 2 Ω .Now we derive expressions for the mean and variance of the pre-activations f ′ in the subsequent layer.The expectation (mean) (7.28)where D h is the dimensionality of the input layer h.We have used the rules for manipu-lating expectations, and we have assumed that the distributions over the hidden units h j and the network weights Ω ij are independent between the second and third lines.Using this result, we see that the variance σ 2 f ′ of the pre-activations f ′ i is: (7.29)where we have used the variance identityVariance identity assumed once more that the distributions of the weights Ω ij and the hidden units h j are independent between lines three and four.Assuming that the input distribution of pre-activations f j is symmetric about zero, half of these pre-activations will be clipped by the ReLU function, and the second moment E[h 2 j ] will be half the variance σ 2 f of f j (see problem 7.14):Problem 7.14(7.30) Ω = 2/D h = 0.02), the variance is stable.However, for larger values, it increases rapidly, and for smaller values, it decreases rapidly (note log scale).b) The variance of the gradients in the backward pass (solid lines) continues this trend; if we initialize with a value larger than 0.02, the magnitude of the gradients increases rapidly as we pass back through the network.If we initialize with a value smaller, then the magnitude decreases.These are known as the exploding gradient and vanishing gradient problems, respectively.This, in turn, implies that if we want the variance σ 2 f ′ of the subsequent pre-activations f ′ to be the same as the variance σ 2 f of the original pre-activations f during the forward pass, we should set:where D h is the dimension of the original layer to which the weights were applied.This is known as He initialization.A similar argument establishes how the variance of the gradients ∂l/∂f k changes during the backward pass.During the backward pass, we multiply by the transpose Ω T of the weight matrix (equation 7.24), so the equivalent expression becomes:where D h ′ is the dimension of the layer that the weights feed into.If the weight matrix Ω is not square (i.e., there are different numbers of hidden units in the two adjacent layers, so D h and D h ′ differ), then it is not possible to choose the variance to satisfy both equations 7.31 and 7.32 simultaneously.One possible compromise is to use the mean (D h + D h ′ )/2 as a proxy for the number of terms, which gives: pass and the variance of the gradients in the backward pass remain stable when the parameters are initialized appropriately.The primary focus of this book is scientific; this is not a guide for implementing deep learning models.Nonetheless, in figure 7.8, we present PyTorch code that implements the ideas explored in this book so far.The code defines a neural network and initializes Problems 7.16-7.17the weights.It creates random input and output datasets and defines a least squares loss function.The model is trained from the data using SGD with momentum in batches of size 10 over 100 epochs.The learning rate starts at 0.01 and halves every 10 epochs.The takeaway is that although the underlying ideas in deep learning are quite complex, implementation is relatively simple.For example, all of the details of the backpropagation are hidden in the single line of code: loss.backward().The previous chapter introduced stochastic gradient descent (SGD), an iterative optimization algorithm that aims to find the minimum of a function.In the context of neural networks, this algorithm finds the parameters that minimize the loss function.SGD relies on the gradient of the loss function with respect to the parameters, which must be initialized before optimization.This chapter has addressed these two problems for deep neural networks.The gradients must be evaluated for a very large number of parameters, for each member of the batch, and at each SGD iteration.It is hence imperative that the gradient computation is efficient, and to this end, the backpropagation algorithm was introduced.Careful parameter initialization is also critical.The magnitudes of the hidden unit activations can either decrease or increase exponentially in the forward pass.The same is true of the gradient magnitudes in the backward pass, where these behaviors are known as the vanishing gradient and exploding gradient problems.Both impede training but can be avoided with appropriate initialization.We've now defined the model and the loss function, and we can train a model for a given task.The next chapter discusses how to measure the model performance.Backpropagation: Efficient reuse of partial computations while calculating gradients in computational graphs has been repeatedly discovered, including by Werbos (1974), Bryson et al. (1979), LeCun (1985), and Parker (1985.However, the most celebrated description of this idea was by Rumelhart et al. (1985) and Rumelhart et al. (1986), who also coined the term "backpropagation."This latter work kick-started a new phase of neural network research in the eighties and nineties; for the first time, it was practical to train networks with hidden layers.However, progress stalled due (in retrospect) to a lack of training data, limited computational power, and the use of sigmoid activations.Areas such as natural language processing and computer vision did not rely on neural network models until the remarkable image classification results of Krizhevsky et al. (2012) ushered in the modern era of deep learning.The implementation of backpropagation in modern deep learning frameworks such as PyTorch and TensorFlow is an example of reverse-mode algorithmic differentiation.This is distinguished from forward-mode algorithmic differentiation in which the derivatives from the chain rule are accumulated while moving forward through the computational graph (see problem 7.13).Further information about algorithmic differentiation can be found in Griewank & Walther (2008) and Baydin et al. (2018).Initialization: He initialization was first introduced by He et al. (2015).It follows closely from Glorot or Xavier initialization (Glorot & Bengio, 2010), which is very similar but does not consider the effect of the ReLU layer and so differs by a factor of two.Essentially the same method was proposed much earlier by LeCun et al. (2012) but with a slightly different motivation; in this case, sigmoidal activation functions were used, which naturally normalize the range of outputs at each layer, and hence help prevent an exponential increase in the magnitudes of the hidden units.However, if the pre-activations are too large, they fall into the flat regions of the sigmoid function and result in very small gradients.Hence, it is still important to initialize the weights sensibly.Klambauer et al. (2017) introduce the scaled exponential linear unit (SeLU) and show that, within a certain range of inputs, this activation function tends to make the activations in network layers automatically converge to mean zero and unit variance.A completely different approach is to pass data through the network and then normalize by the empirically observed variance.Layer-sequential unit variance initialization (Mishkin & Matas, 2016) is an example of this kind of method, in which the weight matrices are initialized as orthonormal.GradInit (Zhu et al., 2021) randomizes the initial weights and temporarily fixes them while it learns non-negative scaling factors for each weight matrix.These factors are selected to maximize the decrease in the loss for a fixed learning rate subject to a constraint on the maximum gradient norm.Activation normalization or ActNorm adds a learnable scaling and offset parameter after each network layer at each hidden unit.They run an initial batch through the network and then choose the offset and scale so that the mean of the activations is zero and the variance one.After this, these extra parameters are learned as part of the model.Closely related to these methods are schemes such as BatchNorm (Ioffe & Szegedy, 2015), in which the network normalizes the variance of each batch as part of its processing at every step.BatchNorm and its variants are discussed in chapter 11.Other initialization schemes have been proposed for specific architectures, including the ConvolutionOrthogonal initializer (Xiao et al., 2018a) for convolutional networks, Fixup (Zhang et al., 2019a) for residual networks, and TFixup (Huang et al., 2020a) and DTFixup (Xu et al., 2021b) for transformers.Training neural networks is memory intensive.We must store both the model parameters and the pre-activations at the hidden units for every member of the batch during the forward pass.Two methods that decrease memory requirements are gradient checkpointing (Chen et al., 2016a) and micro-batching (Huang et al., 2019).In gradient checkpointing, the activations are only stored every N layers during the forward pass.During the backward pass, the intermediate missing activations are recalculated from the nearest checkpoint.In this manner, we can drastically reduce the memory requirements at the computational cost of performing the forward pass twice (problem 7.11).In micro-batching, the batch is subdivided into smaller parts, and the gradient updates are aggregated from each sub-batch before being applied to the network.A completely different approach is to build a reversible network (e.g., Gomez et al., 2017), in which the activations at the previous layer can be computed from the activations at the current one, so there is no need to cache anything during the forward pass (see chapter 16).Sohoni et al. ( 2019) review approaches to reducing memory requirements.For sufficiently large models, the memory requirements or total required time may be too much for a single processor.In this case, we must use distributed training, in which training takes place in parallel across multiple processors.There are several approaches to parallelism.In data parallelism, each processor or node contains a full copy of the model but runs a subset of the batch (see Xing et al., 2015;Li et al., 2020b).The gradients from each node are aggregated centrally and then redistributed back to each node to ensure that the models remain consistent.This is known as synchronous training.The synchronization required to aggregate and redistribute the gradients can be a performance bottleneck, and this leads to the idea of asynchronous training.For example, in the Hogwild!algorithm (Recht et al., 2011), the gradient from a node is used to update a central model whenever it is ready.The updated model is then redistributed to the node.This means that each node may have a slightly different version of the model at any given time, so the gradient updates may be stale; however, it works well in practice.Other decentralized schemes have also been developed.For example, in Zhang et al. (2016a), the individual nodes update one another in a ring structure.Data parallelism methods still assume that the entire model can be held in the memory of a single node.Pipeline model parallelism stores different layers of the network on different nodes and hence does not have this requirement.In a naïve implementation, the first node runs the forward pass for the batch on the first few layers and passes the result to the next node, which runs the forward pass on the next few layers and so on.In the backward pass, the gradients are updated in the opposite order.The obvious disadvantage of this approach is that each machine lies idle for most of the cycle.Various schemes revolving around each node processing microbatches sequentially have been proposed to reduce this inefficiency (e.g., Huang et al., 2019;Narayanan et al., 2021a).Finally, in tensor model parallelism, computation at a single network layer is distributed across nodes (e.g., Shoeybi et al., 2019).A good overview of distributed training methods can be found in Narayanan et al. (2021b), who combine tensor, pipeline, and data parallelism to train a language model with one trillion parameters on 3072 GPUs.Problem 7.1 A two-layer network with two hidden units in each layer can be defined as:where ∂z/∂h is a matrix containing the term ∂zi/∂hj in its i th column and j th row.To do this, first find an expression for the constituent elements ∂zi/∂hj, and then consider the form that the matrix ∂z/∂h must take.Problem 7.7 Consider the case where we use the logistic sigmoid (see equation 7.37) as an activation function, so h = sig[f ].Compute the derivative ∂h/∂f for this activation function.What happens to the derivative when the input takes (i) a large positive value and (ii) a large negative value?Problem 7.8 Consider using (i) the Heaviside function and (ii) the rectangular function as activation functions:Figure 7.9 Computational graph for problem 7.12 and problem 7.13.Adapted from Domke (2010).andDiscuss why these functions are problematic for neural network training with gradient-based optimization methods.Problem 7.9 * Consider a loss function ℓ[f ], where f = β + Ωh.We want to find how the loss ℓ changes when we change Ω, which we'll express with a matrix that contains the derivative ∂ℓ/∂Ωij at the i th row and j th column.Find an expression for ∂fi/∂Ωij and, using the chain rule, show that:Problem 7.10 * Derive the equations for the backward pass of the backpropagation algorithm for a network that uses leaky ReLU activations, which are defined as:where α is a small positive constant (typically 0.1).Consider training a network with fifty layers, where we only have enough memory to store the pre-activations at every tenth hidden layer during the forward pass.Explain how to compute the derivatives in this situation using gradient checkpointing.Problem 7.12 * This problem explores computing derivatives on general acyclic computational graphs.Consider the function:We can break this down into a series of intermediate computations so that:This work is subject to a Creative Commons CC-BY-NC-ND license.(C) MIT Press.Notes 117The associated computational graph is depicted in figure 7.9.Problem 7.17 Change the code in figure 7.8 to tackle a binary classification problem.You will need to (i) change the targets y so they are binary, (ii) change the network to predict numbers between zero and one (iii) change the loss function appropriately.Draft: please send errata to udlbookmail@gmail.com.Chapter 8Previous chapters described neural network models, loss functions, and training algorithms.This chapter considers how to measure the performance of the trained models.With sufficient capacity (i.e., number of hidden units), a neural network model will often perform perfectly on the training data.However, this does not necessarily mean it will generalize well to new test data.We will see that the test errors have three distinct causes and that their relative contributions depend on (i) the inherent uncertainty in the task, (ii) the amount of training data, and (iii) the choice of model.The latter dependency raises the issue of hyperparameter search.We discuss how to select both the model hyperparameters (e.g., the number of hidden layers and the number of hidden units in each) and the learning algorithm hyperparameters (e.g., the learning rate and batch size).We explore model performance using the MNIST-1D dataset (figure 8.1).This consists of ten classes y ∈ {0, 1, . . ., 9}, representing the digits 0-9.The data are derived from 1D templates for each of the digits.Each data example x is created by randomly transforming one of these templates and adding noise.We use a network with D i = 40 inputs and D o = 10 outputs which are passed through a softmax function to produce class probabilities (see section 5.5).The network has two hidden layers with D = 100 hidden units each.It is trained using stochastic gradient descent with batch size 100 and learning rate 0.1 for 6000 steps (150 epochs) with a multiclass cross-entropy loss (equation 5.24).Figure 8.2 shows that the training error decreases as training proceeds.The training data are classified perfectly after about 4000 steps.The training loss also decreases, eventually approaching zero.orized the training set but be unable to predict new examples.To estimate the true performance, we need a separate test set of input/output pairs {x i , y i }.To this end, we generate 1000 more examples using the same process.Figure 8.2a also shows the errors for this test data as a function of the training step.These decrease as training proceeds, but only to around 40%.This is better than the chance error rate of 90% error rate but far worse than for the training set; the model has not generalized well to the test data.The test loss (figure 8.2b) decreases for the first 1500 training steps but then increases Notebook 8.1 MNIST-1D performance again.At this point, the test error rate is fairly constant; the model makes the same mistakes but with increasing confidence.This decreases the probability of the correct answers and thus increases the negative log-likelihood.This increasing confidence is a side-effect of the softmax function; the pre-softmax activations are driven to increasingly extreme values to make the probability of the training data approach one (see figure 5.10).We now consider the sources of the errors that occur when a model fails to generalize.To make this easier to visualize, we revert to a 1D linear least squares regression problem where we know exactly how the ground truth data were generated.Figure 8.3 shows a quasi-sinusoidal function; both training and test data are generated by sampling input values in the range [0, 1], passing them through this function, and adding Gaussian noise with a fixed variance.We fit a simplified shallow neural net to this data (figure 8.4).The weights and biases that connect the input layer to the hidden layer are chosen so that the "joints" of the function are evenly spaced across the interval.If there are D hidden units, then these joints will be at 0, 1/D, 2/D, . . ., (D − 1)/D.This model can represent any piecewise linear function with D equally sized regions in the range [0, 1].As well as being easy to understand, this model also has the advantage that it can be fit in closed form without the need for stochastic optimization algorithms (see problem 8.3).Consequently, we can The weights and biases between the input and hidden layer are fixed (dashed arrows).b-d) They are chosen so that the hidden unit activations have slope one, and their joints are equally spaced across the interval, with joints at x = 0, x = 1/3, and x = 2/3, respectively.Modifying the remaining parameters ϕ = {β, ω1, ω2, ω3} can create any piecewise linear function over x ∈ [0, 1] with joints at 1/3 and 2/3.e-g) Three example functions with different values of the parameters ϕ.Figure 8.5 Sources of test error.a) Noise.Data generation is noisy, so even if the model exactly replicates the true underlying function (black line), the noise in the test data (gray points) means that some error will remain (gray region represents two standard deviations).b) Bias.Even with the best possible parameters, the three-region model (cyan line) cannot exactly fit the true function (black line).This bias is another source of error (gray regions represent signed error).c) Variance.In practice, we have limited noisy training data (orange points).When we fit the model, we don't recover the best possible function from panel (b) but a slightly different function (cyan line) that reflects idiosyncrasies of the training data.This provides an additional source of error (gray region represents two standard deviations).Figure 8.6 shows how this region was calculated.There are three possible sources of error, which are known as noise, bias, and variance respectively (figure 8.5):Noise The data generation process includes the addition of noise, so there are multiple possible valid outputs y for each input x (figure 8.5a).This source of error is insurmountable for the test data.Note that it does not necessarily limit the training performance; we will likely never see the same input x twice during training, so it is still possible to fit the training data perfectly.Noise may arise because there is a genuine stochastic element to the data generation process, because some of the data are mislabeled, or because there are further explanatory variables that were not observed.In rare cases, noise may be absent; for example, a network might approximate a function that is deterministic but requires significant computation to evaluate.However, noise is usually a fundamental limitation on the possible test performance.Bias A second potential source of error may occur because the model is not flexible enough to fit the true function perfectly.For example, the three-region neural network model cannot exactly describe the quasi-sinusoidal function, even when the parameters are chosen optimally (figure 8.5b).This is known as bias.Variance We have limited training examples, and there is no way to distinguish systematic changes in the underlying function from noise in the underlying data.When we fit a model, we do not get the closest possible approximation to the true underlying function.Indeed, for different training datasets, the result will be slightly different each time.This additional source of variability in the fitted function is termed variance (figure 8.5c).In practice, there might also be additional variance due to the stochastic learning algorithm, which does not necessarily converge to the same solution each time.We now make the notions of noise, bias, and variance mathematically precise.Consider a 1D regression problem where the data generation process has additive noise with variance σ 2 (e.g., figure 8.3); we can observe different outputs y for the same input x, so for Appendix C.2 Expectation each x, there is a distribution P r(y|x) with expected value (mean) µ[x]:and fixed noiseHere we have used the notation y[x] to specify that we are considering the output y at a given input position x.Now consider a least squares loss between the model prediction f[x, ϕ] at position x and the observed value y[x] at that position:where we have both added and subtracted the mean µ[x] of the underlying function in the second line and have expanded out the squared term in the third line.The underlying function is stochastic, so this loss depends on the particular y[x] we observe.The expected loss is:where we have made use of the rules for manipulating expectations.In the second line, weReturning to the first term of equation 8.3, we add and subtract f µ [x] and expand:We then take the expectation with respect to the training dataset D:where we have simplified using similar steps as for equation 8.3.Finally, we substitute this result into equation 8.3:This equation says that the expected loss after considering the uncertainty in the training data D and the test data y consists of three additive components.The variance is uncertainty in the fitted model due to the particular training dataset we sample.The bias is the systematic deviation of the model from the mean of the function we are modeling.The noise is the inherent uncertainty in the true mapping from input to output.These three sources of error will be present for any task.They combine additively for linear regression with a least squares loss.However, their interaction can be more complex for other types of problems.In the previous section, we saw that test error results from three sources: noise, bias, and variance.The noise component is insurmountable; there is nothing we can do to circumvent this, and it represents a fundamental limit on model performance.However, it is possible to reduce the other two terms.Recall that the variance results from limited noisy training data.Fitting the model to two different training sets results in slightly different parameters.It follows we can reduce the variance by increasing the quantity of training data.This averages out the inherent noise and ensures that the input space is well sampled.Figure 8.6 shows the effect of training with 6, 10, and 100 samples.For each dataset size, we show the best-fitting model for three training datasets.With only six samples, the fitted function is quite different each time: the variance is significant.As we increase the number of samples, the fitted models become very similar, and the variance reduces.In general, adding training data almost always improves test performance.The bias term results from the inability of the model to describe the true underlying function.This suggests that we can reduce this error by making the model more flexible.This is usually done by increasing the model capacity.For neural networks, this means adding more hidden units and/or hidden layers.In the simplified model, adding capacity corresponds to adding more hidden units so that the interval [0, 1] is divided into more linear regions.Figures 8.7a-c show that (unsurprisingly) this does indeed reduce the bias; as we increase the number of linear regions to ten, the model becomes flexible enough to fit the true function closely.However, figures 8.7d-f show an unexpected side-effect of increasing the model capacity.For a fixed-size training dataset, the variance term increases as the model capacity increases.Consequently, increasing the model capacity does not necessarily reduce the test error.This is known as the bias-variance trade-off.Figure 8.8 explores this phenomenon.In panels a-c), we fit the simplified three-region model to three different datasets of fifteen points.Although the datasets differ, the final model is much the same; the noise in the dataset roughly averages out in each linear region.In panels d-f), we fit a model with ten regions to the same three datasets.This model has more flexibility, but this is disadvantageous; the model certainly fits the data better, and the training error will be lower, but much of the extra descriptive power is devoted to modeling the noise.This phenomenon is known as overfitting.We've seen that as we add capacity to the model, the bias decreases, but the variance increases for a fixed-size training dataset.This suggests that there is an optimal capacity where the bias is not too large and the variance is still relatively small.Figure 8.9 shows how these terms vary numerically for the toy model as we increase the capacity, using Notebook 8.2 Bias-variance trade-off the data from figure 8.8.For regression models, the total expected error is the sum of the bias and the variance, and this sum is minimized when the model capacity is four (i.e., with four hidden units and four linear regions in the range of the data).In the previous section, we examined the bias-variance trade-off as we increased the capacity of a model.Let's now return to the MNIST-1D dataset and see whether this happens in practice.We use 10,000 training examples, test with another 5,000 examples and examine the training and test performance as we increase the capacity (number of parameters) in the model.We train the model with Adam and a step size of 0.005 using a full batch of 10,000 examples for 4000 steps.Figure 8.10a shows the training and test error for a neural network with two hidden layers as the number of hidden units increases.The training error decreases as the capacity grows and quickly becomes close to zero.The vertical dashed line represents the capacity where the model has the same number of parameters as there are training examples, but the model memorizes the dataset before this point.The test error decreases as we add model capacity but does not increase as predicted by the bias-variance trade-off curve; it keeps decreasing.In figure 8.10b, we repeat this experiment, but this time, we randomize 15% of the  training labels.Once more, the training error decreases to zero.This time, there is more randomness, and the model requires almost as many parameters as there are data points to memorize the data.The test error does show the typical bias-variance trade-off as we increase the capacity to the point where the model fits the training data exactly.However, then it does something unexpected; it starts to decrease again.Indeed, if we add enough capacity, the test loss reduces to below the minimal level that we achieved in the first part of the curve.This phenomenon is known as double descent.For some datasets like MNIST, it is present with the original data (figure 8.10c).For others, like MNIST-1D and CIFAR-100 (figure 8.10d), it emerges or becomes more prominent when we add noise to the labels.The first part of the curve is referred to as the classical or under-parameterized regime, and the second part as the modern or over-parameterized regime.The central part where the error increases is termed the critical regime.The discovery of double descent is recent, unexpected, and somewhat puzzling.It results from an interaction of two phenomena.First, the test performance becomes temporarily worse when the model has just enough capacity to memorize the data.Second, the test performance continues to improve with capacity even after the training performance is perfect.The first phenomenon is exactly as predicted by the bias-variance trade-off.The second phenomenon is more confusing; it's unclear why performance should be better in the over-parameterized regime, given that there are now not even enough training data points to constrain the model parameters uniquely.To understand why performance continues to improve as we add more parameters, note that once the model has enough capacity to drive the training loss to near zero, the model fits the training data almost perfectly.This implies that further capacity Problems 8.4-8.5 cannot help the model fit the training data any better; any change must occur between the training points.The tendency of a model to prioritize one solution over another as it extrapolates between data points is known as its inductive bias.The model's behavior between data points is critical because, in high-dimensional space, the training data are extremely sparse.The MNIST-1D dataset has 40 dimensions, and we trained with 10,000 examples.If this seems like plenty of data, consider what would happen if we quantized each input dimension into 10 bins.There would be 10 40 bins in total, constrained by only 10 4 examples.Even with this coarse quantization, there will only be one data point in every 10 35 bins!The tendency of the volume of high-dimensional space to overwhelm the number of training points is termed the curse of dimensionality.The implication is that problems in high dimensions might look more like figure 8.11a; there are small regions of the input space where we observe data with significant gaps between them.The putative explanation for double descent is that as we add capacity to the model, it interpolates between the nearest data points increasingly smoothly.In the absence of information about what happens between the training points, assuming smoothness is sensible and will probably generalize reasonably to new data., then it has to contort itself to pass through the training data, and the output predictions will not be smooth.c-f) However, as we add more hidden units, the model has the ability to interpolate between the points more smoothly (smoothest possible curve plotted in each case).However, unlike in this figure, it is not obliged to.This argument is plausible.It's certainly true that as we add more capacity to the model, it will have the capability to create smoother functions.Figures 8.11b-f show the smoothest possible functions that still pass through the data points as we increase the number of hidden units.When the number of parameters is very close to the number of training data examples (figure 8.11b), the model is forced to contort itself to fit the training data exactly, resulting in erratic predictions.This explains why the peak in the double descent curve is so pronounced.As we add more hidden units, the model has the ability to construct smoother functions that are likely to generalize better to new data.However, this does not explain why over-parameterized models should produce smooth functions.Figure 8.12 shows three functions that can be created by the simplified model with 50 hidden units.In each case, the model fits the data exactly, so the loss is zero.If the modern regime of double descent is explained by increasing smoothness, then what exactly is encouraging this smoothness?Figure 8.12 Regularization.a-c) Each of the three fitted curves passes through the data points exactly, so the training loss for each is zero.However, we might expect the smooth curve in panel (a) to generalize much better to new data than the erratic curves in panels (b) and (c).Any factor that biases a model toward a subset of the solutions with a similar training loss is known as a regularizer.It is thought that the initialization and/or fitting of neural networks have an implicit regularizing effect.Consequently, in the over-parameterized regime, more reasonable solutions, such as that in panel (a), are encouraged.The answer to this question is uncertain, but there are two likely possibilities.First, the network initialization may encourage smoothness, and the model never departs from the sub-domain of smooth function during the training process.Second, the training algorithm may somehow "prefer" to converge to smooth functions.Any factor that biases a solution toward a subset of equivalent solutions is known as a regularizer, so one possibility is that the training algorithm acts as an implicit regularizer (see section 9.2).In the previous section, we discussed how test performance changes with model capacity.Unfortunately, in the classical regime, we don't have access to either the bias (which requires knowledge of the true underlying function) or the variance (which requires multiple independently sampled datasets to estimate).In the modern regime, there is no way to tell how much capacity should be added before the test error stops improving.This raises the question of exactly how we should choose model capacity in practice.For a deep network, the model capacity depends on the numbers of hidden layers and hidden units per layer as well as other aspects of architecture that we have yet to introduce.Furthermore, the choice of learning algorithm and any associated parameters (learning rate, etc.) also affects the test performance.These elements are collectively termed hyperparameters.The process of finding the best hyperparameters is termed hyperparameter search or (when focused on network structure) neural architecture search.Hyperparameters are typically chosen empirically; we train many models with different hyperparameters on the same training set, measure their performance, and retain the best model.However, we do not measure their performance on the test set; this would admit the possibility that these hyperparameters just happen to work well for the test set but don't generalize to further data.Instead, we introduce a third dataset known as a validation set.For every choice of hyperparameters, we train the associated model using the training set and evaluate performance on the validation set.Finally, we select the model that worked best on the validation set and measure its performance on the test set.In principle, this should give a reasonable estimate of the true performance.The hyperparameter space is generally smaller than the parameter space but still too large to try every combination exhaustively.Unfortunately, many hyperparameters are discrete (e.g., the number of hidden layers), and others may be conditional on one another (e.g., we only need to specify the number of hidden units in the tenth hidden layer if there are ten or more layers).Hence, we cannot rely on gradient descent methods as we did for learning the model parameters.Hyperparameter optimization algorithms intelligently sample the space of hyperparameters, contingent on previous results.This procedure is computationally expensive since we must train an entire model and measure the validation performance for each combination of hyperparameters.To measure performance, we use a separate test set.The degree to which performance is maintained on this test set is known as generalization.Test errors can be explained by three factors: noise, bias, and variance.These combine additively in regression problems with least squares losses.Adding training data decreases the variance.When the model capacity is less than the number of training examples, increasing the capacity decreases bias but increases variance.This is known as the bias-variance trade-off, and there is a capacity where the trade-off is optimal.However, this is balanced against a tendency for performance to improve with capacity, even when the parameters exceed the training examples.Together, these two phenomena create the double descent curve.It is thought that the model interpolates more smoothly between the training data points in the over-parameterized "modern regime," although it is unclear what drives this.To choose the capacity and other model and training algorithm hyperparameters, we fit multiple models and evaluate their performance using a separate validation set.Bias-variance trade-off: We showed that the test error for regression problems with least squares loss decomposes into the sum of noise, bias, and variance terms.These factors are all present for models with other losses, but their interaction is typically more complicated (Friedman, 1997;Domingos, 2000).For classification problems, there are some counter-intuitive predictions; for example, if the model is biased toward selecting the wrong class in a region of the input space, then increasing the variance can improve the classification rate as this pushes some of the predictions over the threshold to be classified correctly.We saw that it is typical to divide the data into three parts: training data (which is used to learn the model parameters), validation data (which is used to choose the hyperparameters), and test data (which is used to estimate the final performance).This approach is known as cross-validation.However, this division may cause problems where the total number of data examples is limited; if the number of training examples is comparable to the model capacity, then the variance will be large.One way to mitigate this problem is to use k-fold cross-validation.The training and validation data are partitioned into K disjoint subsets.For example, we might divide these data into five parts.We train with four and validate with the fifth for each of the five permutations and choose the hyperparameters based on the average validation performance.The final test performance is assessed using the average of the predictions from the five models with the best hyperparameters on an entirely different test set.There are many variations of this idea, but all share the general goal of using a larger proportion of the data to train the model, thereby reducing variance.We have used the term capacity informally to mean the number of parameters or hidden units in the model (and hence indirectly, the ability of the model to fit functions of increasing complexity).The representational capacity of a model describes the space of possible functions it can construct when we consider all possible parameter values.When we take into account the fact that an optimization algorithm may not be able to reach all of these solutions, what is left is the effective capacity.The Vapnik-Chervonenkis (VC) dimension (Vapnik & Chervonenkis, 1971) is a more formal measure of capacity.It is the largest number of training examples that a binary classifier can label arbitrarily.Bartlett et al. (2019) derive upper and lower bounds for the VC dimension in terms of the number of layers and weights.An alternative measure of capacity is the Rademacher complexity, which is the expected empirical performance of a classification model (with optimal parameters) for data with random labels.Neyshabur et al. (2017) derive a lower bound on the generalization error in terms of the Rademacher complexity.The term "double descent" was coined by Belkin et al. (2019), who demonstrated that the test error decreases again in the over-parameterized regime for two-layer neural networks and random features.They also claimed that this occurs in decision trees, although Buschjäger & Morik (2021) subsequently provided evidence to the contrary.Nakkiran et al. (2021) show that double descent occurs for various modern datasets (CIFAR-10, CIFAR-100, IWSLT'14 de-en), architectures (CNNs, ResNets, transformers), and optimizers (SGD, Adam).The phenomenon is more pronounced when noise is added to the target labels (Nakkiran et al., 2021) and when some regularization techniques are used (Ishida et al., 2020).A simple approach is to sample the space randomly (Bergstra & Bengio, 2012).However, for continuous variables, it is better to build a model of performance as a function of the hyperparameters and the uncertainty in this function.This can be exploited to test where the uncertainty is great (explore the space) or home in on regions where performance looks promising (exploit previous knowledge).Bayesian optimization is a framework based on Gaussian processes that does just this, and its application to hyperparameter search is described in Snoek et al. (2012).The Beta-Bernoulli bandit (see Lattimore & Szepesvári, 2020) is a roughly equivalent model for describing uncertainty in results due to discrete variables.The sequential model-based configuration (SMAC) algorithm (Hutter et al., 2011) can cope with continuous, discrete, and conditional parameters.The basic approach is to use a random forest to model the objective function where the mean of the tree predictions is the best guess about the objective function, and their variance represents the uncertainty.A completely different approach that can also cope with combinations of continuous, discrete, and conditional parameters is Tree-Parzen Estimators (Bergstra et al., 2011).The previous methods modeled the probability of the model performance given the hyperparameters.In contrast, the Tree-Parzen estimator models the probability of the hyperparameters given the model performance.Hyperband (Li et al., 2017b) is a multi-armed bandit strategy for hyperparameter optimization.It assumes that there are computationally cheap but approximate ways to measure performance (e.g., by not training to completion) and that these can be associated with a budget (e.g., by training for a fixed number of iterations).A number of random configurations are sampled and run until the budget is used up.Then the best fraction η of runs is kept, and the budget is multiplied by 1/η.This is repeated until the maximum budget is reached.This approach has the advantage of efficiency; for bad configurations, it does not need to run the experiment to the end.However, each sample is just chosen randomly, which is inefficient.The BOHB algorithm (Falkner et al., 2018) combines the efficiency of Hyperband with the more sensible choice of hyperparameters from Tree Parzen estimators to construct an even better method.Circles are four samples from this distribution.As the distance from the center increases, the probability decreases, but the volume of space at that radius (i.e., the area between adjacent evenly spaced circles) increases.b) These factors trade off so that the histogram of distances of samples from the center has a pronounced peak.c) In higher dimensions, this effect becomes more extreme, and the probability of observing a sample close to the mean becomes vanishingly small.Although the most likely point is at the mean of the distribution, the typical samples are found in a relatively narrow shell.Problem 8.7 The volume of a hypersphere with radius r in D dimensions is: Draft: please send errata to udlbookmail@gmail.com.Chapter 9Chapter 8 described how to measure model performance and identified that there could be a significant performance gap between the training and test data.Possible reasons for this discrepancy include: (i) the model describes statistical peculiarities of the training data that are not representative of the true mapping from input to output (overfitting), and (ii) the model is unconstrained in areas with no training examples, leading to suboptimal predictions.This chapter discusses regularization techniques.These are a family of methods that reduce the generalization gap between training and test performance.Strictly speaking, regularization involves adding explicit terms to the loss function that favor certain parameter choices.However, in machine learning, this term is commonly used to refer to any strategy that improves generalization.We start by considering regularization in its strictest sense.Then we show how the stochastic gradient descent algorithm itself favors certain solutions.This is known as implicit regularization.Following this, we consider a set of heuristic methods that improve test performance.These include early stopping, ensembling, dropout, label smoothing, and transfer learning.Consider fitting a model f[x, ϕ] with parameters ϕ using a training set {x i , y i } of input/output pairs.We seek the minimum of the loss function L[ϕ] :where the individual terms ℓ i [x i , y i ] measure the mismatch between the network predictions f[x i , ϕ] and output targets y i for each training pair.To bias this minimization toward certain solutions, we include an additional term:where g[ϕ] is a function that returns a scalar that takes a larger value when the parameters are less preferred.The term λ is a positive scalar that controls the relative contribution of the original loss function and the regularization term.The minima of the regularized loss function usually differ from those in the original, so the training procedure converges to different parameter values (figure 9.1).Regularization can be viewed from a probabilistic perspective.Section 5.1 shows how loss functions are constructed from the maximum likelihood criterion:3)The regularization term can be considered as a prior P r(ϕ) that represents knowledge about the parameters before we observe the data and we now have the maximum a posteriori or MAP criterion:This discussion has sidestepped the question of which solutions the regularization term should penalize (or equivalently that the prior should favor).Since neural networks are used in an extremely broad range of applications, these can only be very generic preferences.The most commonly used regularization term is the L2 norm, which penalizes the sum of the squares of the parameter values:where j indexes the parameters.This is also referred to as Tikhonov regularization or Problems 9.1-9.2ridge regression, or (when applied to matrices) Frobenius norm regularization.For neural networks, L2 regularization is usually applied to the weights but not the biases and is hence referred to as a weight decay term.The effect is to encourage smaller weights, so the output function is smoother.To see this, consider that the output prediction is a weighted sum of the activations at the last hidden layer.If the Notebook 9.1 L2 regularization weights have a smaller magnitude, the output will vary less.The same logic applies to the computation of the pre-activations at the last hidden layer and so on, progressing backward through the network.In the limit, if we forced all the weights to be zero, the network would produce a constant output determined by the final bias parameter.Figure 9.2 shows the effect of fitting the simplified network from figure 8.4 with weight decay and different values of the regularization coefficient λ.When λ is small, it has little effect.However, as λ increases, the fit to the data becomes less accurate, and the function becomes smoother.This might improve the test performance for two reasons:• If the network is overfitting, then adding the regularization term means that the network must trade off slavish adherence to the data against the desire to be smooth.One way to think about this is that the error due to variance reduces (the model no longer needs to pass through every data point) at the cost of increased bias (the model can only describe smooth functions).• When the network is over-parameterized, some of the extra model capacity describes areas with no training data.Here, the regularization term will favor functions that smoothly interpolate between the nearby points.This is reasonable behavior in the absence of knowledge about the true function.For large λ (panels e-f), the fitted function is smoother than the ground truth, so the fit is worse.An intriguing recent finding is that neither gradient descent nor stochastic gradient descent moves neutrally to the minimum of the loss function; each exhibits a preference for some solutions over others.This is known as implicit regularization.Consider a continuous version of gradient descent where the step size is infinitesimal.The change in parameters ϕ will be governed by the differential equation:Gradient descent approximates this process with a series of discrete steps of size α:The discretization causes a deviation from the continuous path (figure 9.3).This deviation can be understood by deriving a modified loss term L for the continuous case that arrives at the same place as the discretized version on the original loss L. It can be shown (see end of chapter) that this modified loss is:LGDIn other words, the discrete trajectory is repelled from places where the gradient norm is large (the surface is steep).This doesn't change the position of the minima where the gradients are zero anyway.However, it changes the effective loss function elsewhere and modifies the optimization trajectory, which potentially converges to a different minimum.Implicit regularization due to gradient descent may be responsible for the observation that full batch gradient descent generalizes better with larger step sizes (figure 9.5a).A similar analysis can be applied to stochastic gradient descent.Now we seek a modified loss function such that the continuous version reaches the same place as the average of the possible random SGD updates.This can be shown to be:(9.10) Equation 9.9 reveals an extra regularization term, which corresponds to the variance of the gradients of the batch losses L b .In other words, SGD implicitly favors places where the gradients are stable (where all the batches agree on the slope).Once more, this modifies the trajectory of the optimization process (figure 9.4) but does not necessarily change the position of the global minimum; if the model is over-parameterized, then it may fit all the training data exactly, so all of these gradient terms will all be zero at the global minimum.SGD generalizes better than gradient descent, and smaller batch sizes generally perform better than larger ones (figure 9.5b).One possible explanation is that the inherent randomness allows the algorithm to reach different parts of the loss function.However, Notebook 9.2 Implicit regularization it's also possible that some or all of this performance increase is due to implicit regularization; this encourages solutions where all the data fits well (so the batch variance is small) rather than solutions where some of the data fit extremely well and other data less well (perhaps with the same overall loss, but with larger batch variance).The former solutions are likely to generalize better.We've seen that adding explicit regularization terms encourages the training algorithm to find a good solution by adding extra terms to the loss function.This also occurs implicitly as an unintended (but seemingly helpful) byproduct of stochastic gradient descent.This section describes other heuristic methods used to improve generalization.8.1) for a neural network with two hidden layers.a) Performance is better for large learning rates than for intermediate or small ones.In each case, the number of iterations is 6000/LR, so each solution has the opportunity to move the same distance.b) Performance is superior for smaller batch sizes.In each case, the number of iterations was chosen so that the training data were memorized at roughly the same model capacity.Early stopping refers to stopping the training procedure before it has fully converged.This can reduce overfitting if the model has already captured the coarse shape of the underlying function but has not yet had time to overfit to the noise (figure 9.6).One way of thinking about this is that since the weights are initialized to small values (see section 7.5), they simply don't have time to become large, so early stopping has a similar effect to explicit L2 regularization.A different view is that early stopping reduces the effective model complexity.Hence, we move back down the bias/variance trade-off curve from the critical region, and performance improves (see figures 8.9 and 8.10).Early stopping has a single hyperparameter, the number of steps after which learning is terminated.As usual, this is chosen empirically using a validation set (section 8.5).However, for early stopping, the hyperparameter can be selected without the need to train multiple models.The model is trained once, the performance on the validation set is monitored every T iterations, and the associated models are stored.The stored model where the validation performance was best is selected.Another approach to reducing the generalization gap between training and test data is to build several models and average their predictions.A group of such models is known as an ensemble.This technique reliably improves test performance at the cost of training and storing multiple models and performing inference multiple times.The models can be combined by taking the mean of the outputs (for regression problems) or the mean of the pre-softmax activations (for classification problems).The assumption is that model errors are independent and will cancel out.Alternatively, we can take the median of the outputs (for regression problems) or the most frequent predicted class (for classification problems) to make the predictions more robust.One way to train different models is just to use different random initializations.This may help in regions of input space far from the training data.Here, the fitted function is relatively unconstrained, and different models may produce different predictions, so the average of several models may generalize better than any single model.A second approach is to generate several different datasets by re-sampling the training data with replacement and training a different model from each.This is known as bootstrap aggregating or bagging for short (figure 9.7).It has the effect of smoothing out the data; if a data point is not present in one training set, the model will interpo- b-e) Four models created by re-sampling the data with replacement (bagging) four times (size of orange point indicates number of times the data point was re-sampled).f) When we average the predictions of this ensemble, the result (cyan curve) is smoother than the result from panel (a) for the full dataset (gray curve) and will probably generalize better.late from nearby points; hence, if that point was an outlier, the fitted function will be more moderate in this region.Other approaches include training models with different hyperparameters or training completely different families of models.Dropout randomly clamps a subset (typically 50%) of hidden units to zero at each iteration of SGD (figure 9.8).This makes the network less dependent on any given hidden unit and encourages the weights to have smaller magnitudes so that the change in the function due to the presence or absence of the hidden unit is reduced.This technique has the positive benefit that it can eliminate undesirable "kinks" in the function that are far from the training data and don't affect the loss.For example, consider three hidden units that become active sequentially as we move along the curve (figure 9.9a).The first hidden unit causes a large increase in the slope.A second hidden unit decreases the slope, so the function goes back down.Finally, the third unit cancels out this decrease and returns the curve to its original trajectory.These three units conspire to make an undesirable local change in the function.This will not change the training loss but is unlikely to generalize well.When several units conspire in this way, eliminating one (as would happen in dropout) causes a considerable change to the output function that is propagated to the half-space where that unit was active (figure 9.9b).A subsequent gradient descent step will attempt to compensate for the change that this induces, and such dependencies will be eliminated over time.The overall effect is that large unnecessary changes between training data points are gradually removed even though they contribute nothing to the loss (figure 9.9).At test time, we can run the network as usual with all the hidden units active; however, the network now has more hidden units than it was trained with at any given iteration, so we multiply the weights by one minus the dropout probability to compensate.This is known as the weight scaling inference rule.A different approach to inference is to use Monte Carlo dropout, in which we run the network multiple times with different random subsets of units clamped to zero (as in training) and combine the results.This is closely related to ensembling in that every random version of the network is a different model; however, we do not have to train or store multiple networks here.Figure 9.9 Dropout mechanism.a) An undesirable kink in the curve is caused by a sequential increase in the slope, decrease in the slope (at circled joint), and then another increase to return the curve to its original trajectory.Here we are using full-batch gradient descent, and the model (from figure 8.4) fits the data as well as possible, so further training won't remove the kink.b) Consider what happens if we remove the eighth hidden unit that produced the circled joint in panel (a), as might happen using dropout.Without the decrease in the slope, the right-hand side of the function takes an upwards trajectory, and a subsequent gradient descent step will aim to compensate for this change.c) Curve after 2000 iterations of (i) randomly removing one of the three hidden units that cause the kink and (ii) performing a gradient descent step.The kink does not affect the loss but is nonetheless removed by this approximation of the dropout mechanism.Dropout can be interpreted as applying multiplicative Bernoulli noise to the network activations.This leads to the idea of applying noise to other parts of the network during training to make the final model more robust.One option is to add noise to the input data; this smooths out the learned function Problem 9.3 (figure 9.10).For regression problems, it can be shown to be equivalent to adding a regularizing term that penalizes the derivatives of the network's output with respect to its input.An extreme variant is adversarial training, in which the optimization algorithm actively searches for small perturbations of the input that cause large changes to the output.These can be thought of as worst-case additive noise vectors.A second possibility is to add noise to the weights.This encourages the network to make sensible predictions even for small perturbations of the weights.The result is that the training converges to local minima in the middle of wide, flat regions, where changing the individual weights does not matter much.Finally, we can perturb the labels.The maximum-likelihood criterion for multiclass classification aims to predict the correct class with absolute certainty (equation 5.24).To this end, the final network activations (i.e., before the softmax function) are pushed to very large values for the correct class and very small values for the wrong classes.We could discourage this overconfident behavior by assuming that a proportion ρ of the training labels are incorrect and belong with equal probability to the other classes.This could be done by randomly changing the labels at each training iteration.However, the same end can be achieved by changing the loss function to minimize the crossentropy between the predicted distribution and a distribution where the true label has Problem 9.4 probability 1 − ρ, and the other classes have equal probability.This is known as label smoothing and improves generalization in diverse scenarios.The maximum likelihood approach is generally overconfident; in the training phase, it selects the most likely parameters and bases its predictions on the model defined by these.However, many parameter values may be broadly compatible with the data and only slightly less likely.The Bayesian approach treats the parameters as unknown variables and computes a distribution P r(ϕ|{x i , y i }) over these parameters ϕ conditioned on the training data {x i , y i } using Bayes' rule: .11)where P r(ϕ) is the prior probability of the parameters, and the denominator is a normalizing term.Hence, every parameter choice is assigned a probability (figure 9.11).The prediction y for new input x is an infinite weighted sum (i.e., an integral) of the predictions for each parameter set, where the weights are the associated probabilities: P r(y|x, {x i , y i }) = P r(y|x, ϕ)P r(ϕ|{x i , y i })dϕ.(9.12)This is effectively an infinite weighted ensemble, where the weight depends on (i) the prior probability of the parameters and (ii) their agreement with the data.).The parameters are treated as uncertain.The posterior probability P r(ϕ|{xi, yi}) for a set of parameters is determined by their compatibility with the data {xi, yi} and a prior distribution P r(ϕ).a-c) Two sets of parameters (cyan and gray curves) sampled from the posterior using normally distributed priors with mean zero and three variances.When the prior variance σ 2 ϕ is small, the parameters also tend to be small, and the functions smoother.d-f) Inference proceeds by taking a weighted sum over all possible parameter values where the weights are the posterior probabilities.This produces both a prediction of the mean (cyan curves) and the associated uncertainty (gray region is two standard deviations).The Bayesian approach is elegant and can provide more robust predictions than those that derive from maximum likelihood.Unfortunately, for complex models like neural networks, there is no practical way to represent the full probability distribution Notebook 9.4Bayesian approach over the parameters or to integrate over it during the inference phase.Consequently, all current methods of this type make approximations of some kind, and typically these add considerable complexity to learning and inference.When training data are limited, other datasets can be exploited to improve performance.In transfer learning (figure 9.12a), the network is pre-trained to perform a related sec-ondary task for which data are more plentiful.The resulting model is then adapted to the original task.This is typically done by removing the last layer and adding one or more layers that produce a suitable output.The main model may be fixed, and the new layers trained for the original task, or we may fine-tune the entire model.The principle is that the network will build a good internal representation of the data from the secondary task, which can subsequently be exploited for the original task.Equivalently, transfer learning can be viewed as initializing most of the parameters of the final network in a sensible part of the space that is likely to produce a good solution.Multi-task learning (figure 9.12b) is a related technique in which the network is trained to solve several problems concurrently.For example, the network might take an image and simultaneously learn to segment the scene, estimate the pixel-wise depth, and predict a caption describing the image.All of these tasks require some understanding of the image and, when learned simultaneously, the model performance for each may improve.The above discussion assumes that we have plentiful data for a secondary task or data for multiple tasks to be learned concurrently.If not, we can create large amounts of "free" labeled data using self-supervised learning and use this for transfer learning.There are two families of methods for self-supervised learning: generative and contrastive.In generative self-supervised learning, part of each data example is masked, and the secondary task is to predict the missing part (figure 9.12c).For example, we might use a corpus of unlabeled images and a secondary task that aims to inpaint (fill in) missing parts of the image (figure 9.12c).Similarly, we might use a large corpus of text and mask some words.We train the network to predict the missing words and then fine-tune it for the actual language task we are interested in (see chapter 12).In contrastive self-supervised learning, pairs of examples with commonalities are compared to unrelated pairs.For images, the secondary task might be to identify whether a pair of images are transformed versions of one another or are unconnected.For text, the secondary task might be to determine whether two sentences followed one another in the original document.Sometimes, the precise relationship between a connected pair must be identified (e.g., finding the relative position of two patches from the same image).Transfer learning improves performance by exploiting a different dataset.Multi-task learning improves performance using additional labels.A third option is to expand the dataset.We can often transform each input data example in such a way that the label stays the same.For example, we might aim to determine if there is a bird in an image (figure 9.13).Here, we could rotate, flip, blur, or manipulate the color balance of the image, and the label "bird" remains valid.Similarly, for tasks where the input is text, Notebook 9.5 Augmentation we can substitute synonyms or translate to another language and back again.For tasks where the input is audio, we can amplify or attenuate different frequency bands.ing is used when we have limited labeled data for the primary task (here depth estimation) but plentiful data for a secondary task (here segmentation).We train a model for the secondary task, remove the final layers, and replace them with new layers appropriate to the primary task.We then train only the new layers or fine-tune the entire network for the primary task.The network learns a good internal representation from the secondary task that is then exploited for the primary task.b) In multi-task learning, we train a model to perform multiple tasks simultaneously, hoping that performance on each will improve.c) In generative self-supervised learning, we remove part of the data and train the network to complete the missing information.Here, the task is to fill in (inpaint) a masked portion of the image.This permits transfer learning when no labels are available.Generating extra training data in this way is known as data augmentation.The aim is to teach the model to be indifferent to these irrelevant data transformations.Explicit regularization involves adding an extra term to the loss function that changes the position of the minimum.The term can be interpreted as a prior probability over the parameters.Stochastic gradient descent with a finite step size does not neutrally descend to the minimum of the loss function.This bias can be interpreted as adding additional terms to the loss function, and this is known as implicit regularization.There are also many heuristics for improving generalization, including early stopping, dropout, ensembling, the Bayesian approach, adding noise, transfer learning, multi-task learning, and data augmentation.There are four main principles behind these methods (figure 9.14).We can (i) encourage the function to be smoother (e.g., L2 regularization), (ii) increase the amount of data (e.g., data augmentation), (iii) combine models (e.g., ensembling), or (iv) search for wider minima (e.g., applying noise to network weights).Another way to improve generalization is to choose the model architecture to suit the task.For example, in image segmentation, we can share parameters within the model, so we don't need to independently learn what a tree looks like at every image location.Chapters 10-13 consider architectural variations designed for different tasks.An overview and taxonomy of regularization techniques in deep learning can be found in Kukačka et al. (2017).Notably missing from the discussion in this chapter is BatchNorm (Szegedy et al., 2016) at its variants, which are described in chapter 11.Regularization: L2 regularization penalizes the sum of squares of the network weights.This encourages the output function to change slowly (i.e., become smoother) and is the most used regularization term.It is sometimes referred to as Frobenius norm regularization as it penalizes the Frobenius norms of the weight matrices.It is often also mistakenly referred to as "weight decay," although this is a separate technique devised by Hanson & Pratt (1988) in which the parameters ϕ are updated as: (9.13) where, as usual, α is the learning rate, and L is the loss.This is identical to gradient descent, except that the weights are reduced by a factor of 1−λ ′ before the gradient update.For standard SGD, weight decay is equivalent to L2 regularization (equation 9.5) with coefficient λ = λ ′ /2α.Problem 9.5However, for Adam, the learning rate α is different for each parameter, so L2 regularization and weight decay differ.Loshchilov & Hutter (2019) present AdamW, which modifies Adam to implement weight decay correctly and show that this improves performance.Other choices of vector norm encourage sparsity in the weights.The L0 regularization term where in the second line, we have introduced the correction term (equation 9.16), and in the final line, we have removed terms of greater order than α 2 .Note that the first two terms on the right-hand side ϕ 0 + αg[ϕ 0 ] are the same as the discrete update (equation 9.14).Hence, to make the continuous and discrete versions arrive at the same place, the third term on the right-hand side must equal zero, allowing us to solve for g 1 [ϕ]: This is equivalent to performing continuous gradient descent on the loss function:LGD  2021) both show that using a large learning rate reduces the tendency of typical optimization trajectories to move to "sharper" parts of the loss function (i.e., where at least one direction has high curvature).This implicit regularization effect of large learning rates can be approximated by penalizing the trace of the Fisher Information Matrix, which is closely related to penalizing the gradient norm in equation 9.20 (Jastrzębski et al., 2021).Early stopping: Bishop (1995) and Sjöberg & Ljung (1995) argued that early stopping limits the effective solution space that the training procedure can explore; given that the weights are initialized to small values, this leads to the idea that early stopping helps prevent the weights from getting too large.Goodfellow et al. (2016) show that under a quadratic approximation of the loss function with parameters initialized to zero, early stopping is equivalent to L2 regularization in gradient descent.The effective regularization weight λ is approximately 1/(τ α) where α is the learning rate, and τ is the early stopping time.Ensembling: Ensembles can be trained using different random seeds (Lakshminarayanan et al., 2017), hyperparameters (Wenzel et al., 2020b, or even entirely different families of models.The models can be combined by averaging their predictions, weighting the predictions, or stacking (Wolpert, 1992), in which the results are combined using another machine learning model.Lakshminarayanan et al. (2017) showed that averaging the output of independently trained networks can improve accuracy, calibration, and robustness.Conversely, Frankle et al. (2020) showed that if we average together the weights to make one model, the network fails.et al., 2017a) also store the models from different time steps and average their predictions.The diversity of these models can be improved by cyclically increasing and decreasing the learning rate.Garipov et al. (2018) observed that different minima of the loss function are often connected by a low-energy path (i.e., a path with a low loss everywhere along it).Motivated by this observation, they developed a method that explores low-energy regions around an initial solution to provide diverse models without retraining.This is known as fast geometric ensembling.A review of ensembling methods can be found in Ganaie et al. (2022).Dropout is applied at the level of hidden units.Dropping a hidden unit has the same effect as temporarily setting all the incoming and outgoing weights and the bias to zero.Wan et al.(2013) generalized dropout by randomly setting individual weights to zero.Gal & Ghahramani (2016) and Kendall & Gal (2017) proposed Monte Carlo dropout, in which inference is computed with several dropout patterns, and the results are averaged together.Gal & Ghahramani (2016) argued that this could be interpreted as approximating Bayesian inference.Dropout is equivalent to applying multiplicative Bernoulli noise to the hidden units.Similar benefits derive from using other distributions, including the normal (Srivastava et al., 2014;Shen et al., 2017), uniform (Shen et al., 2017, and beta distributions (Liu et al., 2019b).Adding noise: Bishop (1995) and An (1996) added Gaussian noise to the network inputs to improve performance.Bishop (1995) showed that this is equivalent to weight decay.An (1996) also investigated adding noise to the weights.DeVries & Taylor (2017a) added Gaussian noise to the hidden units.The randomized ReLU (Xu et al., 2015) applies noise in a different way by making the activation functions stochastic.Finding wider minima: It is thought that wider minima generalize better (see figure 20.11).Here, the exact values of the weights are less important, so performance should be robust to errors in their estimates.One of the reasons that applying noise to parts of the network during training is effective is that it encourages the network to be indifferent to their exact values.Bayesian approaches: For some models, including the simplified neural network model in figure 9.11, the Bayesian predictive distribution can be computed in closed form (see Bishop, 2006;Prince, 2012).For neural networks, the posterior distribution over the parameters cannot be represented in closed form and must be approximated.The two main approaches are variational Bayes (Hinton & van Camp, 1993;MacKay, 1995;Barber & Bishop, 1997;Blundell et al., 2015), in which the posterior is approximated by a simpler tractable distribution, and Markov Chain Monte Carlo (MCMC) methods, which approximate the distribution by drawing a set of samples (Neal, 1995;Welling & Teh, 2011;Chen et al., 2014;Ma et al., 2015;Li et al., 2016a).The generation of samples can be integrated into SGD, and this is known as stochastic gradient MCMC (see Ma et al., 2015).It has recently been discovered that "cooling" the posterior distribution over the parameters (making it sharper) improves predictions from these models (Wenzel et al., 2020a), but this is not currently fully understood (see Noci et al., 2021).Self-supervised learning in NLP can be based on predicting masked words (Devlin et al., 2019), predicting the next word in a sentence (Radford et al., 2019;Brown et al., 2020), or predicting whether two sentences follow one another (Devlin et al., 2019).In automatic speech recognition, the Wav2Vec model (Schneider et al., 2019) aims to distinguish an original audio sample from one where 10ms of audio has been swapped out from elsewhere in the clip.Self-supervision has also been applied to graph neural networks (chapter 13).Tasks include recovering masked features (You et al., 2020) , Calimeri et al., 2017).In other cases, the data have been augmented with adversarial examples (Goodfellow et al., 2015a), which are minor perturbations of the training data that cause the example to be misclassified.A review of data augmentation for images can be found in Shorten & Khoshgoftaar (2019).Augmentation methods for acoustic data include pitch shifting, time stretching, dynamic range compression, and adding random noise (e.g., Abeßer et al., 2017;Salamon & Bello, 2017;Xu et al., 2015;Lasseck, 2018), as well as mixing data pairs (Zhang et al., 2017c;Yun et al., 2019), masking features (Park et al., 2019), and using GANs to generate new data (Mun et al., 2017).Augmentation for speech data includes vocal tract length perturbation (Jaitly & Hinton, 2013;Kanda et al., 2013), style transfer (Gales, 1998;Ye & Young, 2004), adding noise (Hannun et al., 2014), and synthesizing speech (Gales et al., 2009).Augmentation methods for text include adding noise at a character level by switching, deleting, and inserting letters (Belinkov & Bisk, 2018;Feng et al., 2020)Problem 9.1 Consider a model where the prior distribution over the parameters is a normal distribution with mean zero and variance σ 2 ϕ so thatwhere j indexes the model parameters.We now maximize I i=1 P r(yi|xi, ϕ)P r(ϕ).Show that the associated loss function of this model is equivalent to L2 regularization.Chapter 10Chapters 2-9 introduced the supervised learning pipeline for deep neural networks.However, these chapters only considered fully connected networks with a single path from input to output.Chapters 10-13 introduce more specialized network components with sparser connections, shared weights, and parallel processing paths.This chapter describes convolutional layers, which are mainly used for processing image data.Images have three properties that suggest the need for specialized model architecture.First, they are high-dimensional.A typical image for a classification task contains 224×224 RGB values (i.e.,150,528 input dimensions).Hidden layers in fully connected networks are generally larger than the input size, so even for a shallow network, the number of weights would exceed 150, 528 2 , or 22 billion.This poses obvious practical problems in terms of the required training data, memory, and computation.Second, nearby image pixels are statistically related.However, fully connected networks have no notion of "nearby" and treat the relationship between every input equally.If the pixels of the training and test images were randomly permuted in the same way, the network could still be trained with no practical difference.Third, the interpretation of an image is stable under geometric transformations.An image of a tree is still an image of a tree if we shift it leftwards by a few pixels.However, this shift changes every input to the network.Hence, a fully connected model must learn the patterns of pixels that signify a tree separately at every position, which is clearly inefficient.Convolutional layers process each local image region independently, using parameters shared across the whole image.They use fewer parameters than fully connected layers, exploit the spatial relationships between nearby pixels, and don't have to re-learn the interpretation of the pixels at every position.A network predominantly consisting of convolutional layers is known as a convolutional neural network or CNN.We argued above that some properties of images (e.g., tree texture) are stable under transformations.In this section, we make this idea more mathematically precise.A  This can be handled by zero padding, in which we assume values outside the input are zero.The final output is treated similarly.d) Alternatively, we could only compute outputs where the kernel fits within the input range ("valid" convolution); now, the output will be smaller than the input.Convolutional networks consist of a series of convolutional layers, each of which is equivariant to translation.They also typically include pooling mechanisms that induce partial invariance to translation.For clarity of exposition, we first consider convolutional networks for 1D data, which are easier to visualize.In section 10.3, we progress to 2D convolution, which can be applied to image data.Convolutional layers are network layers based on the convolution operation.In 1D, a convolution transforms an input vector x into an output vector z so that each output z i is a weighted sum of nearby inputs.The same weights are used at every position and are collectively called the convolution kernel or filter.The size of the region over which inputs are combined is termed the kernel size.For a kernel size of three, we have: (10.3)where ω = [ω 1 , ω 2 , ω 3 ] T is the kernel (figure 10.2). 1 Notice that the convolution oper-Problem 10.1ation is equivariant with respect to translation.If we translate the input x, then the corresponding output z is translated in the same way.With a kernel size of five, we take a weighted sum of the nearest five inputs.d) In dilated or atrous convolution (from the French "à trous" -with holes), we intersperse zeros in the weight vector to allow us to combine information over a large area using fewer weights.Equation 10.3 shows that each output is computed by taking a weighted sum of the previous, current, and subsequent positions in the input.This begs the question of how to deal with the first output (where there is no previous input) and the final output (where there is no subsequent input).There are two common approaches.The first is to pad the edges of the inputs with new values and proceed as usual.Zero padding assumes the input is zero outside its valid range (figure 10.2c).Other possibilities include treating the input as circular or reflecting it at the boundaries.The second approach is to discard the output positions where the kernel exceeds the range of input positions.These valid convolutions have the advantage of introducing no extra information at the edges of the input.However, they have the disadvantage that the representation decreases in size.In the example above, each output was a sum of the nearest three inputs.However, this is just one of a larger family of convolution operations, the members of which are distinguished by their stride, kernel size, and dilation rate.When we evaluate the output at every position, we term this a stride of one.However, it is also possible to shift the kernel by a stride greater than one.If we have a stride of two, we create roughly half the number of outputs (figure 10.3a-b).The kernel size can be increased to integrate over a larger area (figure 10.3c).However, it typically remains an odd number so that it can be centered around the current position.Increasing the kernel size has the disadvantage of requiring more weights.This leads to the idea of dilated or atrous convolutions, in which the kernel values are interspersed with zeros.For example, we can turn a kernel of size five into a dilated kernel of size three by setting the second and fourth elements to zero.We still integrate informa-Problems 10.2-10.4tion from a larger input region but only require three weights to do this (figure 10.3d).The number of zeros we intersperse between the weights determines the dilation rate.A convolutional layer computes its output by convolving the input, adding a bias β, and passing each result through an activation function a [•].With kernel size three, stride one, and dilation rate one, the i th hidden unit h i would be computed as: (10.4)where the bias β and kernel weights ω 1 , ω 2 , ω 3 are trainable parameters, and (with zero padding) we treat the input x as zero when it is out of the valid range.This is a special case of a fully connected layer that computes the i th hidden unit as: and others are constrained to be identical (figure 10.4).If we only apply a single convolution, information will inevitably be lost; we are averaging nearby inputs, and the ReLU activation function clips results that are less than zero.Hence, it is usual to compute several convolutions in parallel.Each convolution produces a new set of hidden variables, termed a feature map or channel.Typically, multiple convolutions are applied to the input x and stored in channels.a) A convolution is applied to create hidden units h1 to h6, which form the first channel.b) A second convolution operation is applied to create hidden units h7 to h12, which form the second channel.The channels are stored in a 2D array H1 that contains all the hidden units in the first hidden layer.c) If we add a further convolutional layer, there are now two channels at each input position.Here, the 1D convolution defines a weighted sum over both input channels at the three closest positions to create each new output channel.Supervised learning models define a mapping from input data to an output prediction.In the following sections, we discuss the inputs, the outputs, the model itself, and what is meant by "training" a model.Supervised learning models define a mapping from input data to an output prediction.In the following sections, we discuss the inputs, the outputs, the model itself, and what is meant by "training" a model.Appendix C.5.1 KL Divergence and p(z) can be evaluated using the Kullback-Leibler (KL) divergence:(5.27)Now consider that we observe an empirical data distribution at points {y i } I i=1 .We can describe this as a weighted sum of point masses:(5.28)where δ [•] is the Dirac delta function.We want to minimize the KL divergence between Lipschitz constant of a model (the fastest the output can change for a small input change).A review of the theory of over-parameterized machine learning can be found in Dar et al. (2021).As dimensionality increases, the volume of space grows so fast that the amount of data needed to densely sample it increases exponentially.This phenomenon is known as the curse of dimensionality.High-dimensional space has many unexpected properties, and caution should be used when trying to reason about it based on low-dimensional examples.This book visualizes many aspects of deep learning in one or two dimensions, but these visualizations should be treated with healthy skepticism.Surprising properties of high-dimensional spaces include: (i) Two randomly sampled data points from a standard normal distribution are very close to orthogonal to one another (relative to Problems 8.6-8.9 the origin) with high likelihood.(ii) The distance from the origin of samples from a standard normal distribution is roughly constant.(iii) Most of a volume of a high-dimensional sphere (hypersphere) is adjacent to its surface (a common metaphor is that most of the volume of a highdimensional orange is in the peel, not in the pulp).(iv) If we place a unit-diameter hypersphere inside a hypercube with unit-length sides, then the hypersphere takes up a decreasing proportion of the volume of the cube as the dimension increases.Since the volume of the cube is fixed atIn this chapter, we argued that model performance could be evaluated using a held-out test set.However, the result won't be indicative of real-world performance if the statistics of the test set don't match those of real-world data.Moreover, the statistics of real-world data may change over time, causing the model to become increasingly stale and performance to decrease.This is known as data drift and means that deployed models must be carefully monitored.There are three main reasons why real-world performance may be worse than the test performance implies.First, the statistics of the input data x may change; we may now be observing parts of the function that were sparsely sampled or not sampled at all during training.This is known as covariate shift.Second, the statistics of the output data y may change; if some output values are infrequent during training, then the model may learn not to predict these in ambiguous situations and will make mistakes if they are more common in the real world.This is known as prior shift.Third, the relationship between input and output may change.This is known as concept shift.These issues are discussed in Moreno-Torres et al. (2012).Hyperparameter search: Finding the best hyperparameters is a challenging optimization task.Testing a single configuration of hyperparameters is expensive; we must train an entire model and measure its performance.We have no easy way to access the derivatives (i.e., how performance changes when we make a small change to a hyperparameter).Moreover, many of the hyperparameters are discrete, so we cannot use gradient descent methods.applies a fixed penalty for every non-zero weight.The effect is to "prune" the network.L0 regularization can also be used to encourage group sparsity; this might apply a fixed penalty if any of the weights contributing to a given hidden unit are non-zero.If they are all zero, we can remove the unit, decreasing the model size and making inference faster.Unfortunately, L0 regularization is challenging to implement since the derivative of the regularization term is not smooth, and more sophisticated fitting methods are required (see Louizos et al., 2018).Somewhere between L2 and L0 regularization is L1 regularization or LASSO (least absolute shrinkage and selection operator), which imposes a penalty on the absolute values of the weights.L2 regularization somewhat discourages sparsity in that the derivative of the squared penalty decreases as the weight becomes smaller, lowering the pressure to make it smaller still.L1 regularization does not have this disadvantage, as the derivative of the penalty is constant.This can produce sparser solutions than L2 regularization but is much easier to Problem 9.6 optimize than L0 regularization.Sometimes both L1 and L2 regularization terms are used, which is termed an elastic net penalty (Zou & Hastie, 2005).A different approach to regularization is to modify the gradients of the learning algorithm without ever explicitly formulating a new loss function (e.g., equation 9.13).This approach has been used to promote sparsity during backpropagation (Schwarz et al., 2021).The evidence on the effectiveness of explicit regularization is mixed.2021) take a different approach and develop an algorithm that constrains the Lipschitz constant of the network to be below a particular value.The gradient descent step is:where g[ϕ 0 ] is the negative of the gradient of the loss function, and α is the step size.As α → 0, the gradient descent process can be described by a differential equation: Consider the first two terms of a Taylor expansion of the modified continuous solution ϕ around initial position ϕ 0 :As dimensionality increases, the volume of space grows so fast that the amount of data needed to densely sample it increases exponentially.This phenomenon is known as the curse of dimensionality.High-dimensional space has many unexpected properties, and caution should be used when trying to reason about it based on low-dimensional examples.This book visualizes many aspects of deep learning in one or two dimensions, but these visualizations should be treated with healthy skepticism.Surprising properties of high-dimensional spaces include: (i) Two randomly sampled data points from a standard normal distribution are very close to orthogonal to one another (relative to Problems 8.6-8.9 the origin) with high likelihood.(ii) The distance from the origin of samples from a standard normal distribution is roughly constant.(iii) Most of a volume of a high-dimensional sphere (hypersphere) is adjacent to its surface (a common metaphor is that most of the volume of a highdimensional orange is in the peel, not in the pulp).(iv) If we place a unit-diameter hypersphere inside a hypercube with unit-length sides, then the hypersphere takes up a decreasing proportion of the volume of the cube as the dimension increases.Since the volume of the cube is fixed atIn this chapter, we argued that model performance could be evaluated using a held-out test set.However, the result won't be indicative of real-world performance if the statistics of the test set don't match those of real-world data.Moreover, the statistics of real-world data may change over time, causing the model to become increasingly stale and performance to decrease.This is known as data drift and means that deployed models must be carefully monitored.There are three main reasons why real-world performance may be worse than the test performance implies.First, the statistics of the input data x may change; we may now be observing parts of the function that were sparsely sampled or not sampled at all during training.This is known as covariate shift.Second, the statistics of the output data y may change; if some output values are infrequent during training, then the model may learn not to predict these in ambiguous situations and will make mistakes if they are more common in the real world.This is known as prior shift.Third, the relationship between input and output may change.This is known as concept shift.These issues are discussed in Moreno-Torres et al. (2012).Hyperparameter search: Finding the best hyperparameters is a challenging optimization task.Testing a single configuration of hyperparameters is expensive; we must train an entire model and measure its performance.We have no easy way to access the derivatives (i.e., how performance changes when we make a small change to a hyperparameter).Moreover, many of the hyperparameters are discrete, so we cannot use gradient descent methods.applies a fixed penalty for every non-zero weight.The effect is to "prune" the network.L0 regularization can also be used to encourage group sparsity; this might apply a fixed penalty if any of the weights contributing to a given hidden unit are non-zero.If they are all zero, we can remove the unit, decreasing the model size and making inference faster.Unfortunately, L0 regularization is challenging to implement since the derivative of the regularization term is not smooth, and more sophisticated fitting methods are required (see Louizos et al., 2018).Somewhere between L2 and L0 regularization is L1 regularization or LASSO (least absolute shrinkage and selection operator), which imposes a penalty on the absolute values of the weights.L2 regularization somewhat discourages sparsity in that the derivative of the squared penalty decreases as the weight becomes smaller, lowering the pressure to make it smaller still.L1 regularization does not have this disadvantage, as the derivative of the penalty is constant.This can produce sparser solutions than L2 regularization but is much easier to Problem 9.6 optimize than L0 regularization.Sometimes both L1 and L2 regularization terms are used, which is termed an elastic net penalty (Zou & Hastie, 2005).A different approach to regularization is to modify the gradients of the learning algorithm without ever explicitly formulating a new loss function (e.g., equation 9.13).This approach has been used to promote sparsity during backpropagation (Schwarz et al., 2021).The evidence on the effectiveness of explicit regularization is mixed.2021) take a different approach and develop an algorithm that constrains the Lipschitz constant of the network to be below a particular value.The gradient descent step is:where g[ϕ 0 ] is the negative of the gradient of the loss function, and α is the step size.As α → 0, the gradient descent process can be described by a differential equation: Consider the first two terms of a Taylor expansion of the modified continuous solution ϕ around initial position ϕ 0 :