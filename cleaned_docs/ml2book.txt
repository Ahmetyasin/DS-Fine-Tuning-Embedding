Preface vii 1 Linear AlgebraWe may have all heard the saying "use it or lose it".We experience it when we feel rusty in a foreign language or sports that we have not practised in a while.Practice is important to maintain skills but it is also key when learning new ones.This is a reason why many textbooks and courses feature exercises.However, the solutions to the exercises feel often overly brief, or are sometimes not available at all.Rather than an opportunity to practice the new skills, the exercises then become a source of frustration and are ignored.This book contains a collection of exercises with detailed solutions.The level of detail is, hopefully, sufficient for the reader to follow the solutions and understand the techniques used.The exercises, however, are not a replacement of a textbook or course on machine learning.I assume that the reader has already seen the relevant theory and concepts and would now like to deepen their understanding through solving exercises.While coding and computer simulations are extremely important in machine learning, the exercises in the book can (mostly) be solved with pen and paper.The focus on penand-paper exercises reduced length and simplified the presentation.Moreover, it allows the reader to strengthen their mathematical skills.However, the exercises are ideally paired with computer exercises to further deepen the understanding.The exercises collected here are mostly a union of exercises that I developed for the courses "Unsupervised Machine Learning" at the University of Helsinki and "Probabilistic Modelling and Reasoning" at the University of Edinburgh.The exercises do not comprehensively cover all of machine learning but focus strongly on unsupervised methods, inference and learning.(a) Given two vectors a 1 and a 2 in R n , show thatare orthogonal to each other.Two vectors u 1 and u 2 of R n are orthogonal if their inner product equals zero.Computing the inner product u 1 u 2 gives 1.4) Hence the vectors u 1 and u 2 are orthogonal.If a 2 is a multiple of a 1 , the orthogonalisation procedure produces a zero vector for u 2 .To see this, let a 2 = αa 1 for some real number α.We then obtain u 2 = a 2 − u 1 a 2 u 1 u 1 u 1 (S.1.5) = αu 1 − αu 1 u 1 u 1 u 1 u 1 (S.1.6) = αu 1 − αu 1 (S.1.7)= 0. (S. 1.8) (b) Show that any linear combination of (linearly independent) a 1 and a 2 can be written in terms of u 1 and u 2 .Solution.Let v be a linear combination of a 1 and a 2 , i.e. v = αa 1 + βa 2 for some real numbers α and β.Expressing u 1 and u 2 in term of a 1 and a 2 , we can write v as v = αa 1 + βa 2 (S.1.9)1.11) = (α + β u 1 a 2 u 1 u 1 )u 1 + βu 2 , (S. 1.12) Since α + β((u 1 a 2 )/(u 1 u 1 )) and β are real numbers, we can write v as a linear combination of u 1 and u 2 .Overall, this means that any vector in the span of {a 1 , a 2 } can be expressed in the orthogonal basis {u 1 , u 2 }.(c) Show by induction that for any k ≤ n linearly independent vectors a 1 , . . ., a k , the vectors u i , i = 1, . . .k, are orthogonal, whereu j a i u j u j u j .(1.3)The calculation of the vectors u i is called Gram-Schmidt orthogonalisation.Solution.We have shown above that the claim holds for two vectors.This is the base case for the proof by induction.Assume now that the claim holds for k vectors.The induction step in the proof by induction then consists of showing that the claim also holds for k + 1 vectors.Assume that u 1 , u 2 , . . ., u k are orthogonal vectors.The linear independence assumption ensures that none of the u i is a zero vector.We then have for u k+1 1.13) and for all i = 1, 2, . . ., k(S. 1.14)By assumption u i u j = 0 if i = j, so that 1.15) = u i a k+1 − u i a k+1 (S. 1.16) = 0, (S. 1.17) which means that u k+1 is orthogonal to u 1 , . . ., u k .(d) Show by induction that any linear combination of (linear independent) a 1 , a 2 , . . ., a k can be written in terms of u 1 , u 2 , . . ., u k .Solution.The base case of two vectors was proved above.Using induction, we assume that the claim holds for k vectors and we will prove that it then also holds for k + 1 vectors: Let v be a linear combination of a 1 , a 2 , . . ., a k+1 , i.e. v = α 1 a 1 + α 2 a 2 + . . .+ α k a k + α k+1 a k+1 for some real numbers α 1 , α 2 , . . ., α k+1 .Using the induction assumption, v can be written as v = β 1 u 1 + β 2 u 2 + . . .+ β k u k + α k+1 a k+1 , (S. 1.18) for some real numbers β 1 , β 2 , . . ., β k Furthermore, using equation (S. 1.13), v can be written as v = β 1 u 1 + . . .+ β k u k + α k+1 u k+1 + α k+1 u 1 a k+1 u 1 u 1 u 1 (S.1.19)(S. 1.20)With γ i = β i + α k+1 (u i a k+1 )/(u i u i ), v can thus be written as v = γ 1 u 1 + γ 2 u 2 + . . .+ γ k u k + α k+1 u k+1 , (S. 1.21) which completes the proof.Overall, this means that the u 1 , u 2 , . . ., u k form an orthogonal basis for span(a 1 , . . ., a k ), i.e. the set of all vectors that can be obtained by linearly combining the a i .(e) Consider the case where a 1 , a 2 , . . ., a k are linearly independent and a k+1 is a linear combination of a 1 , a 2 , . . ., a k .Show that u k+1 , computed according to (1.3), is zero.Starting with (1.3), we haveu j a k+1 u j u j u j .(S. 1.22)By assumption, a k+1 is a linear combination of a 1 , a 2 , . . ., a k .By the previous question, it can thus also be written as a linear combination of the u 1 , . . ., u k .This means that there are some β i so that 1.23) holds.Inserting this expansion into the equation above gives 1.25) because u j u i = 0 if i = j.We thus obtain the desired result: 1.26) = 0 (S.1.27)This property of the Gram-Schmidt process in (1.3) can be used to check whether a list of vectors a 1 , a 2 , . . ., a d is linearly independent or not.If, for example, u k+1 is zero, a k+1 is a linear combination of the a 1 , . . ., a k .Moreover, the result can be used to extract a sublist of linearly independent vectors: We would remove a k+1 from the list and restart the procedure in (1.3) with a k+2 taking the place of a k+1 .Continuing in this way constructs a list of linearly independent a j and orthogonal u j , j = 1, . . ., r, where r is the number of linearly independent vectors among the a 1 , a 2 , . . ., a d .(a) Assume two vectors a 1 and a 2 are in R 2 .Together, they span a parallelogram.Use Exercise 1.1 to show that the squared area S 2 of the parallelogram is given by S 2 = (a T 2 a 2 )(a T 1 a 1 ) − (a T 2 a 1 ) 2 (1.4)Let a 1 and a 2 be the vectors that span the parallelogram.From geometry we know that the area of parallelogram is base times height, which is equivalent to the length of the base vector times the length of the height vector.Denote this by, where is a 1 is the base vector and u 2 is the height vector which is orthogonal to the base vector.Using the Gram-Schmidt process for the vectors a 1 and a 2 in that order, we obtain the vector u 2 as the second output.1.30) = a 2 a 2 − (a 1 a 2 ) 2 a 1 a 1 .(S. 1.31) Thus, S 2 is: 1.32) = (a 1 a 1 )(u 2 u 2 ) (S. 1.33) = (a 1 a 1 ) a 2 a 2 − (a 1 a 2 ) 2 a 1 a 1 (S.1.34) = (a 2 a 2 )(a 1 a 1 ) − (a 1 a 2 ) 2 .(S. 1.35) (b) Form the matrix A = (a 1 a 2 ) where a 1 and a 2 are the first and second column vector, respectively.Show that S 2 = (det A) 2 .(1.5) Solution.We form the matrix A, A = a 1 a 2 = a 11 a 12 a 21 a 22 .(S. 1.36)The determinant of A is det A = a 11 a 22 −a 12 a 21 .By multiplying out (a 2 a 2 ), (a 1 a 1 ) and (a 1 a 2 ) 2 , we get a 2 a 2 = a 2 12 + a 2 22 (S.1.37) = (a 11 a 22 − a 12 a 21 ) 2 , (S. 1.43) which equals (det A) 2 .(c) Consider the linear transform y = Ax where A is a 2 × 2 matrix.Denote the image of the rectangleWhat is U y ?What is the area of U y ?Solution.U y is parallelogram that is spanned by the column vectors a 1 and a 2 of A, when A = (a 1 a 2 ).A rectangle with the same area as U x is spanned by vectors (∆ 1 , 0) and (0, ∆ 2 ).Under the linear transform A these spanning vectors become ∆ 1 a 1 and ∆ 2 a 2 .Therefore a parallelogram with the same area as U y is spanned by ∆ 1 a 1 and ∆ 2 a 2 as shown in the following figure.x 1 + ∆ 1x 2 + ∆ 2x 1x 1x 1 + ∆ 1From the previous question, the A Uy of U y equals the absolute value of the determinant of the matrix (∆ 1 a 1 ∆ 2 a 2 ): where A is such that U x is an axis-aligned (hyper-) rectangle as in the previous question.We can think that, loosely speaking, the two integrals are limits of the following two sums 1.48) where x i = A −1 y i , which means that x and y are related by y = Ax.The set of function values f (y i ) and f (Ax i ) that enter the two sums are exactly the same.The volume vol(∆ x i ) of a small axis-aligned hypercube (in d dimensions) equals d i=1 ∆ i .The image of this small axis-aligned hypercube under A is a parallelogram ∆ y i with volume vol(∆ y i ) = | det A|vol(∆ x i ).Hence(S. 1.49)We must have the term | det A| to compensate for the fact that the volume of U x and U y are not the same.For example, let A be a diagonal matrix diag(10, 100) so that U x is much smaller than U y .The determinant det A = 1000 then compensates for the fact that the x i values are more condensed than the y i .For a square matrix A of size n × n, a vector u i = 0 which satisfiesis called a eigenvector of A, and λ i is the corresponding eigenvalue.For a matrix of size n × n, there are n eigenvalues λ i (which are not necessarily distinct).(a) Show that if u 1 and u 2 are eigenvectors with λ 1 = λ 2 , then u = αu 1 + βu 2 is also an eigenvector with the same eigenvalue.Au = αAu 1 + βAu 2 (S.1.50) = αλu 1 + βλu 2 (S.1.51) = λ(αu 1 + βu 2 ) (S. 1.52) = λu, (S. 1.53) so u is an eigenvector of A with the same eigenvalue as u 1 and u 2 .(b) Assume that none of the eigenvalues of A is zero.Denote by U the matrix where the column vectors are linearly independent eigenvectors u i of A. Verify that (1.7) can be written in matrix form as AU = UΛ, where Λ is a diagonal matrix with the eigenvalues λ i as diagonal elements.Solution.By basic properties of matrix multiplication, we have AU = (Au 1 Au 2 . . .Au n ) (S. 1.54)With Au i = λ i u i for all i = 1, 2, . . ., n, we thus obtain 1.55) = UΛ.(S. 1.56) (c) Show that we can write, withAwhere v i is the i-th column of V.(i) Since the columns of U are linearly independent, U is invertible.Because AU = UΛ, multiplying from the right with the inverse of U gives A = UΛU −1 = UΛV .(ii) Denote by u [i] the ith row of U, v (j) the jth column of V and v [j] the jth row of V and denote B = n i=1 λ i u i v i .Let e [i] be a row vector with 1 in the ith place and 0 elsewhere and e (j) be a column vector with 1 in the jth place and 0 elsewhere.Notice that because A = UΛV , the element in the ith row and jth column is (j) (S. 1.57) = u [i] Λv [j] (S. 1.58)(S. 1.60) On the other hand, for matrix B the element in the ith row and jth column isλ k e [i] u k v k e (j) (S. 1.61)λ k U ik V jk , (S. 1.62) which is the same as A ij .Therefore A = B.(iii) Since Λ is a diagonal matrix with no zeros as diagonal elements, it is invertible.We have thusA −1 = (UΛU −1 ) −1 (S. 1.63) = (ΛU −1 ) −1 U −1 (S. 1.64) = UΛ −1 U −1 (S. 1.65) = UΛ −1 V .(S. 1.66) (iv) This follows from A = UΛV = i u i λ i v i , when λ i is replaced with 1/λ i .1.4 Trace, determinants and eigenvalues  1.75) where, in the last line, we have used that the determinant of a diagonal matrix is the product of its elements.(a) Assume that a matrix A is symmetric, i.e.A = A. Let u 1 and u 2 be two eigenvectors of A with corresponding eigenvalues λ 1 and λ 2 , with λ 1 = λ 2 .Show that the two vectors are orthogonal to each other.Since Au 2 = λ 2 u 2 , we have(S. 1.76) Taking the transpose of u 1 Au 2 gives (u 1 Au 2 ) = (Au 2 ) (u 1 ) = u 2 A u 1 = u 2 Au 1 (S.1.77) = λ 1 u 2 u 1 (S.1.78) because A is symmetric and Au 1 = λ 1 u 1 .On the other hand, the same operation gives 1.79) Therefore λ 1 u 2 u 1 = λ 2 u 2 u 1 , which is equivalent to u 2 u 1 (λ 1 − λ 2 ) = 0.Because λ 1 = λ 2 , the only possibility is that u 2 u 1 = 0. Therefore u 1 and u 2 are orthogonal to each other.The result implies that the eigenvectors of a symmetric matrix A with distinct eigenvalues λ i forms an orthogonal basis.The result extends to the case where some of the eigenvalues are the same (not proven).(b) A symmetric matrix A is said to be positive definite if v T Av > 0 for all non-zero vectors v. Show that positive definiteness implies that λ i > 0, i = 1, . . ., M .Show that, vice versa, λ i > 0, i = 1 . . .M implies that the matrix A is positive definite.Conclude that a positive definite matrix is invertible.Assume that v Av > 0 for all v = 0. Since eigenvectors are not zero vectors, the assumption holds also for eigenvector u k with corresponding eigenvalue λ k .Now 1.80) and because ||u k || > 0, we obtain λ k > 0.Assume now that all the eigenvalues of A, λ 1 , λ 2 , . . ., λ n , are positive and nonzero.We have shown above that there exists an orthogonal basis consisting of eigenvectors u 1 , u 2 , . . ., u n and therefore every vector v can be written as a linear combination of those vectors (we have only shown it for the case of distinct eigenvalues but it holds more generally).Hence for a nonzero vector v and for some real numbers α 1 , α 2 , . . ., α n , we haveα i u i α j λ j u j (S. 1.84) 1.86) where we have used that u T i u j = 0 if i = j, due to orthogonality of the basis.Since (α i ) 2 > 0, ||u i || 2 > 0 and λ i > 0 for all i, we find that v Av > 0.Since every eigenvalue of A is nonzero, we can use Exercise 1.3 to conclude that inverse of A exists and equals i 1/λ i u i u i .We here analyse an algorithm called the "power method".The power method takes as input a positive definite symmetric matrix Σ Σ Σ and calculates the eigenvector that has the largest eigenvalue (the "first eigenvector").For example, in case of principal component analysis, Σ Σ Σ is the covariance matrix of the observed data and the first eigenvector is the first principal component direction.The power method consists in iterating the update equations .10)where ||v k+1 || 2 denotes the Euclidean norm.(a) Let U the matrix with the (orthonormal) eigenvectors u i of Σ Σ Σ as columns.What is the eigenvalue decomposition of the covariance matrix Σ Σ Σ?Solution.Since the columns of U are orthonormal (eigen)vectors, U is orthogonal, i.e.U −1 = U .With Exercise 1.3 and Exercise 1.5, we obtain Σ Σ Σ = UΛU , (S. 1.87) where Λ is the diagonal matrix with eigenvalues λ i of Σ Σ Σ as diagonal elements.Let the eigenvalues be ordered λ 1 > λ 2 > . . .> λ n > 0 (and, as additional assumption, all distinct).(b) Let ṽk = U T v k and wk = U T w k .Write the update equations of the power method in terms of ṽk and wk .This means that we are making a change of basis to represent the vectors w k and v k in the basis given by the eigenvectors of Σ Σ Σ.With 1.88) = UΛU w k (S. 1.89) we obtain U v k+1 = ΛU w k .(S. 1.90) Hence ṽk+1 = Λ wk .The norm of ṽk+1 is the same as the norm of v k+1 : 1.92) = v k+1 UU v k+1 (S. 1.93) = v k+1 v k+1 (S. 1.94) = ||v k+1 || 2 .(S. 1.95) Hence, the update equation, in terms of ṽk and wk , is ṽk+1 = Λ wk , wk+1 = ṽk+1 ||ṽ k+1 || .(S. 1.96) (c) Assume you start the iteration with w0 .To which vector w * does the iteration converge to?Solution.Let w0 = α 1 α 2 . . .α n .Since Λ is a diagonal matrix, we obtain 1.97) and therefore .1.98)where c 1 is a normalisation constant such that w1 = 1 (i.e.c 1 = ṽ1 ).Hence, for wk it holds that .1.99)where ck is again a normalisation constant such that || wk || = 1.As λ 1 is the dominant eigenvalue, |λ j /λ 1 | < 1 for j = 2, 3, . . ., n, so that . . , n, (S.1.100)and hence .1.101)For the normalisation constant ck , we obtain 1.102) and thereforeThe limit of the product of two convergent sequences is the product of the limits so that .1.106)(d) Conclude that the power method finds the first eigenvector.Since w k = U wk , we obtain 1.107) which is the eigenvector with the largest eigenvalue, i.e. the "first" or "dominant" eigenvector.Chapter 2ExercisesFor a function J that maps a column vector w ∈ R n to R, the gradient is defined aswhere ∂J(w)/∂w i are the partial derivatives of J(w) with respect to the i-th element of the vector w = (w 1 , . . ., w n ) (in the standard basis).Alternatively, it is defined to be the column vector ∇J(w) such thatfor an arbitrary perturbation h.This phrases the derivative in terms of a first-order, or affine, approximation to the perturbed function J(w + h).The derivative ∇J is a linear transformation that maps h ∈ R n to R (see e.g. Rudin, 1976, Chapter 9, for a formal treatment of derivatives).Use either definition to determine ∇J(w) for the following functions where a ∈ R n , A ∈ R n×n and f : R → R is a differentiable function.(a) J(w) = a w.First method:Second method:Hence we find again ∇J(w) = a.(b) J(w) = w Aw.First method: We start withwhere we have used that the entry in row i and column k of the matrix A equals the entry in row k and column i of its transpose A .It follows thatwhere we have used that sums like j B ij w j are equal to the i-th element of the matrix-vector product Bw.Second method: .2.13) where we have used that h Aw is a scalar so that h Aw = (h Aw) = w A h.Solution.The easiest way to calculate the gradient of J(w) = w w is to use the previous question with A = I (the identity matrix).Therefore The derivatives ∂w w/∂w k were calculated in the question above so thatSecond method: Let f (w) = w w.From the previous question, we know thatWith z = f (w) and u = 2w h, we thus obtainEither the chain rule or the approach with the Taylor expansion can be used to deal with the outer function f .In any case: .2.28) where f is the derivative of the function f .(f) J(w) = f (w a).We have seen that ∇ w a w = a.Using the chain rule then yields .2.29) =f (w a)a (S.2.30)Assume that in the neighbourhood of w 0 , a function J(w) can be described by the quadratic approximationwhere c = J(w 0 ), g is the gradient of J with respect to w, and H a symmetric positive definite matrix (e.g. the Hessian matrix for J(w) at w 0 if positive definite).(a) Use Exercise 2.1 to determine ∇f (w).We first write f as .2.32) Using now that w Hw 0 is a scalar and that H is symmetric, we have w Hw 0 = (w Hw 0 ) = w 0 H w = w 0 Hw (S. 2.33) and hence .2.34)With the results from Exercise 2.1 and the fact that H is symmetric, we thus obtainThe expansion of f (w) due to the w − w 0 terms is a bit tedious.It is simpler to note that gradients define a linear approximation of the function.We can more efficiently deal with w − w 0 by changing the coordinates and determine the linear approximation of f as a function of v = w − w 0 , i.e. locally around the point w 0 .We then have .2.38)With Exercise 2.1, the derivative is .2.39) and the linear approximation becomesThe linear approximation for f determines a linear approximation of f around w 0 , i.e. .2.41) so that the derivative for f iswhich is the same result as before.(b) A necessary condition for w being optimal (leading either to a maximum, minimum or a saddle point) is ∇f (w) = 0. Determine w * such that ∇f (w) w=w * = 0. Provide arguments why w * is a minimiser of f (w).We set the gradient to zero and solve for w:As we assumed that H is positive definite, the inverse H exists (and is positive definite too).Let us consider f as a function of v around w * , i.e. w = w * +v.With w * +v −w 0 = −H −1 g + v, we haveHence, as we move away from w * , the function increases quadratically, so that w * minimises f (w).(c) In terms of Newton's method to minimise J(w), what do w 0 and w * stand for?.2.46) corresponds to one update step in Newton's method where w 0 is the current value of w in the optimisation of J(w) and w * is the updated value.In practice rather than determining the inverse H −1 , we solve .2.47) for p and then set w * = w 0 − p.The vector p is the search direction, and it is possible include a step-length α so that the update becomes w * = w 0 − αp.The value of α may be set by hand or can be determined via line-search methods (see e.g.Nocedal and Wright, 1999).For functions J that map a matrix W ∈ R n×m to R, the gradient is defined as(2.4)Alternatively, it is defined to be the matrix ∇J such thatThis definition is analogue to the one for vector-valued functions in (2.2).It phrases the derivative in terms of a linear approximation to the perturbed objective J(W + H) and, more formally, tr ∇J is a linear transformation that maps H ∈ R n×m to R (see e.g.Rudin, 1976, Chapter 9, for a formal treatment of derivatives).Let e (i) be column vector which is everywhere zero but in slot i where it is 1.Moreover let e [j] be a row vector which is everywhere zero but in slot j where it is 1.The outer product e (i) e [j] is then a matrix that is everywhere zero but in row i and column j where it is one.For H = e (i) e [j] , we obtainNote that e [i] ∇Je (j) picks the element of the matrix ∇J that is in row i and column j, i.e. e [i] ∇Je (j) = ∂J/∂W ij .Use either of the two definitions to find ∇J(W) for the functions below, where .2.48) and hence ∇J(W) = uv (S.2.49)Second method: .2.53) Hence:Expanding the objective function gives J(W) = u Wv + u Av.The second term does not depend on W. With the previous question, the derivative thus is, where w n are the rows of the matrix W.First method: .2.59) where f operates element-wise on the vector Wv.Second method: .2.61) where e [k] is the unit row vector that is zero everywhere but for element k which equals one.We now perform a perturbation of W by H.f (e [k] Wv + e [k] Hv) (S.2.63)f (e [k] Wv)e [k] Hv + O( 2 ) (S.2.65)The term f (e [k] Wv)e [k] is a row vector that equals (0, . . ., 0, f (e [k] Wv), 0, . . ., 0).Hence, we have .2.66) where f operates element-wise on the column vector Wv.The perturbed objective function thus isHence, the gradient is the transpose of vf (Wv) , i.e.Solution.We first verify the hint:Hence the identity holds up to terms smaller than 2 , which is sufficient we do not care about terms of order 2 and smaller in the definition of the gradient in (2.5).Let us thus make a first-order approximation of the perturbed objective J(W + H):Comparison with (2.5) gives .2.78) and hencewhere W − is the transpose of the inverse of W.The goal of this exercise is to determine the gradient of(2.10) (a) Show that the n-th eigenvalue λ n can be written as (2.11) where u n is the nth eigenvector and v n the nth column vector of U −1 , with U being the matrix with the eigenvectors u n as columns.As in Exercise 1.3, let UΛV be the eigenvalue decomposition of W (with V = U −1 ).Then Λ = V WU andwhere e (n) is the standard basis (unit) vector with a 1 in the n-th slot and zeros elsewhere, and e [n] is the corresponding row vector.(b) Calculate the gradient of λ n with respect to W, i.e. ∇λ n (W).With Exercise 2.3, we have (ii) If W is a matrix with real entries, then Wu = λu implies Wū = λū, i.e. if λ is a complex eigenvalue, then λ (the complex conjugate of λ) is also an eigenvalue.SinceNow we can write J(W) in terms of the eigenvalues:Assume that the real-valued λ j are non-zero so that(2.12)Solution.This follows from Exercise 1.3 where we have found that .2.97) Indeed:Assume we would like to minimise a matrix valued function J(W) by gradient descent, i.e. the update equation iswhere is the step-length.The gradient ∇J(W) was defined in Exercise 2.3.It was there pointed out that the gradient defines a first order approximation to the perturbed objective function J(W + H).With (2.5),For any (nonzero) matrix M, it holds thatwhich means that tr(∇J(W) ∇J(W)) > 0 if the gradient is nonzero, and hencefor small enough .Consequently, ∇J(W) is a descent direction.Show that A A∇J(W)BB for non-zero matrices A and B is also a descent direction or leaves the leaves the objective invariant.As in the introduction to the question, we appeal to (2.5) to obtainConsider the following directed graph:(a) List all trails in the graph (of maximal length)We have (a, q, e) (a, q, z, h) (h, z, q, e)and the corresponding ones with swapped start and end nodes.(b) List all directed paths in the graph (of maximal length) Solution.nondesc(q) = {a, z, h, e} \ {e} = {a, z, h} (e) Which of the following orderings are topological to the graph?• (a,z,h,q,e)• (a,z,e,h,q)• (z,a,q,h,e)• (z,q,e,a,h) Solution.• (a,z,h,q,e): yes• (a,z,e,h,q): no (q is a parent of e and thus has to come before e in the ordering)• (z,a,q,h,e): yes• (z,q,e,a,h): no (a is a parent of q and thus has to come before q in the ordering)We here derive the independencies that hold in the three canonical connections that exist in DAGs, shown in Figure 3.1.x z y This means that if the state or value of z is known (i.e. if the random variable z is "instantiated"), evidence about x will not change our belief about y, and vice versa.We say that the z node is "closed" and that the trail between x and y is "blocked" by the instantiated z.In other words, knowing the value of z blocks the flow of evidence between x and y.(b) For the serial connection, show that the marginal p(x, y) does generally not factorise into p(x)p(y), i.e. that x ⊥ ⊥ y does not hold.There are several ways to show the result.One is to present an example where the independency does not hold.Consider for instance the following modelwhere n z ∼ N (n z ; 0, 1) and n y ∼ N (n y ; 0, 1), both being statistically independent from x.Here N (•; 0, 1) denotes the Gaussian pdf with mean 0 and variance 1, and x ∼ N (x; 0, 1) means that we sample x from the distribution N (x; 0, 1).Hence p(z|x) = N (z; x, 1), p(y|z) = N (y; z, 1) and p(x, y, z) = p(x)p(z|x)p(y|z) = N (x; 0, 1)N (z; x, 1)N (y; z, 1).Whilst we could manipulate the pdfs to show the result, it's here easier to work with the generative model in Equations (S. 3.3) to (S. 3.5).Eliminating z from the equations, by plugging the definition of z into (S.3.5)we have y = x + n z + n y , (S. 3.6) which describes the marginal distribution of (x, y).We see that E[xy] is .3.9) where we have use the linearity of expectation, that x is independent from n z and n y , and that x has zero mean.If x and y were independent (or only uncorrelated), we hadx and y are not independent.In plain English, this means that if the state of z is unknown, then evidence or information about x will influence our belief about y, and the other way around.Evidence can flow through z between x and y.We say that the z node is "open" and the trail between x and y is "active".(c) For the diverging connection, use the ordered Markov property to show that x ⊥ ⊥ y | z.A topological ordering is z, x, y.The predecessors of y are pre y = {x, z} and its parents pa y = {z}.The ordered Markov property y ⊥ ⊥ (pre y \ pa y ) | pa y (S. 3.10) thus becomes again y ⊥ ⊥ x | z, (S. 3.11) which is, since the independence relationship is symmetric, the same as x ⊥ ⊥ z | z.As in the serial connection, if the state or value z is known, evidence about x will not change our belief about y, and vice versa.Knowing z closes the z node, which blocks the trail between x and y.(d) For the diverging connection, show that the marginal p(x, y) does generally not factorise into p(x)p(y), i.e. that x ⊥ ⊥ y does not hold.As for the serial connection, it suffices to give an example where x ⊥ ⊥ y does not hold.We consider the following generative model z ∼ N (z; 0, 1) (S.3.12)x = z + n x (S. 3.13) .3.14) where n x ∼ N (n x ; 0, 1) and n y ∼ N (n y ; 0, 1), and they are independent of each other and the other variables.We haveOn the other hand .3.17) = 1 + 0 + 0 (S.3.18) Hence, E[xy] = E[x]E[y] and we do not have that x ⊥ ⊥ y holds.In a diverging connection, as in the serial connection, if the state of z is unknown, then evidence or information about x will influence our belief about y, and the other way around.Evidence can flow through z between x and y.We say that the z node is open and the trail between x and y is active.(e) For the converging connection, show that x ⊥ ⊥ y.We can here again use the ordered Markov property with the ordering y, x, z.Since pre x = {y} and pa x = ∅, we have Hence p(x, y) factorises into its marginals, which means that x ⊥ ⊥ y.Hence, when we do not have evidence about z, evidence about x will not change our belief about y, and vice versa.For the converging connection, if no evidence about z is available, the z node is closed, which blocks the trail between x and y.(f) For the converging connection, show that x ⊥ ⊥ y | z does generally not hold.Solution.We give a simple example where x ⊥ ⊥ y | z does not hold.x ∼ N (x; 0, 1) (S. 3.28) y ∼ N (y; 0, 1) (S. 3.29) .3.30) where n z ∼ N (n z ; 0, 1), independent from the other variables.From the last equation, we have xy = z − n z (S. 3.31) We thus have .3.33)On the other hand,The intuition here is that if you know the value of the product xy, even if subject to noise, knowing the value of x allows you to guess the value of y and vice versa.More generally, for converging connections, if evidence or information about z is available, evidence about x will influence the belief about y, and vice versa.We say that information about z opens the z-node, and evidence can flow between x and y.Note: information about z means that z or one of its descendents is observed, see exercise 3.9.We continue with the investigation of the graph from Exercise 3.1 shown below for reference.a z q e h (a) The ordering (z, h, a, q, e) is topological to the graph.What are the independencies that follow from the ordered Markov property?A distribution that factorises over the graph satisfies the independenciesfor all orderings of the variables that are topological to the graph.The ordering comes into play via the predecessors pre i = {x 1 , . . ., x i−1 } of the variables x i ; the graph via the parent sets pa i .For the graph and the specified topological ordering, the predecessor sets are pre z = ∅, pre h = {z}, pre a = {z, h}, pre q = {z, h, a}, pre e = {z, h, a, q}The parent sets only depend on the graph and not the topological ordering.They are:The ordered Markov property reads x i ⊥ ⊥ (pre i \ pa i ) | pa i where the x i refer to the ordered variables, e.g.With pre h \ pa h = ∅ pre a \ pa a = {z, h} pre q \ pa q = {h} pre e \ pa e = {z, h, a} we thus obtainThe relation h ⊥ ⊥ ∅ | z should be understood as "there is no variable from which h is independent given z" and should thus be dropped from the list.Note that we can possibly obtain more independence relations for variables that occur later in the topological ordering.This is because the set pre \ pa can only increase when the predecessor set pre becomes larger.(b) What are the independencies that follow from the local Markov property?Solution.The non-descendants are nondesc(a) = {z, h} nondesc(z) = {a} nondesc(h) = {a, z, q, e} nondesc(q) = {a, z, h} nondesc(e) = {a, q, z, h}With the parent sets as before, the independencies that follow from the local Markov property areThe independency relations obtained via the ordered and local Markov property include q ⊥ ⊥ h | {a, z}.Verify the independency using d-separation.The only trail from q to h goes through z which is in a tail-tail configuration.Since z is part of the conditioning set, the trail is blocked and the result follows.(d) Use d-separation to check whether a ⊥ ⊥ h | e holds.The trail from a to h is shown below in red together with the default states of the nodes along the trail.Conditioning on e opens the q node since q in a collider configuration on the path.The trail from a to h is thus active, which means that the relationship does not hold because a ⊥ ⊥ h | e for some distributions that factorise over the graph.(e) Assume all variables in the graph are binary.How many numbers do you need to specify, or learn from data, in order to fully specify the probability distribution?Solution.The graph defines a set of probability mass functions (pmf) that factorise as p(a, z, q, h, e) = p(a)p(z)p(q|a, z)p(h|z)p(e|q)To specify a member of the set, we need to specify the (conditional) pmfs on the right-hand side.The (conditional) pmfs can be seen as tables, and the number of elements that we need to specified in the tables are: -1 for p(a) -1 for p(z) -4 for p(q|a, z) -2 for p(h|z) -2 for p(e|q) In total, there are 10 numbers to specify.This is in contrast to 2 5 − 1 = 31 for a distribution without independencies.Note that the number of parameters to specify could be further reduced by making parametric assumptions.We continue with the investigation of the graph below a z q e h (a) Why can the ordered or local Markov property not be used to check whether a ⊥ ⊥ h | e may hold?Solution.The independencies that follow from the ordered or local Markov property require conditioning on parent sets.However, e is not a parent of any node so that the above independence assertion cannot be checked via the ordered or local Markov property.(b) The independency relations obtained via the ordered and local Markov property include a ⊥ ⊥ {z, h}.Verify the independency using d-separation.All paths from a to z or h pass through the node q that forms a headhead connection along that trail.Since neither q nor its descendant e is part of the conditioning set, the trail is blocked and the independence relation follows.(c) Determine the Markov blanket of z.Solution.The Markov blanket is given by the parents, children, and co-parents.Hence: MB(z) = {a, q, h}.(d) Verify that q ⊥ ⊥ h | {a, z} holds by manipulating the probability distribution induced by the graph.A basic definition of conditional statistical independence) equals the product of the (conditional) marginals p(x 1 | x 3 ) and p(x 2 | x 3 ).In other words, for discrete random variables, .3.34)We thus answer the question by showing that (use integrals in case of continuous random variables) p(q, h|a, z) = h p(q, h|a, z) q p(q, h|a, z) (S. 3.35) First, note that the graph defines a set of probability density or mass functions that factorise as p(a, z, q, h, e) = p(a)p(z)p(q|a, z)p(h|z)p(e|q)We then use the sum-rule to compute the joint distribution of (a, z, q, h), i.e. the distribution of all the variables that occur in p(q, h|a, z) p(a, z, q, h) = e p(a, z, q, h, e) (S.We further see that p(q|a, z) and p(h|z) are the marginals of p(q, h|a, z), i.e.p(q|a, z) = h p(q, h|a, z) (S. 3.47) p(h|z) = q p(q, h|a, z).(S. 3.48) This means that p(q, h|a, z) = h p(q, h|a, z) q p(q, h|a, z) , (S. 3.49) which shows that q ⊥ ⊥ h|a, z.We see that using the graph to determine the independency is easier than manipulating the pmf/pdf.Barber, 2012, Exercise 3.3)The directed graphical model in Figure 3.2 is about the diagnosis of lung disease (t=tuberculosis or l=lung cancer).In this model, a visit to some place "a" is thought to increase the probability of tuberculosis.(a) Explain which of the following independence relationships hold for all distributions that factorise over the graph.• There are two trails from t to s: (t, e, l, s) and (t, e, d, b, s).• The trail (t, e, l, s) features a collider node e that is opened by the conditioning variable d.The trail is thus active and we do not need to check the second trail because for independence all trails needed to be blocked.• The independence relationship does thus generally not hold.Barber, 2012, Exercise 3.3) Consider the directed graphical model in Figure 3.2.(a) Explain which of the following independence relationships hold for all distributions that factorise over the graph.Solution.• There are two trails from a to s: (a, t, e, l, s) and (a, t, e, d, b, s)• The trail (a, t, e, l, s) features a collider node e that blocks the trail (the trail is also blocked by l).• The trail (a, t, e, d, b, s) is blocked by the collider node d.• All trails are blocked so that the independence relation holds.Solution.• There are two trails from a to s: (a, t, e, l, s) and (a, t, e, d, b, s)• The trail (a, t, e, l, s) features a collider node e that is opened by the conditioning variable d but the l node is closed by the conditioning variable l: the trail is blocked  As all trails are blocked we haveThis exercise is about directed graphical models that are specified by the following DAG:These models are called "hidden" Markov models because we typically assume to only observe the y i and not the x i that follow a Markov model.(a) Show that all probabilistic models specified by the DAG factorise as p(x 1 , y 1 , x 2 , y 2 , .p(y i |pa(y i )).The result is then obtained by noting that the parent of y i is given by x i for all i, and that the parent of x i is x i−1 for i = 2, 3, 4 and that x 1 does not have a parent (pa(x 1 ) = ∅).(b) Derive the independencies implied by the ordered Markov property with the topological ordering (x 1 , y 1 , x 2 , y 2 , x 3 , y 3 , x 4 , y 4 ) Solution.Derive the independencies implied by the ordered Markov property with the topological ordering (x 1 , x 2 , . . ., x 4 , y 1 , . . ., y 4 ).For the x i , we use that for i ≥ 2: pre(x i ) = {x 1 , . . ., x i−1 } and pa(x i ) = x i−1 .For the y i , we use that pre(y 1 ) = {x 1 , . . ., x 4 }, that pre(y i ) = {x 1 , . . ., x 4 , y 1 , . . ., y i−1 } for i > 1, and that pa(y i ) = x i .The ordered Markov property then gives:Solution.The trail y 1 − x 1 − x 2 − x 3 − x 4 − y 4 is active: none of the nodes is in a collider configuration, so that their default state is open and conditioning on y 3 does not block any of the nodes on the trail.While x 1 − x 2 − x 3 − x 4 forms a Markov chain, where e.g.x 4 ⊥ ⊥ x 1 | x 3 holds, this not so for the distribution of the y's.We also haveThis exercise is on further properties and characterisations of statistical independence.We consider the joint distribution p(x, y, w|z).By assumption p(x, y, w|z) = p(x|z)p(y, w|z) (S. 3.68) We have to show that x ⊥ ⊥ y|z and x ⊥ ⊥ w|z.For simplicity, we assume that the variables are discrete valued.If not, replace the sum below with an integral.which means that x ⊥ ⊥ y|z.To show that x ⊥ ⊥ w|z, we similarly marginalise p(x, y, w|z) over y to obtain p(x, w|z) = p(x|z)p(w|z), which means that x ⊥ ⊥ w|z.(b) For the directed graphical model below, show that the following two statements hold without using d-separation:x ⊥ ⊥ y and (3.3)The exercise shows that not only conditioning on a collider node but also on one of its descendents activates the trail between x and y.You can use the result that x ⊥ ⊥ y|w ⇔ p(x, y, w) = a(x, w)b(y, w) for some non-negative functions a(x, w) and b(y, w).Solution.The graphical model corresponds to the factorisation p(x, y, z, w) = p(x)p(y)p(z|x, y)p(w|z).For the marginal p(x, y) we have to sum (integrate) over all (z, w)Consider the following directed acyclic graph.x 1x 2x 3x 4x 5x 6x 7x 8x 9For each of the statements below, determine whether it holds for all probabilistic models that factorise over the graph.Provide a justification for your answer.(a) p(x 7 |x 2 ) = p(x 7 )Solution.Yes, it holds.x 2 is a non-descendant of x 7 , pa(x 7 ) = ∅, and hence, by the local Markov property, x 7 ⊥ ⊥ x 2 , so that p(x 7 |x 2 ) = p(x 7 ).Solution.No, does not hold.x 1 and x 3 are d-connected, which only implies independence for some and not all distributions that factorise over the graph.The graph generally only allows us to read out independencies and not dependencies.) for some non-negative functions φ 1 and φ 2 .Solution.Yes, it holds.The statement is equivalent toThere are three trails from x 2 to x 4 , which are all blocked:1. x 2 − x 1 − x 4 : this trail is blocked because x 1 is in a tail-tail connection and it is observed, which closes the node.x 2 − x 3 − x 6 − x 5 − x 4 : this trail is blocked because x 3 , x 6 , x 5 is in a collider configuration, and x 6 is not observed (and it does not have any descendants).this trail is blocked because x 3 , x 6 , x 8 is in a collider configuration, and x 6 is not observed (and it does not have any descendants).Hence, by the global Markov property (d-separation), the independency holds.No, does not hold.Conditioning on x 6 opens the collider node x 4 on the trail x 2 − x 1 − x 4 − x 7 − x 9 , so that the trail is active.(e) x 8 ⊥ ⊥ {x 2 , x 9 } | {x 3 , x 5 , x 6 , x 7 } Solution.Yes, it holds.{x 3 , x 5 , x 6 , x 7 } is the Markov blanket of x 8 , so that x 8 is independent of remaining nodes given the Markov blanket.Solution.Yes, it holds.{x 2 , x 3 , x 4 , x 5 } are non-descendants of x 8 , and x 7 is the parent of x 8 , so thatConsider the following directed acyclic graph:For each of the statements below, determine whether it holds for all probabilistic models that factorise over the graph.Provide a justification for your answer.(a)for some non-negative functions φ A and φ B Solution.Holds.The statement is equivalent to x 1 ⊥ ⊥ y 1 | {θ 1 , u 1 }.The conditioning set {θ 1 , u 1 } blocks all trails from x 1 to y 1 because they are both only in serial configurations in all trails from x 1 to y 1 , hence the independency holds by the global Markov property.Alternative justification: the conditioning set is the Markov blanket of x 1 , and x 1 and y 1 are not neighbours which implies the independency.Holds.The conditioning set is the Markov blanket of v 2 (the set of parents, children, and co-parents): the set of parents is pa(v 2 ) = {m 2 , s 2 }, y 2 is the only child of v 2 , and θ 2 is the only other parent of y 2 .And v 2 is independent of all other variables given its Markov blanket.Solution.Holds.There are four trails from m 1 to m 2 , namely via x 1 , via y 1 , via x 2 , via y 2 .In all trails the four variables are in a collider configuration, so that each of the trails is blocked.By the global Markov property (d-separation), this means thatAlternative justification 1: m 2 is a non-descendent of m 1 and pa(m 2 ) = ∅.By the directed local Markov property, a variable is independent from its non-descendents given the parents, hence m 2 ⊥ ⊥ m 1 .Alternative justification 2: We can choose a topological ordering where m 1 and m 2 are the first two variables.Moreover, their parent sets are both empty.By the directed ordered Markov, we thus haveChapter 4Undirected Graphical ModelsWe here consider the Gibbs distribution p(x 1 , . . ., x 5 ) ∝ φ 12 (x 1 , x 2 )φ 13 (x 1 , x 3 )φ 14 (x 1 , x 4 )φ 23 (x 2 , x 3 )φ 25 (x 2 , x 5 )φ 45 (x 4 , x 5 ) (a) Visualise it as an undirected graph.Solution.We draw a node for each random variable x i .There is an edge between two nodes if the corresponding variables co-occur in a factor.x 1x 2x 3x 4x 5(b) What are the neighbours of x 3 in the graph?The neighbours are all the nodes for which there is a single connecting edge.Thus: ne(x 3 ) = {x 1 , x 2 }. (Note that sometimes, we may denote ne(x 3 ) by ne 3 .)Solution.Yes.The conditioning set {x 1 , x 2 } equals ne 3 , which is also the Markov blanket of x 3 .This means that x 3 is conditionally independent of all the other variables given {x 1 , x 2 }, i.e.(One can also use graph separation to answer the question.)The Markov blanket of a node in a undirected graphical model equals the set of its neighbours: MB(x 4 ) = ne(x 4 ) = ne 4 = {x 1 , x 5 }.This implies, for example, that(e) On which minimal set of variables A do we need to condition to haveWe first identify all trails from x 1 to x 5 .There are three such trails: (x 1 , x 2 , x 5 ), (x 1 , x 3 , x 2 , x 5 ), and (x 1 , x 4 , x 5 ).Conditioning on x 2 blocks the first two trails, conditioning on x 4 blocks the last.We thus have:Consider the undirected graphical model defined by the graph in Figure 4.1.x 1x 2x 3x 4x 5x 6The Gibbs distributions are thusLet p be a pdf that factorises according to the graph.Does p(x 3 |x 2 , x 4 ) = p(x 3 |x 4 ) hold?We can use the graph to check whether this generally holds for pdfs that factorise according to the graph.There are multiple trails from x 3 to x 2 , including the trail (x 3 , x 1 , x 2 ), which is not blocked by x 4 .From the graph, we thus cannot conclude that x 3 ⊥ ⊥ x 2 | x 4 , and p(x 3 |x 2 , x 4 ) = p(x 3 |x 4 ) will generally not hold (the relation may hold for some carefully defined factors φ i ).(c) Explain why x 2 ⊥ ⊥ x 5 | x 1 , x 3 , x 4 , x 6 holds for all distributions that factorise over the graph.Distributions that factorise over the graph satisfy the pairwise Markov property.Since x 2 and x 5 are not neighbours, and x 1 , x 3 , x 4 , x 6 are the remaining nodes in the graph, the independence relation follows from the pairwise Markov property.(d) Assume you would like to approximate E(x 1 x 2 x 5 | x 3 , x 4 ), i.e. the expected value of the product of x 1 , x 2 , and x 5 given x 3 and x 4 , with a sample average.Do you need to have joint observations for all five variables x 1 , . . ., x 5 ?Solution.In the graph, all trails from {x 1 , x 2 } to x 5 are blocked by {x 3 , x 4 }, so that x 1 , x 2 ⊥ ⊥ x 5 | x 3 , x 4 .We thus haveHence, we only need joint observations of ( x 1 , x 2 , x 3 , x 4 ) and (x 3 , x 4 , x 5 ).Variables (x 1 , x 2 ) and x 5 do not need to be jointly measured.Consider the undirected graphical model defined by the following graph, sometimes called a diamond configuration.We can generate the independencies by conditioning on progressively larger sets.Since there is a trail between any two nodes, there are no unconditional independencies.If we condition on a single variable, there is still a trail that connects the remaining ones.Let us thus consider the case where we condition on two nodes.By graph separation, we haveThese are all the independencies that hold for the model, since conditioning on three nodes does not lead to any independencies in a model with four variables.Assume you know the following Markov blankets for all variables x 1 , . . ., x 4 , y 1 , . . .y 4 of a pdf or pmf p(x 1 , . . ., x 4 , y 1 , . . ., y 4 ).Assuming that p is positive for all possible values of its variables, how does p factorise?In undirected graphical models, the Markov blanket for a variable is the same as the set of its neighbours.Hence, when we are given all Markov blankets we know what local Markov property p must satisfy.For positive distributions we have an equivalence between p satisfying the local Markov property and p factorising over the graph.Hence, to specify the factorisation of p it suffices to construct the undirected graph H based on the Markov blankets and then read out the factorisation.We need to build a graph where the neighbours of each variable equals the indicated Markov blanket.This can be easily done by starting with an empty graph and connecting each variable to the variables in its Markov blanket.We see that each y i is only connected to x i .Including those Markov blankets we get the following graph:Connecting the x i to their neighbours according to the Markov blanket thus gives:The graph has maximal cliques of size two, namely the x i − y i for i = 1, . . ., 4, and theGiven the equivalence between the local Markov property and factorisation for positive distributions, we know that p must factorise asThe graphical model corresponds to an undirected version of a hidden Markov model where the x i are the unobserved (latent, hidden) variables and the y i are the observed ones.Note that the x i form a Markov chain.We consider the same setup as in Exercise 4.4 but we now assume that we do not know all Markov blankets but onlyWithout inserting more independencies than those specified by the Markov blankets, draw the graph over which p factorises and state the factorisation.(Again assume that p is positive for all possible values of its variables).We take the same approach as in Exercise 4. 4. In particular, the Markov blankets of a variable are its neighbours in the graph.But since we are not given all Markov blankets and are not allowed to insert additional independencies, we must assume that each y i is connected to all the other y's.For example, if we didn't connect y 1 and y 4 we would assert the additional independencyWe thus have a graph as follows:The factorisation thus is .4.4) where the m i (x i , x i+1 ), g i (x i , y i ) and g(y 1 , . . ., y 4 ) are positive factors.Compared to the factorisation in Exercise 4.4, we still have the Markov structure for the x i , but only a single factor for (y 1 , y 2 , y 3 , y 4 ) to avoid inserting independencies beyond those specified by the given Markov blankets.We here consider Gibbs distributions where the factors only depend on two variables at a time.The probability density or mass functions over d random variables x 1 , . . ., x d then take the formSuch models are sometimes called pairwise Markov networks.(a) Let p(x 1 , . . ., x d ) ∝ exp − 1 2 x Ax − b x where A is symmetric and x = (x 1 , . . ., x d ) .What are the corresponding factors φ ij for i ≤ j? Solution.Denote the (i, j)-th element of A by a ij .We have .4.6) where the second line follows from A = A. Hence,For x ∈ R d , the distribution is a Gaussian with A equal to the inverse covariance matrix.For binary x, the model is known as Ising model or Boltzmann machine.Forfor all i, so that the a ii are constants that can be absorbed into the normalisation constant.This means that for x i ∈ {−1, 1}, we can work with matrices A that have zeros on the diagonal.Solution.The previous question showed that we can write p(x 1 , . . ., x d ) ∝ i≤j φ ij (x i , x j ) with potentials as in Equation (S.4.8).Consider two variables x i and x j for fixed (i, j).They only appear in the factorisation via the potential φ ij .If a ij = 0, the factor φ ij becomes a constant, and no other factor contains x i and x j , which means that there is no edge between x i and x j if a ij = 0.By the pairwise Markov property it then follows that Barber, 2012, Exercise 4.4) The restricted Boltzmann machine is an undirected graphical model for binary variables v = (v 1 , . . ., v n ) and h = (h 1 , . . ., h m ) with a probability mass function equal towhere W is a n × m matrix.Both the v i and h i take values in {0, 1}.The v i are called the "visibles" variables since they are assumed to be observed while the h i are the hidden variables since it is assumed that we cannot measure them.(a) Use graph separation to show that the joint conditional p(h|v) factorises asSolution.Figure 4.2 on the left shows the undirected graph for p(v, h) with n = 3, m = 2.We note that the graph is bi-partite: there are only direct connections between the h i and the v i .Conditioning on v thus blocks all trails between the h i (graph on the right).This means that the h i are independent from each other given v so thatwhere W ji is the (ji)-th element of W, so that j W ji v j is the inner product (scalar product) between the i-th column of W and v.For the conditional pmf p(h i |v) any quantity that does not depend on h i can be considered to be part of the normalisation constant.A general strategy is to first work out p(h i |v) up to the normalisation constant and then to normalise it afterwards.We begin with p(h|v): .4.11)As we are interested in p(h i |v) for a fixed i, we can drop all the terms not depending on that h i , so that .4.14) Since h i only takes two values, 0 and 1, normalisation is here straightforward.Call the unnormalised pmf p(h i |v), .4.15)We then have With that notation, we have(c) Use a symmetry argument to show that .4.24)To derive the result, we note that v and a now take the place of h and b from before, and that we now have W rather than W. In Equation (4.5), we thus replace h i with v i , b i with a i , and W ji with W ij to obtain p(v i = 1|h).In terms of the sigmoid function, we haveNote that while p(v|h) factorises, the marginal p(v) does generally not.The marginal p(v) can here be obtained in closed form up to its normalisation constant.Importantly, each term in the product only depends on a single h j , so that by sequentially applying the distributive law, we have .4.34) and thus .4.35)Note that in the derivation of p(v) we have not used the assumption that the visibles v i are binary.The same expression would thus obtained if the visibles were defined in another space, e.g. the real numbers.While p(v) is written as a product, p(v) does not factorise into terms that depend on subsets of the v i .On the contrary, all v i are present in all factors.Since p(v) does not factorise, computing the normalising Z is expensive.For binary visibles .4.36) where we have to sum over all 2 n configurations of the visibles v.This is computationally expensive, or even prohibitive if(This is a reason why Z is called the partition function when the a i , b i , W ij are free parameters.)It is instructive to write p(v) in the log-domain, .4.37) and to introduce the nonlinearity f (u), 4.38) which is called the softplus function and plotted below.The softplus function is a smooth approximation of max(0, u), see e.g.https://en.wikipedia.org/wiki/Rectifier_(neural_networks)With the softplus function f (u), we can write log p(v) asThe parameter b j plays the role of a threshold as shown in the figure below.The terms f ( i v i W ij + b j ) can be interpreted in terms of feature detection.The sum i v i W ij is the inner product between v and the j-th column of W, and the inner product is largest if v equals the j-th column.We can thus consider the columns of W to be feature-templates, and the f ( i v i W ij + b j ) a way to measure how much of each feature is present in v.i v i W ij + b j is also the input to the sigmoid function when computing p(h j = 1|v).Thus, the conditional probability for h j to be one, i.e. "active", can be considered to be an indicator of the presence of the j-th feature (j-th column of W) in the input v.If v is such that i v i W ij + b j is large for many j, i.e. if many features are detected, then f ( i v i W ij + b j ) will be non-zero for many j, and log p(v) will be large.Consider the following undirected graph for a hidden Markov model where the y i correspond to observed (visible) variables and the x i to unobserved (hidden/latent) variables.The graph implies the following factorisationwhere the φ x i and φ y i are non-negative factors.Let us consider the situation where t i=2 φwith x = (x 1 , .(S. 4.40) where g i (x i ) is φ y i (x i , y i ) for a fixed value of y i .(b) Draw the undirected graph for p(x 1 , .In this setting all factors in (4.8) are conditional pdfs and we are dealing with a directed graphical model that factorises as(S. 4.41) By integrating over the y i , we have p(x 1 , .The normalising constant is given bySince we can use ancestral sampling to sample from f , the above expectation can be easily computed via sampling.(e) Express the expectation of a test function h(x) with respect to p(x 1 , . . ., x t |y 1 , . . ., y t ) as a reweighted expectation with respect to f (x).By definition, the expectation over a test function h(x) isBoth the numerator and denominator can be approximated using samples from f .Since the g i (x i ) = φ y i (x i , y i ) involve the observed variables y i , this has a nice interpretation: We can think we have two models for x: f (x) that does not involve the observations and p(x 1 , . . ., x t |y 1 , . . ., y t ) that does.Note, however, that unless φ y i (x i , y i ) is the conditional pdf p(y i |x i ), f (x) is not the marginal p(x 1 , . . ., x t ) that you would obtain by integrating out the y's from the joint model .We can thus generally think it is a base distribution that got "enhanced" by a change of measure in our expression for p(x 1 , . . ., x t |y 1 , . . ., y t ).If φ y i (x i , y i ) is the conditional pdf p(y i |x i ), the change of measure corresponds to going from the prior to the posterior by multiplication with the likelihood (the terms g i ).From the expression for the expectation, we can see that the "enhancing" leads to a corresponding introduction of weights in the expectation that depend via g i on the observations.This can be particularly well seen when we approximate the expectation as a sample average over n samples x (k) ∼ f (x): .4.53)is the i-th dimension of the vector x (k) .Chapter 5Expressive Power of Graphical Models  Solution.The skeleton of graph 3 is different from the skeleton of graphs 1 and 2, so that graph 3 cannot be I-equivalent to graph 1 or 2, and we do not need to further check the immoralities for graph 3. Graph 1 and 2 have the same skeleton, and they also have the same immorality.Hence, graph 1 and 2 are I-equivalent.Note that node w in graph 1 is in a collider configuration along trail v − w − x but it is not an immorality because its parents are connected (covering edge); equivalently for node v in graph 2. x 1 x 2x 3 x 4 x 5x 6x 7Graph 0For each of the three graphs below, explain whether the graph is a perfect map, an I-map, or not an I-map for U.x 1 x 2x 3 x 4 x 5x 6 x 7Graph 1x 1 x 2x 3 x 4 x 5x 6 x 7Graph 2x 1 x 2x 3 x 4 x 5x 6x 7Graph 3Solution.• Graph 1 has an immorality x 2 → x 5 ← x 7 which graph 0 does not have.The graph is thus not I-equivalent to graph 0 and can thus not be a perfect map.Moreover, graph 1 asserts that x 2 ⊥ ⊥ x 7 |x 4 which is not case for graph 0. Since graph 0 is a perfect map for U, graph 1 asserts an independency that does not hold for U and can thus not be an I-map for U.• Graph 2 has an immorality x 1 → x 3 ← x 7 which graph 0 does not have.Graph 2 thus asserts that x 1 ⊥ ⊥ x 7 , which is not the case for graph 0. Hence, for the same reason as for graph 1, graph 2 is not an I-map for U.• Graph 3 has the same skeleton and set of immoralities as graph 0. It is thus I-equivalent to graph 0, and hence also a perfect map.(a) Assume that the graph G in Figure 5.1 is a perfect I-map for p(a, z, q, e, h).Determine the minimal directed I-map using the ordering (e, h, q, z, a).Is the obtained graph I-equivalent to G?Since the graph G is a perfect I-map for p, we can use G to check whether p satisfies a certain independency.This gives the following recipe to construct the minimal directed I-map:1. Assume an ordering of the variables.Denote the ordered random variables by x 1 , . . ., x d .2. For each i, find a minimal subset of variables π i ⊆ pre i such that3. Construct a graph with parents pa i = π i .Note: For I-maps G that are not perfect, if the graph does not indicate that a certain independency holds, we have to check that the independency indeed does not hold for p.If we don't, we won't obtain a minimal I-map but just an I-map for I(p).This is because p may have independencies that are not encoded in the graph G.Given the ordering (e, h, q, z, a), we build a graph where e is the root.From Figure 5.1 (and the perfect map assumption), we see that h ⊥ ⊥ e does not hold.We thus set e as parent of h, see first graph in Figure 5.2.Then:• We consider q: pre q = {e, h}.There is no subset π q of pre q on which we could condition to make q independent of pre q \ π q , so that we set the parents of q in the graph to pa q = {e, h}.(Second graph in Figure 5.2.)• We consider z: pre z = {e, h, q}.From the graph in Figure 5.1, we see that for π z = {q, h} we have z ⊥ ⊥ pre z \ π z |π z .Note that π z = {q} does not work because z ⊥ ⊥ e, h|q does not hold.We thus set pa z = {q, h}.(Third graph in Figure 5.2.)• We consider a: pre a = {e, h, q, z}.This is the last node in the ordering.To find the minimal set π a for which a ⊥ ⊥ pre a \ π a |π a , we can determine its Markov blanket MB(a).The Markov blanket is the set of parents (none), children (q), and co-parents of a (z) in Since the skeleton in the obtained minimal I-map is different from the skeleton of G, we do not have I-equivalence.Note that the ordering (e, h, q, z, a) yields a denser graph (Figure 5.2) than the graph in Figure 5.1.Whilst a minimal I-map, the graph does e.g.not show that a ⊥ ⊥ z.Furthermore, the causal interpretation of the two graphs is different.(b) For the collection of random variables (a, z, h, q, e) you are given the following Markov blankets for each variable:(i) Draw the undirected minimal I-map representing the independencies.(ii) Indicate a Gibbs distribution that satisfies the independence relations specified by the Markov blankets.Connecting each variable to all variables in its Markov blanket yields the desired undirected minimal I-map.Note that the Markov blankets are not mutually disjoint.For positive distributions, the set of distributions that satisfy the local Markov property relative to a graph (as given by the Markov blankets) is the same as the set of Gibbs distributions that factorise according to the graph.Given the I-map, we can now easily find the Gibbs distribution p(a, z, h, q, e) = 1 Z φ 1 (a, z, q)φ 2 (q, e)φ 3 (z, h),where the φ i must take positive values on their domain.Note that we used the maximal clique (a, z, q).First, note that both graphs share the same skeleton and the only reason that they are not fully connected is the missing edge between x and z.For the DAG, there is also only one ordering that is topological to the graph: x, u, y, z.The missing edge between x and y corresponds to the only independency encoded by the graph:This is the same independency that we get from the directed local Markov property.For the undirected graph, z ⊥ ⊥ x|u, y holds because u, y block all paths between z and x.All variables but z and x are connected to each other, so that no further independency can hold.Hence both graphs only encode z ⊥ ⊥ x|u, y and they are thus I-equivalent.(b) Are the following two graphs, which are directed and undirected hidden Markov models, I-equivalent?Solution.The skeleton of the two graphs is the same and there are no immoralities.Hence, the two graphs are I-equivalent.(c) Are the following two graphs I-equivalent?Solution.The two graphs are not I-equivalent because x 1 − x 2 − x 3 forms an immorality.Hence, the undirected graph encodes x 1 ⊥ ⊥ x 3 |x 2 which is not represented in the directed graph.On the other hand, the directed graph asserts x 1 ⊥ ⊥ x 3 which is not represented in the undirected graph.The following recipe constructs undirected minimal I-maps for I(p):• Determine the Markov blanket for each variable x i• Construct a graph where the neighbours of x i are given by its Markov blanket.We can adapt the recipe to construct an undirected minimal I-map for the independencies I(G) encoded by a DAG G. What we need to do is to use G to read out the Markov blankets for the variables x i rather than determining the Markov blankets from the distribution p.Show that this procedure leads to the following recipe to convert DAGs to undirected minimal I-maps:1.For all immoralities in the graph: add edges between all parents of the collider node.2. Make all edges in the graph undirected.The first step is sometimes called "moralisation" because we "marry" all the parents in the graph that are not already directly connected by an edge.The resulting undirected graph is called the moral graph of G, sometimes denoted by M(G).The Markov blanket of a variable x is the set of its parents, children, and co-parents, as shown in the graph below in sub-figure (a).The parents and children are connected to x in the directed graph, but the co-parents are not directly connected to x.Hence, according to "Construct a graph where the neighbours of x i are its Markov blanket.",we need to introduce edges between x and all its co-parents.This gives the intermediate graph in sub-figure (b).Now, considering the top-left parent of x, we see that for that node, the Markov blanket includes the other parents of x.This means that we need to connect all parents of x, which gives the graph in sub-figure (c).This is sometimes called "marrying" the parents of x.Continuing in this way, we see that we need to "marry" all parents in the graph that are not already married.Finally, we need to make all edges in the graph undirected, which gives sub-figure (d).A simpler approach is to note that the DAG specifies the factorisation p(x) = i p(x i |pa i ).We can consider each conditional p(x i |pa i ) to be a factor φ i (x i , pa i ) so that we obtain the Gibbs distribution p(x)Visualising the distribution by connecting all variables in the same factor φ i (x i |pa i ) leads to the "marriage" of all parents of x i .This corresponds to the first step in the recipe because x i is in a collider configuration with respect to the parent nodes.Not all parents form an immorality but this does here not matter because those that do not form an immorality are already connected by a covering edge in the first place.For the DAG G below find the minimal undirected I-map for I(G).x 3x 4 x 5x 6 x 7Solution.To derive an undirected minimal I-map from a directed one, we have to construct the moralised graph where the "unmarried" parents are connected by a covering edge.This is because each conditional p(x i |pa i ) corresponds to a factor φ i (x i , pa i ) and we need to connect all variables that are arguments of the same factor with edges.Statistically, the reason for marrying the parents is as follows: An independency x ⊥ ⊥ y|{child, other nodes} does not hold in the directed graph in case of collider connections but would hold in the undirected graph if we didn't marry the parents.Hence links between the parents must be added.It is important to add edges between all parents of a node.Here, p(x 4 |x 1 , x 2 , x 3 ) corresponds to a factor φ(x 4 , x 1 , x 2 , x 3 ) so that all four variables need to be connected.Just adding edges x 1 − x 2 and x 2 − x 3 would not be enough.The moral graph, which is the requested minimal undirected I-map, is shown below.x 2 x 1x 3x 4 x 5x 6 x 7Consider the DAG G:A friend claims that the undirected graph below is the moral graph M(G) of G.Is your friend correct?If not, state which edges needed to be removed or added, and explain, in terms of represented independencies, why the changes are necessary for the graph to become the moral graph of G.Solution.The moral graph M(G) is an undirected minimal I-map of the independencies represented by G. Following the procedure of connecting "unmarried" parents of colliders, we obtain the following moral graph of G:We can thus see that the friend's undirected graph is not the moral graph of G.The edge between x 1 and x 6 can be removed.This is because for G, we have e.g. the independencies, z 2 which is not represented by the drawn undirected graph.We need to add edges between x 1 and x 3 , and between x 4 and x 6 .Otherwise, the undirected graph makes the wrong independency assertion that x 1 ⊥ ⊥ x 3 |x 2 , z 1 (and equivalent for x 4 and x 6 ).In Exercise 5.4 we adapted a recipe for constructing undirected minimal I-maps for I(p) to the case of I(G), where G is a DAG.The key difference was that we used the graph G to determine independencies rather than the distribution p.We can similarly adapt the recipe for constructing a directed minimal I-map for I(p) to build a directed minimal I-map for I(H), where H is an undirected graph:1. Choose an ordering of the random variables.2. For all variables x i , use H to determine a minimal subset π i of the predecessors pre i such that3. Construct a DAG with the π i as parents pa i of x i .Remarks: (1) Directed minimal I-maps obtained with different orderings are generally not I-equivalent.(2) The directed minimal I-maps obtained with the above method are always chordal graphs.Chordal graphs are graphs where the longest trail without shortcuts is a triangle (https://en.wikipedia.org/wiki/Chordal_graph).They are thus also called triangulated graphs.We obtain chordal graphs because if we had trails without shortcuts that involved more than 3 nodes, we would necessarily have an immorality in the graph.But immoralities encode independencies that an undirected graph cannot represent, which would make the DAG not an I-map for I(H) any more.(a) Let H be the undirected graph below.Determine the directed minimal I-map for I(H) with the variable ordering x 1 , x 2 , x 3 , x 4 , x 5 .x 1 x 2x 3 x 4x 5Solution.We use the ordering x 1 , x 2 , x 3 , x 4 , x 5 and follow the conversion procedure:• x 2 is not independent from x 1 so that we set pa 2 = {x 1 }.See first graph in Figure 5.4.• Since x 3 is connected to both x 1 and x 2 , we don't have x 3 ⊥ ⊥ x 2 , x 1 .We cannot make x 3 independent from x 2 by conditioning on x 1 because there are two paths from x 3 to x 2 and x 1 only blocks the upper one.Moreover, x 1 is a neighbour of x 3 so that conditioning on x 2 does make them independent.Hence we must set pa 3 = {x 1 , x 2 }.See second graph in Figure 5.4.• For x 4 , we see from the undirected graph, thatThe graph further shows that removing either x 3 or x 2 from the conditioning set is not possible and conditioning on x 1 won't make x 4 independent from x 2 or x 3 .We thus have pa 4 = {x 2 , x 3 }.See fourth graph in Figure 5.4.• The same reasoning shows that pa 5 = {x 3 , x 4 }.See last graph in Figure 5.4.This results in the triangulated directed graph in Figure 5.4 on the right.x 1 x 2x 3 x 4x 5x 1 x 2x 3 x 4x 5x 1 x 2x 3 x 4x 5x 1 x 2x 3 x 4x 5 To see why triangulation is necessary consider the case where we didn't have the edge between x 2 and x 3 as in Figure 5.5.The directed graph would then imply that x 3 ⊥ ⊥ x 2 | x 1 (check!).But this independency assertion does not hold in the undirected graph so that the graph in Figure 5.5 is not an I-map.(b) For the undirected graph from question (a) above, which variable ordering yields the directed minimal I-map below?x 1 x 2x 3 x 4x 5Figure 5.5:Not a directed I-map for the undirected graphical model defined by the graph in Exercise 5.7, Question (a).x 1 x 2x 3 x 4x 5Solution.x 1 is the root of the DAG, so it comes first.Next in the ordering are the children of x 1 : x 2 , x 3 , x 4 .Since x 3 is a child of x 4 , and x 4 a child of x 2 , we must have x 1 , x 2 , x 4 , x 3 .Furthermore, x 3 must come before x 5 in the ordering since x 5 is a child of x 3 , hence the ordering used must have been:x 1 , x 2 , x 4 , x 3 , x 5 .Consider the following probability density function for random variables x 1 , . . ., x 6 .For each of the two graphs below, explain whether it is a minimal I-map, not a minimal I-map but still an I-map, or not an I-map for the independencies that hold for p a .x 1 x 2Solution.The pdf can be visualised as the following directed graph, which is a minimal I-map for it.x 1x 2x 3 x 4 x 5x 6Graph 1 defines distributions that factorise as(S.5.1)Comparing with p a (x 1 , . . ., x 6 ), we see that only the conditionals p(x 4 |x 2 , x 3 ) and p(x 5 |x 1 , x 3 ) are different.Specifically, their conditioning set includes x 3 , which means that Graph 1 encodes fewer independencies than what p a (x 1 , . . ., x 6 ) satisfies.In particular x 4 ⊥ ⊥ x 3 |x 2 and x 5 ⊥ ⊥ x 3 |x 1 are not represented in the graph.This means that we could remove x 3 from the conditioning sets, or equivalently remove the edges x 3 → x 4 and x 3 → x 5 from the graph without introducing independence assertions that do not hold for p a .This means graph 1 is an I-map but not a minimal I-map.Graph 2 is not an I-map.To be an undirected minimal I-map, we had to connect variables x 5 and x 4 that are parents of x 6 .Graph 2 wrongly claims that x 5 ⊥ ⊥ x 4 | x 1 , x 3 , x 6 .We here consider the probabilistic model p(y 1 , y 2 , x 1 , x 2 ) = p(y 1 , y 2 |x 1 , x 2 )p(x 1 )p(x 2 ) where p(y 1 , y 2 |x 1 , x 2 ) factorises as(5.2)In the model, x 1 and x 2 are two independent inputs that each control the interacting variables y 1 and y 2 (see graph below).However, the nature of the interaction between y 1 and y 2 is not modelled.In particular, we do not assume a directionality, i.e. y 1 → y 2 , or y 2 → y 1 .some interactionx 1 x 2 y 1 y 2 (a) Use the basic characterisations of statistical independenceto show that p(y 1 , y 2 , x 1 , x 2 ) satisfies the following independencies(S.5.6)Since p(x 1 ) and p(x 2 ) are the univariate marginals of x 1 and x 2 , respectively, it follows from (5.3) that x 1 ⊥ ⊥ x 2 .ForWe rewrite p(y 1 , y 2 , x 1 , x 2 ) asWith (5.4), we have that x 1 ⊥ ⊥ y 2 | y 1 , x 2 .Note that p(x 2 ) can be associated either with φ A or with φ B .ForWe use here the same approach as for x 1 ⊥ ⊥ y 2 | y 1 , x 2 .(By symmetry considerations, we could immediately see that the relation holds but let us write it out for clarity).We rewrite p(y 1 , y 2 , x 1 , x 2 ) as .5.11) = φA (x 2 , x 1 , y 2 ) φB (y 1 , y 2 , x 1 ) (S.5.12)With (5.4), we have that(b) Is there an undirected perfect map for the independencies satisfied by p(y 1 , y 2 , x 1 , x 2 )?Solution.We writeas a Gibbs distribution .5.13) .5.15) .5.16)(S. 5.17)Visualising it as an undirected graph gives an I-map:Hence the graph is not a perfect map.Note further that removing any edge would result in a graph that is not an I-map for I(p) anymore.Hence the graph is a minimal I-map for I(p) but that we cannot obtain a perfect I-map.(c) Is there a directed perfect map for the independencies satisfied by p(y 1 , y 2 , x 1 , x 2 )?Solution.We construct directed minimal I-maps for p(y 1 , y 2 , x 1 , x 2 ) = p(y 1 , y 2 |x 1 , x 2 )p(x 1 )p(x 2 ) for different orderings.We will see that they do not represent all independencies in I(p) and hence that they are not perfect I-maps.To guarantee unconditional independence of x 1 and x 2 , the two variables must come first in the orderings (either x 1 and then x 2 or the other way around).If we use the ordering x 1 , x 2 , y 1 , y 2 , and thatare in I(p), we obtain the following directed minimal I-map:The graphs missesIf we use the ordering x 1 , x 2 , y 2 , y 1 , and thatare in I(p), we obtain the following directed minimal I-map:The graph missesMoreover, the graphs imply a directionality between y 1 and y 2 , or a direct influence of x 1 on y 2 , or of x 2 on y 1 , in contrast to the original modelling goals.(d) (advanced) The following factor graph represents p(y 1 , y 2 , x 1 , x 2 ):Use the separation rules for factor graphs to verify that we can find all independence relations.The separation rules are (see Barber, 2012, Section 4.4.1), or the original paper by Frey (2003):"If all paths are blocked, the variables are conditionally independent.A path is blocked if one or more of the following conditions is satisfied:1.One of the variables in the path is in the conditioning set.2. One of the variables or factors in the path has two incoming edges that are part of the path (variable or factor collider), and neither the variable or factor nor any of its descendants are in the conditioning set."Remarks:• "one or more of the following" should best be read as "one of the following".• "incoming edges" means directed incoming edges• the descendants of a variable or factor node are all the variables that you can reach by following a path (containing directed or directed edges, but for directed edges, all directions have to be consistent)• In the graph we have dashed directed edges: they do count when you determine the descendants but they do not contribute to paths.For example, y 1 is a descendant of the n(x 1 , x 2 ) factor node but x 1 − n − y 2 is not a path.There are two paths from x 1 to x 2 marked with red and blue below:Both the blue and red path are blocked by condition 2.There are two paths from x 1 to y 2 marked with red and blue below:The observed variables are marked in blue.For the red path, the observed x 2 blocks the path (condition 1).Note that the n(x 1 , x 2 ) node would be open by condition 2. The blue path is blocked by condition 1 too.In directed graphical models, the y 1 node would be open, but here while condition 2 does not apply, condition 1 still applies (note the one or more of ... in the separation rules), so that the path is blocked.x 2 ⊥ ⊥ y 1 | y 2 , x 1 There are two paths from x 2 to y 1 marked with red and blue below:The same reasoning as before yields the result.Finally note that x 1 and x 2 are not independent given y 1 or y 2 because the upper path through n(x 1 , x 2 ) is not blocked whenever y 1 or y 2 are observed (condition 2).Credit: this example is discussed in the original paper by B. Frey (Figure 6).Factor Graphs and Message Passing(a) Draw an undirected graph and an undirected factor graph for p(x 1 , x 2 , x 3 ) = p(x 1 )p(x 2 )p(x 3 |x 1 , x 2 ) Solution.x(b) Draw an undirected factor graph for the directed graphical model defined by the graph below.Solution.The graph specifies probabilistic models that factorise as p(x 1 , . . ., x 4 , y 1 , . . ., y 4 ) = p(x 1 )p(y 1 |x 1 )It is the graph for a hidden Markov model.The corresponding factor graph is shown below.(c) Draw the moralised graph and an undirected factor graph for directed graphical models defined by the graph below (this kind of graph is called a polytree: there are no loops but a node may have more than one parent).x 1 x 2x 3 x 4x 5 x 6Solution.The moral graph is obtained by connecting the parents of the collider node x 4 .See the graph on the left in the figure below.For the factor graph, we note that the directed graph defines the following class of probabilistic modelsThis gives the factor graph on right in the figure below.x 1 x 2x 3 x 4x 5 x 6 p(x 1 )x 6Note:• The moral graph contains a loop while the factor graph does not.The factor graph is still a polytree.This can be exploited for inference.• One may choose to group some factors together in order to obtain a factor graph with a particular structure (see factor graph below)x 4 p(x 5 |x 4 )p(x 6 |x 4 )x 5 x 6We here consider the following factor tree:Let all variables be binary, x i ∈ {0, 1}, and the factors be defined as follows:Mark the graph with arrows indicating all messages that need to be computed for the computation of p(x 1 )..Compute the messages that you have identified.Assuming that the computation of the messages is scheduled according to a common clock, group the messages together so that all messages in the same group can be computed in parallel during a clock cycle.Since the variables are binary, each message can be represented as a two-dimensional vector.We use the convention that the first element of the vector corresponds to the message for x i = 0 and the second element to the message for x i = 1.For example,means that the message µ φ A →x 1 (x 1 ) equals 2 for x 1 = 0, i.e. µ φ A →x 1 (0) = 2.The following figure shows a grouping (scheduling) of the computation of the messages.Clock cycle 1:Clock cycle 2:Message µ φ D →x 3 is defined as= 10 (S. 6.8)= 8 (S.6.12) and thus .6.13)The above computations can be written more compactly in matrix notation.Let φ D φ D φ D be the matrix that contains the outputs of φ D (x 3 , x 4 ) .6.14)We can then write µ φ D →x 3 µ φ D →x 3 µ φ D →x 3 in terms of a matrix vector product,(S.6.15)Representing the factor φ E as matrix .6.16) we can write .6.17) as a matrix vector product,  .6.21)Using to denote element-wise multiplication of two vectors, we have  .6.25) Writing out the sum for x 1 = 0 and x 1 = 1 gives   .6.46)After step 5, variable node x 1 has received all incoming messages and the marginal can be computed.In addition to the messages needed for computation of p(x 1 ) one can compute all messages in the graph in five clock cycles, see Figure 6.1.This means that all marginals, as well as the joints of those variables sharing a factor node, are available after five clock cycles.(c) What is p(x 1 = 1)?Solution.We compute the marginal p(x 1 ) as If we also computed the messages toward the leaf factor nodes, we needed six cycles, but they are not necessary for computation of the marginals so they are omitted.which is in vector notation Note the relatively large numbers in the messages that we computed.In other cases, one may obtain very small ones depending on the scale of the factors.This can cause numerical issues that can be addressed by working in the logarithmic domain.(d) Draw the factor graph corresponding to p(x 1 , x 3 , x 4 , x 5 |x 2 = 1) and provide the numerical values for all factors.Solution.The pmf represented by the original factor graph isThe conditional p(x 1 , x 3 , x 4 , x 5 |x 2 = 1) is proportional to p(x 1 , . . ., x 5 ) with x 2 fixed to x 2 = 1, i.e.p(x 1 , x 3 , x 4 , x 5 |x 2 = 1) ∝ p(x 1 , x 2 = 1, x 3 , x 4 , x 5 ) (S.6.53).6.55) whereThe numerical values of φ x 2 C (x 1 , x 3 ) can be read from the table defining φ C (x 1 , x 2 , x 3 ), extracting those rows where x 2 = 1,The factor graph for p(x 1 , x 3 , x 4 , x 5 |x 2 = 1) is shown below.Factor φ B has disappeared since it only depended on x 2 and thus became a constant.Factor φ C is replaced by φ x 2 C defined above.The remaining factors are the same as in the original factor graph.), re-using messages that you have already computed for the evaluation of p(x 1 = 1).The message µ φ A →x 1 is the same as in the original factor graph and µ x 3 →φ x 2 C = µ x 3 →φ C .This is because the outgoing message from x 3 corresponds to the effective factor obtained by summing out all variables in the sub-trees attached to x 3 (without the φ x 2 C branch), and these sub-trees do not depend on x 2 .The message µ φ x 2 C →x 1 needs to be newly computed.We have 6.56) or in vector notation We thus obtain for the marginal posterior of x 1 given x 2 = 1: and thus p(x 1 = 1|x 2 = 1) = 0.7657.The posterior probability is slightly larger than the prior probability, p(x 1 = 1) = 0.7224.The following factor graph represents a Gibbs distribution over four binary variablesThe factors φ a , φ b , φ d are defined as follows:, and is zero otherwise.For all questions below, justify your answer:(a) Compute the values of µ x 2 →φ b (x 2 ) for x 2 = 0 and x 2 = 1.Messages from leaf-variable nodes to factor nodes are equal to one, so that µ x 2 →φ b (x 2 ) = 1 for all x 2 .(b) Assume the message µ x 4 →φc (x 4 ) equalsCompute the values of φ e (x 4 ) for x 4 = 0 and x 4 = 1.Messages from leaf-factors to their variable nodes are equal to the leaffactors, and variable nodes with single incoming messages copy the message.We thus have µ φe→x 4 (x 4 ) = φ e (x 4 ) (S.6.65)µ x 4 →φc (x 4 ) = µ φe→x 4 (x 4 ) (S. 6.66) and henceCompute the values of µ φc→x 1 (x 1 ) for x 1 = 0 and x 1 = 1.We first compute µ x 3 →φc (x 3 ):The desired message µ φc→x 1 (x 1 ) is by definition, where it equals one, the computations simplify:= 1 (S.6.73)What is the probability that x 1 = 1, i.e. p(x 1 = 1)?Solution.The unnormalised marginal p(x 1 ) is given by the product of the three incoming messages .6.78) it follows thatWe here compute most probable states for the factor graph and factors below.Let all variables be binary, x i ∈ {0, 1}, and the factors be defined as follows:(a) Will we need to compute the normalising constant Z to determine argmax x p(x 1 , . . ., x 5 )?Solution.This is not necessary since argmax x p(x 1 , . . ., x 5 ) = argmax x cp(x 1 , . . ., x 5 ) for any constant c.Algorithmically, the backtracking algorithm is also invariant to any scaling of the factors.(b) Compute argmax x 1 ,x 2 ,x 3 p(x 1 , x 2 , x 3 |x 4 = 0, x 5 = 0) via max-sum message passing.We first derive the factor graph and corresponding factors for p(x 1 , x 2 , x 3 |x 4 = 0, x 5 = 0).For fixed values of x 4 , x 5 , the two variables are removed from the graph, and the factors φ D (x 3 , x 4 ) and φ E (x 3 , x 5 ) are reduced to univariate factors φ x 4 D (x 3 ) and φ x 5 D (x 3 ) by retaining those rows in the table where x 4 = 0 and x 5 = 0, respectively:Since both factors only depend on x 3 , they can be combined into a new factor φ(x 3 ) by element-wise multiplication.x 3 φ 0 24 1 12Moreover, since we work with an unnormalised model, we can rescale the factor so that the maximum value is one, so that) is a constant for fixed value of x 5 and can be ignored.The factor graph for p(x 1 , x 2 , x 3 |x 4 = 0, x 5 = 0) thus isLet us fix x 1 as root towards which we compute the messages.The messages that we need to compute are shown in the following graphNext, we compute the leaf (log) messages.We only have factor nodes as leaf nodes so that .6.88) and similarlySince the variable nodes x 2 and x 3 only have one incoming edge each, we obtainThe message λ φ C →x 1 (x 1 ) equals .6.91) where we wrote the messages in non-vector notation to highlight their dependency on the variables x 2 and x 3 .We now have to consider all combinations of x 2 and x 3x 2 x 3 log φ C (x 1 = 0, x 2 , x 3 )Hence for x 1 = 0, we have The maximal value is log 32 and for backtracking, we also need to keep track of the argmax which is here x2 = x3 = 0.For x 1 = 1, we haveThe maximal value is log 48 and the argmax is (x 2 = 1, x3 = 0).So overall, we have .6.92) and the argmax back-tracking function is 6.93)We now have all incoming messages to the assigned root node x 1 .Ignoring the normalising constant, we obtain The value x 1 for which γ * (x 1 ) is largest is thus x1 = 1.Plugging x1 = 1 into the backtracking function λ * φ C →x 1 (x 1 ) gives .6.96)In this low-dimensional example, we can verify the solution by computing the unnormalised pmf for all combinations of x 1 , x 2 , x 3 .This is done in the following table where we start with the table for φ C and then multiply-in the further factors φ A , φ and φ B .For example, for the column φ c φ A , we multiply each value of φ C (x 1 , x 2 , x 3 ) by φ A (x 1 ), so that the rows with x 1 = 0 get multiplied by 2, and the rows with x 1 = 1 by 4.The maximal value in the final column is achieved for x 1 = 1, x 2 = 1, x 3 = 0, in line with the result above (and 48 • 4 = 192).Since φ B (x 2 ) is a constant, being equal to 4 for all values of x 2 , we could have ignored it in the computation.The formal reason for this is that since the model is unnormalised, we are allowed to rescale each factor by an arbitrary (factor-dependent) constant.This operation does not change the model.So we could divide φ B by 4 which would give a value of 1, so that the factor can indeed be ignored.(c) Compute argmax x 1 ,...,x 5 p(x 1 , . . ., x 5 ) via max-sum message passing with x 1 as root.As discussed in the solution to the answer above, we can drop factor φ B (x 2 ) since it takes the same value for all x 2 .Moreover, we can rescale the individual factors by a constant so they are more amenable to calculations by hand.We normalise them such that the largest value is one, which gives the following factors.Note that this is entirely optional.The factor graph without φ B together with the messages that we need to compute is:The leaf (log) messages are (using vector notation where the top element corresponds to x i = 0 and the bottom one to x i = 1):(S.6.97)The variable node x 5 only has one incoming edge so that λ x 5 →φ E = λ φ F →x 5 .The message λ φ E →x 3 (x 3 ) equalslog φ E (x 3 , x 5 ) + λ x 5 →φ E (x 5 ) (S. 6.98) Writing out log φ E (x 3 , x 5 ) + λ x 5 →φ E (x 5 ) for all x 5 as a function of x 3 we haveTaking the maximum over x 5 as a function of x 3 , we obtain λ φ E →x 3 = log 16 log 8 (S.6.99) and the backtracking function that indicates the maximiser x5 = argmax x 5 log φ E (x 3 , x 5 )+ λ x 5 →φ E (x 5 ) as a function of x 3 equalsWe perform the same kind of operation for λ φ D →x 3 (x 3 )Since λ x 4 →φ D (x 4 ) = 0 for all x 4 , the table with all values of log φ D (x 3 , x 4 ) + λ x 4 →φ D (x 4 ) isTaking the maximum over x 4 as a function of x 3 we thus obtain λ φ D →x 3 = log 4 log 3 (S.6.102) and the backtracking function that indicates the maximiser x4 = argmax x 4 log φ D (x 3 , x 4 )+ λ x 4 →φ D (x 4 ) as a function of x 3 equalsFor the message λ x 3 →φ C (x 3 ) we add together the messages λ φ E →x 3 (x 3 ) and λ φ D →x 3 (x 3 ) which gives λ x 3 →φ C = log 16 + log 4 log 8 + log 3 = log 64 log 24 (S.6.104)Next we compute the message λ φ C →x 1 (x 1 ) by maximising over x 2 and x 3 ,We now have all incoming messages to the assigned root node x 1 .Ignoring the normalising constant, we obtain ) via max-sum message passing with x 3 as root.Solution.With x 3 as root, we need the following messages:The following messages are the same as when x 1 was the root: 6.111) Since x 1 has only one incoming message, we further have .6.112)We next compute λ φ C →x 3 (x 3 ),(S. 6.113)We first form a table for log φThe maximal value as a function of x 3 are highlighted in the table, which gives the message λ φ C →x 3 = log 6 log 6 (S.6.114) and the backtracking function .6.115)We have now all incoming messages for x 3 and can compute γ * (x 3 ) up the normalising constant − log Z (which is not needed if we are interested in the argmax only: We can now start the backtracking which gives: x3 = 0, so that λ * φ C →x 3 (0) = (x 1 = 1, x2 = 1).The backtracking functions λ * φ E →x 3 (x 3 ) and λ * φ D →x 3 (x 3 ) are the same for question (c), which gives λ * φ E →x 3 (0) = x5 = 1 and λ * φ D →x 3 (0) = x4 = 0. Hence, overall, we find argmaxx 1 ,...,x 5 p(x 1 , . . ., x 5 ) = (1, 1, 0, 0, 1).(S. 6.118)Note that this matches the result from question (c) where x 1 was the root.This is because the output of the max-sum algorithm is invariant to the choice of the root.Consider the following factor graph, which contains a loop:Let all variables be binary, x i ∈ {0, 1}, and the factors be defined as follows:Draw the factor graph corresponding to p(x 2 , x 3 , x 4 , x 5 | x 1 = 0, x 6 = 1) and give the tables defining the new factors φ x 1 =0 A (x 2 , x 3 ) and φ x 6 =1 D (x 4 ) that you obtain.Solution.First condition on x 1 = 0:Factor node φ A (x 1 , x 2 , x 3 ) depends on x 1 , thus we create a new factor φ x 1 =0 A (x 2 , x 3 ) from the table for φ A using the rows where x 1 = 0.Next condition on x 6 = 1:Factor node φ D (x 4 , x 6 ) depends on x 6 , thus we create a new factor φ x 6 =1 D (x 4 ) from the table for φ D using the rows where x 6 = 1.x 1 = 0, x 6 = 1) using the elimination ordering (x 4 , x 5 , x 3 ):(i) Draw the graph for p(x 2 , x 3 , x 5 | x 1 = 0, x 6 = 1) by marginalising x 4 Compute the table for the new factor φ4 (x 2 , x 3 , x 5 )(ii) Draw the graph for p(x 2 , x 3 | x 1 = 0, x 6 = 1) by marginalising x 5 Compute the table for the new factor φ45 (x 2 , x 3 ) (iii) Draw the graph for p(x 2 | x 1 = 0, x 6 = 1) by marginalising x 3 Compute the table for the new factor φ453 (x 2 )Solution.Starting with the factor graph for p(x 2 , x 3 , x 4 , x 5 | x 1 = 0, x 6 = 1)Marginalising x 5 modifies the factor φ4A and φ45x 2 φ453We now compute the tables for the new factors φ4 , φ45 , φ453 .First find φ4 (x 2 , x 3 , x 5 ) Finally find φ453 (x 2 )The normalising constant is Z = 1728 + 1632.Our conditional marginal is thus:) with the elimination ordering (x 5 , x 4 , x 3 ):(i) Draw the graph for p(x 2 , x 3 , x 4 , | x 1 = 0, x 6 = 1) by marginalising x 5 Compute the table for the new factor φ5 (x 4 )(ii) Draw the graph for p(x 2 , x 3 | x 1 = 0, x 6 = 1) by marginalising x 4 Compute the table for the new factor φ54 (x 2 , x 3 ) (iii) Draw the graph for p(x 2 | x 1 = 0, x 6 = 1) by marginalising x 3 Compute the table for the new factor φ543 (x 2 )Solution.Starting with the factor graph for p(x 2 , x 3 , x 4 , x 5 | x 1 = 0, x 6 = 1)A and φ54x 2 φ543We now compute the tables for the new factors φ5 , φ54 , and φ543 .First find φ5 (x 4 )Next find φ54 (x 2 , x 3 )x 4 φ5 0 10 1 8 Finally find φ543 (x 2 )As with the ordering in the previous part, we should come to the same result for our conditional marginal distribution.The normalising constant is Z = 1728 + 1632, so that the conditional marginal is p(x 2 | x 1 = 0, x 6 = 1) = 1728/Z 1632/Z = 0.514 0.486 (S.6.120)(d) Which variable ordering, (x 4 , x 5 , x 3 ) or (x 5 , x 4 , x 3 ) do you prefer?Solution.The ordering (x 5 , x 4 , x 3 ) is cheaper and should be preferred over the ordering (x 4 , x 5 , x 3 ) .The reason for the difference in the cost is that x 4 has three neighbours in the factor graph for p(x 2 , x 3 , x 4 , x 5 | x 1 = 0, x 6 = 1).However, after elimination of x 5 , which has only one neighbour, x 4 has only two neighbours left.Eliminating variables with more neighbours leads to larger (temporary) factors and hence a larger cost.We can see this from the tables that were generated during the computation (or numbers that we needed to add together): for the ordering (x 4 , x 5 , x 3 ), the largest table had 2 4 entries while for (x 5 , x 4 , x 3 ), it had 2 3 entries.Choosing a reasonable variable ordering has a direct effect on the computational complexity of variable elimination.This effect becomes even more pronounced when the domain of our discrete variables has a size greater than 2 (binary variables), or if the variables are continuous.We would like to compute the marginal p(x 1 ) by variable elimination for a joint pmf represented by the following factor graph.All variables x i can take K different values.φ f (a) A friend proposes the elimination order x 4 , x 5 , x 6 , x 7 , x 3 , x 2 , i.e. to do x 4 first and x 2 last.Explain why this is computationally inefficient.According to the factor graph, p(x 1 , . . ., x 7 ) factorises as p(x 1 , . . ., x 7 ) ∝ φ a (x 1 , x 4 )φ b (x 2 , x 4 )φ c (x 3 , x 4 )φ d (x 5 , x 4 )φ e (x 6 , x 4 )φ f (x 7 , x 4 ) (S. 6.121)If we choose to eliminate x 4 first, i.e. compute p(x 1 , x 2 , x 3 , x 5 , x 6 , x 7 ) =x 4 p(x 1 , . . ., x 7 ) (S.6.122).6.123) we cannot pull any of the factors out of the sum since each of them depends on x 4 .This means the cost to sum out x 4 for all combinations of the six variables (x 1 , x 2 , x 3 , x 5 , x 6 , x 7 ) is K 7 .Moreover, the new factor .6.124) does not factorise anymore so that subsequent variable eliminations will be expensive too.(b) Propose an elimination ordering that achieves O(K 2 ) computational cost per variable elimination and explain why it does so.Any ordering where x 4 is eliminated last will do.At any stage, elimination of one of the variables x 2 , x 3 , x 5 , x 6 , x 7 is then a O(K 2 ) operation.This is because e.g.p(x 1 , . . ., x 6 ) =x 7 p(x 1 , . . ., x 7 ) (S. 6.125))φ e (x 6 , x 4 ) φ7 (x 4 ) (S. 6.127) where computing φ7 (x 4 ) for all values of x 4 is O(K 2 ).Further, p(x 1 , . . ., x 5 ) =x 6 p(x 1 , . . ., x 6 ) (S. 6.128)x 6φ e (x 6 , x 4 ) (S. 6.129)x 4 ) φ7 (x 4 ) φ6 (x 4 ), (S. 6.130) where computation of φ6 (x 4 ) for all values of x 4 is again O(K ).Continuing in this manner, one obtains p(x 1 , x 4 ) ∝ φ a (x 1 , x 4 ) φ2 (x 4 ) φ3 (x 4 ) φ5 (x 4 ) φ6 (x 4 ) φ7 (x ).(S. 6.131) where each derived factor φ has O(K 2 ) cost.Summing out x and normalising the pmf is again a O(K 2 ) operation.Chapter 7Inference for Hidden Markov ModelsFor the hidden Markov modelassume you have observations for v i , i = 1, . . ., u < d.(a) Use message passing to compute p(h t |v 1:u ) for u < t ≤ d.For the sake of concreteness, you may consider the case d = 6, u = 2, t = 4.The factor graph for d = 6, u = 2, with messages that are required for the computation of p(h t |v 1:u ) for t = 4, is as follows.The messages from the unobserved visibles v i to their corresponding h i , e.g.v 3 to h 3 , are all one.Moreover, the message from the p(h 5 |h 4 ) node to h 4 equals one as well.This is because all involved factors, p(v i |h i ) and p(h i |h i−1 ), sum to one.Hence the factor graph reduces to a chain:Since the variable nodes copy the messages in case of a chain, we only show the factor-to-variable messages.The graph shows that we are essentially in the same situation as in filtering, with the difference that we use the factors p(h s |h s−1 ) for s ≥ u + 1.Hence, we can use filtering to compute the messages until time s = u and then compute the further messages with the p(h s |h s−1 ) as factors.This gives the following algorithm:1. Compute α(h u ) by filtering..7.4) since p(h s |h s−1 ) is normalised.This means that the normalising constant Z above equalswhich is the likelihood.For filtering, we have seen that α(h s ) ∝ p(h s |v 1:s ), s ≤ u.The α(h s ) for all s > u are proportional to p(h s |v 1:u ).This may be seen by noting that the above arguments hold for any t > u.(b) Use message passing to compute p(v t |v 1:u ) for u < t ≤ d.For the sake of concreteness, you may consider the case d = 6, u = 2, t = 4.The factor graph for d = 6, u = 2, with messages that are required for the computation of p(v t |v 1:u ) for t = 4, is as follows.Due to the normalised factors, as above, the messages to the right of h t are all one.Moreover the messages that go up from the v i to the h i , i = t, are also all one.Hence the graph simplifies to a chain.The message in blue is proportional to p(h t |v 1:u ) computed in question (a).Thus assume that we have computed p(h t |v 1:u ).The predictive distribution on the level of the visibles thus is .7.6)This follows from message passing since the last node (h 4 in the graph) just copies the (normalised) message and the next factor equals p(v t |h t ).An alternative derivation follows from basic definitions and operations, together with the independencies in HMMs: .7.7) .7.8)For the hidden Markov modelassume you have observations for v i , i = 1, . . ., t. Use the max-sum algorithm to derive an iterative algorithm to compute ĥ = argmaxAssume that the latent variables h i can take K different values, e.g.The resulting algorithm is known as Viterbi algorithm.Solution.We first form the factors .7.11) where the v i are known and fixed.The posterior p(h 1 , . . ., h t |v 1 , . . ., v t ) is then represented by the following factor graph (assuming t = 4).For the max-sum algorithm, we here choose h t to be the root.We thus initialise the algorithm with γ φ 1 →h 1 (h 1 ) = log φ 1 (h 1 ) = log p(v 1 |h 1 ) + log p(h 1 ) and then compute the messages from left to right, moving from the leaf φ 1 to the root h t .Since we are dealing with a chain, the variable nodes, much like in the sum-product algorithm, just copy the incoming messages.It thus suffices to compute the factor to variable messages shown in the graph, and then backtrack to h 1 .), the factor-to-variable update equation is .7.13)To simplify notation, denote γ φ i →h i (h i ) by V i (h i ).We thus have . . , t (S.7.15)In general, V 1 (h 1 ) and V i (h i ) are functions that depend on h 1 and h i , respectively.Assuming that the h i can take on the values 0, . . ., K − 1, the above equations can be written as .7.16) . . , t, (S.7.17)At the end of the algorithm, we thus have a t × K matrix V with elements v i,k .The maximisation can be performed by computing the temporary matrix A (via broadcasting) where the (m, k)-th element is log φ i (m, k) + v i−1,m .Maximisation then corresponds to determining the maximal value in each column.To support the backtracking, when we compute V i (h i ) by maximising over h i−1 , we compute at the same time the look-up table .7.18)When h i takes on the values 0, . . ., K − 1, this can be written as .7.19)This is the (row) index of the maximal element in each column of the temporary matrix A.After computing v t,k and γ * t,k , we then perform backtracking via S.7.21)This gives recursively ĥ = ( ĥ1 , . . ., ĥt ) = argmax h 1 ,...,ht p(h 1:t |v 1:t ).Consider the hidden Markov model specified by the following DAG.We assume that have already run the alpha-recursion (filtering) and can compute p(h t |v 1:t ) for all t.The goal is now to generate samples p(h 1 , . . ., h n |v 1:n ), i.e. entire trajectories (h 1 , . . ., h n ) from the posterior.Note that this is not the same as sampling from the n filtering distributions p(h t |v 1:t ).Moreover, compared to the Viterbi algorithm, the sampling approach generates samples from the full posterior rather than just returning the most probable state and its corresponding probability.(a) Show that p(h 1 , .Combining the emission distributions p(v s |h s ) (and marginal p(h 1 )) with the transition distributions p(h s |h s−1 ) we obtain the factors . . , n (S.7.23)We see from the factor tree that h t−1 and h t are neighbours, being attached to the same factor node φ t (h t−1 , h t ), e.g.φ 3 in case of p(h 2 , h 3 |v 1:4 ).By the rules of message passing, the joint p(h t−1 , h t |v 1:n ) is thus proportional to φ t times the messages into φ t .The following graph shows the messages for the case of p(h 2 , h 3 |v 1:4 ).Since the variable nodes only receive single messages from any direction, they copy the messages so that the messages into φ t are given by α(h t−1 ) and β(h t ) shown below in red and blue, respectively..7.25) which is the result that we want to show.Solution.The conditional p(h t−1 |h t , v 1:n ) can be written as the ratio The denominator p(h t |v 1:n ) is proportional to α(h t )β(h t ) since it is the smoothing distribution than can be determined via the alpha-beta recursion.Normally, we needed to sum the messages over all values of (h t−1 , h t ) to find the normalising constant of the numerator.For the denominator, we had to sum over all values of h t .Next, I will argue qualitatively that this summation is not needed; the normalising constants are both equal to p(v 1:t ).A more mathematical argument is given below.We started with a factor graph and factors that represent the joint p(h  .7.30)The desired conditional thus is .7.33) which is the result that we wanted to show.Note that β(h t ) cancels out and that p(h t−1 |h t , v 1:n ) only involves the α's, the (forward) transition distribution p(h t |h t−1 ) and the emission distribution at time t.Alternative solution: An alternative, mathematically rigorous solution is as follows.The conditional p(h t−1 |h t , v 1:n ) can be written as the ratio .7.34)We first determine the denominator.From the properties of the alpha and beta recursion, we know that .7.35) Using that v t+1:n ⊥ ⊥ v 1:t |h t , we can thus express the denominator p(h t |v 1:n ) as .7.38)For the numerator, we have .7.43)With the product rule, we have p(h t , v t:The desired conditional thus is .7.51) which is the result that we wanted to show.We thus obtain the following algorithm to generate samples from p(h 1 , . . ., h n |v 1:n ):1. Run the alpha-recursion (filtering) to determine all α(h t ) forward in time for t = 1, . . ., n.This algorithm is known as forward filtering backward sampling (FFBS).Consider a hidden Markov model with three visibles v 1 , v 2 , v 3 and three hidden variables h 1 , h 2 , h 3 which can be represented with the following factor graph:This question is about computing the predictive probability p(v 3 = 1|v 1 = 1).(a) The factor graph below represents p(h. Provide an equation that defines φ A in terms of the factors in the factor graph above.(b) Assume further that all variables are binary, h i ∈ {0, 1}, v i ∈ {0, 1}; that p(h 1 = 1) = 0.5, and that the transition and emission distributions are, for all i, given by:Compute the numerical values of the factor φ A .(c) Given the definition of the transition and emission probabilities, we have φThe variable node h 3 copies the message so that we haveWe thus haveThe requested x and y are thus: x = 0.4, y = 0.6.(f) Compute the numerical value of p(v 3 = 1|v 1 = 1).Inserting the numbers gives α(h 2 = 0) + α(h 2 = 1) = 5/10 = 1/2 so thatWe take here a change of measure perspective on the alpha-recursion.Consider the following directed graph for a hidden Markov model where the y i correspond to observed (visible) variables and the x i to unobserved (hidden/latent) variables.x 1 x 2 x 3 . . . . . .x n y 1 y 2 y 3 y nThe joint model for x = (x 1 , . . ., x n ) and y = (y 1 , . . ., y n ) thus isfor t = 0, . . ., n.We take the case t = 0 to correspond to p(x 1 , .The result for p(x 1 , . . ., x n ) is obtained when we integrate out all y's.(b) Show that p(x 1 , .(7.9)By iterating from t = 1 to t = n, we can thus recursively compute p(x 1 , . . ., x n |y 1 , . . ., y n ), including its normalising constant Z n , which equals the likelihood Z n = p(y 1 , . . ., y n )Solution.We start with (7.8) which shows that by definition of p t (x 1 , . . ., x n ) we have .7.89)For t = 1, we thus haveIntegrating out x 2 , . . ., x n givesThe normalising constant is .7.96)This establishes the result for t = 1.From (7.8), we further haveIntegrating out x t+1 , . . ., x n thus gives .7.103) Noting that the product over the g i does not involve x t and that p(x t |x t−1 ) is a pdf, we have furtherSolution.Let t > 1.With (7.9), we have  .7.119) which proves the "change of measure".Moreover, the normalising constant Z t is the same as before.Hence completing the iteration until t = n yields the likelihood p(y 1 , . . ., y n ) = Z n as a by-product of the recursion.The initialisation of the recursion with p 0 (x 1 ) = p(x 1 ) is also the same as above.We here consider filtering for hidden Markov models with Gaussian transition and emission distributions.For simplicity, we assume one-dimensional hidden variables and observables.We denote the probability density function of a Gaussian random variable x with mean µ and variance σ 2 by N (x|µ, σ 2 ), .16)The transition and emission distributions are assumed to beThe distribution p(h 1 ) is assumed Gaussian with known parameters.The A s , B s , C s , D s are also assumed known.(a) Show that h s and v s as defined in the following update and observation equationsfollow the conditional distributions in (7.17) and (7.18).The random variables ξ s and η s are independent from the other variables in the model and follow a standard normal Gaussian distribution, e.g.ξ s ∼ N (ξ s |0, 1).Hint: For two constants c 1 and c 2 , y = c 1 + c 2 x is Gaussian if x is Gaussian.In other words, an affine transformation of a Gaussian is Gaussian.The equations mean that h s is obtained by scaling h s−1 and by adding noise with variance B 2 s .The observed value v s is obtained by scaling the hidden h s and by corrupting it with Gaussian observation noise of variance D 2 s .By assumption, ξ s is Gaussian.Since we condition on) is a constant, and since B s is a constant too, h s is Gaussian.What we have to show next is that (7.19) defines the same conditional mean and variance as the conditional Gaussian in (7.17):The conditional expectation of h s given h s−1 isThe conditional variance of h s given h s−1 is We see that the conditional mean and variance of h s given h s−1 match those in (7.17).And since h s given h s−1 is Gaussian as argued above, the result follows.Exactly the same reasoning also applies to the case of (7.20).Conditional on h s , v s is Gaussian because it is an affine transformation of a Gaussian.The conditional mean of v s given h s is:The conditional variance of v s given h s is Hence, conditional on h s , v s is Gaussian with mean and variance as in (7.18).(b) Show thatHint: While this result can be obtained by integration, an approach that avoids this is as follows: First note that N (x|µ, σ 2 )N (y|Ax, B 2 ) is proportional to the joint pdf of x and y.We can thus consider the integral to correspond to the computation of the marginal of y from the joint.Using the equivalence of Equations ( 7.17)-( 7.18) and (7.19)-( 7.20), and the fact that the weighted sum of two Gaussian random variables is a Gaussian random variable then allows one to obtain the result.Solution.We follow the procedure outlined above.The two Gaussian densities correspond to the equations .7.132) .7.133) where ξ and η are independent standard normal random variables.The mean of y is .7.135) where we have use the linearity of expectation and E(η) = 0.The variance of y is(by properties of the variance) (S.7.137)Since y is the (weighted) sum of two Gaussians, it is Gaussian itself, and hence its distribution is completely defined by its mean and variance, so that .7.139) Now, the product N (x|µ, σ 2 )N (y|Ax, B 2 ) is proportional to the joint pdf of x and y, so that the integral can be considered to correspond to the marginalisation of x, and hence its result is proportional to the density of y, which is N (y|Aµ, A 2 σ 2 + B 2 ).(c) Show thatwhereHint: Work in the negative log domain.We show the result using a classical technique called "completing the square", see e.g.https://en.wikipedia.org/wiki/Completing_the_square.We work in the (negative) log-domain and use that .7.142) where const indicates terms not depending on x.We thus obtainwhere .7.148) Comparison with (S. 7.142) shows that we can further write .7.149) where .7.151) and hence .7.152)Note that the identity 7.153) is obtained as followsWe can use the "alpha-recursion" to recursively compute p(h t |v 1:t ) ∝ α(h t ) as follows.For continuous random variables, the sum above becomes an integral so thatFor reference, let us denote the integral by I(h s ),Note that I(h s ) is proportional to the predictive distribution p(h s |v 1:s−1 ).For a Gaussian prior distribution for h 1 and Gaussian emission probability p(v 1 |h 1 ), α(h 1 ) = p(h 1 ) • p(v 1 |h 1 ) ∝ p(h 1 |v 1 ) is proportional to a Gaussian.We denote its mean by µ 1 and its variance by σ 2 1 so thatAssuming α(h s−1 ) ∝ N (h s−1 |µ s−1 , σ 2 s−1 ) (which holds for s = 2), use Equation (7.21) to show thatwhereis Gaussian, see Equation (7.17), Equation (7.27) becomes .7.158) Equation ( 7.21) with x ≡ h s−1 and y ≡ h s yields the desired result, .7.159)We can understand the equation as follows: To compute the predictive mean of h s given v 1:s−1 , we forward propagate the mean of h s−1 |v 1:s−1 using the update equation ( 7 .19).This gives the mean term A s µ s−1 .Since h s−1 |v 1:s−1 has variance σ 2 s−1 , the variance of h s |v 1:s−1 is given by A 2 s σ 2 s−1 plus an additional term, B 2 s , due to the noise in the forward propagation.This gives the variance term A 2 s σ 2 s−1 + B 2 s .(e) Use Equation ( 7.22) to show thatwhereHaving computed I(h s ), the final step in the alpha-recursion is .7.160)With Equation ( 7.18) we obtain .7.161)We further note that .7.162) so that we can apply Equation (7.22 whereThese are the Kalman filter equations and K s is called the Kalman filter gain.We start from .7.171) and see that .7.173)For the variance σ 2 s , we have where the posterior mean and variance are recursively computed as .7.183) .7.184) .7.185) and initialised with µ µ µ 1 and Σ Σ Σ 1 equal to the mean and variance of p(h 1 |v 1 ).The matrix K s is then called the Kalman gain matrix.The Kalman filter is widely applicable, see e.g.https://en.wikipedia.org/wiki/Kalman_filter, and has played a role in historic events such as the moon landing, see e.g.(Grewal and Andrews, 2010).An example of the application of the Kalman filter to tracking is shown in Figure 7.1.We have already seen that A s µ s−1 is the predictive mean of h s given v 1:s−1 .The term C s A s µ s−1 is thus the predictive mean of v s given the observations so far, v 1:s−1 .The difference v s − C s A s µ s−1 is thus the prediction error of the observable.Since α(h s ) is proportional to p(h s |v 1:s ) and µ s its mean, we thus see that the posterior mean of h s |v 1:s equals the posterior mean of h s |v 1:s−1 , A s µ s−1 , updated by the prediction error of the observable weighted by the Kalman gain.For D 2 s → 0, K s → C −1 s and (S.7.189) so that the posterior mean of p(h s |v 1:s ) is obtained by inverting the observation equation.Moreover, the variance σ 2 s of h s |v 1:s goes to zero so that the value of h s is known precisely and equals C −1 s v s .Chapter 8Model-Based LearningThe Gaussian pdf parametrised by mean µ and standard deviation σ is given by For iid data, the likelihood function is(S. x i (8.1)and the square root of the sample varianceSince the logarithm is strictly monotonically increasing, the maximiser of the log-likelihood equals the maximiser of the likelihood.It is easier to take derivatives for the log-likelihood function than for the likelihood function so that the maximum likelihood estimate is typically determined using the log-likelihood.Given the algebraic expression of (θ), it is simpler to work with the variance v = σ 2 rather than the standard deviation.Since σ > 0 the function v = g(σ) = σ 2 is invertible, and the invariance of the MLE to re-parametrisation guarantees that σ = √ v.We now thus maximise the function J(µ, v), .8.5) with respect to µ and v.Taking partial derivatives givesA necessary condition for optimality is that the partial derivatives are zero.We thus obtain the conditions .8.10) From the first condition it follows that μ = 1 n n i=1x i (S. 8.11) The second condition thus becomes(x i − μ) 2 = 0 (multiply with v 2 and rearrange) (S.8.12) .8.13) and hence .8.14)We now check that this solution corresponds to a maximum by computing the Hessian matrix .8.15)If the Hessian negative definite at (μ, v), the point is a (local) maximum.Since we only have one critical point, (μ, v), the local maximum is also a global maximum.Taking second derivatives gives(S. 8.16) Substituting the values for (μ, v) gives, (S. 8.17) which is negative definite.Note that the the (negative) curvature increases with n, which means that J(µ, v), and hence the log-likelihood becomes more and more peaked as the number of data points n increases.Given iid data D = {x 1 , . . ., x n }, compute p(µ|D, σ 2 ) for the Bayesian modelwhere σ 2 is a fixed known quantity.Hint: You may use thatwhereSolution.We re-use the expression for the likelihood L(µ) from Exercise 8.1.(S.8.18) which we can write as  .8.29) .8.32)As n increases, σ 2 /n goes to zero so that σ 2 n → 0 and µ n → x.This means that with an increasing amount of data, the posterior of the mean tends to be concentrated around the maximum likelihood estimate x.From (8.7), we also have that .8.33) which shows more clearly that the value of µ n lies on a line with end-points µ 0 (for n = 0) and x (for n → ∞).As the amount of data increases, µ n moves form the mean under the prior, µ 0 , to the average of the observed sample, that is the MLE x.We assume that we are given a parametrised directed graphical model for variables x 1 , . . ., x d ,where the conditionals are represented by parametrised probability tables, For example, if pa 3 = {x 1 , x 2 }, p(x 3 |pa 3 ; θ 3 ) is represented asi =s)(S. 8.42) Swapping the order of the products so that the product over the data points comes first, we obtaini =s)(S. 8.43) We next split the product over j into two products, one for all j where x (j) i = 1, and one for all j where x (j)i =s)(S.8.44)i =s)i =s)(S.8.45)i =s)(S.8.46) .8.47) where .8.48) is the number of times x i = 1 and x i = 0, respectively, with its parents being in state s.(c) Show that the log-likelihood decomposes into sums of terms that can be independently optimised, and that each term corresponds to the log-likelihood for a Bernoulli model.The log-likelihood (θ) equals (θ) = log p(D; θ) (S.8.49)Since the parameters θ s i are not coupled in any way, maximising (θ) can be achieved by maximising each term is (θ s i ) individually, .8.53)Moreover, is (θ s i ) corresponds to the log-likelihood for a Bernoulli model with success probability θ s i and data with n s x i =1 number of ones and n s x i =0 number of zeros.(d) Determine the maximum likelihood estimate θ for the Bernoulli model .11)and iid data x 1 , . . ., x n .Solution.The log-likelihood function is .8.55) Since log(θ) and log(1 − θ) do not depend on i, we can pull them outside the sum and the log-likelihood function can be written as .8.56) where (θ) (S. 8.57)There are multiple ways to solve the problem.One option is to determine the unconstrained optimiser and then check whether it satisfies the constraint.The first derivative equals .8.58) and the second derivative isThe second derivative is always negative for θ ∈ (0, 1), which means that (θ) is strictly concave on (0, 1) and that an optimiser that is not on the boundary corresponds to a maximum.Setting the first derivative to zero gives the condition For n x=1 < n, we have θ ∈ (0, 1) so that the constraint is actually not active.In the derivation, we had to exclude boundary cases where θ is 0 or 1.We note that e.g.θ = 1 is obtained when n x=1 = n, i.e. when we only observe 1's in the data set.In that case, n x=0 = 0 and the log-likelihood function equals n log(θ), which is strictly increasing and hence attains the maximum at θ = 1.A similar argument shows that if n x=1 = 0, the maximum is at θ = 0. Hence, the maximum likelihood estimate θ = n x=1 n (S. 8.66) is valid for all n x=1 ∈ {0, . . ., n}.An alternative approach to deal with the constraint is to reparametrise the objective function and work with the log-odds η, .8.67)The log-odds take values in R so that η is unconstrained.The transformation from θ to η is invertible and .8.68)The optimisation problem then becomesComputing the second derivative shows that the objective is concave for all η and the maximiser η can be determined by setting the first derivative to zero.The maximum likelihood estimate of θ is then given by θ = exp(η) 1 + exp(η) (S. 8.69) The reason for this is as follows: Let J(η) = (g −1 (η)) be the log-likelihood seen as a function of η.Since g and g −1 are invertible, we have that .8.70) .8.71) (e) Returning to the fully observed directed graphical model, conclude that the maximum likelihood estimates are given bySolution.Given the result from question (c), we can optimise each term is (θ s i ) separately.Each term formally corresponds to a log-likelihood for a Bernoulli model, so that we can use the results from question (d) to obtaini = s) and we further have .8.75) Hence, to determine θs i , we first count the number of times the parents of x i are in state s, which gives the denominator, and then among them, count the number of times x i = 1, which gives the numerator.The distribution of a and s are Bernoulli distributions with parameter (success probability) θ a and θ s , respectively, i.e. (8.13) and the distribution of c given the parents is parametrised as specified in the following tableThe free parameters of the model are (θ a , θ s , θ 1 c , . . ., θ 4 c ).Assume we observe the following iid data (each row is a data point).a s c 0 1 1 0 0 0 1 0 1 0 0 0 0 1 0 (a) Determine the maximum-likelihood estimates of θ a and θ s Solution.The maximum likelihood estimate (MLE) θa is given by the fraction of times that a is 1 in the data set.Hence θa = 1/5.Similarly, the MLE θs is 2/5.(b) Determine the maximum-likelihood estimates of θ 1 c , . . ., θ 4 c .Solution.With (S.8.75), we haveThis because, for example, we have two observations where (a, s) = (0, 0), and among them, c = 1 never occurs, so that the MLE for p(c = 1|a, s) is zero.This example illustrates some issues with maximum likelihood estimates: We may get extreme probabilities, zero or one, or if the parent configuration does not occur in the observed data, the estimate is undefined.Consider the Bayesian modelwhere x ∈ {0, 1}, θ ∈ [0, 1], α 0 = (α 0 , β 0 ), andwhere n x=1 denotes the number of ones and n x=0 the number of zeros in the data.Solution.This follows from p(θ|D) ∝ L(θ)p(θ; α 0 ) (S. 8.76) and from the expression for the likelihood function of the Bernoulli model, which is (S.8.80)where n x=1 = n i=1 x i denotes the number of 1's in the data, andInserting the expressions for the likelihood and prior into (S.8.76) gives .8.83) which is the desired result.Since α 0 and β 0 are updated by the counts of ones and zeros in the data, these hyperparameters are also referred to as "pseudo-counts".Alternatively, one can think that they are the counts that are observed in another iid data set which has been previously analysed and used to determine the prior.(b) Compute the mean of a Beta random variable f , .15) using thatwhere B(α, β) denotes the Beta function and where the Gamma function Γ(t) is defined as and satisfies Γ(t + 1) = tΓ(t).Hint: It will be useful to represent the partition function in terms of the Beta function.Solution.We first write the partition function of p(f ; α, β) in terms of the Beta function .8.85)We then have that the mean E[f ] is given by .8.92) where we have used the definition of the Beta function in terms of the Gamma function and the property Γ(t + 1) = tΓ(t).(c) Show that the predictive posterior probability p(x = 1|D) for a new independently observed data point x equals the posterior mean of p(θ|D), which in turn is given byThis is the Bayesian analogue of Exercise 8.3 and the notation follows that exercise.We consider the Bayesian model .20)where p(x i |pa i , θ i ) is defined via (8.9), α 0 is a vector of hyperparameters containing all α s i,0 , β 0 the vector containing all β s i,0 , and as before B denotes the Beta distribution.Under the prior, all parameters are independent.(a) For iid data D = {x (1) , . . . , x (n)where 8.22) and that the parameters are also independent under the posterior.Solution.We start with p(θ|D) ∝ p(D|θ)p(θ; α 0 , β 0 ).(S. 8.102) Inserting the expression for p(D|θ) given in (8.10) and the assumed form of the prior gives .8.107)It can be immediately verified that B(θ s i ;) is proportional to the marginal p(θ s i |D) so that the parameters are independent under the posterior too.(b) For a variable x i with parents pa i , compute the posterior predictive probability p(x i = 1|pa i , D)Solution.The solution is analogue to the solution for question (c), using the sum rule, independencies, and properties of beta random variables: .8.114) where n s = n s x i =0 + n s x i =1 denotes the number of times the parent configuration s occurs in the observed data D. (1 + 0)/(1 + 1 + 2) = 1/4 0 0Compared to the MLE solution in Exercise (b) question (b), we see that the estimates are less extreme.This is because they are a combination of the prior knowledge and the observed data.Moreover, when we do not have any data, the posterior equals the prior, unlike for the mle where the estimate is not defined.We consider the directed graphical model shown below on the left for the four binary variables t, b, s, x, each being either zero or one.Assume that we have observed the data shown in the table on the right.x s t b 0 1 0 1 0 0 0 0 0 1 0 1 0 1 0 1 0 0 0 0 0 0 0 0 0 1 0 1 0 1 0 1 0 0 0 1 1 1 1 0We assume the (conditional) pmf of s|t, b is specified by the following parametrised probability table:(a) What are the maximum likelihood estimates for p(s = 1|b = 0, t = 0) and p(s = 1|b = 0, t = 1), i.e. the parameters θ 1 s and θ 3 s ?Solution.The maximum likelihood estimates (MLEs) are equal to the fraction of occurrences of the relevant events..8.118) (b) Assume each parameter in the table for p(s|t, b) has a uniform prior on (0, 1).Compute the posterior mean of the parameters of p(s = 1|b = 0, t = 0) and p(s = 1|b = 0, t = 1) and explain the difference to the maximum likelihood estimates.A uniform prior corresponds to a Beta distribution with hyperparameters α 0 = β 0 = 1.With Exercise 8.6 question (b), we have .8.120) Compared to the MLE, the posterior mean is less extreme.It can be considered a "smoothed out" or regularised estimate, where α 0 > 0 and β 0 > 0 provides regularisation (see https://en.wikipedia.org/wiki/Additive_smoothing).We can see a pull of the parameters towards the prior predictive mean, which equals 1/2.A friend proposes to improve the factor analysis model by working with correlated latent variables.The proposed model iswhere C is some H × H covariance matrix, F is the D × H matrix with the factor loadings, Ψ Ψ Ψ = diag(Ψ 1 , . . ., Ψ D ), c ∈ R D and the dimension of the latents H is less than the dimension of the visibles D. N (x; µ µ µ, Σ Σ Σ) denotes the pdf of a Gaussian with mean µ µ µ and covariance matrix Σ Σ Σ.The standard factor analysis model is obtained when C is the identity matrix.(a) What is marginal distribution of the visibles p(v; θ) where θ stands for the parameters C, F, c, Ψ Ψ Ψ?Solution.The model specifications are equivalent to the following data generating process: and we denote it by C 1/2 .Show that the matrix square root of C equals, so that the extra parameters given by the covariance matrix C are actually redundant and nothing is gained with the richer parametrisation.We verify that the model has the same distribution for the visibles.As before E[v] = c, and the covariance matrix is .8.134)By the ICA model, V(h) = I, so that Ã must satisfy .8.150) which means that Ã is orthonormal.In the original ICA model, the number of parameters is given by the number of elements of the matrix A, which is D 2 if v is D-dimensional.An orthogonal matrix contains D(D − 1)/2 degrees of freedom (see e.g.https://en.wikipedia.org/wiki/Orthogonal_matrix), so that we can think that whitening "solves half of the ICA problem".Since whitening is a relatively simple standard operation, many algorithms (e.g."fastICA", Hyvärinen, 1999) first reduce the complexity of the estimation problem by whitening the data.Moreover, due to the properties of the orthogonal matrix, the log-likelihood for the ICA model also simplifies for whitened data: The log-likelihood for ICA model without whitening is While the log-likelihood takes a simpler form, the optimisation problem is now a constrained optimisation problem: B is constrained to be orthonormal.For further information, see e.g.(Hyvärinen et al., 2001, Chapter 9).The objective function J(θ) that is minimised in score matching is .32)where ψ j is the partial derivative of the log model-pdf log p(x; θ) with respect to the j-th coordinate (slope) and ∂ j ψ j its second partial derivative (curvature).The observed data are denoted by x 1 , . . ., x n and x ∈ R m .The goal of this exercise is to show that for statistical models of the form (8.33) the score matching objective function becomes a quadratic form, which can be optimised efficiently (see e.g.Barber, 2012, Appendix A.5.3).The set of models above are called the (continuous) exponential family, or also log-linear models because the models are linear in the parameters θ k .Since the exponential family generally includes probability mass functions as well, the qualifier "continuous" may be used to highlight that we are here considering continuous random variables only.The functions F k (x) are assumed to be known (they are called the sufficient statistics).(a) Denote by K(x) the matrix with elements K kj (x), (8.34) and by H(x) the matrix with elements H kj (x), .35)Furthermore, let h j (x) = (H 1j (x), . . ., H Kj (x)) be the j-th column vector of H(x).Show that for the continuous exponential family, the score matching objective in Equation ( 8.32) becomeswhere .8.154) the first derivative with respect to x j , the j-th element of x, is .8.157)The second derivative is .8.160) which we can write more compactly as .8.161)The score matching objective in Equation (8.32) features the sum j ψ j (x; θ) 2 .The term ψ j (x; θ) 2 equals .8.165) which can be more compactly expressed using matrix notation.Noting that .8.166) we can write .8.168) where we have used that for some matrix A S.8.169) where [A] k,k is the (k, k ) element of the matrix A.Inserting the expressions into Equation (8.32) gives .8.174) which is the desired result.(b) The pdf of a zero mean Gaussian parametrised by the variance σ 2 isThe (multivariate) Gaussian is a member of the exponential family.By comparison with Equation ( 8.33), we can re-parametrise the statistical model {p(x; σ 2 )} σ 2 and work with .39)instead.The two parametrisations are related by θ = −1/(2σ 2 ).Using the previous result on the (continuous) exponential family, determine the score matching estimate θ, and show that the corresponding σ2 is the same as the maximum likelihood estimate.This result is noteworthy because unlike in maximum likelihood estimation, score matching does not need the partition function Z(θ) for the estimation.By comparison with Equation (8.33), the sufficient statistics F (x) is x 2 .We first determine the score matching objective function.For that, we need to determine the quantities r and M in Equation (8.37).Here, both r and M are scalars, and so are the matrices K and H that define r and M. By their definitions, we obtain The only parameter value that satisfies the condition is .8.185)The second derivative of J(θ) is .8.186) which is positive (as long as all data points are non-zero).Hence θ is a minimiser.From the relation θ = −1/(2σ 2 ), we obtain that the score matching estimate of the variance σ .8.187)We can obtain the score matching estimate σ2 from θ in this manner for the same reason that we were able to work with transformed parameters in maximum likelihood estimation.For zero mean Gaussians, the second moment m 2 is the maximum likelihood estimate of the variance, which shows that the score matching and maximum likelihood estimate are here the same.While the two methods generally yield different estimates, the result also holds for multivariate Gaussians where the score matching estimates also equal the maximum likelihood estimates, see the original article on score matching by Hyvärinen (2005).Consider the Ising model for two binary random variables (x 1 , x 2 ), Solution.The definition of the partition function is .8.188) where have have to sum over (x 1 , x 2 ) ∈ {−1, 1} 2 = {(−1, 1), (1, 1), (1, −1) (−1 − 1)}.This gives -Denoting the i-th observed data point by (x i 1 , x i 2 ), the log-likelihood is .8.191) Inserting the definition of the p(x 1 , x 2 ; θ) yields .8.193) Its derivative with respect to the θ is .8.195) Setting it to zero yields .8.196)An alternative approach is to start with the more general relationship that relates the gradient of the partition function to the gradient of the log unnormalised model.For example, ifwe have .8.198) Setting the derivative to zero gives, .8.200) From the graph, we see that f (θ) takes on the value −1/3 for θ = −1, which is the desired MLE.Let p(x; A) ∝ exp(−x Ax) be a parametric statistical model for x = (x 1 , . . ., x 100 ), where the parameters are the elements of the matrix A. Assume that A is symmetric and positive semi-definite, i.e.A satisfies x Ax ≥ 0 for all values of x.(a) For n iid data points x 1 , . . ., x n , a friend proposes to estimate A by maximising J(A), .40)Explain why this procedure cannot give reasonable parameter estimates.We have that x k Ax k ≥ 0 so that exp −x k Ax k ≤ 1. Hence exp −x k Ax k is maximal if the elements of A are zero.This means that J(A) is maximal if A = 0 whatever the observed data, which does not correspond to a meaningful estimation procedure (estimator).(b) Explain why maximum likelihood estimation is easy when the x i are real numbers, i.e. x i ∈ R, while typically very difficult when the x i are binary, i.e. x i ∈ {0, 1}.We would like to use importance sampling to compute the probability that a standard Gaussian random variable x takes on a value larger than 5, i.eWe know that the probability equalswhere Φ(.) is the cumulative distribution function of a standard normal random variable.(a) With the indicator function 1 x>5 (x), which equals one if x is larger than 5 and zero otherwise, we can write P(x > 5) in form of the expectationwhere the expectation is taken with respect to the density N (x; 0, 1) of a standard normal random variable, .6)This suggests that we can approximate P(x > 5) by a Monte Carlo averagex i ∼ N (x; 0, 1).(9.7)Explain why this approach does not work well.In this approach, we essentially count how many times the x i are larger than 5.However, we know that the chance that x i > 5 is only 2.87 • 10 −7 .That is, we only get about one value above 5 every 20 million simulations!The approach is thus very sample inefficient.(b) Another approach is to use importance sampling with an importance distribution q(x) that is zero for x < 5. We can then write P(x > 5) as(9.10) and estimate P(x > 5) as a sample average.We here use an exponential distribution shifted by 5 to the right.It has pdfFor background on the exponential distribution, see e.g.https://en.wikipedia.org/wiki/Exponential_distribution.Provide a formula that approximates P(x > 5) as a sample average over n samples x i ∼ q(x).Solution.The provided equation(S.9.1) can be approximated as a sample average as follows: Plot the estimate against the sample size and compare with the ground truth value.The following figure shows the importance sampling estimate as a function of the sample size (numbers do depend on the random seed used).We can see that we can obtain a good estimate with a few hundred samples already.Python code is as follows.A standard Cauchy distribution has the density function (pdf)(9.12) with x ∈ R. A friend would like to verify that p(x)dx = 1 but doesn't quite know how to solve the integral analytically.They thus use importance sampling and approximate the integral aswhere q is the density of the auxiliary/importance distribution.Your friend chooses a standard normal density for q and produces the following figure:The figure shows two independent runs.In each run, your friend computes the approximation with different sample sizes by subsequently including more and more x i in the approximation, so that, for example, the approximation with n = 2000 shares the first 1000 samples with the approximation that uses n = 1000.Your friend is puzzled that the two runs give rather different results (which are not equal to one), and also that within each run, the estimate very much depends on the sample size.Explain these findings.We thus can write the inverse F −1 x (y) of the cdf y = F x (α) as To generate n iid samples from x, we first generate n iid samples y (i) that are uniformly distributed on [0, 1], and then compute for each F −1 x (y (i) ).The properties of inverse transform sampling guarantee that the x (i) ,x (y (i) ), (S.9.44) are independent and Laplace distributed.We can sample from it by sampling a Laplace variable with variance 1 as in Exercise 9.5 and then scaling the sample by √ 2b.Rejection sampling then repeats the following steps:• Generate x ∼ q(x; b)• Accept x with probability f (x) = 1 M p(x) q(x) , i.e. generate u ∼ U (0, 1) and accept x if u ≤ f (x).Given the independencies between the hiddens given the visibles and vice versa, we have .9.74) so that the expressions for p(h i = 1|v) and p(v i = 1|h) allow us to implement the Gibbs sampler.Given the independencies, it makes further sense to sample the h and v variables in blocks: first we sample all the h i given v, and then all the v i given the h (or vice versa).This is also known as block Gibbs sampling.In summary, given a sample (h (k) , v (k) ), we thus generate the next sample (h (k+1) , v (k+1) ) in the sequence as follows:• For all h i , i = 1, . . ., m:compute p h i = p(h i = 1|v (k) ) -sample u i from a uniform distribution on [0, 1] and set h (k+1) i to 1 if u i ≤ p h i .• For all v i , i = 1, . . ., n:compute p v i = p(v i = 1|h (k+1) ) -sample u i from a uniform distribution on [0, 1] and set v (k+1) i to 1 if u i ≤ p v i .As final step, after sampling S pairs (h (k) , v (k) ), k = 1, . . ., S, the set of visibles v (k) form samples from the marginal p(v).This exercise is on sampling and approximate inference by Markov chain Monte Carlo (MCMC).MCMC can be used to obtain samples from a probability distribution, e.g. a posterior distribution.The samples approximately represent the distribution, as illustrated in Figure 9.3, and can be used to approximate expectations.We denote the density of a zero mean Gaussian with variance σ 2 by N (x; µ, σ 2 ), i.e.  for some function g(θ).If d is small, e.g.d ≤ 3, deterministic numerical methods can be used to approximate the integral to high accuracy, see e.g.https://en.wikipedia.org/wiki/Numerical_integration.But for higher dimensions, these methods are generally not applicable any more.The expectation, however, can be approximated as a sample average if we have samples θ (i) from p(θ | D):g(θ (i) ) (9.25)Note that in MCMC methods, the samples θ (1) , . . ., θ (S) used in the above approximation are typically not statistically independent.Metropolis-Hastings is an MCMC algorithm that generates samples from a distribution p(θ), where p(θ) can be any distribution on the parameters (and not only posteriors).The algorithm is iterative and at iteration t, it uses:• a proposal distribution q(θ; θ (t) ), parametrised by the current state of the Markov chain, i.e. θ (t) ;• a function p * (θ), which is proportional to p(θ).In other words, p * (θ) is unnormalised 1 and the normalised density p(θ) is .26)For all tasks in this exercise, we work with a Gaussian proposal distribution q(θ; θ (t) ), whose mean is the previous sample in the Markov chain, and whose variance is 2 .That is, at iteration t of our Metropolis-Hastings algorithm, q(θ; θ (t−1) ) = (a) Read Section 27.4 of Barber (2012) to familiarise yourself with the Metropolis-Hastings algorithm.(b) Write a function mh implementing the Metropolis Hasting algorithm, as given in Algorithm 27.3 in Barber (2012), using the Gaussian proposal distribution in (9.27) above.The function should take as arguments• p_star: a function on θ that is proportional to the density of interest p(θ);• param_init: the initial sample -a value for θ from where the Markov chain starts;• num_samples: the number S of samples to generate;• vari: the variance 2 for the Gaussian proposal distribution q;and return [θ (1) , . . ., θ (S) ] -a list of S samples from p(θ) ∝ p * (θ).As we are using a symmetrical proposal distribution, q(θ | θ * ) = q(θ * | θ), and one could simplify the algorithm by having a = p * (θ * ) p * (θ) , where θ is the current sample and θ * is the proposed sample.In practice, it is desirable to implement the function in the log domain, to avoid numerical problems.That is, instead of p * , mh will accept as an argument log p * , and a will be calculated as: a = (log q(θ | θ * ) + log p * (θ * )) − (log q(θ * | θ) + log p * (θ)) (c) Test your algorithm by sampling 5, 000 samples from p(x, y) = N (x; 0, 1)N (y; 0, 1).Initialise at (x = 0, y = 0) and use 2 = 1.Generate a scatter plot of the obtained samples.The plot should be similar to Figure 9.3b.Highlight the first 20 samples only.Do these 20 samples alone adequately approximate the true density?Sample another 5, 000 points from p(x, y) = N (x; 0, 1)N (y; 0, 1) using mh with 2 = 1, but this time initialise at (x = 7, y = 7).Generate a scatter plot of the drawn samples and highlight the first 20 samples.If everything went as expected, your plot probably shows a "trail" of samples, starting at (x = 7, y = 7) and slowly approaching the region of space where most of the probability mass is.Solution.Figure 9.4 shows the two scatter plots of draws from N (x; 0, 1)N (y; 0, 1):• Figure 9.4a highlights the first 20 samples obtained by the chain when starting at (x = 0, y = 0).They appear to be representative samples from the distribution, however, they are not enough to approximate the distribution on their own.This would mean that a sample average computed with 20 samples only would • param_init = (α init , β init ) = (0, 0),• vari = 1, and• number of warm-up steps W = 1000.Plot the drawn samples with x-axis α and y-axis β and report the posterior mean of α and β, as well as their correlation coefficient under the posterior.see that if the samples are strongly correlated, ∞ k=1 ρ(k) is large and the effective sample size is small.On the other hand, if ρ(k) = 0 for all k, the effective sample size is S.ESS, as defined above, is the number of independent samples which are needed to obtain a sample average that has the same variance as the sample average computed from correlated samples.To illustrate how correlation between samples is related to a reduction of sample size, consider two pairs of samples (θ 1 , θ 2 ) and (ω 1 , ω 2 ).All variables have variance σ 2 and the same mean µ, but ω 1 and ω 1 are uncorrelated while the covariance matrix for θ 1 , θ 2 is C, .35)with ρ > 0. The variance of the average ω = 0.5(ω 1 + ω 2 ) is .36)where the 2 in the denominator is the sample size.Derive an equation for the variance of θ = 0.5(θ 1 + θ 2 ) and compute the reduction α of the sample size when working with the correlated (θ 1 , θ 2 ).In other words, derive an equation of α in Because of the strong correlation, we effectively only have one sample and not two if ρ → 1.Variational InferenceLet L x (q) be the evidence lower bound for the marginal p(x) of a joint pdf/pmf p(x, y), L x (q) = E q(y|x) log p(x, y) q(y|x) .(10.1)Mean field variational inference assumes that the variational distribution q(y|x) fully factorises, i.e.q(y|x) = d i=1 q i (y i |x), (10.2) when y is d-dimensional.An approach to learning the q i for each dimension is to update one at a time while keeping the others fixed.We here derive the corresponding update equations.(a) Show that the evidence lower bound L x (q) can be written as L x (q) = E q 1 (y 1 |x) E q(y \1 |x) [log p(x, y)] − d i=1 E q i (y i |x) [log q i (y i |x)] (10.3)where q(y \1 |x) = d i=2 q i (y i |x) is the variational distribution without q 1 (y 1 |x).Solution.This follows directly from the definition of the ELBO and the assumed factorisation of q(y|x).We have L x (q) = E q(y|x) log p(x, y) − E q(y|x) log q(y|x) (S.E q i (y i |x) log q i (y i |x) (S.10.3)= E q 1 (y 1 |x) E d i=2 q i (y i |x) log p(x, y) − d i=1 E q i (y i |x) log q i (y i |x) (S.10.4) = E q 1 (y 1 |x) E q(y \1 |x) [log p(x, y)] − d i=1 E q i (y i |x) [log q i (y i |x)] (S. 10.5)We have here used the linearity of expectation.In case of continuous random variables, for instance, we have q i (y i |x) log q i (y i |x)dy i j =i q j (y j |x)dy j =1 (S.10.8)E q i (y i |x) log q i (y i |x) (S.10.9)For discrete random variables, the integral is replaced with a sum and leads to the same result.(b) Assume that we would like to update q 1 (y 1 |x) and that the variational marginals of the other dimensions are kept fixed.Show that argmax q 1 (y 1 |x)L x (q) = argmin where Z is the normalising constant.Note that variables y 2 , . . ., y d are marginalised out due to the expectation with respect to q(y \1 |x).Solution.Starting from L x (q) = E q 1 (y 1 |x) E q(y \1 |x) [log p(x, y)] − d i=1 E q i (y i |x) [log q i (y i |x)] (S. 10.10) we drop terms that do not depend on q 1 .We then obtain J(q 1 ) = E q 1 (y 1 |x) E q(y \1 |x) [log p(x, y)] − E q 1 (y 1 |x) [log q 1 (y 1 |x)] (S.10.11)= E q 1 (y 1 |x) log p(y 1 |x) − E q 1 (y 1 |x) [log q 1 (y 1 |x)] + const (S.10.12)= E q 1 (y 1 |x) log p(y 1 |x) q 1 (y 1 |x) (S.Explanation: We can divide the domain of p and q into the areas where p is small (zero) and those where p has significant mass.Since the objective features q in the numerator while p is in the denominator, an optimal q needs to be zero where p is zero.Otherwise, it would incur a large penalty (division by zero).Since we take the expectation with respect to q, however, regions where p > 0 do not need to be covered by q; cutting them out does not incur a penalty.Hence, optimal unimodal q only cover one peak of the bimodal p. where we have used that for zero mean x i , E q [x 2 i ] = V(x i ) = λ 2 .We similarly obtainFor h 1 = 1, h 2 = 0, we obtain φ A (h 1 = 1, h 2 = 0) = p(v 1 = 1|h 1 = 1)p(h 1 = 1)p(h 2 = 0|h 1 = 1) (S.HenceDenote the message from variable node h 2 to factor node p(h 3 |h 2 ) by α(h 2 ).Use message passing to compute α(h 2 ) for h 2 = 0 and h 2 = 1.Report the values of any intermediate messages that need to be computed for the computation of α(h 2 ).The message from h 1 to φ A is one.The message from φ A to h 2 is 7.60) = 0.3 (S. 7.61) 7.62) = 0.2 (S. 7.63) Since v 2 is not observed and p(v 2 |h 2 ) normalised, the message from p(v 2 |h 2 ) to h 2 equals one.This means that the message from h 2 to p(h 3 |h 2 ), which is α(h 2 ) equals µ φ A →h 2 (h 2 ), i.e.α(h 2 = 0) = 0.3 (S. 7.64) α(h 2 = 1) = 0.2 (S. 7.65) (e) With α(h 2 ) defined as above, use message passing to show that the predictive probability p(v 3 = 1|v 1 = 1) can be expressed in terms of α(h 2 ) as p(v 3 = 1|v 1 = 1) = xα(h 2 = 1) + yα(h 2 = 0) α(h 2 = 1) + α(h 2 = 0) (7.4)and report the values of x and y.Hence p t−1 (x 1 , . . ., x t ) = p t−1 (x 1 , . . ., x t−1 )p(x t |x t−1 ) (S. 7.106) Note that we can have an equal sign since p(x t |x t−1 ) is a pdf and hence integrates to one.This is sometimes called the "extension" since the inputs for p t−1 are extended from (x 1 , . . ., x t−1 ) to x 1 , ..7.111)This recursion, and some slight generalisations, forms the basis for what is known as the "forward recursion" in particle filtering and sequential Monte Carlo.An excellent introduction to these topics is book (Chopin and Papaspiliopoulos, 2020).(d) Use the recursion above to derive the following form of the alpha recursion:(7.12)(7.13)(change of measure) (7.14) Z t = p t−1 (x t )g t (x t )dx t (7.15) with p 0 (x 1 ) = p(x 1 ).The term p t (x t ) corresponds to α(x t ) from the alpha-recursion after normalisation.Moreover, p t−1 (x t ) is the predictive distribution for x t given observations until time t−1.Multiplying p t−1 (x t ) with g t (x t ) gives the new α(x t ).The term g t (x t ) = p(y t |x t ) is sometimes called the "correction" term.We see here that the correction has the effect of a change of measure, changing the predictive distribution p t−1 (x t ) into the filtering distribution p t (x t ).which is the desired result.The filtering result generalises to vector valued latents and visibles where the transition and emission distributions in (7.17) and (7.18) become .7.179) where N () denotes multivariate Gaussian pdfs, e.g.(S. 7.180) We then have .7.181) with θ 3 = (θ 1 3 , θ 2 3 , θ 3 3 , θ 4 3 ), and where the superscripts j of θ j 3 enumerate the different states that the parents can be in.(a) Assuming that x i has m i parents, verify that the table parametrisation of p(x i |pa i ; θ i ) is equivalent to writing p(x i |pa i ; θ i ) aswhere S i = 2 m i is the total number of states/configurations that the parents can be in, and 1(x i = 1, pa i = s) is one if x i = 1 and pa i = s, and zero otherwise.The number of configurations that m binary parents can be in is given by S i .The questions thus boils down to showing that p( .8.36) (b) For iid data D = {x (1) , . . . , x (n) } show that the likelihood can be represented aswhere n s x i =1 is the number of times the pattern (x i = 1, pa i = s) occurs in the data D, and equivalently for n s x i =0 .Since the data are iid, we have .8.38) where each term p(x (j) ; θ) factorises as in (8.8),i denoting the i-th element of x (j) and pa (j)i the corresponding parents.The conditionals p(x (j) i |pa (j) i ; θ i ) factorise further according to (8.9), p(xi =s) , (S. 8.40) We assume that the prior over the parameters of the model, (θ a , θ s , θ 1 c , . . ., θ 4 c ), factorises and is given by beta distributions with hyperparameters α 0 = 1 and β 0 = 1 (same for all parameters).Assume we observe the following iid data (each row is a data point).a s c 0 1 1 0 0 0 1 0 1 0 0 0 0 1 0 (a) Determine the posterior predictive probabilities p(a = 1|D) and p(s = 1|D).Solution.With Exercise 8.5 question (c), we have where we have used that C 1/2 is a symmetric matrix.This means that the correlation between the h can be absorbed into the factor matrix F and the set of pdfs defined by the proposed model equals the set of pdfs of the original factor analysis model.Another way to see the result is to consider the data generating process and noting that we can sample h from N (h; 0, C) by first sampling h from N (h ; 0, I) and then transforming the sample by .8.135)This follows again from the basic properties of linear transformations of Gaussians, i.e.To generate samples from the proposed factor analysis model, we would thus proceed as follows:and since h follows N (h ; 0, I), we are back at the original factor analysis model.(a) Whitening corresponds to linearly transforming a random variable x (or the corresponding data) so that the resulting random variable z has an identity covariance matrix, i.e.The matrix V is called the whitening matrix.We do not make a distributional assumption on x, in particular x may or may not be Gaussian.Given the eigenvalue decomposition C = EΛE , show thatis a whitening matrix..8.141) where we have used that E E = I.Sincewe further have .8.145) so that V is indeed a valid whitening matrix.Note that whitening matrices are not unique.For example,is also a valid whitening matrix.More generally, if V is a whitening matrix, then RV is also a whitening matrix when R is an orthonormal matrix.This is becausewhere we have used that V is a whitening matrix so that Vx has identity covariance matrix.(b) Consider the ICA model .30)where the matrix A is invertible and the h i are independent random variables of mean zero and variance one.Let V be a whitening matrix for v. Show that z = Vv follows the ICA modelwhere Ã is an orthonormal matrix.with Ã = VA.By the whitening operation, the covariance matrix of z is identity, so thatis unbiased by construction, we have to check whether its second moment is finite.Otherwise, we have an invalid estimator that behaves erratically in practice.The ratio w(x) between p(x) and q(x) equals 9.6) which can be simplified toThe second moment of w(x) under q(x) thus isThe exponential function grows more quickly than any polynomial so that the integral becomes arbitrarily large.Hence, the second moment (and the variance) of În is unbounded, which explains the erratic behaviour of the curves in the plot.A less formal but quicker way to see that, for this problem, a standard normal is a poor choice of an importance distribution is to note that its density decays more quickly than the Cauchy pdf in (9.12), which means that the standard normal pdf is "small" when the Cauchy pdf is still "large" (see Figure 9.1).This leads to large variance of the estimate.The overall conclusion is that the integral p(x)dx should not be approximated with importance sampling with a Gaussian importance distribution.The cumulative distribution function (cdf) F x (α) of a (continuous or discrete) random variable x indicates the probability that x takes on values smaller or equal to α, .14)For continuous random variables, the cdf is defined via the integralFigure 9.1: Exercise 9.2.Comparison of the log pdf of a standard normal (blue) and the Cauchy random variable (red) for positive inputs.The Cauchy pdf has much heavier tails than a Gaussian so that the Gaussian pdf is already "small" when the Cauchy pdf is still "large".where p x denotes the pdf of the random variable x (u is here a dummy variable).Note that F x maps the domain of x to the interval [0, 1].For simplicity, we here assume that F x is invertible.For a continuous random variable x with cdf F x show that the random variableImportantly, this implies that for a random variable y which is uniformly distributed on [0, 1], the transformed random variable F −1 x (y) has cdf F x .This gives rise to a method called "inverse transform sampling" to generate n iid samples of a random variable x with cdf F x .Given a target cdf F x , the method consists of:• calculating the inverse F −1 x • sampling n iid random variables uniformly distributed on [0, 1]: y (i) ∼ U(0, 1), i = 1, . . ., n.• transforming each sample by F −1 x :By construction of the method, the x (i) are n iid samples of x.We start with the cumulative distribution function (cdf) F y for y, F y (β) = P(y ≤ β).(S.9.11)Let α be the value of x that F x maps to β, i.e.F x (α) = β, which means α = F −1 x (β).Since F x is a non-decreasing function, we have(S.9.12)The cdf F y is thus given byThe exponential distribution has the densitywhere λ is a parameter of the distribution.Use inverse transform sampling to generate n iid samples from p(x; λ).We first compute the cumulative distribution function.for x, which gives:To generate samples x (i) ∼ p(x; λ), we thus first sample y (i) ∼ U (0, 1), and then set(S.9.23)Inverse transform sampling can be used to generate samples from many standard distributions.For example, it allows one to generate Gaussian random variables from uniformly distributed random variables.A Laplace random variable x of mean zero and variance one has the density p(x)Use inverse transform sampling to generate n iid samples from x.The main task is to compute the cumulative distribution function (cdf) F x of x and its inverse.The cdf is by definition(S.9.24)We first consider the case where α ≤ 0. Since −|u| = u for u ≤ 0, we have(S.9.27)For α > 0, we have .9.29) where we have used the fact that the pdf has to integrate to one.For values of u > 0, −|u| = −u, so that(S.9.32)In total, for α ∈ R, we thus have 9.33) which is positive so that the b = 1 is a minimum.The smallest value of M thus is .9.62) where e = exp(1).The maximal acceptance probability thus is .9.64)This means for each sample x generated from q(x; 1), there is chance of 0.76 that it gets accepted.In other words, for each accepted sample, we need to generate 1/0.76 = 1.32 samples from q(x; 1).The variance of the Laplace distribution for b = 1 equals 2. Hence the variance of the auxiliary distribution is larger (twice as large) as the variance of the distribution we would like to sample from.(c) Assume you sample from p(x 1 , . . ., x d ) = d i=1 p(x i ) using q(x 1 , . . ., x d ) = d i=1 q(x i ; b) as auxiliary distribution without exploiting any independencies.How does the acceptance probability scale as a function of d?You may denote the acceptance probability in case of d = 1 by A.We have to determine the maximal ratiox 1 ,...,x d p(x 1 , . . ., x d ) q(x 1 , . . ., x d ) (S.9.65)Plugging-in the factorisation givesHence, the acceptance probability is .9.70)Note that A ≤ 1 since it is a probability.This means that, unless A = 1, we have an acceptance probability that decays exponentially in the number of dimensions if the target and auxiliary distributions factorise and we do not exploit the independencies.The restricted Boltzmann machine (RBM) is a model for binary variables v = (v 1 , . . ., v n ) and h = (h 1 , . . ., h m ) which asserts that the joint distribution of (v, h) can be described by the probability mass functionwhere W is a n × m matrix, and a and b vectors of size n and m, respectively.Both the v i and h i take values in {0, 1}.The v i are called the "visibles" variables since they are assumed to be observed while the h i are the hidden variables since it is assumed that we cannot measure them.Explain how to use Gibbs sampling to generate samples from the marginal p(v), .20)for any given values of W, a, and b.Hint: You may use thatIn order to generate samples v (k) from p(v) we generate samples (v (k) , h (k) ) from p(v, h) and then ignore the h (k) .Gibbs sampling is a MCMC method to produce a sequence of samples x (1) , x (2) , x (3) , . . .that follow a pdf/pmf p(x) (if the chain is run long enough).Assuming that x is ddimensional, we generate the next sample x (k+1) in the sequence from the previous sample x (k) by: 1. picking (randomly) an index i ∈ {1, . . ., d}For the RBM, the tuple (h, v) corresponds to x so that a x i in the above steps can either be a hidden variable or a visible.Hence .9.71) (h \i denotes the vector h with element h i removed, and equivalently for v \i ) have high variance, i.e. its value would depend strongly on the values of the 20 samples used to compute the average.• Figure 9.4b highlights the first 20 samples obtained by the chain when starting at (x = 7, y = 7).One can clearly see the "burn-in" tail which slowly approaches the region where most of the probability mass is.(d) In practice, we don't know where the distribution we wish to sample from has high density, so we typically initialise the Markov Chain somewhat arbitrarily, or at the maximum a-posterior (MAP) sample if available.The samples obtained in the beginning of the chain are typically discarded, as they are not considered to be representative of the target distribution.This initial period between initialisation and starting to collect samples is called "warm-up", or also "burn-in".Extended your function mh to include an additional warm-up argument W , which specifies the number of MCMC steps taken before starting to collect samples.Your function should still return a list of S samples as in (b).We can extend the mh function with a warm-up argument by, for example, iterating for num_samples + warmup steps, and start recording samples only after the warm-up period: A scatter plot showing 5, 000 samples from the posterior is shown on Figure 9.5.The posterior mean of α is 0.84, the posterior mean of β is -0.2, and posterior correlation coefficient is -0.63.Note that the numerical values are sample-specific.Under weak conditions, an MCMC algorithm is an asymptotically exact inference algorithm, meaning that if it is run forever, it will generate samples that correspond to the desired probability distribution.In this case, the chain is said to converge.In practice, we want to run the algorithm long enough to be able to approximate the posterior adequately.How long is long enough for the chain to converge varies drastically depending on the algorithm, the hyperparameters (e.g. the variance vari), and the target posterior distribution.It is impossible to determine exactly whether the chain has run long enough, but there exist various diagnostics that can help us determine if we can "trust" the sample-based approximation to the posterior.A very quick and common way of assessing convergence of the Markov chain is to visually inspect the trace plots for each parameter.A trace plot shows how the drawn samples evolve through time, i.e. they are a time-series of the samples generated by the Markov chain.Figure 9.6 shows examples of trace plots obtained by running the Metropolis Hastings algorithm for different values of the hyperparameters vari and param_init.Ideally, the time series covers the whole domain of the target distribution and it is hard to "see" any structure in it so that predicting values of future samples from the current one is difficult.If so, the samples are likely independent from each other and the chain is said to be well "mixed".(a) Consider the trace plots in Figure 9.6: Is the variance vari used in Figure 9.6b larger or smaller than the value of vari used in Figure 9.6a?Is vari used in Figure 9.6c larger or smaller than the value used in Figure 9.6a?In both cases, explain the behaviour of the trace plots in terms of the workings of the Metropolis Hastings algorithm and the effect of the variance vari.Solution.MCMC methods are sensitive to different hyperparameters, and we usually need to carefully diagnose the inference results to ensure that our algorithm adequately approximates the target posterior distribution.(i) Figure 9.6b uses a small variance (vari was set to 0.001) .The trace plots show that the samples for β are very highly correlated and evolve very slowly through time.This is because the introduced randomness is quite small compared to the scale of the posterior, thus the proposed sample at each MCMC iteration will be very close to the current sample and hence likely accepted.More mathematical explanation: for a symmetric proposal distribution, the acceptance ratio a becomes a = p * (θ * ) p * (θ) , (S.9.77)where θ is the current sample and θ * is the proposed sample.For variances that are small compared to the (squared) scale of the posterior, a is close to one and the proposed sample θ * gets likely accepted.This then gives rise to the slowly changing time series shown in Figure 9.6b.(ii) In Figure 9.6c, the variance is larger than the reference (vari was set to 50) .The trace plots suggest that many iterations of the algorithm result in the proposed sample being rejected, and thus we end up copying the same sample over and over again.This is because if the random perturbations are large compared to the scale of the posterior, p * (θ * ) may be very different from p * (θ) and a may be very small.(b) In Metropolis-Hastings, and MCMC in general, any sample depends on the previously generated sample, and hence the algorithm generates samples that are generally statistically dependent.The effective sample size of a sequence of dependent samples is the number of independent samples that are, in some sense, equivalent to our number of dependent samples.A definition of the effective sample size (ESS) iswhere S is the number of dependent samples drawn and ρ(k) the correlation coefficient between two samples in the Markov chain that are k time points apart.We can Hence argmax .10.15) (c) Conclude that given q i (y i |x), i = 2, . . ., d, the optimal q 1 (y 1 |x) equals p(y 1 |x).This then leads to an iterative updating scheme where we cycle through the different dimensions, each time updating the corresponding marginal variational distribution according to:where q(y \i |x) = j =i q(y j |x) is the product of all marginals without marginal q i (y i |x).Solution.This follows immediately from the fact that the KL divergence is minimised when q 1 (y 1 |x) = p(y 1 |x).Side-note: The iterative update rule can be considered to be coordinate ascent optimisation in function space, where each "coordinate" corresponds to a q i (y i |x).Assume random variables y 1 , y 2 , x are generated according to the following process y 1 ∼ N (y 1 ; 0, 1) y 2 ∼ N (y 2 ; 0, 1) (10.8) n ∼ N (n; 0, 1) x = y 1 + y 2 + n (10.9)where y 1 , y 2 , n are statistically independent.(a) y 1 , y 2 , x are jointly Gaussian.Determine their mean and their covariance matrix.The expected value of y 1 and y 2 is zero.By linearity of expectation, the expected value of x is .10.16)The variance of y 1 and y 2 is 1.Since y 1 , y 2 , n are statistically independent, .10.17)The covariance between y 1 and x is .10.18) .10.20) = 1 + 0 + 0 (S.10.21) where we have used that y 1 and x have zero mean and the independence assumptions.The covariance between y 2 and x is computed in the same way and equals 1 too.We thus obtain the covariance matrix Σ Σ Σ,Since x is the sum of three random variables that have the same distribution, it makes intuitive sense that the mean assigns 1/3 of the observed value of x to y 1 and y 2 .Moreover, y 1 and y 2 are negatively corrected since an increase in y 1 must be compensated with a decrease in y 2 .Let us now approximate the posterior p(y 1 , y 2 |x) with mean field variational inference.Determine the optimal variational distribution using the method and results from Exercise 10.1.You may use thatSolution.The mean field assumption means that the variational distribution is assumed to factorise as q(y 1 , y 2 |x) = q 1 (y 1 |x)q 2 (y 2 |x) (S. 10.23) From Exercise 10.1, the optimal q 1 (y 1 |x) and q 2 (y 2 |x) satisfy .10.24) .10.25)Note that these are coupled equations: q 2 features in the equation for q 1 via p(y 1 |x), and q 1 features in the equation for q 2 via p(y 2 |x).But we have two equations for two unknowns, which for the Gaussian joint model p(x, y 1 , y 2 ) can be solved in closed form.Given the provided equation for p(y 1 , y 2 , x), we have that  .10.32) where we have absorbed all terms not involving y 1 into the constant.Moreover, we set E q 2 (y 2 |x) [y 2 ] = m 2 .Note that an arbitrary Gaussian density N (y; m, σ 2 ) with mean m and variance σ 2 can be written in the log-domain as .10.34)Comparison with (S. 10.32) shows that p(y 1 |x), and hence q 1 (y 1 |x), is Gaussian with variance and mean equal to .10.35)Note that we have not made a Gaussianity assumption on q 1 (y 1 |x).The optimal q 1 (y 1 |x) turns out to be Gaussian because the model p(y 1 , y 2 , x) is Gaussian.The equation for p(y 2 |x) gives similarly .10.40) where we have absorbed all terms not involving y 2 into the constant.Moreover, we set E q 1 (y 1 |x) [y 1 ] = m 1 .With (S. 10.34), this is defines a Gaussian distribution with variance and mean equal to .10.41) Hence the optimal marginal variational distributions q 1 (y 1 |x) and q 2 (y 2 |x) are both Gaussian with variance equal to 1/2.Their means satisfy .10.42)These are two equations for two unknowns.We can solve them as follows .10.44) .10.45) .10.46) .10.48)In summary, we find .10.49) and the optimal variational distribution q(y 1 , y 2 |x) = q 1 (y 1 |x)q 2 (y 2 |x) is Gaussian.We have made the mean field (independence) assumption but not the Gaussianity assumption.Gaussianity of the variational distribution is a consequence of the Gaussianity of the model p(y 1 , y 2 , x).Comparison with the true posterior shows that the mean field variational distribution q(y 1 , y 2 |x) has the same mean but ignores the correlation and underestimates the marginal variances.The true posterior and the mean field approximation are shown in Figure 10.1.We have seen that maximising the evidence lower bound (ELBO) with respect to the variational distribution q minimises the Kullback-Leibler divergence to the true posterior p.We here assume that q and p are probability density functions so that the Kullback-Leibler divergence between them is defined as KL(q||p) = q(x) log q(x) p(x) dx = E q log q(x) p(x) .(10.12) (a) You can here assume that x is one-dimensional so that p and q are univariate densities.Consider the case where p is a bimodal density but the variational densities q are unimodal.Sketch a figure that shows p and a variational distribution q that has been learned by minimising KL(q||p).Explain qualitatively why the sketched q minimises KL(q||p).Assume further that the variational density q(x 1 , x 2 ; λ 2 ) is parametrised as q(x 1 , x 2 ; λ 2 ) = 1 2πλ 2 exp − x 2 1 + x 2 2 2λ 2 (10.14)where λ 2 is the variational parameter that is learned by minimising KL(q||p).If σ 2 2 is much larger than σ 2 1 , do you expect λ 2 to be closer to σ 2 2 or to σ 2 1 ?Provide an explanation.The learned variational parameter will be closer to σ 2 1 (the smaller of the two σ 2 i ).Explanation: First note that the σ 2 i are the variances along the two different axes, and that λ 2 is the single variance for both x 1 and x 2 .The objective penalises q if it is non-zero where p is zero (see above).The variational parameter λ 2 thus will get adjusted during learning so that the variance of q is close to the smallest of the two σ 2 i .We have seen that maximising the evidence lower bound (ELBO) with respect to the variational distribution minimises the Kullback-Leibler divergence to the true posterior.We here investigate the nature of the approximation if the family of variational distributions does not include the true posterior.(a) Assume that the true posterior for x = (x 1 , x 2 ) is given by p(x) = N (x 1 ; σ 2 1 )N (x 2 ; σ 2 2 ) (10.15) and that our variational distribution q(x; λ 2 ) is q(x; λ 2 ) = N (x 1 ; λ 2 )N (x 2 ; λ 2 ), (10.16)where λ > 0 is the variational parameter.Provide an equation for J(λ) = KL(q(x; λ 2 )||p(x)), (10.17)where you can omit additive terms that do not depend on λ.Solution.We write KL(q(x; λ 2 )||p(x)) = E q log q(x; λ 2 ) p(x) (S.10.50)= E q log q(x; λ 2 ) − E q log p(x) (S.10.51)= E q log N (x 1 ; λ 2 ) + E q log N (x 2 ; λ 2 ) − E q log N (x 1 ; σ 2 1 ) − E q log N (x 2 ; σ 2 2 ) (S. 10.52) .10.66)This is a minimum because the second derivative of J(λ) 10.67) is positive for all λ > 0.The result has an intuitive explanation: the optimal variance λ 2 is the harmonic mean of the variances σ 2 i of the true posterior.In other words, the optimal precision 1/λ 2 is given by the average of the precisions 1/σ 2 i of the two dimensions.If the variances are not equal, e.g. if σ 2 2 > σ 2 1 , we see that the optimal variance of the variational distribution strikes a compromise between two types of penalties in the KL-divergence: the penalty of having a bad fit because the variational distribution along dimension two is too narrow; and along dimension one, the penalty for the variational distribution to be nonzero when p is small.The message from h 1 to φ A is one.The message from φ A to h 2 is 7.60) = 0.3 (S. 7.61) 7.62) = 0.2 (S. 7.63) Since v 2 is not observed and p(v 2 |h 2 ) normalised, the message from p(v 2 |h 2 ) to h 2 equals one.This means that the message from h 2 to p(h 3 |h 2 ), which is α(h 2 ) equals µ φ A →h 2 (h 2 ), i.e.α(h 2 = 0) = 0.3 (S. 7.64) α(h 2 = 1) = 0.2 (S. 7.65) (e) With α(h 2 ) defined as above, use message passing to show that the predictive probability p(v 3 = 1|v 1 = 1) can be expressed in terms of α(h 2 ) as p(v 3 = 1|v 1 = 1) = xα(h 2 = 1) + yα(h 2 = 0) α(h 2 = 1) + α(h 2 = 0) (7.4)and report the values of x and y.Hence p t−1 (x 1 , . . ., x t ) = p t−1 (x 1 , . . ., x t−1 )p(x t |x t−1 ) (S. 7.106) Note that we can have an equal sign since p(x t |x t−1 ) is a pdf and hence integrates to one.This is sometimes called the "extension" since the inputs for p t−1 are extended from (x 1 , . . ., x t−1 ) to x 1 , ..7.111)This recursion, and some slight generalisations, forms the basis for what is known as the "forward recursion" in particle filtering and sequential Monte Carlo.An excellent introduction to these topics is book (Chopin and Papaspiliopoulos, 2020).(d) Use the recursion above to derive the following form of the alpha recursion:(7.12)(7.13)(change of measure) (7.14) Z t = p t−1 (x t )g t (x t )dx t (7.15) with p 0 (x 1 ) = p(x 1 ).The term p t (x t ) corresponds to α(x t ) from the alpha-recursion after normalisation.Moreover, p t−1 (x t ) is the predictive distribution for x t given observations until time t−1.Multiplying p t−1 (x t ) with g t (x t ) gives the new α(x t ).The term g t (x t ) = p(y t |x t ) is sometimes called the "correction" term.We see here that the correction has the effect of a change of measure, changing the predictive distribution p t−1 (x t ) into the filtering distribution p t (x t ).which is the desired result.The filtering result generalises to vector valued latents and visibles where the transition and emission distributions in (7.17) and (7.18) become .7.179) where N () denotes multivariate Gaussian pdfs, e.g.(S. 7.180) We then have .7.181) with θ 3 = (θ 1 3 , θ 2 3 , θ 3 3 , θ 4 3 ), and where the superscripts j of θ j 3 enumerate the different states that the parents can be in.(a) Assuming that x i has m i parents, verify that the table parametrisation of p(x i |pa i ; θ i ) is equivalent to writing p(x i |pa i ; θ i ) aswhere S i = 2 m i is the total number of states/configurations that the parents can be in, and 1(x i = 1, pa i = s) is one if x i = 1 and pa i = s, and zero otherwise.The number of configurations that m binary parents can be in is given by S i .The questions thus boils down to showing that p( .8.36) (b) For iid data D = {x (1) , . . . , x (n) } show that the likelihood can be represented aswhere n s x i =1 is the number of times the pattern (x i = 1, pa i = s) occurs in the data D, and equivalently for n s x i =0 .Since the data are iid, we have .8.38) where each term p(x (j) ; θ) factorises as in (8.8),i denoting the i-th element of x (j) and pa (j)i the corresponding parents.The conditionals p(x (j) i |pa (j) i ; θ i ) factorise further according to (8.9), p(xi =s) , (S. 8.40) We assume that the prior over the parameters of the model, (θ a , θ s , θ 1 c , . . ., θ 4 c ), factorises and is given by beta distributions with hyperparameters α 0 = 1 and β 0 = 1 (same for all parameters).Assume we observe the following iid data (each row is a data point).a s c 0 1 1 0 0 0 1 0 1 0 0 0 0 1 0 (a) Determine the posterior predictive probabilities p(a = 1|D) and p(s = 1|D).Solution.With Exercise 8.5 question (c), we have where we have used that C 1/2 is a symmetric matrix.This means that the correlation between the h can be absorbed into the factor matrix F and the set of pdfs defined by the proposed model equals the set of pdfs of the original factor analysis model.Another way to see the result is to consider the data generating process and noting that we can sample h from N (h; 0, C) by first sampling h from N (h ; 0, I) and then transforming the sample by .8.135)This follows again from the basic properties of linear transformations of Gaussians, i.e.To generate samples from the proposed factor analysis model, we would thus proceed as follows:and since h follows N (h ; 0, I), we are back at the original factor analysis model.(a) Whitening corresponds to linearly transforming a random variable x (or the corresponding data) so that the resulting random variable z has an identity covariance matrix, i.e.The matrix V is called the whitening matrix.We do not make a distributional assumption on x, in particular x may or may not be Gaussian.Given the eigenvalue decomposition C = EΛE , show thatis a whitening matrix..8.141) where we have used that E E = I.Sincewe further have .8.145) so that V is indeed a valid whitening matrix.Note that whitening matrices are not unique.For example,is also a valid whitening matrix.More generally, if V is a whitening matrix, then RV is also a whitening matrix when R is an orthonormal matrix.This is becausewhere we have used that V is a whitening matrix so that Vx has identity covariance matrix.(b) Consider the ICA model .30)where the matrix A is invertible and the h i are independent random variables of mean zero and variance one.Let V be a whitening matrix for v. Show that z = Vv follows the ICA modelwhere Ã is an orthonormal matrix.with Ã = VA.By the whitening operation, the covariance matrix of z is identity, so thatis unbiased by construction, we have to check whether its second moment is finite.Otherwise, we have an invalid estimator that behaves erratically in practice.The ratio w(x) between p(x) and q(x) equals 9.6) which can be simplified toThe second moment of w(x) under q(x) thus isThe exponential function grows more quickly than any polynomial so that the integral becomes arbitrarily large.Hence, the second moment (and the variance) of În is unbounded, which explains the erratic behaviour of the curves in the plot.A less formal but quicker way to see that, for this problem, a standard normal is a poor choice of an importance distribution is to note that its density decays more quickly than the Cauchy pdf in (9.12), which means that the standard normal pdf is "small" when the Cauchy pdf is still "large" (see Figure 9.1).This leads to large variance of the estimate.The overall conclusion is that the integral p(x)dx should not be approximated with importance sampling with a Gaussian importance distribution.The cumulative distribution function (cdf) F x (α) of a (continuous or discrete) random variable x indicates the probability that x takes on values smaller or equal to α, .14)For continuous random variables, the cdf is defined via the integralFigure 9.1: Exercise 9.2.Comparison of the log pdf of a standard normal (blue) and the Cauchy random variable (red) for positive inputs.The Cauchy pdf has much heavier tails than a Gaussian so that the Gaussian pdf is already "small" when the Cauchy pdf is still "large".where p x denotes the pdf of the random variable x (u is here a dummy variable).Note that F x maps the domain of x to the interval [0, 1].For simplicity, we here assume that F x is invertible.For a continuous random variable x with cdf F x show that the random variableImportantly, this implies that for a random variable y which is uniformly distributed on [0, 1], the transformed random variable F −1 x (y) has cdf F x .This gives rise to a method called "inverse transform sampling" to generate n iid samples of a random variable x with cdf F x .Given a target cdf F x , the method consists of:• calculating the inverse F −1 x • sampling n iid random variables uniformly distributed on [0, 1]: y (i) ∼ U(0, 1), i = 1, . . ., n.• transforming each sample by F −1 x :By construction of the method, the x (i) are n iid samples of x.We start with the cumulative distribution function (cdf) F y for y, F y (β) = P(y ≤ β).(S.9.11)Let α be the value of x that F x maps to β, i.e.F x (α) = β, which means α = F −1 x (β).Since F x is a non-decreasing function, we have(S.9.12)The cdf F y is thus given byThe exponential distribution has the densitywhere λ is a parameter of the distribution.Use inverse transform sampling to generate n iid samples from p(x; λ).We first compute the cumulative distribution function.for x, which gives:To generate samples x (i) ∼ p(x; λ), we thus first sample y (i) ∼ U (0, 1), and then set(S.9.23)Inverse transform sampling can be used to generate samples from many standard distributions.For example, it allows one to generate Gaussian random variables from uniformly distributed random variables.A Laplace random variable x of mean zero and variance one has the density p(x)Use inverse transform sampling to generate n iid samples from x.The main task is to compute the cumulative distribution function (cdf) F x of x and its inverse.The cdf is by definition(S.9.24)We first consider the case where α ≤ 0. Since −|u| = u for u ≤ 0, we have(S.9.27)For α > 0, we have .9.29) where we have used the fact that the pdf has to integrate to one.For values of u > 0, −|u| = −u, so that(S.9.32)In total, for α ∈ R, we thus have 9.33) which is positive so that the b = 1 is a minimum.The smallest value of M thus is .9.62) where e = exp(1).The maximal acceptance probability thus is .9.64)This means for each sample x generated from q(x; 1), there is chance of 0.76 that it gets accepted.In other words, for each accepted sample, we need to generate 1/0.76 = 1.32 samples from q(x; 1).The variance of the Laplace distribution for b = 1 equals 2. Hence the variance of the auxiliary distribution is larger (twice as large) as the variance of the distribution we would like to sample from.(c) Assume you sample from p(x 1 , . . ., x d ) = d i=1 p(x i ) using q(x 1 , . . ., x d ) = d i=1 q(x i ; b) as auxiliary distribution without exploiting any independencies.How does the acceptance probability scale as a function of d?You may denote the acceptance probability in case of d = 1 by A.We have to determine the maximal ratiox 1 ,...,x d p(x 1 , . . ., x d ) q(x 1 , . . ., x d ) (S.9.65)Plugging-in the factorisation givesHence, the acceptance probability is .9.70)Note that A ≤ 1 since it is a probability.This means that, unless A = 1, we have an acceptance probability that decays exponentially in the number of dimensions if the target and auxiliary distributions factorise and we do not exploit the independencies.The restricted Boltzmann machine (RBM) is a model for binary variables v = (v 1 , . . ., v n ) and h = (h 1 , . . ., h m ) which asserts that the joint distribution of (v, h) can be described by the probability mass functionwhere W is a n × m matrix, and a and b vectors of size n and m, respectively.Both the v i and h i take values in {0, 1}.The v i are called the "visibles" variables since they are assumed to be observed while the h i are the hidden variables since it is assumed that we cannot measure them.Explain how to use Gibbs sampling to generate samples from the marginal p(v), .20)for any given values of W, a, and b.Hint: You may use thatIn order to generate samples v (k) from p(v) we generate samples (v (k) , h (k) ) from p(v, h) and then ignore the h (k) .Gibbs sampling is a MCMC method to produce a sequence of samples x (1) , x (2) , x (3) , . . .that follow a pdf/pmf p(x) (if the chain is run long enough).Assuming that x is ddimensional, we generate the next sample x (k+1) in the sequence from the previous sample x (k) by: 1. picking (randomly) an index i ∈ {1, . . ., d}For the RBM, the tuple (h, v) corresponds to x so that a x i in the above steps can either be a hidden variable or a visible.Hence .9.71) (h \i denotes the vector h with element h i removed, and equivalently for v \i ) have high variance, i.e. its value would depend strongly on the values of the 20 samples used to compute the average.• Figure 9.4b highlights the first 20 samples obtained by the chain when starting at (x = 7, y = 7).One can clearly see the "burn-in" tail which slowly approaches the region where most of the probability mass is.(d) In practice, we don't know where the distribution we wish to sample from has high density, so we typically initialise the Markov Chain somewhat arbitrarily, or at the maximum a-posterior (MAP) sample if available.The samples obtained in the beginning of the chain are typically discarded, as they are not considered to be representative of the target distribution.This initial period between initialisation and starting to collect samples is called "warm-up", or also "burn-in".Extended your function mh to include an additional warm-up argument W , which specifies the number of MCMC steps taken before starting to collect samples.Your function should still return a list of S samples as in (b).We can extend the mh function with a warm-up argument by, for example, iterating for num_samples + warmup steps, and start recording samples only after the warm-up period: A scatter plot showing 5, 000 samples from the posterior is shown on Figure 9.5.The posterior mean of α is 0.84, the posterior mean of β is -0.2, and posterior correlation coefficient is -0.63.Note that the numerical values are sample-specific.Under weak conditions, an MCMC algorithm is an asymptotically exact inference algorithm, meaning that if it is run forever, it will generate samples that correspond to the desired probability distribution.In this case, the chain is said to converge.In practice, we want to run the algorithm long enough to be able to approximate the posterior adequately.How long is long enough for the chain to converge varies drastically depending on the algorithm, the hyperparameters (e.g. the variance vari), and the target posterior distribution.It is impossible to determine exactly whether the chain has run long enough, but there exist various diagnostics that can help us determine if we can "trust" the sample-based approximation to the posterior.A very quick and common way of assessing convergence of the Markov chain is to visually inspect the trace plots for each parameter.A trace plot shows how the drawn samples evolve through time, i.e. they are a time-series of the samples generated by the Markov chain.Figure 9.6 shows examples of trace plots obtained by running the Metropolis Hastings algorithm for different values of the hyperparameters vari and param_init.Ideally, the time series covers the whole domain of the target distribution and it is hard to "see" any structure in it so that predicting values of future samples from the current one is difficult.If so, the samples are likely independent from each other and the chain is said to be well "mixed".(a) Consider the trace plots in Figure 9.6: Is the variance vari used in Figure 9.6b larger or smaller than the value of vari used in Figure 9.6a?Is vari used in Figure 9.6c larger or smaller than the value used in Figure 9.6a?In both cases, explain the behaviour of the trace plots in terms of the workings of the Metropolis Hastings algorithm and the effect of the variance vari.Solution.MCMC methods are sensitive to different hyperparameters, and we usually need to carefully diagnose the inference results to ensure that our algorithm adequately approximates the target posterior distribution.(i) Figure 9.6b uses a small variance (vari was set to 0.001) .The trace plots show that the samples for β are very highly correlated and evolve very slowly through time.This is because the introduced randomness is quite small compared to the scale of the posterior, thus the proposed sample at each MCMC iteration will be very close to the current sample and hence likely accepted.More mathematical explanation: for a symmetric proposal distribution, the acceptance ratio a becomes a = p * (θ * ) p * (θ) , (S.9.77)where θ is the current sample and θ * is the proposed sample.For variances that are small compared to the (squared) scale of the posterior, a is close to one and the proposed sample θ * gets likely accepted.This then gives rise to the slowly changing time series shown in Figure 9.6b.(ii) In Figure 9.6c, the variance is larger than the reference (vari was set to 50) .The trace plots suggest that many iterations of the algorithm result in the proposed sample being rejected, and thus we end up copying the same sample over and over again.This is because if the random perturbations are large compared to the scale of the posterior, p * (θ * ) may be very different from p * (θ) and a may be very small.(b) In Metropolis-Hastings, and MCMC in general, any sample depends on the previously generated sample, and hence the algorithm generates samples that are generally statistically dependent.The effective sample size of a sequence of dependent samples is the number of independent samples that are, in some sense, equivalent to our number of dependent samples.A definition of the effective sample size (ESS) iswhere S is the number of dependent samples drawn and ρ(k) the correlation coefficient between two samples in the Markov chain that are k time points apart.We can Hence argmax .10.15) (c) Conclude that given q i (y i |x), i = 2, . . ., d, the optimal q 1 (y 1 |x) equals p(y 1 |x).This then leads to an iterative updating scheme where we cycle through the different dimensions, each time updating the corresponding marginal variational distribution according to:where q(y \i |x) = j =i q(y j |x) is the product of all marginals without marginal q i (y i |x).Solution.This follows immediately from the fact that the KL divergence is minimised when q 1 (y 1 |x) = p(y 1 |x).Side-note: The iterative update rule can be considered to be coordinate ascent optimisation in function space, where each "coordinate" corresponds to a q i (y i |x).Assume random variables y 1 , y 2 , x are generated according to the following process y 1 ∼ N (y 1 ; 0, 1) y 2 ∼ N (y 2 ; 0, 1) (10.8) n ∼ N (n; 0, 1) x = y 1 + y 2 + n (10.9)where y 1 , y 2 , n are statistically independent.(a) y 1 , y 2 , x are jointly Gaussian.Determine their mean and their covariance matrix.The expected value of y 1 and y 2 is zero.By linearity of expectation, the expected value of x is .10.16)The variance of y 1 and y 2 is 1.Since y 1 , y 2 , n are statistically independent, .10.17)The covariance between y 1 and x is .10.18) .10.20) = 1 + 0 + 0 (S.10.21) where we have used that y 1 and x have zero mean and the independence assumptions.The covariance between y 2 and x is computed in the same way and equals 1 too.We thus obtain the covariance matrix Σ Σ Σ,Since x is the sum of three random variables that have the same distribution, it makes intuitive sense that the mean assigns 1/3 of the observed value of x to y 1 and y 2 .Moreover, y 1 and y 2 are negatively corrected since an increase in y 1 must be compensated with a decrease in y 2 .Let us now approximate the posterior p(y 1 , y 2 |x) with mean field variational inference.Determine the optimal variational distribution using the method and results from Exercise 10.1.You may use thatSolution.The mean field assumption means that the variational distribution is assumed to factorise as q(y 1 , y 2 |x) = q 1 (y 1 |x)q 2 (y 2 |x) (S. 10.23) From Exercise 10.1, the optimal q 1 (y 1 |x) and q 2 (y 2 |x) satisfy .10.24) .10.25)Note that these are coupled equations: q 2 features in the equation for q 1 via p(y 1 |x), and q 1 features in the equation for q 2 via p(y 2 |x).But we have two equations for two unknowns, which for the Gaussian joint model p(x, y 1 , y 2 ) can be solved in closed form.Given the provided equation for p(y 1 , y 2 , x), we have that  .10.32) where we have absorbed all terms not involving y 1 into the constant.Moreover, we set E q 2 (y 2 |x) [y 2 ] = m 2 .Note that an arbitrary Gaussian density N (y; m, σ 2 ) with mean m and variance σ 2 can be written in the log-domain as .10.34)Comparison with (S. 10.32) shows that p(y 1 |x), and hence q 1 (y 1 |x), is Gaussian with variance and mean equal to .10.35)Note that we have not made a Gaussianity assumption on q 1 (y 1 |x).The optimal q 1 (y 1 |x) turns out to be Gaussian because the model p(y 1 , y 2 , x) is Gaussian.The equation for p(y 2 |x) gives similarly .10.40) where we have absorbed all terms not involving y 2 into the constant.Moreover, we set E q 1 (y 1 |x) [y 1 ] = m 1 .With (S. 10.34), this is defines a Gaussian distribution with variance and mean equal to .10.41) Hence the optimal marginal variational distributions q 1 (y 1 |x) and q 2 (y 2 |x) are both Gaussian with variance equal to 1/2.Their means satisfy .10.42)These are two equations for two unknowns.We can solve them as follows .10.44) .10.45) .10.46) .10.48)In summary, we find .10.49) and the optimal variational distribution q(y 1 , y 2 |x) = q 1 (y 1 |x)q 2 (y 2 |x) is Gaussian.We have made the mean field (independence) assumption but not the Gaussianity assumption.Gaussianity of the variational distribution is a consequence of the Gaussianity of the model p(y 1 , y 2 , x).Comparison with the true posterior shows that the mean field variational distribution q(y 1 , y 2 |x) has the same mean but ignores the correlation and underestimates the marginal variances.The true posterior and the mean field approximation are shown in Figure 10.1.We have seen that maximising the evidence lower bound (ELBO) with respect to the variational distribution q minimises the Kullback-Leibler divergence to the true posterior p.We here assume that q and p are probability density functions so that the Kullback-Leibler divergence between them is defined as KL(q||p) = q(x) log q(x) p(x) dx = E q log q(x) p(x) .(10.12) (a) You can here assume that x is one-dimensional so that p and q are univariate densities.Consider the case where p is a bimodal density but the variational densities q are unimodal.Sketch a figure that shows p and a variational distribution q that has been learned by minimising KL(q||p).Explain qualitatively why the sketched q minimises KL(q||p).Assume further that the variational density q(x 1 , x 2 ; λ 2 ) is parametrised as q(x 1 , x 2 ; λ 2 ) = 1 2πλ 2 exp − x 2 1 + x 2 2 2λ 2 (10.14)where λ 2 is the variational parameter that is learned by minimising KL(q||p).If σ 2 2 is much larger than σ 2 1 , do you expect λ 2 to be closer to σ 2 2 or to σ 2 1 ?Provide an explanation.The learned variational parameter will be closer to σ 2 1 (the smaller of the two σ 2 i ).Explanation: First note that the σ 2 i are the variances along the two different axes, and that λ 2 is the single variance for both x 1 and x 2 .The objective penalises q if it is non-zero where p is zero (see above).The variational parameter λ 2 thus will get adjusted during learning so that the variance of q is close to the smallest of the two σ 2 i .We have seen that maximising the evidence lower bound (ELBO) with respect to the variational distribution minimises the Kullback-Leibler divergence to the true posterior.We here investigate the nature of the approximation if the family of variational distributions does not include the true posterior.(a) Assume that the true posterior for x = (x 1 , x 2 ) is given by p(x) = N (x 1 ; σ 2 1 )N (x 2 ; σ 2 2 ) (10.15) and that our variational distribution q(x; λ 2 ) is q(x; λ 2 ) = N (x 1 ; λ 2 )N (x 2 ; λ 2 ), (10.16)where λ > 0 is the variational parameter.Provide an equation for J(λ) = KL(q(x; λ 2 )||p(x)), (10.17)where you can omit additive terms that do not depend on λ.Solution.We write KL(q(x; λ 2 )||p(x)) = E q log q(x; λ 2 ) p(x) (S.10.50)= E q log q(x; λ 2 ) − E q log p(x) (S.10.51)= E q log N (x 1 ; λ 2 ) + E q log N (x 2 ; λ 2 ) − E q log N (x 1 ; σ 2 1 ) − E q log N (x 2 ; σ 2 2 ) (S. 10.52) .10.66)This is a minimum because the second derivative of J(λ) 10.67) is positive for all λ > 0.The result has an intuitive explanation: the optimal variance λ 2 is the harmonic mean of the variances σ 2 i of the true posterior.In other words, the optimal precision 1/λ 2 is given by the average of the precisions 1/σ 2 i of the two dimensions.If the variances are not equal, e.g. if σ 2 2 > σ 2 1 , we see that the optimal variance of the variational distribution strikes a compromise between two types of penalties in the KL-divergence: the penalty of having a bad fit because the variational distribution along dimension two is too narrow; and along dimension one, the penalty for the variational distribution to be nonzero when p is small.