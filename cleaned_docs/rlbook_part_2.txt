Approximation in Value Space -Rollout Algorithms Chap. 2produced by the base heuristic starting from ỹ0 , i.e., G(ỹ N ) ≤ G R(ỹ 0 ) .Note that T k (ỹ k , u k ) is not guaranteed to be feasible for any given u k (i.e., may not belong to C), but we will assume that the constraint set U k (ỹ k ) of problem (2.42) is nonempty, so that our rollout algorithm is welldefined.We will later modify our algorithm so that it is well-defined under the weaker assumption that just the complete trajectory generated by the base heuristic starting from the initial state ỹ0 is feasible, i.e., R(ỹ 0 ) ∈ C.The algorithm starts at stage 0 and sequentially proceeds to the last stage.At the typical stage k, it has constructed a partial trajectory ỹk = (x 0 , ũ0 , x1 , . . ., ũk−1 , xk )(2.43) that starts at the given initial state ỹ0 = x0 , and is such thatand it is optimal to stop at time k for states x in the setConsider now the rather primitive base policy π, whereby we stop at every state x.Thus we have for all x k and k,The rollout policy is stationary and can be computed on-line relatively easily, since J k,π is available in closed form.In particular, the rollout policy is to stop ati.e., if x k is in the set SN−1, and otherwise to continue.The rollout policy also has an intuitive interpretation: it stops at the states for which it is better to stop rather than continue for one more stage and then stop.A policy of this type turns out to be optimal in several types of stopping applications.Let us provide a condition that guarantees its optimality.We have from the DP Eqs.(2.67)-(2.68),and using this fact in the DP equation (2.68), we obtain inductivelyfor all x and k.Using this fact and the definition of S k we see that(2.69)We will now consider a condition guaranteeing that all the stopping sets S k are equal.Suppose that the set SN−1 is absorbing in the sense that if a state belongs to SN−1 and we decide to continue, the next state will also be in SN−1: f (x, w) ∈ SN−1, for all x ∈ SN−1, w.(2.70)We will show that equality holds in Eq. (2.69) and for all k we haveIndeed, by the definition of SN−1, we havefor all x ∈ SN−1,We will now introduce sequential consistency and sequential improvement conditions guaranteeing that the control set U k (ỹ k ) in the minimization (2.44) is nonempty, and that the costs of the complete trajectories T k (ỹ k , ũk ) are improving with each k in the sense that G T k+1 (ỹ k+1 , ũk+1 ) ≤ G T k (ỹ k , ũk ) , k= 0, 1, . . ., N − 1, while at the first step of the algorithm we have G T 0 (ỹ 0 , ũ0 ) ≤ G R(ỹ 0 ) .It will then follow that the cost improvement propertyholds.Approximation in Value Space -Rollout Algorithms Chap. 2s Terminal State t 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 20 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 20 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 20 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 20 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 20 1 2 3 4 6 7 8 9 10 11 12 13 14 15 20 1 2 3 4 5 6 7 8 10 11 12 13 14 15 20 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 20 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 20 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 20 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 20 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 201 2 3 4 5 6 7 8 9 10 11 12 13 14 15 20 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 20 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 20 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 20 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 20 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 20 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 20 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 20 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 20 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 20 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 20 1 2 3 4 5 6 7 8 9 11 12 13 14 15 20 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 20 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 20 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 20 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 20 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 20 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 20 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 20 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 20 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 20 Definition 2.5.1:We say that the base heuristic is sequentially consistent if whenever it generates a partial trajectory (x k , u k , x k+1 , u k+1 , . . ., u N −1 , x N ), starting from a partial trajectory y k , it also generates the partial trajectory (x k+1 , u k+1 , x k+2 , u k+2 , . . ., u N −1 , x N ), starting from the partial trajectory y k+1 = y k , u k , x k+1 .As we have noted in the context of unconstrained rollout, greedy heuristics tend to be sequentially consistent.Also any policy [a sequence of feedback control functions µ k (y k ), k = 0, 1, . . ., N −1] for the DP problem of minimizing the terminal cost G(y N ) subject to the system equationand the feasibility constraint y N ∈ C can be seen to be sequentially consistent.For an example where sequential consistency is violated, consider the base heuristic of the traveling salesman Example 2.5.1.From Fig. 2.5.3, it can be seen that the base heuristic at A generates ACDBA, but from AC it generates ACBDA, thus violating sequential consistency.For a given partial trajectory y k , let us denote by y k ∪ R(y k ) the complete trajectory obtained by joining y k with the partial trajectory generated by the base heuristic starting from y k .Thus if y k = (x 0 , u 0 , . . ., u k−1 , x k ) and R(y k ) = (x k , u k , . . ., u N −1 , x N ), we have y k ∪ R(y k ) = (x 0 , u 0 , . . ., u k−1 , x k , u k , . . ., u N −1 , x N ).Definition 2.5.2:We say that the base heuristic is sequentially improving if for every k = 0, 1, . . ., N − 1 and partial trajectory y k for which y k ∪ R(y k ) ∈ C, the set Ûk (y k ) is nonempty, and we have(2.45)Note that for a base heuristic that is not sequentially consistent, the condition y k ∪ R(y k ) ∈ C does not imply that the set Ûk (y k ) is nonempty.The reason is that starting from the next statethe base heuristic may generate a different trajectory than from y k , even if it applies u k at y k .Thus we need to include nonemptiness of Ûk (y k ) as a requirement in the preceding definition of sequential improvement (in the fortified version of the algorithm to be discussed shortly, this requirement will be removed).On the other hand, if the base heuristic is sequentially consistent, it is also sequentially improving.The reason is that for a sequentially consistent heuristic, y k ∪ R(y k ) is equal to one of the trajectories contained in the setOur main result is contained in the following proposition.Proposition 2.5.1:(Cost Improvement for Constrained Rollout) Assume that the base heuristic is sequentially improving and generates a feasible complete trajectory starting from the initial state ỹ0 = x0 , i.e., R(ỹ 0 ) ∈ C. Then for each k, the set U k (ỹ k ) is nonempty, and we have G R(ỹ 0 ) ≥ G T 0 (ỹ 0 , ũ0 )where T k (ỹ k , ũk ) = ỹk , ũk , R(ỹ k+1 ) ; cf.Eq. (2.39).In particular, the final trajectory ỹN generated by the constrained rollout algorithm is feasible and has no larger cost than the trajectory R(ỹ 0 ) generated by the base heuristic starting from the initial state.Proof: Consider R(ỹ 0 ), the complete trajectory generated by the base heuristic starting from ỹ0 .Since ỹ0 ∪ R(ỹ 0 ) = R(ỹ 0 ) ∈ C by assumption, it follows from the sequential improvement definition, that the set U 0 (ỹ 0 ) is nonempty and we have G R(ỹ 0 ) ≥ G T 0 (ỹ 0 , ũ0 ) ,[cf.Eq. (2.45)], while T 0 (ỹ 0 , ũ0 ) ∈ C.The preceding argument can be repeated for the next stage, by replacing ỹ0 with ỹ1 , and R(ỹ 0 ) with T 0 (ỹ 0 , ũ0 ).Since ỹ1 ∪R(ỹ 1 ) = T 0 (ỹ 0 , ũ0 ) ∈ C, from the sequential improvement definition, the set U 1 (ỹ 1 ) is nonempty and we have G T 0 (ỹ 0 , ũ0 ) = G ỹ1 ∪ R(ỹ 1 ) ≥ G T 1 (ỹ 1 , ũ1 ) ,[cf.Eq. (2.45)], while T 1 (ỹ 1 , ũ1 ) ∈ C. Similarly, the argument can be successively repeated for every k, to verify that U k (ỹ k ) is nonempty and that G T k (ỹ k , ũk ) ≥ G T k+1 (ỹ k+1 , ũk+1 ) for all k.Q.E.D.Proposition 2.5.1 establishes the fundamental cost improvement property for constrained rollout under the sequential improvement condition.On the other hand we may construct examples where the sequential improvement condition (2.45) is violated and the cost of the solution produced by rollout is larger than the cost of the solution produced by the base heuristic starting from the initial state (cf. the unconstrained rollout Example 2.3.3).In the case of the traveling salesman Example 2.5.1, it can be verified that the base heuristic specified in Fig. 2.5.3 is sequentially improving.However, if the travel cost D→A were larger, say 25, then it can be verified that the definition of sequential improvement would be violated at A, and the tour produced by constrained rollout would be more costly than the one produced by the base heuristic starting from A.We will now discuss some variations and extensions of the constrained rollout algorithm.Let us first consider the case where the sequential improvement assumption is not satisfied.Then it may happen that given the current partial trajectory ỹk , the set of controls U k (ỹ k ) that corresponds to feasible trajectories T k (ỹ k , u k ) [cf.Eq. (2.40)] is empty, in which case the rollout algorithm cannot extend the partial trajectory ỹk further.To bypass this difficulty, we introduce a fortified constrained rollout algorithm, patterned after the fortified algorithm given earlier.For validity of this algorithm, we require that the base heuristic generates a feasible complete trajectory R(ỹ 0 ) starting from the initial state ỹ0 .The fortified constrained rollout algorithm, in addition to the current partial trajectory ỹk = (x 0 , ũ0 , x1 , . . ., ũk−1 , xk ), maintains a complete trajectory Tk , called tentative best trajectory, which is feasible (i.e., Tk ∈ C) and agrees with ỹk up to state xk , i.e., Tk has the form Tk = (x 0 , ũ0 , x1 , . . ., ũk−1 , xk , u k , x k+1 , . . ., u N −1 , x N ), (2.46)for some u k , x k+1 , . . ., u N −1 , x N such thatInitially, T0 is the complete trajectory R(ỹ 0 ), generated by the base heuristic starting from ỹ0 , which is assumed to be feasible.At stage k, the algorithm forms the subset Ûk (ỹ k ) of controls u k ∈ U k (ỹ k ) such that the corresponding T k (ỹ k , u k ) is not only feasible, but also has cost that is no larger than the one of the current tentative best trajectory:There are two cases to consider at state k:(1) The set Ûk (ỹ k ) is nonempty.Then the algorithm forms the partial trajectory ỹk+1 = (ỹ k , ũk , xk+1 ), where ũk ∈ arg minand sets T k (ỹ k , ũk ) as the new tentative best trajectory, i.e., Tk+1 = T k (ỹ k , ũk ).(2) The set Ûk (ỹ k ) is empty.Then, the algorithm forms the partial trajectory ỹk+1 = ỹk , ũk , xk+1 ), where ũk = u k , xk+1 = x k+1 , and u k , x k+1 are the control and state subsequent to xk in the current tentative best trajectory Tk [cf.Eq. (2.46)], and leaves Tk unchanged, i.e., Tk+1 = Tk .It can be seen that the fortified constrained rollout algorithm will follow the initial complete trajectory T0 , the one generated by the base heuristic starting from ỹ0 , up to a stage k where it will discover a new feasible complete trajectory with smaller cost to replace T0 as the tentative best trajectory.Similarly, the new tentative best trajectory Tk may be subsequently replaced by another feasible trajectory with smaller cost, etc.Note that if the base heuristic is sequentially improving, and the fortified rollout algorithm will generate the same complete trajectory as the (nonfortified) rollout algorithm given earlier, with the tentative best trajectory Tk+1 being equal to the complete trajectory T k (ỹ k , ũk ) for all k.The reason is that if the base heuristic is sequentially improving, the controls ũk generated by the nonfortified algorithm belong to the set Ûk (ỹ k ) [by Prop.2.5.1, case (1) above will hold].However, it can be verified that even when the base heuristic is not sequentially improving, the fortified rollout algorithm will generate a complete trajectory that is feasible and has cost that is no worse than the cost of the complete trajectory generated by the base heuristic starting from ỹ0 .This is because each tentative best trajectory has a cost that is no worse than the one of its predecessor, and the initial tentative best trajectory is just the trajectory generated by the base heuristic starting from the initial condition ỹ0 .It is possible to improve the performance of the rollout algorithm at the expense of maintaining more than one partial trajectory.In particular, instead of the partial trajectory ỹk of Eq. (2.43), we can maintain a tree of partial trajectories that is rooted at ỹ0 .These trajectories need not have equal length, i.e., they need not involve the same number of stages.At each step of the algorithm, we select a single partial trajectory from this tree, and execute the rollout algorithm's step as if this partial trajectory were the only one.Let this partial trajectory have k stages and denote it by ỹk .Then we extend ỹk similar to our earlier rollout algorithm, with possibly multiple feasible trajectories.There is also a fortified version of this algorithm where a tentative best trajectory is maintained, which is the minimum cost complete trajectory generated thus far.The aim of the tree-based algorithm is to obtain improved performance, essentially because it can go back and extend partial trajectories that were generated and temporarily abandoned at previous stages.The net result is a more flexible algorithm that is capable of examining more alternative trajectories.Note also that there is considerable freedom to select the number of partial trajectories maintained in the tree.We finally mention a drawback of the tree-based algorithm: it is suitable for off-line computation, but it cannot be applied in an on-line context, where the rollout control selection is made after the current state becomes known as the system evolves in real-time .As noted in Section 2.1, general discrete optimization problems may be formulated as DP problems, which in turn can be addressed with rollout.The following is an example of a classical problem that involves both discrete and continuous variables.It can also be viewed as an instance of a 0-1 integer programming problem, and in fact this is the way it is usually addressed in the literature; see e.g., the book [DrH01].The author's rollout book [Ber20a] contains additional examples.Example 2.5.2 (Facility Location)We are given a candidate set of N locations, and we want to place in some of these locations a "facility" that will serve the needs of a total of M "clients."Each client i = 1, . . ., M has a demand di for services that may be satisfied at a location k = 0, . . ., N − 1 at a cost a ik per unit.If a facility is placed at location k, it has capacity to serve demand up to a known level c k .We introduce a 0-1 integer variable u k to indicate with u k = 1 that a facility is placed at location k at a cost b k and with u k = 0 that a facility is not placed at location k.Thus if y ik denotes the amount of demand of client i to be served at facility k, the constraints are (2.49)We wish to minimize the costsubject to the preceding constraints; see Fig. 2.5.4.The essence of the problem is to place enough facilities at favorable locations to satisfy the clients' demand at minimum cost.This can be a very difficult mixed integer programming problem.On the other hand, when all the variables u k are fixed at some 0 or 1 values, the problem belongs to the class of linear transportation problems (see e.g., [Ber98]), and can be solved by fast polynomial algorithms.Thus the essential difficulty of the problem is how to select the integer variables u k , k = 0, . . ., N − 1.This can be viewed as a discrete optimization problem of the type discussed in Section 1.6.3(cf.Fig. 1.6.2).In terms of the notation of this figure, the control components are u0, . . ., uN−1, where u k can take the values 0 or 1.To address the problem suboptimally by rollout, we must define a base heuristic at a "state" (u0, . . ., u k ), where uj = 1 or uj = 0 specifies that a facility is or is not placed at location j, respectively.A suitable base heuristic at that state is to place a facility at all of the remaining locations (i.e., uj = 1 for j = k +1, . . ., N − 1), and its cost is obtained by solving the corresponding linear transportation problem of minimizing the cost (2.50)subject to the constraints (2.47)-(2.49),with the variables uj , j = 0, . . ., k, fixed at the previously chosen values, and the variables uj , j = k + 1, . . ., N, fixed at 1.To illustrate, at the initial state where no placement decision has been made, we set u0 = 1 (a facility is placed at location 0) or u0 = 0 (a facility is not placed at location 0), we solve the two corresponding transportation problems, and we fix u0, depending on which of the two resulting costs is smallest.Having fixed the status of location 0, we repeat with location 1: set the variable u1 to 1 and to 0, solve the corresponding two transportation problems, and fix u1, depending on which of the two resulting costs is smallest, etc.It is easily seen that if the initial base heuristic choice (placing a facility at every candidate location) is feasible, i.e., M i=1 di ≤ N−1 k=0 c k , the rollout algorithm will yield a feasible solution with cost that is no larger than the cost corresponding to the initial application of the base heuristic.In fact it can be verified that the base heuristic here is sequentially consistent, so it is not necessary to use the fortified version of the algorithm.Regarding computational costs, the number of transportation problems to be solved is at first count 2N , but it can be reduced to N + 1 by exploiting the fact that one of the two transportation problems at each stage after the first has been solved at an earlier stage.It is worth noting, for readers that are familiar with the integer programming method of branch-and-bound, that the graph of Fig. 2.1.4corresponds to the branch-and-bound tree for the problem, so the rollout algorithm amounts to a quick (and imperfect) method to traverse the branch-and-bound tree.This observation may be useful if we wish to use integer programming techniques to add improvements to the rollout algorithm.We finally note that the rollout algorithm requires the solution of many linear transportation problems, which are defined by fairly similar data.It is thus important to use an algorithm that is capable of using effectively the final 0 1 2 3 4 5 6 0 1 2 3 4 5 6 0 1 2 3 4 5 6 0 1 2 3 4 5 6 0 1 2 3 4 5 6 0 1 2 3 4 5 6 0 1 2 3 4 5 6 solution of one transportation problem as a starting point for the solution of the next.The auction algorithm for transportation problems (Bertsekas and Castañon [BeC89]) is particularly well-suited for this purpose.Example 2.5. 3Let us consider a spanning tree-type problem involving a directed graph with nodes 0, 1, . . ., N. At each node k ∈ {0, . . ., N − 1} there is a set of outgoing arcs u k ∈ U k .Node N is special: it is viewed as a "root" node and has no outgoing arc.We are interested in collections of arcs involving a single outgoing arc per node, u = (u0, . . ., uN−1) with u k ∈ U k , k = 0, . . ., N − 1.We require that these arcs do not form a cycle, so that u specifies a directed spanning tree that is rooted at node N .Note that for every node k, such a spanning tree specifies a unique path that starts at k, lies on the spanning tree, and ends at node N .We wish to find u that minimizes a given cost function G(u) subject to certain additional constraints, which we do not specify further.The set of all constraints on u (including the constraint that the arcs form a directed spanning tree) is denoted abstractly as u ∈ U , so the problem comes within our constrained optimization framework of this section.Note that this problem contains as a special case the classical shortest path problem, where we have a length for every arc and the objective is to find a tree of shortest paths to node N from all the nodes 0, . . ., N − 1.Here U is just the constraint that the set of arcs u = (u0, . . ., uN−1) form a directed spanning tree that is rooted at node N , and G(u) is the sum of the lengths of all the paths specified by u, summed over all the start nodes k = 0, . . ., N − 1.Other shortest path-type problems, involving constraints, are included as special cases.For example, there may be a constraint that all the paths to N that are specified by the spanning tree corresponding to u contain a number of arcs that does not exceed a given upper bound.Suppose that we have an initial solution/directed spanning tree ū = (ū0, . . ., ūN−1), which is feasible (note here that finding such an initial solution may be a challenge).Let us apply the constrained rollout algorithm with a base heuristic that operates as follows: given a partial trajectory y k = (u0, . . ., u k−1 ), i.e., a sequence of k arcs, each outgoing from one of the nodes 0, . . ., k − 1, it generates the complete trajectory/directed spanning tree (u0, . . ., u k−1 , ūk , . . ., ūN−1).Thus the rollout algorithm, given a partial trajectory ỹk = (ũ0, . . ., ũk−1 ), considers the set Ûk (ỹ k ) of all outgoing arcs u k from node k, such that the complete trajectory (ỹ k , u k , ūk+1 , . . ., ūN−1) is feasible.It then selects the arc u k ∈ Ûk (ỹ k ) that minimizes the cost G(ỹ k , u k , ūk+1 , . . ., ūN−1); see Fig. 2.5.5.It can be seen by induction, starting from ū, that the set of arcs Ûk (ỹ k ) is nonempty, and that the algorithm generates a sequence of feasible solutions/spanning trees, each with cost no worse than the preceding one.Note that throughout the rollout process, a rooted spanning tree is maintained, and at each stage k, a single arc ūk that is outgoing from node k is replaced by the outgoing arc ũk .Thus two successive rooted spanning trees generated by the algorithm, differ by at most a single arc.An interesting aspect of this rollout algorithm is that it can be applied multiple times with the final solution of one rollout application used to specify the base heuristic of the next rollout application.Moreover, a different order of nodes may be used in each rollout application.This can be viewed as a form of policy iteration, of the type that we have discussed.The algorithm will eventually terminate, in the sense that it can make no further progress.More irregular/heuristic orders of node selections are also possible; for example some nodes may be selected multiple times before others will be selected for the first time.However, there is no guarantee that the final solution thus obtained will be optimal.Let us consider the deterministic one-step approximation in value space scheme μk (x k ) ∈ arg min(2.51)In the context of rollout, Jk+1 f k (x k , u k ) is either the cost of the trajectory generated by the base heuristic starting from the next state f k (x k , u k ), or some approximation that may involve truncation and terminal cost function approximation, as in the truncated rollout scheme of Section 2.3.5.There is a special difficulty within this context, which is often encountered in practice.It arises when the cost per stage g k (x k , u k ) is either 0 or is small relative to the cost-to-go approximation Jk+1 f k (x k , u k ) .Then there is a potential pitfall to contend with: the cost approximation errors that are inherent in the term Jk+1 f k (x k , u k ) may overwhelm the first stage cost term g k (x k , u k ), with unpredictable consequences for the quality of the one-step-lookahead policy π = {μ 0 , . . ., μN−1 }.The most straightforward way to address this issue is to use longer lookahead; this is typically what is done in the context of MPC (cf.Section 1.6.7).The difficulty here is that long lookahead minimization may require extensive on-line computation.In this case, creative application of the lookahead tree pruning and incremental rollout ideas, discussed in Section 2.4.2, may be helpful.We will next discuss the difficulty with small stage costs for an alternative context, which arises from discretization of a continuous-time optimal control problem.Consider a problem that involves a vector differential equation of the form ẋ(t) = h x(t), u(t), t , 0 ≤ t ≤ T, (2.52) where x(t) ∈ n is the state vector at time t, ẋ(t) ∈ n is the vector of first order time derivatives of the state at time t, u(t) ∈ U ⊂ m is the control vector at time t, where U is the control constraint set, and T is a given terminal time.Starting from a given initial state x(0), we want to find a feasible control trajectory u(t) | t ∈ [0, T ] , which together with its corresponding state trajectory x(t) | t ∈ [0, T ] , minimizes a cost function of the form G x(T ) + T 0 g x(t), u(t), t dt, (2.53)where g represents cost per unit time, and G is a terminal cost function.This is a classical problem with a long history.Let us consider a simple conversion of the preceding continuous-time problem to a discrete-time problem, while treading lightly over some of the associated mathematical fine points.We introduce a small discretization increment δ > 0, such that T = δN where N is a large integer, and we replace the differential equation (2.52) byk= 0, . . ., N − 1.Here the function h k is given bywhere we view {x k | k = 0, . . ., N − 1} and {u k | k = 0, . . ., N − 1} as state and control trajectories, respectively, which approximate the corresponding continuous-time trajectories:We also replace the cost function (2.53) bywhere g N (x N ) = G x(N δ) , g k (x k , u k ) = g x(kδ), u(kδ), kδ .Thus the approximation in value space scheme with time discretization takes the form μk (x k ) ∈ arg min(2.54)where Jk+1 is the function that approximates the cost-to-go starting from a state at time k + 1.We note here that the ratio of the terms δ • g k (x k , u k ) and Jk+1 x k + δ •h k (x k , u k ) is likely to tend to 0 as δ → 0, since Jk+1 x k + δ •h k (x k , u k ) ordinarily stays roughly constant at a nonzero level as δ → 0. This suggests that the one-step lookahead minimization may be degraded substantially by discretization, and other errors, including rollout truncation and terminal cost approximation.Note that a similar sensitivity to errors may occur in other discrete-time models that involve frequent selection of decisions, with cost per stage that is very small relative to the cumulative cost over many stages and/or the terminal cost.To deal with this difficulty, we subtract the constant Jk (x k ) in the one-step-lookahead minimization (2.54), and write μk (x k ) ∈ arg min(2.55) since Jk (x k ) does not depend on u k , the results of the minimization are not affected.Assuming Jk is differentiable with respect to its argument, we can writewhere ∇ x Jk denotes the gradient of J k (a column vector), and prime denotes transposition.By dividing with δ, and taking informally the limit as δ → 0, we can write the one-step lookahead minimization (2.55) as μ(t) ∈ arg min u(t)∈U g x(t), u(t), t + ∇ x Jt x(t) h x(t), u(t), t , (2.56) where Jt (x) is the continuous-time cost function approximation and ∇ x Jt (x) is its gradient with respect to x.This is the correct analog of the approximation in value space scheme (2.51) for continuous-time problems.In view of the value approximation scheme of Eq. (2.56), it is natural to speculate that the continuous-time analog of rollout with a base policy of the form π = µ t x(t) | 0 ≤ t ≤ T , (2.57) where µ t x(t) ∈ U for all x(t) and t, has the form μt x(t) ∈ arg min u(t)∈U g x(t), u(t), t + ∇ x J π,t x(t) h x(t), u(t), t .(2.58)Here J π,t x(t) is the cost of the base policy π starting from state x(t) at time t, and satisfies the terminal conditionComputationally, the inner product in the right-hand side of the above minimization can be approximated using the finite difference formula ∇ x J π,t x(t) h x(t), u(t), t ≈ J π,t x(t) + δ • h x(t), u(t), t − J π,t x(t) δ ,dt Given Point Given Line Given Point Given LinePoint (0, 0) 0) Optimal Solution 0) Optimal Trajectory which can be calculated by running the base policy π starting from x(t) and from x(t) + δ • h x(t), u(t), t .(This finite differencing operation may involve tricky computational issues, but we will not get into this.)An important question is how to select the base policy π.A choice that is often sensible and convenient is to choose π to be a "short-sighted" policy, which takes into account the "short term" cost from the current state (say for a very small horizon starting from the current time t), but ignores the remaining cost.An extreme case is the myopic policy, given by µ t x(t) ∈ arg min u∈U g x(t), u(t), t .This policy is the continuous-time analog of the greedy policy that we discussed in the context of discrete-time problems, and the traveling salesman Example 1.2.3 in particular.The following example illustrates the rollout algorithm (2.58) with a problem that has a special property: the base policy cost J π,t x(t) is independent of x(t) (it depends only on t), so that ∇ x J π,t x(t) ≡ 0. In this case, in view of Eq. (2.56), the rollout policy is myopic.It turns out that the optimal policy in this example is also myopic, so that the rollout policy is optimal, even though the base policy is very poor.Example 2.6.1 (A Calculus of Variations Problem) This is a simple example from the classical context of calculus of variations (see [Ber17a], Example 7.1.3).The problem is to find a minimum length curve that starts at a given point and ends at a given line.Without loss of generality, let (0, 0) be the given point, and let the given line be the vertical line that passes through (T, 0), as shown in Fig. 2.6.1.Let t, x(t) be the points of the curve, where 0 ≤ t ≤ T .The portion of the curve joining the points t, x(t) and t + dt, x(t + dt) can be approximated, for small dt, by the hypotenuse of a right triangle with sides dt and ẋ(t)dt.Thus the length of this portion isThe length of the entire curve is the integral over [0, T ] of this expression, so the problem is to minimizeTo reformulate the problem as a continuous-time optimal control problem, we introduce a control u and the system equation ẋ(t) = u(t),x(0) = 0.Our problem then takes the form minimizeThis is a problem that fits our continuous-time optimal control framework, with h x(t), u(t), t = u(t), g x(t), u(t), t = 1 + u(t), G x(T ) = 0.Consider now a base policy π whereby the control depends only on t and not on x.Such a policy has the form µt x(t) = β(t), for all x(t),where β(t) is some scalar function.For example, β(t) may be constant, β(t) ≡ β for some scalar β, which yields a straight line trajectory that starts at (0, 0) and makes an angle φ with the horizontal with tan(φ) = β.The cost function of the base policy iswhich is independent of x(t), so that ∇xJπ,t x(t) ≡ 0. Thus, from the minimization of Eq. (2.58), we have μt x(t) ∈ arg minand the rollout policy is μt x(t) ≡ 0. This is the optimal policy: it corresponds to the horizontal straight line that starts at (0, 0) and ends at (T, 0).An extension of the rollout algorithm (2.58) is to use a more general base heuristic whose cost function H t x(t) can be evaluated by simulation.This rollout algorithm has the form μ(t) ∈ arg min u(t)∈U g x(t), u(t), t + ∇ x H t x(t) h x(t), u(t), t .Here the policy cost function J π,t is replaced by a more general differentiable function H t , obtainable through a base heuristic, which may lack the sequential consistency property that is inherent in policies.We will now show a cost improvement property of the rollout algorithm based on the natural condition(2.59) and the assumption min(2.60) for all x(t), t , where ∇ x H t denotes gradient with respect to x, and ∇ t H t denotes gradient with respect to t.This assumption is the continuous-time analog of the sequential improvement condition of Definition 2.3.2 [cf.Eq. (2.13)].Under this assumption, we will show thati.e., the cost of the rollout policy starting from the initial state x(0) is no worse than the base heuristic cost starting from the same initial state.Indeed, let x(t) | t ∈ [0, T ] and ũ(t) | t ∈ [0, T ] be the state and control trajectories generated by the rollout policy starting from x(0).Then the sequential improvement condition (2.60) yields g x(t), ũ(t), t + ∇ t H t x(t) + ∇ x H t x(t) h x(t), ũ(t), t ≤ 0 for all t, and by integration over [0, T ], we obtain(2.62) The second integral above can be written asand its integrand is the total differential with respect to time: d dt H t x(t) .Thus we obtain from Eq. (2.62)(2.63)Eq. (2.59)] and x(0) = x(0), from Eq.(2.63) [which is a direct consequence of the sequential improvement condition (2.60)], it follows thatthus proving the cost improvement property (2.61).Note that the sequential improvement condition (2.60) is satisfied if H t is the cost function J π,t corresponding to a base policy π.The reason is that for any policy π = µ t (x(t)) | 0 ≤ t ≤ T [cf.Eq. (2.57)], the analog of the DP algorithm (under the requisite mathematical conditions) is(2.64) In continuous-time optimal control theory, this is known as the Hamilton-Jacobi-Bellman equation.It is a partial differential equation, which may be viewed as the continuous-time analog of the DP algorithm for a single policy; there is also a Hamilton-Jacobi-Bellman equation for the optimal cost function J * t x(t) (see optimal control textbook accounts, such as [Ber17a], Section 7.2, and the references cited there).As illustration, the reader may verify that the cost function of the base policy used in the calculus of variations problem of Example 2.6.1 satisfies this equation.It can be seen from the Hamilton-Jacobi-Bellman Eq. (2.64) that when H t = J π,t , the sequential improvement condition (2.60) and the cost improvement property (2.61) hold.The preceding analysis suggests that when dealing with a discrete-time problem with a long horizon N , a system equation x k+1 = f k (x k , u k ), and a small cost per stage g k (x k , u k ) relative to the optimal cost-to-go function, it is worth considering an alternative implementation of the approximation in value space scheme.In particular, we should consider approximating the cost differencesinstead of approximating the optimal cost-to-go functions J ⇤ k+1 f k (x k , u k ) .The one-step-lookahead minimization (2.51) should then be replaced by μk (x k ) 2 arg minwhere Dk is the approximation to D ⇤ k .Note also that while for continuous-time problems, the idea of approximating the gradient of the optimal cost function is essential and comes out naturally from the analysis, for discrete-time problems, approximating cost-to-go di↵erences rather than cost functions is optional and should be considered in the context of a given problem, possibly in conjunction with increased lookahead.Methods along this line include advantage updating, cost shaping, biased aggregation, and the use of baselines, for which we refer to the books [BeT96], [Ber19a], and [Ber20a].A special method to explicitly approximate cost function di↵erences is di↵erential training, which was proposed in the author's paper [Ber97b], and was also discussed in Section 4.3.4 of the book [Ber20a].The most extreme and challenging case of small stage costs arises when the cost per stage is zero for all states, while a nonzero cost may be incurred only at termination.This type of cost structure occurs, among others, in games such as chess and backgammon.It also occurs in several other contexts, including constraint programming problems (Section 2.1), where there is not even a terminal cost, just constraints to be satisfied.Under these circumstances, the idea of approximating cost-to-go differences that we have just discussed may not be e↵ective, and applying approximation in value space may involve serious challenges.An advisable remedy is to resort to long lookahead, either through multistep lookahead minimization (possibly augmented by pruning or incremental rollout ideas; cf.Section 2.4), or through some form of truncated rollout, as in the TD-Gammon program.It may also be important to introduce a terminal cost function approximation by using problem simplification (solving a simpler problem, in place of the original), or neural network training.Aggregation, discussed in Section 3.6, is another possibility along this line.We will now discuss the extension of the rollout algorithm to stochastic DP problems with a finite number of control and disturbances at every stage.We will restrict ourselves to the case where the base heuristic is a policy π = {µ 0 , . . ., µ N −1 }.The rollout policy applies at state x k the control μk (x k ) given by the minimization μk (x k ) ∈ arg minEquivalently, the rollout policy π = {μ 0 , . . ., μN−1 } is obtained by minimization over the Q-factors Q k,π (x k , u k ) of the base policy:whereWe first establish that the cost improvement property that we showed for deterministic problems under the sequential consistency condition carries through for stochastic problems.In particular, let us denote by J k,π (x k ) the cost corresponding to starting the base policy at state x k , and by J k,π (x k ) the cost corresponding to starting the rollout algorithm at state x k .We claim thatfor all x k and k.(2.65)We prove this inequality by induction similar to the deterministic case [cf.Eq. (2.12)].Clearly it holds for k = N , sinceAssuming that it holds for index k + 1, we have for all x k , The induction proof of the cost improvement property is thus complete.The preceding cost improvement argument assumes that the cost functions J k+1,π of the base policy are calculated exactly.In practice, truncated rollout with terminal cost function approximation and limited simulation may be used to approximate J k+1,π .In this case the cost function of the rollout policy can still be viewed as the result of a Newton step in the context of an approximation in value space scheme.Moreover, the cost improvement property can still be proved under some conditions that we will not discuss in this book; see the books [Ber12], [Ber19a], and [Ber20a].Similar to deterministic problems, it has been observed empirically that for stochastic problems the rollout policy not only does not deteriorate the performance of the base policy, but also typically produces substantial cost improvement, thanks to its underlying Newton step; see also the case studies referenced at the end of the chapter.To emphasize this point, we provide here an example of an nontrivial optimal stopping problem where the rollout policy is actually optimal, despite the fact that the base policy is rather naive.Such behavior is of course special and nontypical, but highlights the nature of the cost improvement property of rollout.Example 2.7.1 (Optimal Stopping and Rollout Optimality)Optimal stopping problems are characterized by the availability, at each state, of a control that stops the evolution of the system.We will consider a problem with two control choices: at each stage we observe the current state of the system and decide whether to continue or to stop the process.We formulate this as an N -stage problem where stopping is mandatory at or before stage N .Consider a stationary version of the problem (state and disturbance spaces, disturbance distribution, control constraint set, and cost per stage are the same for all times).At each state x k and at time k, if we stop, the system moves to a termination state at a cost C(x k ) and subsequently remains there at no cost.If we do not stop, the system moves to state x k+1 = f (x k , w k ) at cost g(x k , w k ).The terminal cost, assuming stopping has not occurred by the last stage, is C(xN ).An example is a problem of optimal exercise of a financial option where x is the asset's price, C(x) = x, and g(x, w) ≡ 0.The DP algorithm (for states other than the termination state) is given byand using Eq.(2.70) we obtain for x ∈ SN−1Therefore, stopping is optimal for all xN−2 ∈ SN−1 or equivalently SN−1 ⊂ SN−2.This together with Eq. (2.69) implies SN−2 = SN−1.Proceeding similarly, we obtain S k = SN−1 for all k.Thus the optimal policy is to stop if and only if the state is within the set SN−1, which is precisely the set of states where the rollout policy stops.In conclusion, if condition (2.70) holds (the one-step stopping set SN−1 is absorbing), the rollout policy is optimal.Moreover, the preceding analysis [cf.Eq. (2.69)] can be used to show that even if the one-step stopping set SN−1 is not absorbing, the rollout policy stops and is optimal within the set of states x ∈ ∩ k S k , and correctly continues within the set of states x / ∈ SN−1.Contrary to the optimal policy, it also stops within the subset of states x ∈ SN−1 that are not in ∩ k S k .Thus, even in the absence of condition (2.70), the rollout policy is quite sensible even though the base policy is not.We next discuss a special case of the preceding example.Again the one-step lookahead/rollout policy is optimal, despite the fact that the base policy is poor.Related examples can be found in Chapter 3 of the DP textbook [Ber17a].Example 2.7.2 (The Rational Burglar)A burglar may at any night k choose to retire with his accumulated earnings x k or enter a house and bring home a random amount w k .However, in the latter case he gets caught with probability p, and then he is forced to terminate his activities and forfeit all of his earnings thus far.The amounts w k are independent, identically distributed with mean w.The problem is to find a policy that maximizes the burglar's expected earnings over N nights.We can formulate this problem as a stopping problem with two actions (retire or continue) and a state space consisting of the real line, the retirement state, and a special state corresponding to the burglar getting caught.The DP algorithm is given byThe one-step stopping set is(more accurately this set together with the special state corresponding to the burglar's arrest).Since this set is absorbing in the sense of Eq. (2.70), we see that the one-step lookahead/rollout policy by which the burglar retires when his earnings reach or exceed (1 − p)w/p is optimal.Note that the base policy of the burglar is the "timid" policy of always retiring, regardless of his accumulated earnings, which is far from optimal.The cost improvement property (2.65) also holds for the simplified version of the rollout algorithm (cf.Section 2.3.4)where the rollout policy is defined by μk (x k ) ∈ arg minThe proof is obtained by replacing the last inequality in the argument of Eq. (2.66), minwith the inequality minThe simplified rollout algorithm (2.71) may be implemented in a number of ways, including control constraint discretization/approximation, a random search algorithm, or a one-agent-at-a-time minimization process, as in multiagent rollout.The simplified rollout idea can also be used within the infinite horizon policy iteration (PI) context.In particular, instead of the minimization μ(x) ∈ arg minfor all x, (2.72) in the policy improvement operation, it is sufficient for cost improvement to generate a new policy μ that satisfies for all x, E w g x, μ(x), w +αJ µ f (x, μ(x), w) ≤ E w g(x, u, w)+αJ µ f (x, u, w) .This cost improvement property is the critical argument for proving convergence of the PI algorithm and its variations to the optimal cost function and policy; see the corresponding proofs in the books [Ber17a] and [Ber19a].As in the case of deterministic DP problems, it is possible to use -step lookahead, with the aim to improve the performance of the policy obtained through approximation in value space.This, however, can be computationally expensive, because the lookahead graph expands fast as increases, due to the stochastic character of the problem.Using certainty equivalence (CE for short) is an important approximation approach for dealing with this difficulty, as it reduces the size of the -step lookahead graph.Moreover, CE mitigates the potentially excessive simulation because it reduces the stochastic variance of the Q-factors calculated at each stage.In the pure but somewhat flawed version of this approach, when solving the -step lookahead minimization problem, we simply replace all of the uncertain quantities w k , w k+1 , . . ., w k+ −1 , . . ., w N −1 by some nominal value w, thus making that problem fully deterministic.Unfortunately, this affects significantly the character of the approximation: when w k is replaced by a deterministic quantity the Newton step interpretation of the underlying approximation in value space scheme is lost to a great extent.Still, we may largely correct this difficulty, while retaining substantial simplification, by using CE for only after the first stage of the -step lookahead.We can do this with a CE scheme whereby only the uncertain quantities w k+1 , . . ., w N −1 are replaced by a deterministic value w, while w k is treated as a stochastic quantity.†The CE approach, first proposed in the paper by Bertsekas and Castañon [BeC99], has an important property: it maintains the Newton step character of the approximation in value space scheme.In particular, the function J μ of the -step lookahead policy μ obtained is generated by a Newton step, applied to the function obtained by the last −1 minimization steps (modified by CE, and applied to the terminal cost function approximation); see the monograph [Ber20a] for a discussion.Thus the benefit of the fast convergence of Newton's method is restored.In fact based on insights derived from this Newton step interpretation, it appears that the performance penalty for the CE approximation is typically small.At the same time the -step lookahead minimization involves only one stochastic step, the first one, and hence potentially a much "thinner" lookahead graph, than the -step minimization that does not involve any CE-type approximations; see Fig. 2.7.1.Moreover, the ideas of tree pruning and iterative deepening, which we have discussed in Section 2.4 for deterministic multistep lookahead, come into play when the CE approximation is used.A conceptually straightforward way to compute the rollout control at a given state x k and time k is to consider each possible control u k ∈ U k (x k ), and to generate a "large" number of simulated trajectories of the system starting from (x k , u k ).Thus a simulated trajectory is obtained fromwhere {µ k+1 , . . ., µ N −1 } is the tail portion of the base policy, the starting state of the simulated trajectory is  and the disturbance sequence {w k , . . ., w N −1 } is obtained by random sampling.The costs of the trajectories corresponding to a pair (x k , u k ) can be viewed as samples of the Q-factorwhere J k+1,π is the cost-to-go function of the base policy, i.e., J k+1,π (x k+1 ) is the cost of using the base policy starting from x k+1 .For problems with a large number of stages, it is also common to truncate the rollout trajectories and add a terminal cost function approximation as compensation for the resulting error.By Monte Carlo averaging of the costs of the sample trajectories plus the terminal cost (if any), we obtain an approximation to the Q-factor given position and roll of the dice, the set of all possible moves is generated, and the outcome of the game for each move is evaluated by "rolling out" (simulating to the end) many games using a suboptimal/heuristic backgammon player (the TD-Gammon player was used for this purpose in [TeG96]), and by Monte Carlo averaging the scores.The move that results in the best average score is selected for play.We then compute the (approximate) rollout control μk (x k ) with the minimization μk (x k ) ∈ arg min(2.73)Example 2.7.3 (Backgammon)The first impressive application of rollout was given for the ancient two-player game of backgammon, in the paper by Tesauro and Galperin [TeG96]; see Fig. 2.7.2.They implemented a rollout algorithm, which attained a level of play that was better than all computer backgammon programs, and eventually better than the best humans.Tesauro had proposed earlier the use of one-step and two-step lookahead with lookahead cost function approximation provided by a neural network, resulting in a backgammon program called TD-Gammon [Tes89a], [Tes89b], [Tes92], [Tes94], [Tes95], [Tes02].TD-Gammon was trained with an approximate policy iteration method, and was used as the base policy (for each of the two players) to simulate game trajectories.The rollout algorithm also involved truncation of long game trajectories, using a terminal cost function approximation based on TD-Gammon's position evaluation.Game trajectories are of course random, since they involve the use of dice at each player's turn.Thus the scores of many trajectories have to be generated and averaged with the Monte Carlo method to assess the probability of a win from a given position.An important issue to consider here is that backgammon is a two-player game and not an optimal control problem that involves a single decision maker.While there is a DP theory for sequential zero-sum games, this theory has not been covered in this book.Thus how are we to interpret rollout algorithms in the context of two-player games, with both players using some base policy?The answer is to view the game as a (one-player) optimal control problem, where one of the two players passively uses the base policy exclusively (TD-Gammon in the present example).The other player takes the role of the optimizer, and actively tries to improve on his base policy (TD-Gammon) by using rollout.Thus "policy improvement" in the context of the present example means that when playing against a TD-Gammon opponent, the rollout player achieves a better score on the average than if he/she were to play with the TD-Gammon strategy.In particular, the theory does not guarantee that a rollout player that is trained using TD-Gammon for both players will do better than TD-Gammon would against a non-TD-Gammon opponent.While this is a plausible practical hypothesis, it is one that can only be tested empirically.In fact relevant counterexamples have been constructed for the game of Go using "adversarial" optimization techniques; see Wang et al. [WGB22], and also our discussion on minimax problems in Section 2.12.Most of the currently existing computer backgammon programs descend from TD-Gammon.Rollout-based backgammon programs are the most powerful in terms of performance, consistent with the principle that a rollout algorithm performs better than its base heuristic.However, they are too timeconsuming for real-time play (without parallel computing hardware), because of the extensive on-line simulation requirement at each move.† They have been used in a limited diagnostic way to assess the quality of neural networkbased programs (many articles and empirical works on computer backgammon are posted on-line; see e.g., http://www.bkgm.com/articles/page07.html).When using simulation, sampling is often organized to effect variance reduction.By this we mean that for a given problem, the collection and use of samples is structured so that the variance of the simulation error is made smaller, with the same amount of simulation effort.There are several methods of this type for which we refer to textbooks on simulation (see, e.g., Ross [Ros12], and Rubinstein and Kroese [RuK1]).In this section we discuss a method to reduce the effects of the simulation error in the calculation of the Q-factors in the context of rollout.The key idea is that the selection of the rollout control depends on the values of the Q-factor differences Qk,π (x k , u k ) − Qk,π (x k , ûk ) † The situation in backgammon is exacerbated by its high branching factor, i.e., for a given position, the number of possible successor positions is quite large, as compared for example with chess.for all pairs of controls (u k , ûk ).These values must be computed accurately, so that the controls u k and ûk can be accurately compared.On the other hand, the simulation/approximation errors in the computation of the individual Q-factors Qk,π (x k , u k ) may be magnified through the preceding differencing operation.An approach to counteract this type of simulation error magnification is to approximate the Q-factor difference Qk,π (x k , u k ) − Qk,π (x k , ûk ) by sampling the difference(2.74)where w k = (w k , w k+1 , . . ., w N −1 ) is the same disturbance sequence for the two controls u k and ûk , andwith {µ k+1 , . . ., µ N −1 } being the tail portion of the base policy.† For a simple example that illustrates how this form of variance reduction works, suppose we want to calculate the difference q 1 − q 2 of two numbers q 1 and q 2 by subtracting two simulation samples s 1 = q 1 + w 1 and s 2 = q 2 + w 2 , where w 1 and w 2 are zero mean random variables.Then s 1 − s 2 is unbiased in the sense that its mean is equal to q 1 − q 2 .However, the variance of s 1 − s 2 decreases as the correlation of w 1 and w 2 increases.It is maximized when w 1 and w 2 are uncorrelated, and it is minimized (it is equal to 0) when w 1 and w 2 are equal.The preceding example suggests a simulation scheme that is based on the difference (2.74) and involves a common disturbance w k for u k and ûk .In particular, it may be far more accurate than the one obtained by differencing samples of C k (x k , u k , w k ) and C k (x k , ûk , ŵk ), which involve two different disturbances w k and ŵk .Indeed, by introducing the zero mean sample errorsit can be seen that the variance of the error in estimating Qk,π (x k , u k ) − Qk,π (x k , ûk ) with the former method will be no larger than with the latter method if and only if2 .† For this to be possible, we need to assume that the probability distribution of each disturbance wi does not depend on xi and ui.By expanding the quadratic forms and using the fact E D k (x k , u k , w k ) = 0, we see that this condition is equivalent toi.e., the errors D k (x k , u k , w k ) and D k (x k , ûk , w k ) being nonnegatively correlated.A little thought should convince the reader that this property is likely to hold in many types of problems.Roughly speaking, the relation (2.75) holds if changes in the value of u k (at the first stage) have little effect on the value of the error D k (x k , u k , w k ) relative to the effect induced by the randomness of w k .To see this, suppose that there exists a scalar γ < 1 such that, for all x k , u k , and ûk , there holds(2.76) Then we have, by using the generic relation ab ≥ a 2 − |a| • |b − a| for two scalars a and b,where for the second inequality we use the generic relationfor two scalars a and b, and for the third inequality we use Eq.(2.76).Thus, under the assumption (2.76), the condition (2.75) holds and guarantees that by averaging cost difference samples rather than differencing (independently obtained) averages of cost samples, the simulation error variance does not increase.Let us finally note the potential benefit of using Q-factor differences in contexts other than rollout.In particular when approximating Q-factors Q k,π (x k , u k ) using parametric architectures (Section 3.3 in the next chapter), it may be important to approximate and compare instead the differencesThe function A k,π (x k , u k ) is also known as the advantage of the pair (x k , u k ), and can serve just as well as Q k,π (x k , u k ) for the purpose of comparing controls, but may work better in the presence of approximation errors.The use of advantages will be discussed further in Chapter 3.In our earlier discussion of simulation-based rollout implementation, we implicitly assumed that once we reach state x k , we generate the same large number of trajectories starting from each pair (x k , u k ), with u k ∈ U (x k ), to the end of the horizon.The drawback of this is threefold:(a) The trajectories may be too long because the horizon length N is large (or infinite, in an infinite horizon context).(b) Some of the controls u k may be clearly inferior to others, and may not be worth as much sampling effort.(c) Some of the controls u k that appear to be promising, may be worth exploring better through multistep lookahead.This has motivated multistep lookahead variants, generally referred to as Monte Carlo tree search (MCTS for short), which aim to trade off computational economy with a hopefully small risk of degradation in performance.Such variants involve, among others, early discarding of controls deemed to be inferior based on the results of preliminary calculations, and simulation that is limited in scope (either because of a reduced number of simulation samples, or because of a shortened horizon of simulation, or both).A simple remedy for (a) above is to use rollout trajectories of reasonably limited length, with some terminal cost approximation at the end (in an extreme case, the rollout may be skipped altogether for some states, i.e., rollout trajectories have zero length).The terminal cost function may be very simple (such as zero) or may be obtained through some auxiliary calculation.In fact the base policy used for rollout may be used to construct the terminal cost function approximation, as noted for the rollout-based backgammon algorithm of Example 2.7.3.In particular, an approximation to the cost function of the base policy may be obtained by training some approximation architecture, such as a neural network (see Chapter 3), and may be used as a terminal cost function.A simple but less straightforward remedy for (b) is to use some heuristic or statistical test to discard some of the controls u k , as soon as this is suggested by the early results of simulation.Similarly, to implement (c) one may use some heuristic to increase the length of lookahead selectively for some of the controls u k .This is similar to the incremental multistep rollout scheme for deterministic problems that we discussed in Section 2.4.3;see Fig. 2.4.6.The MCTS approach can be based on sophisticated procedures for implementing and combining the ideas just described.The general idea is to use the interim results of the computation and statistical tests to focus the simulation effort along the most promising directions.Thus to implement MCTS with multistep lookahead, one needs to maintain a lookahead tree, which is expanded as the relevant Q-factors are evaluated by simulation, and which balances the competing desires of exploitation and exploration (generate and evaluate controls that seem most promising in terms of performance versus assessing the potential of inadequately explored controls).Ideas that were developed in the context of multiarmed bandit problems have played an important role in the construction of this type of MCTS procedures (see the end-of-chapter references).In the simple case of one-step lookahead, with Q-factors calculated by Monte Carlo simulation, MCTS fundamentally aims to find efficiently the minimum of the expected values of a finite number of random variables.This is illustrated in the following example.Example 2.7.4 (Statistical Tests for Adaptive Sampling with One-Step Lookahead)Let us consider a typical one-step lookahead selection strategy that is based on adaptive sampling.We are at a state x k and we try to find a control ũk that minimizes an approximate Q-factorby averaging samples of the expression within braces.We assume that U k (x k ) contains m elements, which for simplicity are denoted 1, . . ., m.At the th sampling period, knowing the outcomes of the preceding sampling periods, we select one of the m controls, say i , and we draw a sample of Qk (x k , i ), whose value is denoted by Si .Thus after the nth sampling period we have an estimate Qi,n of the Q-factor of each control i = 1, . . ., m that has been sampled at least once, given by-Illustration of one-step lookahead MCTS at a state x k .The Q-factor sampled next corresponds to the control i with minimum sum of exploitation index (here taken to be the running average Q i,n ) and exploration index (R i,n , possibly given by the UCB rule).Thus Qi,n is the empirical mean of the Q-factor of control i (total sample value divided by total number of samples), assuming that i has been sampled at least once.After n samples have been collected, with each control sampled at least once, we may declare the control i that minimizes Qi,n as the "best" one, i.e., the one that truly minimizes the Q-factor Q k (x k , i).However, there is a positive probability that there is an error: the selected control may not minimize the true Q-factor.In adaptive sampling, roughly speaking, we want to design the sample selection strategy and the criterion to stop the sampling, in a way that keeps the probability of error small (by allocating some sampling effort to all controls), and the number of samples limited (by not wasting samples on controls i that appear inferior based on their empirical mean Qi,n).Intuitively, a good sampling policy will balance at time n the desires of exploitation and exploration (i.e., sampling controls that seem most promising, in the sense that they have a small empirical mean Qi,n, versus assessing the potential of inadequately explored controls, those i that have been sampled a small number of times).Thus it makes sense to sample next the control i that minimizes the sum Ti,n + Ri,n of two indexes: an exploitation index Ti,n and an exploration index Ri,n.Usually the exploitation index is chosen to be the empirical mean Qi,n; see Fig. 2.7.3.The exploration index is based on a confidence interval formula and depends on the sample countwhere c is a positive constant that is selected empirically (some analysis suggests values near c = √ 2, assuming that Qi,n is normalized to take values in the range [−1, 0]).The UCB rule, first proposed in the paper by Auer, Cesa-Bianchi, and Fischer [ACF02], has been extensively discussed in the literature both for one-step and for multistep lookahead [where it is called UCT (UCB applied to trees; see Kocsis and Szepesvari [KoS06])].† Its justification is based on probabilistic analyses that relate to the multiarmed bandit problem, and is beyond our scope.Alternatives to the UCB formula have been suggested, and in fact in the AlphaZero program, the exploitation term has a different form than the one above, and depends on the depth of lookahead (see Silver et al. [SHS17]).Sampling policies for MCTS with multistep lookahead are based on similar sampling ideas to the case of one-step lookahead.A simulated trajectory is run from a node i of the lookahead tree that minimizes the sum T i,n + R i,n of an exploitation index and an exploration index.There are several schemes of this type, but the details are beyond our scope and are often problem-dependent (see the end-of-chapter references).A major success has been the use of MCTS in two-player game contexts, such as the AlphaGo program (Silver et al. [SHM16]), which performs better than the best humans in the game of Go.This program integrates several of the techniques discussed in this book, including MCTS and rollout using a base policy that is trained off-line using a deep neural network.The AlphaZero program, which has performed spectacularly well against humans and other programs in the games of Go and chess (Silver et al. [SHS17]), bears some similarity with AlphaGo, and critically relies on MCTS, but does not use rollout in its on-line playing mode (it relies primarily on very long lookahead).We have described rollout and MCTS as schemes for policy improvement: start with a base policy, and compute an improved policy based on the results of one-step lookahead or multistep lookahead followed by simulation with the base policy.We have implicitly assumed that both the base policy and the rollout policy are deterministic in the sense that they map each state x k into a unique control μk (x k ) [cf.Eq. (2.73)].In some (even † The paper [ACF02] refers to the rule given here as UCB1 and credits its motivation to the paper by Agrawal [Agr95].The book by Lattimore and Szepesvari [LaS20] provides an extensive discussion of the UCB rule and its generalizations.nonstochastic) contexts, success has been achieved with randomized policies, which map a state x k to a probability distribution over the set of controls U k (x k ), rather than mapping onto a single control.In particular, the AlphaGo and AlphaZero programs use MCTS to generate and use for training purposes randomized policies, which specify at each board position the probabilities with which the various moves are selected.A randomized policy can be used as a base policy in a rollout context in exactly the same way as a deterministic policy: for a given state x k , we just generate sample trajectories and associated sample Q-factors, using probabilistically selected controls, starting from each leaf-state of the lookahead tree that is rooted at x k .We then average the corresponding Q-factor samples.The rollout/improved policy, as described here, is a deterministic policy, i.e., it applies at x k the control μk (x k ) that is "best" according to the results of the rollout [cf.Eq. (2.73)].Still, however, if we wish to generate an improved policy that is randomized, we can simply change the probabilities of different controls in the direction of the deterministic rollout policy.This can be done by increasing by some amount the probability of the "best" control μk (x k ) from its base policy level, while proportionally decreasing the probabilities of the other controls.The use of MCTS provides a related method to "improve" a randomized policy.In the process of the adaptive simulation that is used in MCTS, we generate frequency counts of the different controls in U k (x k ), i.e., the proportion of rollout trajectories associated with each u k ∈ U k (x k ).We can then obtain the rollout randomized policy by moving the probabilities of the base policy in the direction suggested by the frequency counts, i.e., increase the probability of high-count controls and reduce the probability of the others.This type of policy improvement is reminiscent of gradient-type methods, and has been successful in some contexts; see the end-of-chapter references for such policy improvement implementations in AlphaGo, AlphaZero, and other applications.We have considered so far finite control space applications of rollout, so there is a finite number of relevant Q-factors at each state x k , which are evaluated by simulation and are exhaustively compared.When the control constraint set is infinite, to implement this approach the constraint set must be replaced by a finite set, obtained by some form of discretization or random sampling, which can be inconvenient and ineffective.In this section we will discuss an alternative approach to deal with an infinite number of controls and Q-factors at x k .The idea is to use a base heuristic that involves a continuous optimization, and to rely on a linear or nonlinear programming method to solve the corresponding lookahead optimization problem.The base heuristic is to solve an ( − 1)-stage deterministic optimal control problem, which together with the kth stage minimization over u k ∈ U k (x k ), seamlessly forms an -stage continuous spaces optimal control/nonlinear programming problem that starts at state x k .To develop the basic idea of how to deal with infinite control spaces, we first consider deterministic problems, involving a system x k+1 = f k (x k , u k ), and a cost per stage g k (x k , u k ).The rollout minimization is μk (x k ) ∈ arg min(2.77)where Qk (x k , u k ) is the approximate Q-factorIllustration of a simple supply chain system for Example 2.8.1.particular, suppose that H k+1 (x k+1 ) is the optimal cost of some ( − 1)stage deterministic optimal control problem that is related to the original problem.Then the rollout algorithm (2.77)-(2.78)can be implemented by solving the -stage deterministic optimal control problem, which seamlessly concatenates the first stage minimization over u k [cf.Eq. (2.77)], with the ( − 1)-stage minimization of the base heuristic; see Fig. 2.8.1.This -stage problem may be solvable on-line by standard continuous spaces nonlinear programming or optimal control methods.† A major paradigm of methods of this type is model predictive control, which we have discussed in Chapter 1 (cf.Section 1.6.7).In the present section we will discuss a few other possibilities.The following is a simple example of an important class of inventory storage and supply chain management processes.Example 2.8.1 (Supply Chain Management)Let us consider a supply chain system, where a certain item is produced at a production center and fulfilled at a retail center.Stock of the item is shipped from the production center to the retail center, where it arrives with a delay of τ ≥ 1 time units, and is used to fulfill a known stream of demands d k over an N -stage horizon; see Fig. 2.8.2.We denote:x 1 k : The stock at hand at the production center at time k.x 2 k : The stock at hand at the retail center at time k, and used to fulfill demand (both positive and negative x 2 k are allowed; a negative value indicates that there is backordered demand).u 1 k : The amount produced at time k.u 2 k : The amount shipped at time k (and arriving at the retail center τ time units later).The state at time k is the stock available at the production and retail centers, x 1 k , x 2 k , plus the stock amounts that are in transit and have not yet arrived at the retail center u) is † Note, however, that for this to be possible, it is necessary to have a mathematical model of the system; a simulator is not sufficient.Another difficulty occurs when the control space is the union of a discrete set and a continuous set.Then it may be necessary to use some type of mixed integer programming technique to solve the -stage problem.Alternatively, it may be possible to handle the discrete part by brute force enumeration, followed by continuous optimization.chosen from some constraint set that may depend on the current state, and is subject to production capacity and transport availability constraints.The system equation isand involves the delayed control component u 2 k−τ .Thus the exact DP algorithm involves state augmentation as introduced in Section 1.6.3,and may thus be much more complicated than in the case where there are no delays.†The cost at time k consists of three components: a production cost that depends on x 1 k and u 1 k , a transportation cost that depends on u 2 k , and a fulfillment cost that depends on x 2 k [which includes positive costs for both excess inventory (i.e., x 2 k > d k ) and for backordered demand (i.e., x 2 k < d k )].The precise forms of these cost components are immaterial for the purposes of this example.Here the control vector u k is often continuous (or a mixture of discrete and continuous components), so it may be essential for the purposes of rollout to use the continuous optimization framework of this section.In particular, at the current stage k, we know the current state, which includes x 1 k , x 2 k , and the amounts of stock in transit together with their scheduled arrival times at the retail center.We then apply some heuristic optimization to determine the stream of future production and shipment levels over steps, and use the first component of this stream as the control applied by rollout.As an example we may use as base policy one that brings the retail inventory to some target value stages ahead, and possibly keep it at that value for a portion of the remaining periods.This is a nonlinear programming or mixed integer programming problem that may be solvable with available software far more efficiently than by a discretized form of DP.A major benefit of rollout in the supply chain context is that it can readily incorporate on-line replanning.This is necessary when unexpected demand changes, production or transport equipment failures occur, or updated forecasts become available.The following example deals with a common class of problems of resource allocation over time.Example 2.8.2 (Multistage Linear and Mixed Integer Programming)Let us consider a deterministic optimal control problem with linear system equationk= 0, . . ., N − 1, † Despite the fact that with large delays, the size of the augmented state space can become very large (cf.Section 1.6.3), the implementation of rollout schemes is not affected much by this increase in size.For this reason, rollout can be very well suited for problems involving delayed effects of past states and controls.where A k and B k are known matrices of appropriate dimension, d k is a known vector, and x k and u k are column vectors.The cost function is linear of the formwhere c k and d k are known column vectors of appropriate dimension, and a prime denotes transpose.The terminal state and state-control pairs (x k , u k ) are constrained bywhere T and P k , k = 0, . . ., N − 1, are given sets, which are specified by linear and possibly integer constraints.As an example, consider a multi-item production system, where the state is x k = (x 1 k , . . ., x n k ) and x i k represents stock of item i available at the start of period k.The state evolves according to the system equationwhere u ij k is the amount of product i that is used during time k for the manufacture of product j, a ij k are known scalars that are related to the underlying production process, and d i k is a deterministic demand of product i that is fulfilled at time k.One constraint here is thatand there are additional linear and integer constraints on (x k , u k ), which are collected in a general constraint of the form (x k , u k ) ∈ P k (e.g., nonnegativity, production capacity, storage constraints, etc).Note that the problem may be further complicated by production delays, as in the preceding supply chain Example 2.8.1.Moreover, while in this section we focus on deterministic problems, we may envision a stochastic version of the problem where the demands d i k are random with given probability distributions, which are subject to revisions based on randomly received forecasts.The problem may be solved using a linear or mixed integer programming algorithm, but this may be very time-consuming when N is large.Moreover, the problem will need to be resolved on-line if some of the problem data changes and replanning is necessary.A suboptimal alternative is to use truncated rollout with an -stage mixed integer optimization, and a polyhedral terminal cost function Jk+ to provide a terminal cost optimization.A simple possibility is no terminal cost [ Jk+ (x k+ ) ≡ 0], and another possibility is a polyhedral lower bound approximation that can be based on relaxing the integer constraints after stage k + , or some kind of training approach that uses data.We will next discuss how rollout can accommodate stochastic disturbances by using deterministic optimization ideas based on certainty equivalence (cf.Section 2.7.4) and the methodology of stochastic programming.We have focused so far in this section on rollout that relies on deterministic continuous optimization.There is an important class of methods, known as stochastic programming, which can be used for stochastic optimal control, but bears a close connection to continuous spaces deterministic optimization.We will first describe this connection for two-stage problems, then discuss extensions to many-stages problems, and finally show how rollout can be brought to bear for their approximate solution.Example 2.8.3 (Two-Stage Stochastic Programming)Consider a stochastic problem of optimal decision making over two stages: In the first stage we will choose a finite-dimensional vector u0 from a subset U0 with cost g0(u0).Then an uncertain event represented by a random variable w0 will occur, whereby w0 will take one of the values w 1 , . . ., w m with corresponding probabilities p 1 , . . ., p m .Once w0 occurs, we will know its value w i , and we must then choose at the second stage a vector u i 1 from a subset U1(u0, w i ) at a cost g1(u i 1 , w i ).The objective is to minimize the expected costWe can view this problem as a two-stage DP problem, where x1 = w0 is the system equation, the disturbance w0 can take the values w 1 , . . ., w m with probabilities p 1 , . . ., p m , the cost of the first stage is g0(u0), the cost of the second stage is g1(x1, u1), and the terminal cost is 0. The intuitive meaning is that since at time 0 we don't know yet which of the m values w i of w0 will occur, we must calculate (in addition to u0) a separate second stage decision u i 1 for each i, which will be used after we know that the value of w0 is w i .However, if u0 and u1 take values in a continuous space such as the Euclidean spaces d 0 and d 1 , respectively, we can also equivalently view the problem as a nonlinear programming problem of dimension (d0 + md1) (the optimization variables are u0 and u i 1 , i = 1, . . ., m).For a generalization of the preceding example, consider the stochastic DP problem of Section 1.3 for the case where there are only two stages, and the disturbances w 0 and w 1 can independently take one of the m values w 1 , . . ., w m with corresponding probabilities p 1 0 , . . ., p m 0 and p 1 1 , . . ., p m 1 , respectively.The optimal cost function J 0 (x 0 ) is given by the two-stage DP algorithm + minBy bringing the inner minimization outside the inner brackets, we see that this DP algorithm is equivalent to solving the nonlinear programming problem minimize(2.79)If the controls u 0 and u i 1 are elements of d , this problem involves d(1 + m) scalar variables.An example is the multi-item production problem described in Example 2.8.2 in the case where the demands w i k and/or the production coefficients a ij k are stochastic.We can also consider an N -stage stochastic optimal control problem.A similar reformulation as a nonlinear programming problem is possible.It converts the N -stage stochastic problem into a deterministic optimization problem of dimension that grows exponentially with the number of stages N .In particular, for an N -stage problem, the number of control variables expands by a factor m with each additional stage.The total number of variables is bounded bywhere m is the maximum number of values that a disturbance can take at each stage and d is the dimension of the control vector.The dimension of the preceding nonlinear programming formulation of the multistage stochastic optimal control problem with continuous control spaces can be very large.This motivates a variant of a rollout algorithm that relies on a stochastic optimization for the current stage, and a deterministic optimization that relies on (assumed) certainty equivalence for the remaining stages, where the base policy is used.In this way, the dimension of the nonlinear programming problem to be solved by rollout is drastically reduced.This rollout algorithm operates as follows: Given a state x k and control u k ∈ U k (x k ), we consider the next states x i k+1 that correspond to the m possible values w i k , i = 1, . . ., m, which occur with the known probabilities p i k , i = 1, . . ., m.We then consider the approximate Q-factorswhere Hk+1 (x i k+1 ) is the cost of a base policy, which starting at stage k + 1 from, optimizes the cost-to-go starting from x i k+1 , while assuming that the future disturbances w k+1 , . . ., w N −1 , will take some nominal (nonrandom) values wk+1 , . . ., wN−1 .The rollout control μk (x k ) computed by this algorithm is μk (x k ) ∈ arg min(2.81)Note that this rollout algorithm does not have the cost improvement property, because it involves an approximation: the cost Hk+1 (x i k+1 ) used in Eq. (2.80) is an approximation to the cost of a policy.It is the cost of a policy applied to the certainty equivalent version of the original stochastic problem.The key fact now is that the problem (2.81) can be viewed as a seamless (N − k)-stage deterministic optimization, which involves the control u 0 , and for each value w i k of the disturbance w k , the sequence of controls (u i k+1 , . . ., u i N −1 ).If the controls are elements of d , this deterministic optimization involves a total of(2.82) scalar variables.Currently available deterministic optimization software can deal with quite large numbers of variables, particularly in the context of linear programming, so by using rollout in combination with certainty equivalence, very large problems with continuous state and control variables may be addressed.Another possibility is to use multistep lookahead that aims to represent better the stochastic character of the uncertainty.Here at state x k we solve an (N − k)-stage optimal control problem, where the uncertainty is fully taken into account in the first stages, similar to stochastic programming, and in the remaining N − k − stages, the uncertainty is dealt with by certainty equivalence, by fixing the disturbances w k+ , . . ., w N −1 at some nominal values (we assume here for simplicity that < N − k).If the controls are elements of d , and the number of values that the disturbances w 0 , . . ., w N −1 can take is m, the total number of control variables of this problem is[this is the -step lookahead generalization of the formula (2.82)].Once the optimal policy {ũ k , μk+1 , μk+2 , . ..} for this problem is obtained, the first control component ũk is applied at x k and the remaining components {μ k+1 , μk+2 , . ..} are discarded.Note also that this multistep lookahead approach may be combined with the ideas of multiagent rollout, which will be discussed in the next section.We will now consider a special structure of the control space, whereby the control u k consists of m components, u k = (u 1 k , . . ., u m k ), with a separable control constraint structure u k ∈ U k (x k ), = 1, . . ., m.The control constraint set is the Cartesian product(2.83)Conceptually, each component u k , = 1, . . ., m, is chosen at stage k by a separate "agent" (a decision making entity), and for the sake of the following discussion, we assume that each set U k (x k ) is finite.We discussed this type of problem briefly in Section 1.6.5, and we will discuss it in this section in greater detail.The one-step lookahead minimization ũk ∈ arg minwhere π is a base policy, involves as many as This motivates an alternative and more efficient rollout algorithm, called multiagent rollout also referred to as agent-by-agent rollout , that still achieves the cost improvement propertywhere J k,π (x k ), k = 0, . . ., N, is the cost-to-go of the rollout policy π starting from state x k .Indeed we will exploit the multiagent structure to construct an algorithm that maintains the cost improvement property at much smaller computational cost, namely requiring order O(nm) base policy cost computations per stage.A key idea here is that the computational requirements of the rollout one-step minimization (2.84) are proportional to the size of the control space and are independent of the size of the state space.We consequently reformulate the problem so that control space complexity is traded off with state space complexity, as discussed in Section 1.6.5.This is done by "unfolding" the control u k into its m components u 1 k , u 2 k , . . ., u m k .At the same time, between x k and the next state x k+1 = f k (x k , u k , w k ), we introduce artificial intermediate "states" and corresponding transitions; see Fig. 2.9.1, given in Section 1.6.5 and repeated here for convenience.It can be seen that this reformulated problem is equivalent to the original, since any control choice that is possible in one problem is also possible in the other problem, while the cost structure of the two problems is the same: each policy of the reformulated problem corresponds to a policy of the original problem, with the same cost function, and reversely.† † A fine point here is that policies of the original problem involve functions of x k , while policies of the reformulated problem involve functions of the choices of the preceding agents, as well as x k .However, by successive substitution of ....9.1 Equivalent formulation of the N -stage stochastic optimal control problem for the case where the control u k consists of m components u 1 k , u 2 k , . . ., u m k :cf. Section 1.6.5.The figure depicts the kth stage transitions.Starting from state x k , we generate the intermediate statesConsider now the standard rollout algorithm applied to the reformulated problem of Fig. 2.9.1, with a given base policy π = {µ 0 , . . ., µ N −1 }, which is also a policy of the original problem [so that µ k = (µ 1 k , . . ., µ m k ), with each µ k , = 1, . . ., m, being a function of just x k ].The algorithm involves a minimization over only one control component at the states x k and at the intermediate statesIn particular, for each stage k, the algorithm requires a sequence of m minimizations, once over each of the agent controls u 1 k , . . ., u m k , with the past controls determined by the rollout policy, and the future controls determined by the base policy.Assuming a maximum of n elements in the constraint sets U k (x k ), the computation required at each stage k is of order O(n) for each of the "states"the control functions of the preceding agents, we can view control functions of each agent as depending exclusively on x k .It follows that the multi-transition structure of the reformulated problem cannot be exploited to reduce the cost function beyond what can be achieved with a single-transition structure.for a total of order O(nm) computation.To elaborate, at (x k , u 1 k , . . ., u −1 k ) with ≤ m, and for each of the controls u k ∈ U k (x k ), we generate by simulation a number of system trajectories up to stage N , with all future controls determined by the base policy.We average the costs of these trajectories, thereby obtaining the Q-factors corresponding to (x k , u 1 k , . . ., u −1 k , u k ), for all values u k ∈ U k (x k ) (with the preceding controls u 1 k , . . ., u −1 k held at the values computed earlier, and the future controls u +1 k , . . ., u m k , u k+1 , . . ., u N −1 determined by the base policy).We then select the control u k ∈ U k (x k ) that corresponds to the minimal Q-factor.Prerequisite assumptions for the preceding algorithm to work in an on-line multiagent setting are: (a) All agents have access to the current state x k as well as the base policy (including the control functions µ n , = 1, . . ., m, n = 0, . . ., N − 1 of all agents).(b) There is an order in which agents compute and apply their local controls.(c) The agents share their information, so agent knows the local controls u 1 k , . . ., u −1 k computed by the predecessor agents 1, . . ., − 1 in the given order.Note that the rollout policy obtained from the reformulated problem may be different from the rollout policy obtained from the original problem.However, the former rollout algorithm is far more efficient than the latter in terms of required computation, while still maintaining the cost improvement property (2.85).The following spiders-and-flies example illustrates how multiagent rollout may exhibit intelligence and agent coordination that is totally lacking from the base policy.This behavior has been supported by computational experiments and analysis with larger (two-dimensional) spiders-andflies problems.Example 2.9.1 (Spiders and Flies)We have two spiders and two flies moving along integer locations on a straight line.For simplicity we assume that the flies' positions are fixed at some integer locations, although the problem is qualitatively similar when the flies move randomly.The spiders have the option of moving either left or right by one unit; see Fig. 2.9.2.The objective is to minimize the time to capture both flies.The problem has essentially a finite horizon since the spiders can force the capture of the flies within a known number of steps.The salient feature of the optimal policy here is to move the two spiders towards different flies.The minimal time to capture is the maximum of the initial distances of the two spider-fly pairs of the optimal policy.Multiagent rollout with the given base policy starts with spider 1 at location n, and calculates the two Q-factors of moving to locations n − 1 and n + 1, assuming that the remaining moves of the two spiders will be made using the go-towards-the-nearest-fly base policy.The Q-factor of going to n−1 is smallest because it saves in unnecessary moves of spider 1 towards fly 2, so spider 1 will move towards fly 1.The trajectory generated by multiagent rollout is to move spiders 1 and 2 towards flies 1 and 2, respectively, then spider 2 first captures fly 2, and then spider 1 captures fly 1.Let us apply multiagent rollout with the base policy that directs each spider to move one unit towards the closest fly position (a tie is broken by moving towards the right-side fly).The base policy is poor because it may unnecessarily move both spiders in the same direction, when in fact only one is needed to capture the fly.This limitation is due to the lack of coordination between the spiders: each acts selfishly, ignoring the presence of the other.We will see that rollout restores a significant degree of coordination between the spiders through an optimization that takes into account the long-term consequences of the spider moves.According to the multiagent rollout mechanism, the spiders choose their moves one-at-a-time, optimizing over the two Q-factors corresponding to the right and left moves, while assuming that future moves will be chosen according to the base policy.Let us consider a stage, where the two flies are alive, while both spiders are closest to fly 2, as in Fig. 2.9.2.Then the rollout algorithm will start with spider 1 and calculate two Q-factors corresponding to the right and left moves, while using the base heuristic to obtain the next move of spider 2, and the remaining moves of the two spiders.Depending on the values of the two Q-factors, spider 1 will move to the right or to the left, and it can be seen that it will choose to move away from spider 2 even if doing so increases its distance to its closest fly contrary to what the base heuristic will do.Then spider 2 will act similarly and the process will continue.Intuitively, at the state of Fig. 2.9.2, spider 1 moves away from spider 2 and fly 2, because it recognizes that spider 2 will capture earlier fly 2, so it might as well move towards the other fly.Thus the multiagent rollout algorithm induces implicit move coordination, i.e., each spider moves in a way that takes into account future moves of the other spider.In fact it can be verified that the algorithm will produce an optimal sequence of moves starting from any initial spider positions.It can also be seen that ordinary rollout (both flies move at once) will also produce an optimal move sequence.The example illustrates how a poor base heuristic can produce an ex-cellent rollout solution, something that can be observed frequently in many other problems.Intuitively, the key fact is that rollout is "farsighted" in the sense that it can benefit from control calculations that reach far into future stages.A two-dimensional generalization of the example is also interesting.Here the flies are at two corners of a square in the plane.It can be shown that the two spiders, starting from the same position within the square, will separate under the rollout policy, with each moving towards a different spider, while under the base policy, they will move in unison along the shortest path to the closest surviving fly.Again this will happen for both standard and multiagent rollout.Let us consider another example of a discrete optimization problem that can be solved efficiently with multiagent rollout.Example 2.9.2 (Multi-Vehicle Routing)Consider a multi-vehicle routing problem, whereby m vehicles move along the arcs of a given graph, aiming to perform tasks located at the nodes of the graph; see Fig. 2.9.3.When a vehicle reaches a task, it performs it, and can move on to perform another task.We wish to perform the taks in a minimum number of individual vehicle moves.For a large number of vehicles and a complicated graph, this is a nontrivial combinatorial problem.The problem can be formulated as a discrete deterministic optimization problem, and addressed by approximate DP methods.The state at a given stage is the m-tuple of current positions of the vehicles together with the list of pending tasks, but the number of these states can be enormous (it increases exponentially with the number of nodes and the number of vehicles).Moreover the number of joint move choices by the vehicles also increases exponentially with the number of vehicles.We are thus motivated to use a multiagent rollout approach.We define a base heuristic as follows: at a given stage and state (vehicle positions and pending tasks), it finds the closest pending task (in terms of number of moves needed to reach it) for each of the vehicles and moves each vehicle one step towards the corresponding closest pending task (this is a legitimate base heuristic: it assigns to each state a vehicle move for every vehicle).† † There is an alternative version of the base heuristic, which makes selections one-vehicle-at-a-time: at a given stage and state (vehicle positions and pending tasks), it finds the closest pending task (in terms of number of moves needed to reach it) for vehicle 1 and moves this vehicle one step towards this closest pending task.Then it finds the closest pending task for vehicle 2 (the pending status of the tasks, however, may have been affected by the move of vehicle 1) and moves this vehicle one step towards this closest pending task, and continues similarly for vehicles 3, . . ., n.There is a subtle difference between the two base heuristics: for example they may make different choices when vehicle 1 reaches a pending task in a single move, thereby changing the status of that task, and affecting the choice of the base heuristic for vehicle 2, etc. 1 2 3 4 5 6 7 8 9 1 2 3 4 5 6 7 8 9 1 2 3 4 5 6 7 8 9 1 2 3 4 5 6 7 8 9 1 2 3 4 5 6 7 8 9 1 2 3 4 5 6 7 8 9 1 2 3 4 5 6 7 8 9 Vehicle 1 Vehicle 2 1 2 3 4 5 6 7 8 9 Vehicle 1 Vehicle 2 10 11 1 2 3 4 5 6 7 8 9 1 2 3 4 5 6 7 8 9 1 2 3 4 5 6 7 8 9 10 11 10 11 12Capacity=1 Optimal SolutionMove each vehicle one step at a time towards its nearest pending task, Move each vehicle one step at a time towards its nearest pending task, until all tasks are performed) Base heuristic Here, we should avoid sending both vehicles to node 4, towards the task at node 7; sending only vehicle 2 towards that task, while sending vehicle 1 towards the task at node 9 is clearly optimal.However, the base heuristic has "limited vision" and does not perceive this.By contrast the standard and the one-vehicle-at-a-time rollout algorithms look beyond the first move and avoid this inefficiency: they examine both moves of vehicle 1 to nodes 3 and 4, and use the base heuristic to explore the corresponding trajectories to the end of the horizon, and discover that vehicle 2 can reach quickly node 7, and that it is best to send vehicle 1 towards node 9.In particular, the one-vehicle-at-a-time rollout algorithm will operate as follows: given the starting position pair (1, 2) of the vehicles and the current pending tasks at nodes 7 and 9, we first compare the Q-factors of the two possible moves of vehicle 1 (to nodes 3 and 4), assuming that all the remaining moves will be selected by the base heuristic at the beginning of each stage.Thus vehicle 1 will choose to move to node 3. Then with knowledge of the move of vehicle 1 from 1 to 3, we select the move of vehicle 2 by comparing the Q-factors of its two possible moves (to nodes 4 and 5), taking also into account the fact that the remaining moves will be made according to the base heuristic.Thus vehicle 2 will choose to move to node 4.We then continue at the next state [vehicle positions at (3,4) and pending tasks at nodes 7 and 9], select the base heuristic moves of vehicles 1 and 2 on the path to the closest pending tasks [(9 and 7), respectively], etc. Eventually the rollout finds the optimal solution (move vehicle 1 to node 9 in three moves and move vehicle 2 to node 7 in two moves), which has a total cost of 5.By contrast it can be seen that the base heuristic at the initial state will move both vehicles to node 4 (towards the closest pending task), and generate a trajectory that moves vehicle 1 along the path 1 → 4 → 7 and vehicle 2 along the path 2 → 4 → 7 → 10 → 12 → 9, while incurring a total cost of 7.In the multiagent rollout algorithm, at a given stage and state, we take up each vehicle in the order 1, . . ., n, and we compare the Q-factors of the available moves to that vehicle while assuming that all the remaining moves will be made according to the base heuristic, and taking into account the moves that have been already made and the tasks that have already been performed; see the illustration of Fig. 2.9.3.In contrast to all-vehicles-at-once rollout, the one-vehicle-at-a-time rollout algorithm considers a polynomial (in m) number of moves and corresponding shortest path problems at each stage.In the example of Fig. 2.9.3, the one-vehicle-at-a-time rollout finds the optimal solution, while the base heuristic starting from the initial state does not.Generally, it is unclear how the two rollout policies (standard/all-agents-atonce and agent-by-agent) perform relative to each other in terms of attained cost.† On the other hand, both rollout policies perform no worse than the base policy, since the performance of the base policy is identical for both the reformulated and the original problems.This cost improvement property can also be shown analytically as follows by induction, by modifying the standard rollout cost improvement proof; cf.Section 2.7.Proposition 2.9.1: (Cost Improvement for Multiagent Rollout) The rollout policy π = {μ 0 , . . ., μN−1 } obtained by multiagent rollout satisfiesfor all x k and k, (2.86)where π is the base policy.† For an example where the standard rollout algorithm works better, consider a single-stage problem, where the objective is to minimize the first stage cost g0(u 1 0 , . . ., u m 0 ).Let u0 = (u 1 0 , . . ., u m 0 ) be the control applied by the base policy, and assume that u0 is not optimal.Suppose that starting at u0, the cost cannot be improved by varying any single control component.Then the multiagent rollout algorithm stays at the suboptimal u0, while the standard rollout algorithm finds an optimal control.Thus, for one-stage problems, the standard rollout algorithm will perform no worse than the multiagent rollout algorithm.The example just given is best seen within the framework of the classical coordinate descent method for minimizing a function of m components.This method can get stuck at a nonoptimal point in the absence of appropriate conditions on the cost function, such as differentiability and/or convexity.However, within our context of multistage rollout and possibly stochastic disturbances, it appears that the consequences of such a phenomenon may not be serious.In fact, one can construct multi-stage examples where multiagent rollout performs better than the standard rollout.Proof: We will show the inequality (2.86) by induction, but for simplicity, we will give the proof for the case of just two agents, i.e., m = 2. Clearly the inequality holds for k = N , since J N,π = J N,π = g N .Assuming that it holds for index k + 1, we have for all x k , (e) The fourth equality is the DP equation for the base policy π.The induction proof of the cost improvement property (2.86) is thus complete for the case m = 2.The proof for an arbitrary number of agents m is entirely similar.Q.E.D.In the multiagent rollout algorithm described so far, the agents optimize the control components sequentially in a fixed order.It is possible to improve performance by trying to optimize at each stage k the order of the agents.An efficient way to do this is to first optimize over all single agent Qfactors, by solving the m minimization problems that correspond to each of the agents = 1, . . ., m being first in the multiagent rollout order.If 1 is the agent that produces the minimal Q-factor, we fix 1 to be the first agent in the multiagent rollout order.Then we optimize over all single agent Qfactors, by solving the m − 1 minimization problems that correspond to each of the agents = 1 being second in the multiagent rollout order.Let 2 be the agent that produces the minimal Q-factor, fix 2 to be the second agent in the multiagent rollout order, and continue in this manner.In the end, afterminimizations, we obtain an agent order 1 , . . ., m that produces a potentially much reduced Q-factor value, as well as the corresponding rollout control component selections.The method just described likely produces substantially better performance, and eliminates the need for guessing a good agent order, but it increases the number of Q-factor calculations needed per stage roughly by a factor (m + 1)/2.Still this is much better than the all-agents-atonce approach, which requires an exponential number of Q-factor calculations.Moreover, the Q-factor minimizations of the above process can be parallelized, so with m parallel processors, we can perform the number of m(m + 1)/2 minimizations derived above in just m batches of parallel minimizations, which require about the same time as in the case where the agents are selected for Q-factor minimization in a fixed order.We finally note that our earlier cost improvement proof goes through again by induction, when the order of agent selection is variable at each stage k.The agent-by-agent rollout algorithm admits several variants.We describe briefly a few of these variants.(c) When the problem is deterministic there are additional possible variants of the multiagent rollout algorithm.In particular, for deterministic problems, we may use a more general base policy, i.e., a heuristic that is not defined by an underlying policy; cf.Section 2.3.1.In this case, if the sequential improvement assumption for the modified problem of Fig. 2.9.1 is not satisfied, then the cost improvement property may not hold.However, cost improvement may be restored by introducing fortification, as discussed in Section 2.3.2.(d) The multiagent rollout algorithm can be simply modified to apply to infinite horizon problems.In this context, we may also consider policy iteration methods, which can be viewed as repeated rollout.These methods may involve agent-by-agent policy improvement, and value and policy approximations of intermediately generated policies (see the RL book [Ber19a], Section 5.7.3).(e) The multiagent rollout algorithm can be simply modified to apply to deterministic continuous-time optimal control problems; cf.Section 2.6.The idea is again to simplify the minimization over u(t) in the case where u(t) consists of multiple components u 1 (t), . . ., u m (t).(f) We can implement within the agent-by-agent rollout context the use of Q-factor differences.The motivation is similar: deal with the approximation errors that are inherent in the estimated cost of the base policy, Jk+1,π f k (x k , u k ) , and may overwhelm the current stage cost term g k (x k , u k ).As noted in Section 2.3.7,this may seriously degrade the quality of the rollout policy; see also the discussion of advantage updating and differential training in Chapter 3.Let us consider a special structure of the control space, where the control u k consists of m components, u k = (u 1 k , . . ., u m k ), each belonging to a corresponding set U k (x k ), = 1, . . ., m.Thus the control space at stage k is the Cartesian productWe refer to this as the multiagent case, motivated by the special case where each component u k , = 1, . . ., m, is chosen by a separate agent at stage k.Similar to the unconstrained case, we can introduce a modified but equivalent problem, involving one-at-a-time agent control selection.In particular, at the generic state x k , we break down the control u k into the se-quence of the m controls u 1 k , u 2 k , . . ., u m k , and between x k and the next stateand corresponding transitions.The choice of the last control component) marks the transition at cost g k (x k , u k ) to the next state x k+1 = f k (x k , u k ) according to the system equation.It is evident that this reformulated problem is equivalent to the original, since any control choice that is possible in one problem is also possible in the other problem, with the same cost.By working with the reformulated problem, we can consider a rollout algorithm requires a sequence of m minimizations per stage, one over each of the control components u 1 k , . . ., u m k , with the past controls already determined by the rollout algorithm, and the future controls determined by running the base heuristic.Assuming a maximum of n elements in the control component spaces U k (x k ), = 1, . . ., m, the computation required for the m single control component minimizations is of order O(nm) per stage.By contrast the standard rollout minimization (2.44) involves the computation of as many as n m terms G T k (ỹ k , u k ) per stage.In this section we consider multiagent rollout algorithms that are distributed and asynchronous in the sense that the agents may compute their rollout controls in parallel rather than in sequence, aiming at computational speedup.An example of such an algorithm is obtained when at a given stage, agent computes the rollout control ũ k before knowing the rollout controls of some of the agents 1, . . ., − 1, and uses the controls µ 1 k (x k ), . . ., µ −1 k (x k ) of the base policy in their place.This algorithm may work well for some problems, but it does not possess the cost improvement property, and may not work well for other problems.In fact we can construct a simple example involving a single state, two agents, and two controls per agent, where the second agent does not take into account the control applied by the first agent, and as a result the rollout policy performs worse than the base policy for some initial states.Example 2.9. 3Consider a problem with two agents (m = 2) and a single state.Thus the state does not change and the costs of different stages are decoupled (the problem is essentially static).Each of the two agents has two controls:Then it can be seen that when executing rollout, the first agent applies u 1 k = 1, and in the absence of knowledge of this choice, the second agent also applies u 2 k = 1 (thinking that the first agent will use the base policy control u 1 k = 0).Thus the cost of the rollout policy is 2 per stage, while the cost of the base policy is 1 per stage.By contrast the rollout algorithm that takes into account the first agent's control when selecting the second agent's control applies u 1 k = 1 and u 2 k = 0, thus resulting in a rollout policy with the optimal cost of 0 per stage.The difficulty here is inadequate coordination between the two agents.In particular, each agent uses rollout to compute the local control, thinking that the other will use the base policy control.If instead the two agents coordinated their control choices, they would have applied an optimal policy.The simplicity of the preceding example raises serious questions as to whether the cost improvement property (2.86) can be easily maintained by a distributed rollout algorithm where the agents do not know the controls applied by the preceding agents in the given order of local control selection, and use instead the controls of the base policy.One may speculate that if the agents are naturally "weakly coupled" in the sense that their choice of control has little impact on the desirability of various controls of other agents, then a more flexible inter-agent communication pattern may be sufficient for cost improvement.† An important question is to clarify the extent to which agent coordination is essential.In what follows in this section, we will discuss a distributed asynchronous multiagent rollout scheme, which is based on the use of a signaling policy that provides estimates of coordinating information once the current state is known.An interesting possibility for autonomous control selection by the agents is to use a distributed rollout algorithm, which is augmented by a precomputed signaling policy that embodies agent coordination.‡ The idea is to assume that the agents do not communicate their computed rollout control † In particular, one may divide the agents in "coupled" groups, and require coordination of control selection only within each group, while the computation of different groups may proceed in parallel.Note that the "coupled" group formations may change over time, depending on the current state.For example, in applications where the agents' locations are distributed within some geographical area, it may make sense to form agent groups on the basis of geographic proximity, i.e., one may require that agents that are geographically near each other (and hence are more coupled) coordinate their control selections, while agents that are geographically far apart (and hence are less coupled) forego any coordination.‡ The general idea of coordination by sharing information about the agents' policies arises also in other multiagent algorithmic contexts, including some that involve forms of policy gradient methods and Q-learning; see the surveys of the components to the subsequent agents in the given order of local control selection.Instead, once the agents know the state, they use precomputed (or easily computed) approximations to the control components of the preceding agents, and compute their own control components in parallel and asynchronously.We call this algorithm autonomous multiagent rollout .While this type of algorithm involves a form of redundant computation, it allows for additional speedup through parallelization.The algorithm at the kth stage uses a base policy}, called the signaling policy, which is computed off-line, is known to all the agents for on-line use, and is designed to play an agent coordination role.Intuitively, µ k (x k ) provides an intelligent "guess" about what agent will do at state x k .This is used in turn by all other agents i = to compute asynchronously their own rollout control components on-line.More precisely, the autonomous multiagent rollout algorithm uses the base and signaling policies to generate a rollout policy π = {μ 0 , . . ., μN−1 } as follows.At stage k and statek (x k ), . . ., μm k (x k ) can be done asynchronously and in parallel, and without direct agent coordination, since the signaling policy values µ 1 k (x k ), . . ., µ m−1 k (x k ) are precomputed and are known to all the agents.The simplest choice is to use as signaling policy µ the base policy µ.However, this choice does not guarantee policy improvement as evidenced by Example 2.9.3.In fact performance deterioration with this choice is not uncommon, and can be observed in more complicated examples, including the following.Example 2.9.4 (Spiders and Flies -Use of the Base Policy for Signaling)Consider the problem of Example 2.9.1, which involves two spiders and two flies on a line, and the base policy µ that moves a spider towards the closest surviving fly (and in case where a spider starts at the midpoint between the two flies, moves the spider to the right).Assume that we use as signaling policy µ the base policy µ.It can then be verified that if the spiders start from different positions, the rollout policy will be optimal (will move the spiders in opposite directions).If, however, the spiders start from the same position, a completely symmetric situation is created, whereby the rollout controls move both flies in the direction of the fly furthest away from the spiders' position (or to the left in the case where the spiders start at the midpoint between the two flies).Thus, the flies end up oscillating around the middle of the interval between the flies and never catch the flies!The preceding example is representative of a broad class of counterexamples that involve multiple identical agents.If the agents start at the same initial state, with a base policy that has identical components, and use the base policy for signaling, the agents will select identical controls under the corresponding multiagent rollout policy, ending up with a potentially serious cost deterioration.This example also highlights an effect of the sequential choice of the control components u 1 k , . . ., u m k , based on the reformulated problem of Fig. 2.9.1: it tends to break symmetries and "group think" that guides the agents towards selecting the same controls under identical conditions.Generally, any sensible multiagent policy must be able to deal in some way with this "group think" issue.One simple possibility is for each agent to randomize somehow the control choices of other agents j = when choosing its own control, particularly in "tightly coupled" cases where the choice of agent is "strongly" affected by the choices of the agents j = .An alternative idea is to choose the signaling policy µ k to approximate the sequential multiagent rollout policy (the one computed with each agent knowing the controls applied by the preceding agents), or some other policy that is known to embody coordination between the agents.In particular, we may obtain µ k as the multiagent rollout policy for a related but simpler problem, such as a certainty equivalent version of the original problem, whereby the stochastic system is replaced by a deterministic one.Another interesting possibility is to compute µ k = ( µ 1 k , . . ., µ m k ) by off-line training of a neural network (or m networks, one per agent) with training samples generated through the sequential multiagent rollout policy.We intuitively expect that if the neural network provides a signaling policy that approximates well the sequential multiagent rollout policy, we would obtain better performance than the base policy.This expectation was confirmed in a case study involving a large-scale multi-robot repair application (see [BKB20]).The advantage of autonomous multiagent rollout with neural network or other approximations is that it may lead to approximate policy improvement, while at the same time allowing asynchronous agent operation without coordination through communication of their rollout control values (but still assuming knowledge of the exact state by all agents).In this section, we discuss a wide class of problems that has been studied intensively in statistics and related fields since the 1940s.Roughly speaking, in these problems we use observations and sampling for the purpose of inference, but the number and the type of observations are not fixed in advance.Instead, the outcomes of the observations are sequentially evaluated on-line with a view towards stopping or modifying the observation process.This involves sequential decision making, thus bringing to bear exact and approximate DP.A central issue here is to estimate an m-dimensional random vector θ, using optimal sequential selection of observations, which are based on feedback from preceding observations; see Fig. 2.10.1.Here is a simple but historically important illustrative example, where θ represents a binary hypothesis.Example 2.10.1 (Hypothesis Testing -Sequential Probability Ratio Test)Consider a hypothesis testing problem whereby we can make observations, at a cost C each, relating to two hypotheses.Given a new observation, we can either accept one of the hypotheses or delay the decision for one more period, pay the cost C, and obtain a new observation.At issue is trading off the cost of observation with the higher probability of accepting the wrong hypothesis.As an example, in a quality control setting, the two hypotheses may be that a certain product meets or does not meet a certain level of quality, while the observations may consist of quantitative tests of the quality of the product.Intuitively, one expects that once the conditional probability of one of the hypotheses, given the observations thus far, gets sufficiently close to 1, we should stop the observations.Indeed classical DP analyses bear this out; see e.g., the books by Chernoff [Che72], DeGroot [DeG70], Whittle [Whi82], and the references quoted therein.In particular, the simple version of the hypothesis testing problem just described admits a simple and elegant optimal solution, known as the sequential probability ratio test.On the other hand more complex versions of the problem, involving for example multiple hypotheses and/or multiple types of observations, are computationally intractable, thus necessitating the use of suboptimal approaches.future observations.If this is so, the problem can often be viewed most fruitfully as a combined estimation and control problem, and is related to a type of adaptive control problem that we will discuss in the next section.As an example we will consider there sequential decoding, whereby we search for a hidden code word by using a sequence of queries, in the spirit of the Wordle puzzle and the family of Mastermind games [see, e.g., the Wikipedia page for "Mastermind (board game)"].If the observation choices are "independent" and do not affect the cost or availability of future observations, the problem is substantially simplified.We will discuss problems of this type in the present section, starting with the case of surrogate and Bayesian optimization.Surrogate optimization refers to a collection of methods, which address suboptimally a broad range of minimization problems, beyond the realm of DP.The problem is to minimize approximately a function that is given as a "black box."By this we mean a function whose analytical expression is unknown, and whose values at any one point may be hard-to-compute, e.g., may requite costly simulation or experimentation.The idea is to replace such a cost function with a "surrogate" whose values are easier to compute.Here we introduce a model of the cost function that is parametrized by a parameter θ; see Fig. 2.10.2.We observe sequentially the cost function at a few observation points, construct a model of the cost function (the surrogate) by estimating θ based on the results of the observations, and minimize the surrogate to obtain a suboptimal solution.The question is how to select observation points sequentially, using feedback from previous observations.This selection process often embodies an exploration-  function f whose values are hard-to-compute.We replace f with a parametric model that involves a parameter θ to be estimated by using observations at some points.The points are selected sequentially, using the results of earlier observations.Eventually, the observation process is stopped (often when an observation/computation budget limit is reached), and the final estimate of θ is used to construct the surrogate to be minimized in place of f .exploitation tradeoff : Observing at points likely to have near-optimal value vs observing at points in relatively unexplored areas of the search space.Surrogate optimization at its core involves construction from data of functions of interest.Thus the ideas to be presented apply to other domains, e.g., the construction of probability density functions from data.Bayesian optimization (BO) has been used widely for the approximate optimization of functions whose values at given points can only be obtained through time-consuming calculation, simulation, or experimentation.A classical application from geostatistical interpolation, pioneered by the statisticians Matheron and Krige, was to identify locations of high gold distribution in South Africa based on samples from a few boreholes (the name "kriging" is often used to refer to this type of application; see the review by Kleijnen [Kle09]).As another example, BO has been used to select the hyperparameters of machine-learning models, including the architectural parameters of the deep neural network of AlphaZero; see [SHS17].In this section, we will focus on a relatively simple BO formulation that can be viewed as the special case of surrogate optimization.In particular, we will discuss the case where the surrogate function is parametrized by the collection of its values at the points where it is defined.† See the references cited later in this section.Formally, we want to minimize a † More complex forms of surrogates are obtained through linear combinations real-valued function f , defined over a set of m points, which we denote by 1, . . ., m.These m points lie in some space, which we leave unspecified for the moment.† The values of the function are not readily available, but can be estimated with observations that may be imperfect.However, the observations are so costly that we can only hope to observe the function at a limited number of points.Once the function has been estimated with this type of observation process, we obtain a surrogate cost function, which may be minimized to obtain an approximately optimal solution.We denote the value of f at a point u by θ u :Thus the m-dimensional vector θ = (θ 1 , . . ., θ m ) belongs to m and represents the function f .We assume that we obtain sequentially noisy observations of values f (u) = θ u at suitably selected points u.These values are used to estimate the vector θ (i.e., the function f ), and to ultimately minimize (approximately) f over the m points u = 1, . . ., m.The essence of the problem is to select points for observation based on an explorationexploitation tradeoff (exploring the potential of relatively unexplored candidate solutions and improving the estimate of promising candidate solutions).The fundamental idea of the BO methodology is that the function value changes relatively slowly, so that observing the function value at some point provides information about the function values at neighboring points.Thus a limited number of strategically chosen observations can provide reasonable approximation to the true cost function over a large portion of the search space.For a mathematical formulation of a BO framework, we assume that at each of N successive times k = 1, . . ., N, we select a single point u k ∈ {1, . . ., m}, and observe the corresponding component θ u k of θ (i.e., the function value at u k ) with some noise w u k , i.e.,(2.89) see Fig. 2.10.3.We view the observation points u 1 , . . ., u N as the optimization variables (or controls/actions in a DP/RL context), and consider policies for selecting u k with knowledge of the preceding observations z u 1 , . . ., z u k−1 that have resulted from the selections u 1 , . . ., u k−1 .We assume that the noise random variables w u , u ∈ {1, . . ., m} are independent of some basis functions, with the parameter vector θ consisting of the weights of the basis functions.† We restrict the domain of definition of f to be the finite set {1, . . ., m} in order to facilitate the implementation of the rollout algorithm to be discussed in what follows.However, in a more general and sometimes more convenient formulation, the domain of f can be an infinite set, such as a subset of a finitedimensional Euclidean space.An important special case arises when b 0 and the distributions of w u , u ∈ {1, . . ., m}, are Gaussian.In this case b 0 is a multidimensional Gaussian distribution, defined by its mean (based on prior knowledge, or an equal value for all u = 1, . . ., m in case of absence of such knowledge) and its covariance matrix [implying greater correlation for pairs (u, u ) that are "close" to each other in some problem-specific sense, e.g., exponentially decreasing with the Euclidean distance between u and u ].A key consequence of this assumption is that the posterior distribution b k is multidimensional Gaussian, and can be calculated in closed form by using well-known formulas.More generally, b k evolves according to an equation of the formThus given the set of observations up to time k, and the next choice u k+1 , resulting in an observation value z u k+1 , the function B k gives the formula for updating b k to b k+1 , and may be viewed as a recursive estimator of b k .In the Gaussian case, the function B k can be written in closed form, using standard formulas for Gaussian random vector estimation.In other cases where no closed form expression is possible, B k can be implemented through simulation that computes (approximately) the new posterior b k+1 using samples generated from the current posterior b k .At the end of the sequential estimation process, after the complete observation set {z u 1 , . . ., z u N } has been obtained, we have the posterior distribution b N of θ, which we can use to compute a surrogate of f .As an example we may use as surrogate the posterior mean θ = ( θ1 , . . ., θm ), and declare as minimizer of f over u the point u * with minimum posterior mean:There is a large literature relating to the surrogate and Bayesian optimization methodology and its applications, particularly for the Gaussian case.We refer to the books by Rasmussen and Williams [RaW06], Powell and Ryzhov [PoR12], the highly cited papers by Saks et al.  [Fra18], and the references quoted there.Our purpose here is to focus on the aspects of the subject that are most closely connected to exact and approximate DP.The sequential estimation problem just described, viewed as a DP problem, involves a state at time k, which is the posterior (or belief state) b k , and a control/action at time k, which is the point index u k+1 selected for observation.The transition equation according to which the state evolves, iscf. Eq. (2.90).To complete the DP formulation, we need to introduce a cost structure.To this end, we assume that observing θ u , as per Eq.(2.89), incurs a cost c(u), and that there is a terminal cost G(b N ) that depends of the final posterior distribution; as an example, the function G may involve the mean and covariance corresponding to b N .The corresponding DP algorithm is given by(2.91) and proceeds backwards from the terminal condition(2.92)The expected value in the right side of the DP equation (2.91) is taken with respect to the conditional distribution of z u k+1 , given b k and the choice u k+1 .The observation cost c(u) may be 0 or a constant for all u, but it can also have a more complicated dependence on u.The terminal cost G(b N ) may be a suitable measure of surrogate "fidelity" that depends on the posterior mean and covariance of θ corresponding to b N .Generally, executing the DP algorithm (2.91) is practically infeasible, because the space of posterior distributions is infinite-dimensional.In the Gaussian case where the a priori distribution b 0 is Gaussian and the noise variables w u are Gaussian, the posterior b k is m-dimensional Gaussian, so it is characterized by its mean and covariance, and can be specified by a finite set of numbers.Despite this simplification, the DP algorithm (2.91) is prohibitively time-consuming even under Gaussian assumptions, except for simple special cases.We consequently resort to approximation in value space, whereby the function J * k+1 in the right side of Eq. (2.91) is replaced by an approximation Jk+1 .The most popular BO methodology makes use of a myopic/greedy policy µ k+1 , which at each time k and given b k , selects a point ûk+1 = µ k+1 (b k ) for the next observation, using some calculation involving an acquisition function.This function, denoted A k (b k , u k+1 ), quantifies some form of "expected benefit" for an observation at u k+1 , given the current posterior b k .† The myopic policy selects the next point at which to observe, ûk+1 , by maximizing the acquisition function:(2.93)Several ways to define suitable acquisition functions have been proposed, and an important issue is to be able to calculate economically its values A k (b k , u k+1 ) for the purposes of the maximization in Eq. (2.93).Another important issue of course is to be able to calculate the posterior b k economically.Approximation in value space is an alternative approach, which is based on the DP formulation of the preceding section.In particular, in this approach we approximate the DP algorithm (2.91) by replacing J * k+1 with an approximation Jk+1 in the minimization of the right side.Thus we select the next observation at point ũk+1 according to ũk+1 ∈ arg minwheregiven by(2.95) The expected value in the preceding equation is taken with respect to the conditional probability distribution of z u k+1 given (b k , u k+1 ), which can be † A common type of acquisition function is the upper confidence bound , which has the formA special case of approximation in value space is the rollout algorithm, whereby the function J * k+1 in the right side of the DP Eq. (2.91) is replaced by the cost function of some base policy µ k+1 (b k ), k = 0, . . ., N − 1.Thus, given a base policy the rollout algorithm uses the cost function of this policy as the function Jk+1 in the approximation in value space scheme (2.94)-(2.95).The values of Jk+1 needed for the Q-factor calculations in Eq. (2.95) can be computed or approximated by simulation.Greedy/myopic policies based on an acquisition function [cf.Eq. (2.93)] have been suggested as base policies in various rollout proposals.†In particular, given b k , the rollout algorithm computes for each , where it is also referred to as "nonmyopic BO" or "nonmyopic sequential experimental design."For related work, see Gerlach, Hoffmann, and Charlish [GHC21].These papers also discuss various approximations to the rollout approach, and generally report encouraging computational results.Section 3.5 of the author's book [Ber20a] focuses on rollout algorithms for surrogate and Bayesian optimization.One  The simulation of the Q-factor values may also involve other approximations, some of which have been suggested in various proposals for rolloutbased BO.For example, if the number of possible observations m is very large, we may compute and compare the Q-factors of only a subset.In particular, at a given time k, we may rank the observations by using an acquisition function, select a subset U k+1 of most promising observations, compute their Q-factors Q k (b k , u k+1 ), u k+1 ∈ U k+1 , and select the observation whose Q-factor is minimal; this idea has been used in the case of the Wordle puzzle in the papers by Bhambri, Bhattacharjee, and Bertsekas [BBB22], [BBB23], which will be discussed in the next section.In some BO applications there arises the possibility of simultaneously performing multiple observations before receiving feedback about the corresponding observation outcomes.This occurs, among others, in two important contexts:(a) In parallel computation settings, where multiple processors are used to perform simultaneously expensive evaluations of the function f at multiple points u.These evaluations may involve some form of truncated simulation, so they yield evaluations of the form z u = θ u + w u , where w u is the simulation noise.(b) In distributed sensor systems, where a number of sensors provide in parallel relevant information about the random vector θ that we want to estimate; see e.g., the recent paper by Li, Krakow, and Gopalswamy [LKG21], which describes related multisensor estimation problems, based on the multiagent rollout methodology of Section 2.9.Of course in such cases we may treat the entire set of simultaneous observations as a single observation within an enlarged Cartesian product space of observations, but there is a fundamental difficulty: the size of the observation space (and hence the number of Q-factors to be calculated by rollout at each time step) grows exponentially with the number of simultaneous observations.This in turn greatly increases the computational requirements of the rollout algorithm.To address this difficulty, we may employ the methodology of multiagent rollout whereby the policy improvement is done one-agent-at-a-time in a given order, with (possibly partial) knowledge of the choices of the preceding agents in the order.As a result, the amount of computation for each policy improvement grows linearly with the number of agents, as opposed to exponentially for the standard all-agents-at-once method.At the same time the theoretical cost improvement property of the rollout algorithm can be shown to be preserved, while the empirical evidence suggests that great computational savings are achieved with hardly any performance degradation.Aside from BO, there are several other types of simple sequential estimation problems, which involve "independent sampling," i.e., problems where the choice of an observation type does not affect the quality, cost, or availability of observations of other types.A common class of problems that contains BO as a special case and admits a similar treatment, is to sequentially estimate an m-dimensional random vector θ = (θ 1 , . . ., θ m ) by using N linear observations of θ of the formwhere n is some integer.Here w u are independent random variables with given probability distributions, the m-dimensional vectors a u are known, and a u θ denotes the inner product of a u and θ.Similar to the case of BO, the problem simplifies if the given a priori distribution of θ is Gaussian, and the random variables w u are independent and Gaussian.Then, the posterior distribution of θ, given any subset of observations, is Gaussian (thanks to the linearity of the observations), and can be calculated in closed form.Observations are generated sequentially at times 1, . . ., N, one at a time and with knowledge of the outcomes of the preceding observations, by choosing an index u k ∈ {1, . . ., n} at time k, at a cost c(u k ).Thus u k are the optimization variables, and affect both the quality of estimation of θ and the observation cost.The objective, roughly speaking, is to select N observations to estimate θ in a way that minimizes an appropriate cost function; for example, one that penalizes some form of estimation error plus the cost of the observations.We can similarly formulate the corresponding optimization problem in terms of N -stage DP, and develop rollout algorithms for its approximate solution.In this section, we discuss various approaches for the approximate solution of Partially Observed Markovian Decision Problems (POMDP) with a special structure, which is well-suited for adaptive control, as well as other contexts that involve search for a hidden object.† It is well known that POMDP are among the most challenging DP problems, and nearly always require the use of approximations for (suboptimal) solution.The application and implementation of rollout and approximate PI methods to general finite-state POMDP is described in the author's RL book [Ber19a] (Section 5.7.3).Here we will focus attention on a special class of POMDP where the state consists of two components: (a) A perfectly observed component x k that evolves over time according to a discrete-time equation.(b) An unobserved component θ that stays constant and is estimated through the perfect observations of the component x k .We view θ as a parameter in the system equation that governs the evolution of x k , hence the connection with adaptive control.Thus we havewhere u k is the control at time k, selected from a set U k (x k ), and w k is a random disturbance with given probability distribution that depends † In Section 1.6.6,we discussed the indirect adaptive control approach, which enforces a separation of the controller into a system identification algorithm and a policy reoptimization algorithm.The POMDP approach of this section (also summarized in Section 1.6.6),does not assume such an a priori separation, and is thus founded on a more principled algorithmic framework.11.1 Illustration of an adaptive control scheme involving perfect state observation of a system with an unknown parameter θ.At each time a decision is made to select a control and (possibly) one of several observation types, each of different cost.on (x k , θ, u k ).We will assume that θ can take one of m known values θ 1 , . . ., θ m : θ ∈ {θ 1 , . . ., θ m }, see Fig. 2.11.1.The a priori probability distribution of θ is given and is updated based on the observed values of the state components x k and the applied controls u k .In particular, we assume that the information vectorwhere B k is an appropriate function, which can be viewed as a recursive estimator of θ.There are several approaches to implement this estimator (perhaps with some approximation error), including the use of Bayes' rule and the simulation-based method of particle filtering.The preceding mathematical model forms the basis for a classical adaptive control formulation, where each θ i represents a set of system parameters, and the computation of the belief probabilities b k,i can be viewed as the outcome of a system identification algorithm.In this context, the problem becomes one of dual control , a classical type of combined identification and control problem, whose optimal solution is notoriously difficult.Another interesting context arises in search problems, where θ specifies the locations of one or more objects of interest within a given space.Some puzzles, including the popular Wordle game, fall within this category, as we will discuss briefly later in this section.We will now describe an exact DP algorithm that operates in the space of information vectors I k .To describe this algorithm, let us denote by J k (I k ) the optimal cost starting at information vector I k at time k.We can view I k as a state of the POMDP, which evolves over time according to the equationViewing this as a system equation, whose right hand side involves the state I k , the control u k , and the disturbance w k , the DP algorithm takes the form(2.97) for k = 0, . . ., N − 1, with J N (I N ) = g N (x N ); see e.g., the DP textbook [Ber17a], Section 4.1.The algorithm (2.97) is typically very hard to implement, in part because of the dependence of J * k+1 on the entire information vector I k+1 , which expands in size according toTo address this difficulty, we may use approximation in value space, based on replacing J * k+1 (I k+1 ) in the DP algorithm (2.97) with some function Jk+1 (I k+1 ) such that the expected valuecan be obtained with a tractable computation for any (I k , u k ).A useful possibility arises when the cost function approximationscan be obtained for each fixed value of θ i with a tractable computation.In this case, we may compute the cost function approximation (2.98) by using the formulawhich follows from the law of iterated expectations,We will now discuss some choices of functions Jk+1 with a structure that facilitates the implementation of the corresponding approximation in value space scheme.One possibility is to use the optimal cost functions corresponding to the m parameters θ i , Ĵi k+1 (x k+1 ), i= 1, . . ., m.(2.99)In particular, Ĵi k+1 (x k+1 ) is the optimal cost that would be obtained starting from state x k+1 under the assumption that θ = θ i ; this corresponds to a perfect state information problem.Then an approximation in value space scheme with one-step lookahead minimization is given by ũk ∈ arg min(2.100) In particular, instead of the optimal control, which minimizes the optimal Q-factor of (I k , u k ) appearing in the right side of Eq. (2.97), we apply control ũk that minimizes the expected value over θ of the optimal Qfactors that correspond to fixed values of θ.In the case where the horizon is infinite, it is reasonable to expect that an improving estimate of the parameter θ can be obtained over time, and that with a suitable estimation scheme, it converges asymptotically to the correct value of θ, call it θ * , i.e., limThen it can be seen that the generated one-step lookahead controls ũk are asymptotically obtained from the Bellman equation that corresponds to the correct parameter θ * , and are typically optimal in some asymptotic sense.Schemes of this type have been extensively discussed in the adaptive control literature since the 70s; see the end-of-chapter references and discussion.Generally, the optimal costs Ĵi k+1 (x k+1 ) of Eq. (2.99), which correspond to the different parameter values θ i , may be hard to compute, despite the fact that they correspond to perfect state information problems.† An alternative possibility is to use off-line trained approximations to Ĵi k+1 (x k+1 ) involving neural networks or other approximation architectures.Still another possibility, described next, is to use a rollout approach.A simpler possibility for approximation in value space is to use the cost of a given policy π i in place of the optimal cost Ĵi k+1 (x k+1 ) of Eq. (2.99) that corresponds to θ i .In this case the one-step lookahead scheme (2.100) takes the form ũk ∈ arg min(2.101) with π i = {µ i 0 , . . ., µ i N −1 }, i = 1, . . ., m, being known policies, with components µ i k that depend only on x k .Here, the termin Eq. (2.101) is the cost of the base policy π i , calculated starting from the next stateunder the assumption that θ will stay fixed at the value θ = θ i until the end of the horizon.Note that the cost function of π i , conditioned on θ = θ i , x k , and u k , which is needed in Eq. (2.101), can be calculated by Monte Carlo simulation.This is made possible by the fact that the componentsThe preceding scheme has the character of a rollout algorithm, but strictly speaking, it does not qualify as a rollout algorithm because the † In favorable special cases, such as linear quadratic problems, the optimal costs Ĵi k+1 (x k+1 ) may be easily calculated in closed form.Still, however, even in such cases the calculation of the belief probabilities b k,i may not be simple, and may require the use of a system identification algorithm.policy components µ i k involve a dependence on i in addition to the dependence on x k .On the other hand if we restrict all the policies π i to be the same for all i, the corresponding functions µ k depend only on x k and not on i, thus defining a legitimate base policy.In this case the rollout scheme (2.101) amounts to replacingSimilar to Section 2.7, a cost improvement property can then be shown.Within our rollout context, a policy π such that π i = π for all i should be a robust policy, in the sense that it should work adequately well for all parameter values θ i .The method to obtain such a policy is likely problem-dependent.On the other hand robust policies have a long history in the context of adaptive control, and have been discussed widely (see e.g., the book by Jiang and Jiang [JiJ17], and the references quoted therein).Let us now consider the case where the system (2.96) is deterministic of the formThen, while the problem still has a stochastic character due to the uncertainty about the value of θ, the DP algorithm (2.97) and its approximation in value space counterparts are greatly simplified because there is no expectation over w k to contend with.Indeed, given a state x k , a parameter θ i , and a control u k , the on-line computation of the control of the rollout-like algorithm (2.101), takes the form ũk ∈ arg min(2.103) The computation of Ĵi k+1,π i f k (x k , θ i , u k ) involves a deterministic propagation from the state x k+1 of Eq. (2.102) up to the end of the horizon, using the base policy π i , while assuming that θ is fixed at the value θ i .In particular, the termappearing on the right side of Eq. (2.103) is viewed as a Q-factor that must be computed for every pair (u k , θ i ), u k ∈ U k (x k ), i = 1, . . ., m, using the base policy π i .The expected value of this Q-factor,) . . .) . . .) . . .Price Rise Base Policy π 1Price Rise Base Policy π 1Base Policy π 2Base Policy π 2must then be calculated for every u k ∈ U k (x k ), and the computation of the rollout control ũk is obtained from the minimization ũk ∈ arg min(2.105) cf.Eq. (2.103).This computation is illustrated in Fig. 2.11.2.The case of a deterministic system is particularly interesting because we can typically expect that the true parameter θ * is identified in a finite number of stages, since at each stage k, we are receiving a noiseless measurement relating to θ, namely the state x k .Once this happens, the problem becomes one of perfect state information.An illustration similar to the one of Fig. 2.11.2 applies to the rollout scheme (2.101) for the case of a stochastic system.In this case, a Q-factor) must be calculated for every triplet (u k , θ i , w k ), using the base policy π i .The rollout control ũk is obtained by minimizing the expected value of this Q-factor [averaged using the distribution of (θ, w k )]; cf.Eq. (2.101).An interesting and intuitive example that demonstrates the deterministic system case is the popular Wordle puzzle.Example 2.11.1 (The Wordle Puzzle)In the classical form of this puzzle, we try to guess a mystery word ✓ ⇤ out of a known finite collection of 5-letter words.This is done with sequential guesses each of which provides additional information on the correct word ✓ ⇤ , by using certain given rules to shrink the current mystery list (the smallest list that contains ✓ ⇤ , based on the currently available information).The objective is to minimize the number of guesses to find ✓ ⇤ (using more than 6 guesses is considered to be a loss).This type of puzzle descends from the classical family of Mastermind puzzles that centers around decoding a secret sequence of objects (e.g., letters or colors) using partial observations.The rules for shrinking the mystery list relate to the common letters between the word guesses and the mystery word ✓ ⇤ , and they will not be described here (there is a large literature regarding the Wordle puzzle).Moreover, ✓ ⇤ is assumed to be chosen from the initial collection of 5-letter words according to a uniform distribution.Under this assumption, it can be shown that the belief distribution b k at stage k continues to be uniform over the mystery list.As a result, we may use as state x k the mystery list at stage k, which evolves deterministically according to an equation of the form (2.102), where u k is the guess word at stage k.There are several base policies to use in the rollout-like algorithm (2.103), which are described in the papers by Bhambri, Bhattacharjee, and Bertsekas [BBB22], [BBB23], together with computational results, which show that the corresponding rollout algorithm (2.103) performs remarkably close to the optimal policy (first obtained with a very computationally intensive exact DP calculation by Selby in 2022).The rollout approach also applies to several variations of the Wordle puzzle.Such variations may include for example a larger length `> 5 of mystery words, and/or a known nonuniform distribution over the initial collection of `-letter words; see [BBB22].We finally note that the adaptive control framework of this section contains as a special case the sequential estimation framework of the preceding section.Here the problem formulation involves a dynamic system of the formwhere the state x k+1 is the observation at time k+1 and exhibits no explicit dependence on the preceding observation x k , but depends on the stochastic disturbance w k , and on the decision u k ; cf.Figs.2.11.1 and 2.11.3.This decision may involve a cost and determines the type of next observation out of a collection of possible types.While the rollout methodology of the present section applies to sequential estimation problems, other rollout algorithms may also be used, depending on the problem's detailed structure.In particular, the rollout algorithms for Bayesian optimization of the works noted in Section 2.10 involve base policies that depend on the current belief state b k , ratherA view of sequential estimation as an adaptive control problem.The system function f k does not depend on the current state x k , so the system provides a decision-dependent noisy observation of θ.than the current state x k .Another example of rollout for adaptive control, which uses a base policy that depends on the current belief state is given in Section 6.7 of the book [Ber22a].For work on related stochastic optimal control problems that involve observation costs and the rollout approach, see Antunes and Heemels [AnH14], and Khashooei, Antunes, and Heemels [KAH15].The problem of optimal control of uncertain systems is usually treated within a stochastic framework, whereby all disturbances w 0 , . . ., w N −1 are described by probability distributions, and the expected value of the cost is minimized.However, in many practical situations a stochastic description of the disturbances may not be available, but one may have information with less detailed structure, such as bounds on their magnitude.In other words, one may know a set within which the disturbances are known to lie, but may not know the corresponding probability distribution.Under these circumstances one may use a minimax approach, whereby the worst possible values of the disturbances within the given set are assumed to occur.Within this context, we take the view that the disturbances are chosen by an antagonistic opponent.The minimax approach is also connected with two-player games, when in lack of information about the opponent, we adopt a worst case viewpoint during on-line play, as well as with contexts where we wish to guard against adversarial attacks.† † The minimax approach to decision and control has its origins in the 50s and 60s.It is also referred to by other names, depending on the underlying To be specific, consider a finite horizon context, and assume that the disturbances w 0 , w 1 , . . ., w N −1 do not have a probabilistic description but rather are known to belong to corresponding given sets W k (x k , u k ) ⊂ D k , k = 0, 1, . . ., N − 1, which may depend on the current state x k and control u k .The minimax control problem is to find a policy π = {µ 0 , . . ., µ N −1 } with µ k (x k ) ∈ U k (x k ) for all x k and k, which minimizes the cost functionThe DP algorithm for this problem takes the following form, which resembles the one corresponding to the stochastic DP problem (maximization is used in place of expectation):(2.107)This algorithm can be explained by using a principle of optimality type of argument.In particular, we consider the tail subproblem whereby we are at state x k at time k, and we wish to minimize the "cost-to-go" max w t ∈W t (x t ,µ t (x t )) t=k,k+1,...,N−1We argue that if π * = {µ * 0 , µ * 1 , . . ., µ * N −1 } is an optimal policy for the minimax problem, then the tail of the policy {µ * k , µ * k+1 , . . ., µ * N −1 } is optimal for the tail subproblem.The optimal cost of this subproblem is J * k (x k ), as given by the DP algorithm (2.106)-(2.107).The algorithm expresses the intuitive fact that when at state x k at time k, then regardless of what happened in the past, we should choose u k that minimizes the worst/maximum value over w k of the sum of the current stage cost plus the optimal cost of the tail subproblem that starts from the next state.This argument requires a mathematical proof, which turns out to involve a few fine points.For a detailed mathematical derivation, we refer to the author's textbook [Ber17a], Section 1.6.However, the DP algorithm (2.106)-(2.107) is correct assuming finite state and control spaces, among other cases.context, such as robust control , robust optimization, control with a set membership description of the uncertainty, and games against nature.In this book, we will be using the minimax control name.The approximation ideas for stochastic optimal control are also relevant within the minimax context.In particular, approximation in value space with one-step lookahead applies at state x k a control ũk ∈ arg min(2.108) where Jk+1 (x k+1 ) is an approximation to the optimal cost-to-go J * k+1 (x k+1 ) from state x k+1 .Rollout is obtained when this approximation is the tail cost of some base policy π = {µ 0 , . . ., µ N −1 }:Given π, we can compute J k+1,π (x k+1 ) by solving a deterministic maximization DP problem with the disturbances w k+1 , . . ., w N −1 playing the role of "optimization variables/controls."For finite state, control, and disturbance spaces, this is a longest path problem defined on an acyclic graph, since the control variables u k+1 , . . ., u N −1 are determined by the base policy.It is then straightforward to implement rollout: at x k we generate all next states of the formWe then run the maximization/longest path problem described above to compute Jk+1 (x k+1 ) from each of these possible next states x k+1 .Finally, we obtain the rollout control ũk by solving the minimax problem in Eq. (2.108).Moreover, it is possible to use truncated rollout to approximate the tail cost of the base policy.† Note that like all rollout algorithms, the minimax rollout algorithm is well-suited for on-line replanning in problems where data may be changing or may be revealed during the process of control selection.We mentioned earlier that deterministic problems allow a more general form of rollout, whereby we may use a base heuristic that need not be a legitimate policy, i.e., it need not be sequentially consistent.For cost improvement it is sufficient that the heuristic be sequentially improving.A similarly more general view of rollout is not easily constructed for stochastic problems, but is possible for minimax control.In particular, suppose that at any state x k there is a heuristic that generates a sequence of feasible controls and disturbances, and corresponding states, {u k , w k , x k+1 , u k+1 , w k+1 , x k+2 , . . ., u N −1 , w N −1 , x N }, † For a more detailed discussion of this implementation, see the author's paper [Ber19b] (Section 5.4).with corresponding costThen the rollout algorithm applies at state x k a control ũk ∈ arg minThis does not preclude the possibility that the disturbances w k , . . ., w N −1 are chosen by an antagonistic opponent, but allows more general choices of disturbances, obtained for example, by some form of approximate maximization.For example, when the disturbance involves multiple components, w k = (w 1 k , . . ., w m k ), corresponding to multiple opponent agents, the heuristic may involve an agent-by-agent maximization strategy.The sequential improvement condition, similar to the deterministic case, is that for all x k and k,It guarantees cost improvement, i.e., that for all x k and k, the rollout policyThus, generally speaking, minimax rollout is fairly similar to rollout for deterministic as well as stochastic DP problems.The main difference with deterministic (or stochastic) problems is that to compute the Q-factor of a control u k , we need to solve a maximization problem, rather than carry out a deterministic (or Monte-Carlo, respectively) simulation with the given base policy.Example 2.12.1 (Pursuit-Evasion Problems)Consider a pursuit-evasion problem with state x k = (x 1 k , x 2 k ), where x 1 k is the location of the minimizer/pursuer and x 2 k is the location of the maximizer/evader, at stage k, in a (finite node) graph defined in two-or threedimensional space.There is also a cost-free and absorbing termination state that consists of a subset of pairs (x 1 , x 2 ) that includes all pairs with x 1 = x 2 .The pursuer chooses one out of a finite number of actions u k ∈ U k (x k ) at each stage k, when at state x k , and if the state is x k and the pursuer selects u k , the evader may choose from a known set X k+1 (x k , u k ) of next states x k+1 , which depends on (x k , u k ).The objective of the pursuer is to minimize a nonnegative terminal cost g(x 1 N , x 2 N ) at the end of N stages (or reach the termination state, which has cost 0 by assumption).A reasonable base policy for the pursuer can be precomputed by DP as follows: given the current (nontermination) state x k = (x 1 k , x 2 k ), make a move along the path that starts from x 1 k and minimizes the terminal cost after N − k stages, under the assumption that the evader will stay motionless at his current location x 2 k .(In a variation of this policy, the DP computation is done under the assumption that the evader will follow some nominal sequence of moves.)For the on-line computation of the rollout control, we need the maximal value of the terminal cost that the evader can achieve starting from every x k+1 ∈ X k+1 (x k , u k ), assuming that the pursuer will follow the base policy (which has already been computed).We denote this maximal value by J k+1 (x k+1 ).The required values J k+1 (x k+1 ) can be computed by an (N − k)-stage DP computation involving the optimal choices of the evader, while assuming the pursuer uses the (already computed) base policy.Then the rollout control for the pursuer is obtained from the minimization μk (x k ) ∈ arg minNote that the preceding algorithm can be adapted for the imperfect information case where the pursuer knows x 2 k imperfectly.This is possible by using a form of assumed certainty equivalence: the pursuer's base policy and the evader's maximization can be computed by using an estimate of the current location x 2 k instead of the unknown true location.In the preceding pursuit-evasion example, the choice of the base policy was facilitated by the special structure of the problem.Generally, however, finding a suitable base policy that can be conveniently implemented is an important problem-dependent issue.Several of the variants of rollout discussed earlier have analogs in the minimax context, e.g., truncation with terminal cost approximation, multistep and selective step lookahead, and multiagent rollout.In particular, in the -step lookahead variant, we solve the -stage problem minwe find an optimal solution ũk , μk+1 , . . ., μk+ −1 , and we apply the first component ũk of that solution.As an example, this type of problem is solved at each move of chess programs like AlphaZero, where the terminal cost function is encoded through a position evaluator.In fact when multistep lookahead is used, special techniques such as alpha-beta pruning may be used to accelerate the computations by eliminating unnecessary portions of the lookahead graph.These techniques are well-known in the context of the two-person computer game methodology, and are used widely in games such as chess.It is interesting to note that, contrary to the case of stochastic optimal control, there is an on-line constrained form of rollout for minimax control.Here there are some additional trajectory constraints of the formwhere C is an arbitrary set.The modification needed is similar to the one of Section 6.6: at partial trajectory ỹk = (x 0 , ũ0 , . . ., ũk−1 , xk ), generated by rollout, we use a heuristic with cost function H k+1 to compute the Q-factorfor each u k in the set Ũ k (ỹ k ) that guarantee feasibility [we can check feasibility here by running some algorithm that verifies whether the future disturbances w k , . . ., w N −1 can be chosen to violate the constraint under the base policy, starting from (ỹ k , u k )].Once the set of "feasible controls" Ũ k (ỹ k ) is computed, we can obtain the rollout control by the Q-factor minimization: ũk ∈ arg minWe may also use fortified versions of the unconstrained and constrained rollout algorithms, which guarantee a feasible cost-improved rollout policy.This requires the assumption that the base heuristic at the initial state produces a trajectory that is feasible for all possible disturbance sequences.Similar to the deterministic case, there are also truncated and multiagent versions of the minimax rollout algorithm.Example 2.12.2 (Multiagent Minimax Rollout)Let us consider a minimax problem where the minimizer's choice involves the collective decision of m agents, u = (u 1 , . . ., u m ), with u corresponding to agent , and constrained to lie within a finite set U .Thus u must be chosen from within the setwhich is finite but grows exponentially in size with m.The maximizer's choice w is constrained to belong to a finite set W .We consider multiagent rollout for the minimizer, and for simplicity, we focus on a two-stage problem.However, there are straightforward extensions to a more general multistage framework.In particular, we assume that the minimizer knowing an initial state x0, chooses u = (u 1 , . . ., u m ), with u ∈ U , = 1, . . ., m, and a state transition The exact DP algorithm for this problem is given byThis DP algorithm is computationally intractable for large m.The reason is that the set of possible minimizer choices u grows exponentially with m, and for each of these choices the value of J * 1 f0(x0, u) must be computed.However, the problem can be solved approximately with multiagent rollout, using a base policy µ = (µ 1 , . . ., µ m ).Then the number of times J * 1 f0(x0, u) needs to be computed is dramatically reduced.This computation is done sequentially, one-agent-at-a-time, as follows:Approximation in Value Space -Rollout Algorithms Chap. 2In this algorithm, the number of times for which J * 1 f0(x0, u) must be computed grows linearly with m.When the number of stages is larger than two, a similar algorithm can be used.Essentially, the one-stage maximizer's cost function J * 1 must be replaced by the optimal cost function of a multistage maximization problem, where the minimizer is constrained to use the base policy (see also the paper [Ber19b], Section 5.4).An interesting question is how do various algorithms work when approximations are used in the min-max and max-min problems?We can certainly improve the minimizer's policy assuming a fixed policy for the maximizer .However, it is unclear how to improve both the minimizer's and the maximizer's policies simultaneously.In practice, in symmetric games, like chess, a common policy is trained for both players.In particular, in the AlphaZero and TD-Gammon programs this strategy is computationally expedient and has worked well.However, there is no reliable theory to guide the simultaneous training of policies for both maximizer and minimizer, and it is quite plausible that unusual behavior may arise in exceptional cases.† Even exact policy iteration methods for Markov games encounter serious convergence difficulties, and need to be modified for reliable behavior.The author's paper [Ber21c] and book [Ber22b] (Chapter 5) address these convergence issues with modified versions of the policy iteration method, and give many earlier references.We finally note another source of difficulty in minimax control: Newton's method applied to solution of the Bellman equation for minimax problems exhibits more complex behavior than its expected value counterpart.The reason is that the Bellman operator T for infinite horizon problems, given by (T J)(x) = min u∈U(x) max w∈W (x,u) g(x, u, w) + αJ f (x, u, w) , for all x, is neither convex nor concave as a function of J. To see this, note that the function maxviewed as a function of J [for fixed (x, u)], is convex, and when minimized over u ∈ U (x), it becomes neither convex nor concave.As a result there are special difficulties in connection with convergence of Newton's method and the natural form of policy iteration, given by Pollatschek and Avi-Itzhak [PoA69]; see also Chapter 5 of the author's abstract DP book [Ber22a].† Indeed such exceptional cases have been reported for the AlphaGo program in late 2022, when humans defeated an AlphaGo look-alike, KataGo, "by using adversarial techniques that take advantage of KataGo's blind spots" (according to the reports); see Wang et al. [WGB22].Zero-sum game problems are viewed as fundamental in the field of economics, and there is an extensive and time-honored theory around them.In the case where the game involves a dynamic systemand a cost functionthere are two players, the minimizer choosing u k ∈ U k (x k ), and the maximizer choosing w k ∈ W k (x k ), at each stage k.Such zero-sum games involve two minimax control problems:(a) The min-max problem, where the minimizer chooses a policy first and the maximizer chooses a policy second with knowledge of the minimizer's policy.The DP algorithm for this problem has the form(b) The max-min problem, where the maximizer chooses policy first and the minimizer chooses policy second with knowledge of the maximizer's policy.The DP algorithm for this problem has the formA basic and easily seen fact is that Max-Min optimal value ≤ Min-Max optimal value.Game theory is particularly interested on conditions that guarantee that Max-Min optimal value = Min-Max optimal value.(2.109)However, this question is of limited interest in engineering contexts that involve worst case design.Moreover, the validity of the minimax equality (2.109) is beyond the range of practical RL.This is so primarily because once approximations are introduced, the delicate assumptions that guarantee this equality are disrupted.Section 2.1: In this chapter, we have placed emphasis on finite horizon problems, possibly involving a nonstationary system and cost per stage.However, the insights that can be obtained from the infinite horizon/stationary context fully apply.These include the interpretation of approximation in value space as a Newton step, and of rollout as a single step of the policy iteration method.The reason is that an N -step finite horizon/nonstationary problem can be converted to an infinite horizon/stationary problem with a termination state to which the system moves at the N th stage; see Section 1.6.2.Section 2.2: Approximation in value space has been considered in an ad hoc manner since the early days of DP, motivated by the curse of dimensionality.Moreover, the idea of -step lookahead minimization with horizon truncation beyond the steps has a long history and is often referred to as "rolling horizon" or "receding horizon" optimization.Approximation in value space was reframed in the late 80s and was coupled with model-free simulation methods that originated in artificial intelligence.Section 2.3: The main idea of rollout algorithms, obtaining an improved policy starting from some other suboptimal policy, has appeared in several DP contexts, including games; see e.g., Abramson [Abr90], and Tesauro and Galperin [TeG96].The name "rollout" was coined by Tesauro [TeG96] in the context of backgammon; see Example 2.7.3.The use of the name "rollout" has gradually expanded beyond its original context; for example the samples collected through trajectory simulation are referred to as "rollouts" by some authors.In this book, we will adopt the original intended meaning: rollout is an algorithm that provides policy improvement starting from a base policy, which is evaluated with some form of Monte Carlo simulation, perhaps augmented by some other calculation that may include a terminal cost function approximation.The author's rollout book [Ber20a] provides a more extensive discussion of rollout algorithms and their applications.There has been a lot of research on rollout algorithms, which we list selectively in chronological order: Christodouleas These references collectively include a large number of computational studies, discuss variants and problem-specific adaptations of rollout algorithms for a broad variety of practical problems, and consistently report favorable computational experience.The size of the cost improvement over the base policy is often impressive, evidently owing to the fast convergence rate of Newton's method that underlies rollout.Moreover these works illustrate some of the other important advantages of rollout: reliability, simplicity, suitability for on-line replanning, and the ability to interface with other RL techniques, such as neural network training, which can be used to provide suitable base policies and/or approximations to their cost functions.The adaptation of rollout algorithms to discrete deterministic optimization problems, the notions of sequential consistency, sequential improvement, fortified rollout, and the use of multiple heuristics for parallel rollout were first given in the paper by Bertsekas, Tsitsiklis, and Wu [BTW97], and were also discussed in the neuro-dynamic programming book [BeT96].Rollout algorithms for stochastic problems were further formalized in the papers by Bertsekas [Ber97b], and Bertsekas and Castañon [BeC99].Extensions to constrained rollout were first given in the author's papers [Ber05a], [Ber05b].A survey of rollout in discrete optimization was given by the author in [Ber13a].The model-free rollout algorithm, in the form given here, was first discussed in the RL book [Ber19a].It is inspired by the method of comparison training, proposed by Tesauro [Tes89a], [Tes89b], [Tes01], and subsequently used by several other authors (see [DNW16], [TCW19]).This is a general method for training an approximation architecture to choose between two alternatives, using a dataset of expert choices in place of an explicit cost function.The material on most likely sequence generation for n-grams, HMMs, and Markov Chains is recent, and was developed in the paper by Li and Bertsekas [LiB24].Section 2.4: Our discussion of rollout, iterative deepening, and pruning in the context of multistep approximation in value space for deterministic problems contains some original ideas.In particular, the incremental multistep rollout algorithm and variations of Section 2.4.2 are presented here for the first time.Note also that the multistep lookahead approximations described in Section 2.4 can be used more broadly within algorithms that employ forms of multistep lookahead search as subroutines.In particular, local search algorithms, such as tabu search, genetic algorithms, and others, which are commonly used for discrete and combinatorial optimization, may be modified along the lines of Section 2.4 to incorporate RL and approximate DP ideas.Section 2.5: Constrained forms of rollout were introduced in the author's papers [Ber05a] and [Ber05b].The paper [Ber05a] also discusses rollout and approximation in value space for stochastic problems in the context of so-called restricted structure policies.The idea here is to simplify the problem by selectively restricting the information and/or the controls available to the controller, thereby obtaining a restricted but more tractable problem structure, which can be used conveniently in a one-step lookahead context.An example of such a structure is one where fewer observations are obtained, or one where the control constraint set is restricted to a single or a small number of given controls at each state.Section 2.6: Rollout for continuous-time optimal control was first discussed in the author's rollout book [Ber20a].A related discussion of policy iteration, including the motivation for approximating the gradient of the optimal cost-to-go ∇ x J t rather than the optimal cost-to-go J t , has been given in Section 6.11 of the neuro-dynamic programming book [BeT96].This discussion also includes the use of value and policy networks for approximate policy evaluation and policy improvement for continuous-time optimal control.The underlying ideas have long historical roots, which are recounted in detail in the book [BeT96].Section 2.7: The idea of the certainty equivalence approximation in the context of rollout for stochastic systems (Section 2.7.3) was proposed in the paper by Bertsekas and Castañon [BeC99], together with extensive empirical justification.However, the associated theoretical insight into this idea was established more recently, through the interpretation of approximation in value space as a Newton step, which suggests that the lookahead min-imization after the first step can be approximated with small degradation of performance.The idea of variance reduction in the context of rollout (Section 2.7.4) was proposed by the author in the paper [Ber97b].See also the DP textbook [Ber17a], Section 6.5.Adaptive sampling and MCTS may be viewed within the context of a broader class of on-line lookahead minimization techniques, sometimes called on-line search methods.These techniques are based on a variety of ideas, such as random search and intelligent pruning of the lookahead tree.One may naturally combine them with approximation in value space and (possibly) rollout, although it is not necessary to do so (the multistep minimization horizon may extend to the terminal time N ).For representative works, some of which apply to continuous spaces problems, including POMDP, see Hansen  Another rollout idea for stochastic problems, which we have not discussed in this book, is the open-loop feedback controller (OLFC), a suboptimal control scheme that dates to the 60s; see Dreyfus [Dre65].The OLFC applies to POMDP as well, and uses an open-loop optimization over the future evolution of the system.In particular, it uses the current information vector I k to determine the belief state b k .It then solves the open-loop problem of minimizingand applies the first control u k in the optimal open-loop control sequence {u k , u k+1 , . . ., u N −1 }.It is easily seen that the OLFC is a rollout algorithm that uses as base policy the optimal open-loop policy for the problem (the one that ignores any state or observation feedback).For a detailed discussion of the OLFC, we refer to the author's survey paper [Ber05a] (Section 4) and DP textbook [Ber17a] (Section 6.4.4).The survey [Ber05a] discusses also a generalization of the OLFC, called partial open-loop-feedback-control , which calculates the control input on the basis that some (but not necessarily all) of the observations will in fact be taken in the future, and the remaining observations will not be taken.This method often allows one to deal with those observations that are troublesome and complicate the solution, while taking into account the future availability of other observations that can be reasonably dealt with.A computational case study for hydrothermal power system scheduling is given by Martinez and Soares [MaS02].A variant of the OLFC, which also applies to minimax control problems, is given in the author's paper [Ber72b], together with a proof of a cost improvement property over the optimal open-loop policy.Section 2.8: The role of stochastic programming in providing a link between stochastic DP and continuous spaces deterministic optimization (cf.Section 2.8) is well known; see the texts by Birge and Louveaux [BiL97], Kall and Wallace [KaW94], and Prekopa [Pre95], and the survey by Ruszczynski and Shapiro [RuS03].Stochastic programming has been applied widely, and there is much to be gained from its combination with RL.The material of this section comes from the author's rollout book [Ber20a], Section 2.5.2.For a computational study that has tested the ideas of this section on a problem of maintenance scheduling, see Hu et al. [HWP22].Section 2.9: The multiagent rollout algorithm was proposed in the author's papers [Ber19c], [Ber20b].The paper [Ber21a] provides an extensive overview of this research.See also the notes and sources for Chapter 1.Section 2.10: The material on rollout for Bayesian optimization and sequential estimation comes from a recent paper by the author [Ber22d].This paper is also the basis for the adaptive control material of Section 2.11, and has been included in the book [Ber22a].The paper by Bhambri, Bhattacharjee, and Bertsekas [BBB22] discusses this material for the case of a deterministic system, applies rollout to sequential decoding in the context of the challenging Wordle puzzle, and provides an implementation using some popular base heuristics, with performance that is very close to optimal.For related work see Loxley and Cheung [LoC23].Section 2.11: The POMDP framework for adaptive control dates to the 60s, and has stimulated substantial theoretical investigations; see Mandl [Man74], Borkar and Varaiya [BoV79], Doshi and Shreve [DoS80], Kumar and Lin [KuL82], and the survey by Kumar [Kum85].Some of the pitfalls of performing parameter estimation while simultaneously applying adaptive control have been described by Borkar and Varaiya [BoV79], and by Kumar [Kum83]; see [Ber17a], Section 6.8 for a related discussion.The papers just mentioned have proposed on-line estimation of the unknown parameter θ and the use at each time period of a policy that is optimal for the current estimate.The papers provide nontrivial analyses that assert asymptotic optimality of the resulting adaptive control schemes under appropriate conditions.If parameter estimation schemes are similarly used in conjunction with rollout, as suggested in Section 2.11 [cf.Eq. (2.101)], one may conjecture that an asymptotic cost improvement property can be proved for the rollout policy, again under appropriate conditions.Section 2.12: The treatment of sequential minimax problems by DP (cf.Section 2.12) has a long history.For some early influential works, see Blackwell and Girshick [BlG54], Shapley [Sha53], and Witsenhausen [Wit66].In minimax control problems, the maximizer is assumed to make choices with perfect knowledge of the minimizer's policy.If the roles of maximizer and minimizer are reversed, i.e., the maximizer has a policy (a sequence of functions of the current state) and the minimizer makes choices with perfect knowledge of that policy, the minimizer gains an advantage, the problem may genuinely change, and the optimal value may be reduced.Thus "minmax" and "max-min" are generally two different problems.In classical two-person zero-sum game theory, however, the main focus is on situations where the min-max and max-min are equal.By contrast, in engineering worst case design contexts, the min-max and max-min values are typically unequal.There is substantial literature on sequential zero-sum games in the context of DP, often called Markov games.The classical paper by Shapley [Sha53] addresses discounted infinite horizon games.A PI algorithm for finite-state Markov games was proposed by Pollatschek and Avi-Itzhak [PoA69], and was interpreted as a Newton method for solving the associated Bellman equation.They have also shown that the algorithm may not converge to the optimal cost function.Computational studies have verified that the Pollatschek and Avi-Itzhak algorithm converges much faster than its competitors, when it converges (see Breton et al. [BFH86], and also Filar and Tolwinski [FiT91], who proposed a modification of the algorithm).Related methods have been discussed for Markov games by van der Wal [Van78] and Tolwinski [Tol89].The paper by Raghavan and Filar [RaF91], and the textbook by Filar and Vrieze [FiV96] provide extensive surveys of the research up to that time.The author's paper [Ber21b] has explained the reason behind the unreliable behavior of the Pollatschek and Avi-Itzhak algorithm.This explanation relies on the Newton step interpretation of PI given in Chapter 1: in the case of Markov games, the Bellman operator does not have the concavity property that is typical of one-player games.The paper [Ber21b] has also provided a modified algorithm with solid convergence properties under a totally asynchronous implementation, which applies to very general types of sequential zero-sum games and minimax control.Related aggregation-based RL algorithms were also given.The algorithms, variations, and analysis of the paper [Ber21b] were incorporated as Chapter 5 in the 3rd edition of the author's abstract DP book [Ber22b].The paper by Yu [Yu14] provides an analysis of stochastic shortest path games, where the termination state may not be reachable under some policies, following the earlier paper by Patek and Bertsekas [PaB99].The paper [Yu14] also includes a rigorous analysis of the Q-learning algorithm for stochastic shortest path games (without any cost function approximation).The papers by Perolat et al. [PSP15], [PPG16], and the survey by Zhang, Yang, and Basar [ZYB21] discuss alternative RL methods for games.The author's paper [Ber19b] develops VI, PI, and Dijkstra-like finitely terminating algorithms for exact solution of shortest path minimax problems.It also discusses related rollout algorithms for approximate solution.(a) Assume that the base heuristic is chosen to be the farthest neighbor heuristic, which completes a partial tour by successively moving to the farthest neighbor city not visited thus far.Show that this base heuristic is sequentially consistent.What are the tours produced by this base heuristic and the corresponding rollout algorithm?Answer : The base heuristic will produce the tour A→AD→ADB→ADBC→A with cost 45.The rollout algorithm will produce the tour A→AB→ABD→ABDC→A with cost 13.(b) Assume that the base heuristic at city A is the nearest neighbor heuristic, while at the partial tours AB, AC, and AD it is the farthest neighbor heuristic.Show that this base heuristic is sequentially improving but not sequentially consistent.Compute the final tour generated by rollout.Clearly the base heuristic is not sequentially consistent, since from A it generates A→AC→ACD→ACDB→A, but from AC it generates AC→ACB→ACBD→A.However, it is seen that the sequential improvement criterion (2.13) holds at each of the states A, AB, AC, and AD (and also trivially for the remaining states).The base heuristic at A is the nearest neighbor heuristic so it generates A→AC→ACD→ACDB→A with cost 28.The rollout algorithm at state A looks at the three successor states AB, AC, AD, and runs the farthest neighbor heuristic from each, and generates: A→AB→ABD→ABDC→A with cost 13, A→AC→ACB→ACBD→A with cost 45, A→AD→ADB→ADBC→A with cost 45, so the rollout algorithm will move from A to AB.Then the rollout algorithm looks at the two successor states ABC, ABD, and runs the base heuristic (whatever that may be; it does not matter) from each.The paths generated are: AB→ABC→ABCD→A with cost 26, AB→ABD→ABDC→A with cost 13, so the rollout algorithm will move from AB to ABD.Thus the final tour generated by the rollout algorithm is A→AB→ABD→ABDC→A, with cost 13.Consider a rollout algorithm for a deterministic problem with a base heuristic that produces an optimal control sequence at the initial state x0, and uses the (optimal) first control u0 of this sequence to move to the (optimal) next state x1.Suppose that the base heuristic produces a strictly suboptimal sequence from every successor state x2 = f1(x1, u1), u1 ∈ U1(x1), so that the rollout yields a control u1 that is strictly suboptimal.Show that the trajectory produced by the rollout algorithm starting from the initial state x0 is strictly inferior to the one produced by the base heuristic starting from x0, while the sequential improvement condition does not hold.In this computational exercise we consider a more complex, imperfect state information version of the one-directional parking problem of Example 1.6.1.Recall that in this problem a driver is looking for a free parking space in an area consisting of N spaces arranged in a line, with a garage at the end of the line (space N ).The driver starts at space 0 and traverses the parking spaces sequentially, i.e., from each space he/she goes to the next space, up to when he/she decides to park in space k at cost c(k), if space k is free.Upon reaching the garage, parking is mandatory at cost C.In Example 1.6.1,we assumed that the driver knows the probabilities p(k + 1), . . ., p(N − 1) of the parking spaces (k + 1), . . ., (N − 1), respectively, being free.Under this assumption, the state at stage k is either the termination state t (if already parked), or it is F (location k free), or it is F (location k taken), and the DP algorithm has the formfor the states other than the termination state t, while for t we have J * k (t) = 0 for all k.We will now consider the more complex variant of the problem where the probabilities p(0), . . ., p(N − 1) do not change over time, but are unknown to the driver, so that he/she cannot use the exact DP algorithm (2.110)-(2.111).Instead, the driver considers a one-step lookahead approximation in value space scheme, which uses empirical estimates of these probabilities that are based on the ratio f k k+1 , where f k is the number of free spaces seen up to space k, after the free/taken status of spaces 0, . . ., k has been observed.In particular, these empirical estimates are given bywhere f k is the number of free spaces seen up to space k, and γ and p(m) are fixed numbers between 0 and 1.Of course the values f k observed by the driver evolve according to the true (and unknown) probabilities p(0), . . ., p(N − 1) according toFor the solution of this exercise you may assume any reasonable values you wish for N , p(m), p(m), and γ.Recommended values are N ≥ 100, and probabilities p(m) and p(m) that are nonincreasing with m.The decision made by the approximation in value space scheme is to park at space k if and only if it is free and in additionwhere Jk+1 (F ) and J k+1 (F ) are the cost-to-go approximations from stage k + 1.Consider the following two different methods to compute Jk+1 (F ) and Jk+1 (F ) for use in Eq. (2.114):(1) Here the approximate cost function values Jk+1 (F ) and J k+1 (F ) are obtained by using problem approximation, whereby at time k it is assumed that the probabilities of free/taken status at the future spaces m = k + 1, . . ., N − 1 are b k (m, f k ), m = k + 1, . . ., N − 1, as given by Eq. (2.112).More specifically, J k+1 (F ) and J k+1 (F ) are obtained by solving optimally the problem whereby we use the probabilities b k (m, f k ) of Eq. (2.112) in place of the unknown p(m) in the DP algorithm (2.110)-(2.111):where Ĵ k+1 (F ) and Ĵ k+1 (F ) are given at the last step of the DP algorithm(2) Here for each k, the approximate cost function values J k+1 (F ) and Jk+1 (F ) are obtained by using rollout with a greedy base heuristic (park as soon as possible), and Monte Carlo simulation.In particular, according to this greedy heuristic, we have J k+1 (F ) = c(k + 1).To compute Jk+1 (F ) we generate many random trajectories by running the greedy heuristic forward from space k + 1 assuming the probabilities b k (m + 1, f k ) of Eq. (2.112) in place of the unknown p(m + 1), m = k + 1, . . ., N − 1, and we average the cost results obtained.(a) Use Monte Carlo simulation to compute the expected cost from spaces 0, . . ., N − 1, when using each of the two schemes (1) and ( 2).(b) Compare the performance of the schemes of part (a) with the following:(i) The optimal expected costs J * k (F ) and J * k (F ) from k = 0, . . ., N − 1, using the DP algorithm (2.110)-(2.111),and the probabilities p(m), m = 0, . . ., N − 1, that you used for the random generation of the numbers of free spaces f k [cf.Eq. (2.113)].(ii) The expected costs Ĵ k (F ) and Ĵk (F ) from k = 0, . . ., N − 1 that are attained by using the greedy base heuristic.Argue that these are given by Ĵk(c) Argue that scheme (1) becomes superior to scheme (2) in terms of cost attained as γ ≈ 1 and p(m) ≈ p(m).Are your computational results in rough agreement with this assertion?(d) Argue that as γ ≈ 0 and N >> 1, scheme (1) becomes superior to scheme (2) in terms of cost attained from parking spaces k >> 1.(e) What happens if the probabilities p(m) do not change much with m?Consider the breakthrough problem of Example 2.3.2 with the difference that instead of the greedy heuristic, we use the random heuristic, which at a given node selects one of the two outgoing arcs with equal probability.Denote bythe probability of success of the random heuristic in a graph of k stages, and by R k the probability of success of the corresponding rollout algorithm.Show that for allConclude that R k /D k increases exponentially with k.Consider the breakthrough problem of Example 2.3.2 and consider a truncated rollout algorithm that uses a greedy base heuristic with -step lookahead.This is the same algorithm as the one described in Example 2.3.2,except that if both outgoing arcs of the current node at stage k are free, the rollout algorithm considers the two end nodes of these arcs, and from each of them it runs the greedy algorithm for min{l, N − k − 1} steps.Consider a Markov chain with l + 1 states, where states i = 0, . . ., l − 1 correspond to the path generated by the greedy algorithm being blocked after i arcs.State corresponds to the path generated by the greedy algorithm being unblocked after arcs.(a) Derive the transition probabilities for this Markov chain so that it models the operation of the rollout algorithm.(b) Use computer simulation to generate the probability of a breakthrough, and to demonstrate that for large values of N , the optimal value of is roughly constant and much smaller than N (this can also be justified analytically, by using properties of Markov chains).Consider a discrete N -stage optimization problem, involving a tree with a root node s that plays the role of an artificial initial state, and N layers of states x1 = (u0), x2 = (u0, u1), . . ., xN = (u0, . . ., uN−1), as shown in Fig. 2.1.4.We allow deadend nodes in the graph of this problem, i.e., states that have no successor states, and thus cannot be part of any feasible solution.We also assume that all stage costs as well as the terminal cost are 0. The problem is to find a feasible solution, i.e., a sequence of N transitions through the graph that starts at the initial state s and ends at some node of the last layer of states xN .(a) Argue that this is a discrete spaces formulation of a constraint programming problem, such as the one described in Section 2.1.(b) Describe in detail an incremental rollout algorithm with -step lookahead minimization and m-step rollout truncation, which is similar to the IMR algorithm of Section 2.4.2 and operates as follows: The algorithm maintains a connected subtree S that contains the initial state s.The base policy at a state x k either generates a feasible sequence of m arcs starting at x k , where m is an integer that satisfies 1 ≤ m ≤ min{m, N − k}, or determines that such a sequence does not exist.In the former case the node x k is expanded by adding all of its neighbor nodes to S. In the latter case, the node x k is deleted from S. The algorithm terminates once a state xN of the last layer is added to S.(c) Argue that since the algorithm cannot keep deleting nodes indefinitely, one of two things will eventually happen:(1) The graph S will be reduced to just the root node s, proving that there is no feasible solution.(2) The algorithm will terminate with a feasible solution.(d) Suppose that the algorithm is operated so that the selected node x k at each iteration is a leaf node of S, which is at maximum arc distance from s (the number of arcs of the path connecting s and x k is maximized).Show that the subtree S always consists of just a path of nodes, together with all the neighbor nodes of the nodes of the path.Conclude that in this case, the algorithm can be implemented so that it requires O(Nd) memory storage, where d is the maximum node degree.How does this algorithm compare with a depth-first search algorithm for finding a feasible solution?(e) Describe an adaptation of the algorithm of part (d) for the case where most of the arcs have cost 0 but there are some arcs with positive cost.Consider a market that makes available for purchase m products over N time periods, and a buyer that may or may not buy any one of these products subject to cash availability.For each product i = 1, . . ., m and time period k = 0, . . ., N −1, we denote:The asking price of product i at time k (the case where product i is unavailable for purchase is modeled by setting a i k to ∞). v i k : The value to the buyer of product i at time k.u i k : The decision to buy (u i k = 1) or not to buy (u i k = 0) product i at time k.The conditional distributions P (a i k+1 | a i k , u i k = 1) and P (a i k+1 | a i k , u i k = 0) are given.(Thus when u i k = 1, product i will be made available at the next time period at a possibly different price a i k+1 ; however, it may also be unavailable, i.e., a i k+1 = ∞.)The amount of cash available to the buyer at time k is denoted by c k , and evolves according toThe initially available cash c0 is a given positive number.Moreover, we have the constrainti.e., the buyer may not borrow to buy products.The buyer aims to maximize the total value obtained over the N time periods, plus the remaining cash at time N :(a) Formulate the problem as a finite horizon DP problem by identifying the state, control, and disturbance spaces, the system equation, the cost function, and the probability distribution of the disturbance.Write the corresponding exact DP algorithm.(b) Introduce a suitable base policy and formulate a corresponding multiagent rollout algorithm for addressing the problem.Consider a problem of sequentially searching for a treasure of known value v among n given locations.At each time period we may either select a location i to search at cost ci > 0, or we may stop searching.Moreover, if the search location i is different from the location j where we currently are, we incur an additional switching cost sij ≥ 0. If the treasure is at location i, a search at that location will find it with known probability βi < 1.Our initial location is given, and the a priori probabilities pi, i = 1, . . ., n, that the treasure is at location i are also given.We assume that n i=1 pi < 0, so there is positive probability that there is no treasure at any one of the n locations.(a) Formulate the problem as a special case of the adaptive control problem of Section 2.11, with the parameter θ taking one of (n + 1) values, θ 0 , θ 1 , . . ., θ n , where θ 0 corresponds to the case where there is no treasure at any location, and θ i , i = 1, . . ., n, corresponds to the case where the treasure is at location i. Use as state the current location together with the current probability distribution of θ, and use as control the choice between stopping the search or continuing the search at one of the n locations.(b) Consider the special case where there is only one location.Show that the optimal policy is to continue searching up to the point where the conditional expected benefit of the search falls below a certain threshold, and that the optimal cost can be computed very simply.(The proof is given in Example 4.3.1 of the DP textbook [Ber17a].)(c) Formulate the rollout algorithm of Section 2.11 with two different base policies:(1) A policy that is optimal among the policies that never switch to another location (they continue to search the same location up to stopping).(2) A policy that is optimal among the policies that may stay at the current location or may switch to the location that is most likely to contain the treasure according to the current probability distribution of θ.(d) Implement the preceding two rollout algorithms using reasonable problem data of your choice.In this chapter, we will discuss the methods and objectives of off-line training through the use of parametric approximation architectures such as neural networks.We begin with a general discussion of parametric architectures and their training in Section 3.1.We then consider the training of neural networks in Section 3.2, and their use in the context of finite horizon approximate DP in Section 3.3.In Sections 3.4 and 3.5, we discuss the training of policies.Finally, in Section 3.6, we discuss aggregation methods.For the success of approximation in value space, it is important to select a class of lookahead function approximations Jk that is suitable for the problem at hand.In the preceding two chapters we discussed several methods for choosing Jk , based mostly on some form of rollout.We will now discuss how Jk can be obtained by off-line training from a parametric class of functions, possibly involving a neural network, with the parameters "optimized" with the use of some algorithm.Training Data Approximation Architecture Parameter Approximation Architecture Parameter J(x) Training Data x s , J(x s ) s = 1, . . ., q , . . ., q Parameter r r J(x, r) r Approximating Function Approximating Function The general structure for parametric cost approximation.We approximate the target cost function J(x) with a member from a parametric class J(x, r) that depend on a parameter vector r.We use training data x s , J(x s ) , s = 1, . . ., q, and a form of optimization that aims to find a parameter r that "minimizes" the size of the errors J(x s ) − J(x s , r), s = 1, . . ., q.As we have noted in Chapter 1, the most popular structure for parametric cost function approximation involves a target function J(x) that we want to approximate with a member of a parametric class of functions J(x, r) that depend on a parameter vector r (see Fig. 3.1.1).In particular, we collect training data x s , J(x s ) , s = 1, . . ., q, which we use to determine a parameter r that leads to a good "fit" between the data J(x s ) and the predictions J(x s , r) of the parametrized function.This is usually done through an optimization approach, aiming to minimize the size of the errors J(x s ) − J (x s , r), s = 1, . . ., q.Approximation of a target policy µ with a policy from a parametric class μ(x, r) is largely similar.Here, the training data may be obtained, for example, from rollout control calculations, thus enabling the construction of both value and policy networks that can be combined for use in a perpetual rollout scheme.An important difference, however, is that the  approximate cost values J(x, r) are real numbers, whereas the approximate policy values μ(x, r) are elements of a control space U .Thus if U consists of m dimensional vectors, μ(x, r) consists of m numerical components.In this case the parametric approximation problems for cost functions and for policies are fairly similar, and both involve continuous space approximations.On the other hand, the case where the control space is finite U = {u 1 , . . ., u m } is quite different.In this case, for any x, μ(x, r) consists of one of the m possible controls u 1 , . . ., u m .This connects policy space approximation with traditional classification schemes, whereby objects x are classified as belonging to one of the categories u 1 , . . ., u m .In particular, µ(x) defines the category of x, and can be viewed as a classifier.Some of the most prominent classification schemes actually produce randomized outcomes, i.e., x is associated with a probability distribution {μ(u 1 , r), . . ., μ(u m , r)},which is a randomized policy in our policy approximation context; see Fig. 3.1.2.This is typically done algorithmic convenience, since many optimization methods, including least squares regression, require that the optimization variables are continuous.Then, the randomized policy (3.1) can be converted to a nonrandomized policy using a maximization operation: associate x with the control of maximum probability (cf.Fig. 3.1.2),μ(x, r) ∈ arg max i=1,...,m μi (x, r).(3.2)We will discuss the use of classification methods for approximation in policy space in Section 3.4, following our discussion of parametric approximation in value space.For the remainder of this section, as well as Sections 3.2 and 3.3, we will focus on approximation in value space schemes, where the approximate cost functions are selected from a parametric class of functions Jk (x k , r k ) that for each k, depend on the current state x k and a vector r k = (r 1,k , . . ., r m k ,k ) of m k "tunable" scalar parameters.By adjusting the parameters, one can change the "shape" of Jk so that it is a reasonably good approximation to some target function, usually the true optimal cost-to-go function J * k , or the cost-to-go function J k,π of some policy π.The class of functions Jk (x k , r k ) is called an approximation architecture, and the process of choosing the parameter vectors r k is commonly called training or tuning the architecture.We will focus initially on approximation of cost functions, hence the use of the Jk notation.In Section 3.4 we will consider the other major use of parametric approximation architectures, of the form μk (x k , r k ), where the target function is a control function µ k that is part of some policy.The simplest training approach for parametric architectures is to do some form of semi-exhaustive or semi-random search in the space of parameter vectors and adopt the parameters that result in best performance of the associated one-step lookahead controller (according to some criterion).There are methods of this type that have been used primarily in cases where the number of parameters is relatively small.Random search and Bayesian optimization methods have also been used to tune hyperparameters of an approximation architecture; for example, the number of layers in a neural network, or the number of clusters in the context of partitioning discrete spaces into clusters, etc.We refer to the research literature for further discussion.Other systematic approaches are based on numerical optimization, such as a least squares fit that aims to match the cost approximation produced by the architecture to a "training set," i.e., a large number of pairs of state and cost values that are obtained through some form of sampling process.Throughout Sections 3.1-3.3we will focus primarily on this approach.There is a large variety of approximation architectures, based for example on polynomials, wavelets, radial basis functions, discretization/interpolation schemes, neural networks, and others.A particularly interesting type of cost approximation involves feature extraction, a process that maps the state x k into some vector φ k (x k ), called the feature vector associated with x k at time k.The vector φ k (x k ) consists of scalar componentswhere r k is a parameter vector and Ĵk is some function.Thus, the cost approximation depends on the state x k through its feature vector φ k (x k ).Note that we are allowing for different features φ k (x k ) and different parameter vectors r k for each stage k.This is necessary for nonstationary problems (e.g., if the state space changes over time), and also to capture the effect of proximity to the end of the horizon.On the other hand, for stationary problems with a long or infinite horizon, where the state space does not change with k, it is common to use the same features and parameters for all stages.The subsequent discussion can easily be adapted to infinite horizon methods, as we will discuss later.Features are often handcrafted, based on whatever human intelligence, insight, or experience is available, and are meant to capture the most important characteristics of the current state.There are also systematic ways to construct features, including the use of data and neural networks, which we will discuss shortly.In this section, we provide a brief and selective presentation of architectures.One idea behind using features is that the optimal cost-to-go functions J * k may be complicated nonlinear mappings, so it is sensible to try to break their complexity into smaller, less complex pieces.In particular, if the features encode much of the nonlinearity of J * k , we may be able to use a relatively simple architecture Ĵk to approximate J * k .For example, with a well-chosen feature vector φ k (x k ), a good approximation to the cost-to-go is often provided by linearly weighting the features, i.e.,where r ,k and φ ,k (x k ) are the th components of r k and φ k (x k ), respectively, and r k φ k (x k ) denotes the inner product of r k and φ k (x k ), viewed as column vectors of m k (a prime denotes transposition, so r k is a row vector); see Fig. 3.1.3.This is called a linear feature-based architecture, and the scalar parameters r ,k are also called weights.Among other advantages, these architectures admit simpler training algorithms than their nonlinear counterparts; see the NDP book [BeT96].Mathematically, the approximating function Jk (x k , r k ) can be viewed as a member of the subspace spanned by the features φ ,k (x k ), = 1, . . ., m k , which for this reason are also referred to as basis functions.We provide a few examples, where for simplicity we drop the index k.with its own weight r .Suppose that the state space is partitioned into subsets S1, . . ., Sm, so that every state belongs to one and only one subset.Let the th feature be defined by membership to the set S , i.e.,Consider the architecturewhere r is the vector consists of the m scalar parameters r1, . . ., rm.It can be seen that J (x, r) is the piecewise constant function that has value r for all states within the set S ; see Fig. 3.1.4.The piecewise constant approximation is an example of a linear feature-based architecture that involves exclusively local features.These are features that take a nonzero value only for a relatively small subset of states.Thus a change of a single weight causes a change of the value of J(x, r) for relatively few states x.At the opposite end we have linear feature-based architectures that involve global features.These are features that take nonzero values for a large number of states.The following is a common example.An important case of linear architecture is one that uses polynomial basis functions.Suppose that the state consists of n components x 1 , . . ., x n , each taking values within some range of integers.For example, in a queueing system, x i may represent the number of customers in the ith queue, where i = 1, . . ., n. Suppose that we want to use an approximating function that is quadratic in the components x i .Then we can define a total of 1 + n + n 2 basis functions that depend on the state x = (x 1 , . . ., x n ) viaA linear approximation architecture that uses these functions is given bywhere the parameter vector r has components r0, ri, and rij , with i, j = 1, . . ., n.Indeed, any kind of approximating function that is polynomial in the components x 1 , . . ., x n can be constructed similarly.A more general polynomial approximation may be based on some other known features of the state.For example, we may start with a feature vector φ(x) = φ1(x), . . ., φm(x) , and transform it with a quadratic polynomial mapping.In this way we obtain approximating functions of the formwhere the parameter r has components r0, ri, and rij , with i, j = 1, . . ., m.This can also be viewed as a linear architecture that uses the basis functionsThe preceding example architectures are generic in the sense that they can be applied to many different types of problems.Other architectures rely on problem-specific insight to construct features, which are then combined into a relatively simple architecture.We present two examples involving games.The shapes are generated according to some stochastic process.As a given block falls, the player can move horizontally and rotate the block in all possible ways, subject to the constraints imposed by the sides of the grid and the top of the wall.When a row of full squares is created, this row is removed, the bricks lying above this row move one row downward, and the player scores a point.The player's objective is to maximize the score attained (total number of rows removed) within N steps or up to termination of the game, whichever occurs first.Let us consider the game of tetris, which we formulated in Example 1.6.2 as a stochastic shortest path problem with the termination state being the end of the game (see Fig. 3.1.5).The state is the pair of the board position x and the shape of the current falling block y.We viewed as control, the horizontal positioning and rotation applied to the falling block.The optimal cost-togo function is a vector of huge dimension (there are 2 200 board positions in a "standard" tetris board of width 10 and height 20).However, it has been successfully approximated in practice by low-dimensional linear architectures.In particular, the following features have been proposed in the paper by Bertsekas and Ioffe [BeI96]: the heights of the columns, the height differentials of adjacent columns, the wall height (the maximum column height), the number of holes of the board, and the constant 1 (the unit is often included as a feature in cost approximation architectures, as it allows for a constant shift in the approximating function).These features are readily recognized by tetris players as capturing important aspects of the board position.† There † The use of feature-based approximate DP methods for the game of tetris was first suggested in the paper by Tsitsiklis and Van Roy [TsV96], which introduced just two features (in addition to the constant 1): the wall height and the number of holes of the board.Most studies have used the set of features of [BeI96] described here, but other sets of features have also been used; see [ThS09] and the discussion in [GGS13].are a total of 22 features for a "standard" board with 10 columns.Of course the 2 200 × 22 matrix of feature values cannot be stored in a computer, but for any board position, the corresponding row of features can be easily generated, and this is sufficient for implementation of the associated approximate DP algorithms.For recent works involving approximate DP methods and the preceding 22 features, see [Sch13], [GGS13], and [SGG15], which reference several other related papers.In the works mentioned above the shapes of the falling blocks are stochastically independent.In a more challenging version of the problem, which has not been considered in the literature thus far, successive shapes are correlated.Then the state of the problem would become more complex, since past shapes would be useful in predicting future shapes.As a result, we may need to introduce state estimation and additional features in order to properly deal with the effects of correlations.Computer chess programs that involve feature-based architectures have been available for many years, and are still used widely (they have been upstaged in the mid-2010s by alternative types of chess programs, which use neural network techniques that will be discussed later).These programs are based on approximate DP for minimax problems, a feature-based parametric architecture, and multistep lookahead.The fundamental principles on which all computer chess programs (as well as most two-person game programs) are based were laid out by Shannon [Sha50], before Bellman started his work on DP.Shannon proposed multistep lookahead and evaluation of the end positions by means of a "scoring function" (in our terminology this plays the role of a cost function approximation).This function may involve, for example, the calculation of a numerical value for each of a set of major features of a position that chess players easily recognize (such as material balance, mobility, pawn structure, and other positional factors), together with a method to combine these numerical values into a single score.Shannon then went on to describe various strategies of exhaustive and selective search over a multistep lookahead tree of moves.We may view the scoring function as a feature-based architecture for evaluating a chess position/state (cf.Fig. 3.1.6).In most computer chess programs, the features are weighted linearly, i.e., the architecture J (x, r) that is used for multistep lookahead is linear [cf.Eq. (3.3)].In many cases, the weights have been determined manually, by trial and error based on experi-ence.However, in some programs, the weights have been determined with supervised learning techniques that use examples of grandmaster play, i.e., by adjustment to bring the play of the program as close as possible to the play of chess grandmasters.This is a technique that applies more broadly in artificial intelligence; see Tesauro [Tes89b], [Tes01].In a recent computer chess breakthrough, the entire idea of extracting features of a position through human expertise was abandoned in favor of feature discovery through self-play and the use of neural networks.The first program of this type to attain supremacy over humans, as well as over the best computer programs that use human expertise-based features, was AlphaZero (Silver et al. [SHS17]).This program, described in Section 1.1, is based on DP principles of approximate policy iteration and multistep lookahead based on Monte Carlo tree search.Our next example relates to a methodology for feature construction, where the number of features may increase as more data is collected.For a simple example, consider the piecewise constant approximation of Example 3.1.1,where more pieces are progressively added based on new data, possibly using some form of exploration-exploitation tradeoff.We have viewed so far feature vectors φ(x) as functions of x, obtained through some unspecified process that is based on prior knowledge about the cost function being approximated.On the other hand, features may also be extracted from data.For example suppose that with some preliminary calculation using data, we have identified some suitable states x( ), = 1, . . ., m, that can serve as "anchors" for the construction of Gaussian basis functions of the formwhere σ is a scalar "variance" parameter, and • denotes the standard Euclidean norm.This type of function is known as a radial basis function.It is concentrated around the state x( ), and it is weighed with a scalar weight r to form a parametric linear feature-based architecture, which can be trained using additional data.Several other types of data-dependent basis functions, such as support vector machines, are used in machine learning, where they are often referred to as kernels.While it is possible to use a preliminary calculation to obtain the anchors x( ) in Eq. (3.4), and then use additional data for training, one may also consider enrichment of the set of basis functions simultaneously with training.In this case the number of the basis functions increases as the training data is collected.A motivation here is that the quality of the approximation may increase with additional basis functions.This idea underlies a field of machine learning, known as kernel methods or sometimes nonparametric methods.A further discussion is outside our scope.We refer to the literature; see e.  [BMM18].In what follows, for the sake of simplicity, we will focus on parametric architectures with a fixed and given feature vector, since the choice of approximation architecture is somewhat peripheral to our main focus.The next example considers a feature extraction strategy that is particularly relevant to problems of partial state information.The concept of a sufficient statistic, which originated in inference methodologies, plays an important role in DP.As discussed in Section 1.6, it refers to quantities that summarize all the essential content of the state x k for optimal control selection at time k.In particular, consider a partial information context where at time k we have accumulated the information vector (also called the past history)which consists of the past controls u0, . . ., u k−1 and the state-related measurements z0, . . ., z k obtained at the times 0, . . ., k.The control u k is allowed to depend only on I k , and the optimal policy is a sequence of the form µ * 0 (I0), . . ., µ * N−1 (IN−1) .We say that a function S k (I k ) is a sufficient statistic at time k if the control function µ * k depends on I k only through S k (I k ), i.e., for some function μk , we havewhere µ * k is optimal.There are several examples of sufficient statistics, and they are typically problem-dependent.A trivial possibility is to view I k itself as a sufficient statistic, and a more sophisticated possibility is to view the belief state b k as a sufficient statistic (this is the conditional probability distribution of x k given I k ; cf.Section 1.6.4).For a proof that b k is indeed a sufficient statistic and for a more detailed discussion of other possible sufficient statistics, see [Ber17a], Chapter 4. For a mathematically more advanced discussion, see [BeS78], Chapter 10.Since a sufficient statistic contains all the relevant information for optimal control purposes, an idea that suggests itself is to introduce features of a given sufficient statistic and to train a corresponding approximation architecture accordingly.As examples of potentially good features, one may consider some special characteristic of I k (such as whether some alarm-like "special" event has been observed), or a partial history (such as the last m measurements and controls in I k , or more sophisticated versions based on the concept of a finite-state controller proposed by White [Whi91], and White and Scherer [WhS94], and further discussed by Hansen [Han98], Kaelbling, Littman, and Cassandra [KLC98], Meuleau et al. [MPK99], Poupart and Boutilier [PoB04], Yu and Bertsekas [YuB08], Saldi, Yuksel, and Linder [SYL17]).In the case where the belief state b k is used as a sufficient statistic, examples of good features may be a point estimate based on b k , the variance of this estimate, and other quantities that can be simply extracted from b k .The paper by Bhattacharya et al. [BBW20] considers another type of feature vector that is related to the belief state.This is a sufficient statistic, denoted by y k , which subsumes the belief state b k , in the sense that b k can be computed exactly knowing y k .One possibility is for y k to be the union of b k and some identifiable characteristics of the belief state, or some compact representation of the measurement history up to the current time (such as a number of most recent measurements, or the state of a finite-state controller).Even though the information content of y k is no different than the information content of b k for the purposes of exact optimization, a sufficient statistic y k that is specially designed for the problem at hand may lead to improved performance in the presence of cost and policy approximations.We finally note a related idea, which is to supplement a sufficient statistic with features of other sufficient statistics, and thus obtain an enlarged/richer sufficient statistic.In problem-specific contexts, and in the presence of approximations, this may yield improved results.The use of a feature vector φ(x) to represent the state x in an approximation architecture of the form J φ(x), r implicitly involves state aggregation, i.e., the grouping of states into subsets.We will discuss aggregation in some detail in Section 3.6.Here we will give a summary of a special type of aggregation architecture.In particular, let us assume that the feature vector can take only a finite number of values, and define for each possible value v, the subset of states Sv whose feature vector is equal to v:We refer to the sets Sv as the aggregate states induced by the feature vector.These sets form a partition of the state space.An approximate cost-to-go function of the form J φ(x), r is piecewise constant with respect to this partition; that is, it assigns the same cost-to-go value J(v, r) to all states in the set Sv.An often useful approach to deal with problem complexity in DP is to introduce an "aggregate" DP problem, whose states are some suitably defined feature vectors φ(x) of the original problem.The precise form of the aggregate problem may depend on intuition and/or heuristic reasoning, based on our understanding of the original problem.Suppose now that the aggregate problem is simple enough to be solved exactly by DP, and let Ĵ(v)  be its optimal cost-to-go when the initial value of the feature vector is v. Then Ĵ φ(x) provides an approximation architecture for the original problem, i.e., the architecture that assigns to state x the (exactly) optimal cost-togo Ĵ φ(x) of the feature vector φ(x) in the aggregate problem.There is considerable freedom on how one formulates and solves aggregate problems.We refer to the DP textbooks [Ber12], [Ber17a], and the RL textbook [Ber19a], Chapter 6, for a detailed treatment; see also the discussion of Section 3.6.The next example relates to an architecture that is particularly useful when parallel computation is available.A simple method to construct complex and sophisticated approximation architectures, is to partition the state space into several subsets and construct a separate approximation in each subset.For example, by using a separate linear or quadratic polynomial approximation in each subset of the partition, we can construct piecewise linear or piecewise quadratic approximations over the entire state space.Similarly, we may use a separate neural network architecture on each set of the partition.An important issue here is the choice of the method for partitioning the state space.Regular partitions (e.g., grid partitions) may be used, but they often lead to a large number of subsets and very time-consuming computations.Generally speaking, each subset of the partition should contain "similar" states so that the variation of the optimal cost-to-go over the states of the subset is relatively smooth and can be approximated with smooth functions.An interesting possibility is to use features as the basis for partition.In particular, one may use a more or less regular partition of the space of features, which induces a possibly irregular partition of the original state space.In this way, each subset of the irregular partition contains states with "similar features;" see Fig. 3.1.7.As an illustration consider the game of chess.The state here consists of the board position, but the nature of the position progresses over time through opening, middlegame, and endgame phases.Moreover each of these phases may be affected differently by special features of the position.For example there are several different types of endgames (rook endgames, kingand-pawn endgames, minor-piece endgames, etc), which are characterized by identifiable features and call for different playing strategies.It would thus make sense to partition the set of chess positions according to their features, and use a separate strategy on each set of the partition.Indeed this is done to some extent in a number of chess programs.A potential difficulty with partitioned architectures is that there is discontinuity of the approximation along the boundaries of the partition.For this reason, a variant, called soft partitioning, is sometimes employed, whereby the subsets of the partition are allowed to overlap and the discontinuity is smoothed out over their intersection.In particular, once a function approximation is obtained in each subset, the approximate cost-to-go in the overlapping regions is taken to be a smoothly varying linear combination of the function approximations of the corresponding subsets.Partitioning and local approximations can also be used to enhance the quality of approximation in parts of the space where the target function has some special character.For example, suppose that the state space S is partitioned in subsets S1, . . ., SM and consider approximations of the formwhere each φ k,m (x) is a basis function which is local, in the sense that it contributes to the approximation only on the set Sm; that is, it takes the value 0 for x / ∈ Sm.Here Ĵ(x, r) is an architecture of the type discussed earlier, and the parameter vector r consists of r and the coefficients rm(k) of the basis functions.Thus the portion Ĵ(x, r) of the architecture is used to capture "global" aspects of the target function, while each portionis used to capture aspects of the target function that are "local" to the subset Sm.The book [BeT96] (Section 3.1.3)discusses the training of local-global approximation architectures with methods that are tailored to their special structure.Unfortunately, in practice we often do not know an adequate set of features, so it is important to have methods that construct features automatically, to supplement whatever features may already be available.Indeed, there are architectures that do not rely on the knowledge of good features.We have noted the kernel methods of Example 3.1.5 in this connection.Another very popular possibility is neural networks, which we will describe in Section 3.2.Some of these architectures involve training that constructs simultaneously both the feature vectors φ(x) and the parameter vectors r that weigh them.Generally, architectures that construct features automatically do not preclude the use of additional features that are based on a priori knowledge or understanding of the problem at hand.In particular these architectures may, in addition to x, use as inputs additional hand-crafted features that are relevant for the problem at hand.Another possibility is to combine automatically constructed features with other a priori known good features into a (mixed) linear architecture that involves both types of features.The weights of the latter linear architecture may be obtained with a separate second stage training process, following the first stage training process that constructs automatically suitable features using a nonlinear architecture such as a neural network.In this section, we discuss briefly the training process of choosing the parameter vector r of a parametric architecture J(x, r), focusing primarily on incremental gradient methods.The most common type of training is based on a least squares optimization, also known as least squares regression.Here a set of state-cost training pairs (x s , β s ), s = 1, . . ., q, called the training set , is collected and r is determined by solving the problem min r q s=1 J(x s , r) − β s 2 .(3.6) Thus r is chosen to minimize the sum of squared errors between the sample costs β s and the architecture-predicted costs J(x s , r).Here there is some target cost function J that we aim to approximate with J(•, r), and the sample cost β s is the value J(x s ) plus perhaps some error or "noise."The cost function of the training problem (3.6) is generally nonconvex, and can be quite complicated.This may pose challenges, since there may exist multiple local minima.However, for a linear architecture the cost function is convex quadratic, and the training problem admits a closedform solution.In particular, for the linear architecture J(x, r) = r φ(x), the problem becomes minBy setting the gradient of the quadratic objective to 0, we obtainThus by matrix inversion we obtain the minimizing parameter vector r = q s=1 φ(x s )φ(x s ) −1 q s=1 φ(x s )β s .(3.7)If the inverse above does not exist, an additional quadratic in r, called a regularization function, is added to the least squares objective to deal with this, and also to help with other issues to be discussed later.A singular value decomposition approach may also be used to deal with the matrix inversion issue; see [BeT96], Section 3.2.2.Thus a linear architecture has the important advantage that the training problem can be solved exactly and conveniently with the formula (3.7) (of course it may be solved by any other algorithm that is suitable for linear least squares problems, including iterative algorithms).By contrast, if we use a nonlinear architecture, such as a neural network, the associated least squares problem is nonquadratic and also nonconvex, so it is hard to solve in principle.Despite this fact, through a combination of sophisticated implementation of special gradient algorithms, called incremental , and powerful computational resources, neural network methods have been successful in practice.We will now discuss briefly special methods for solution of the nonlinear least squares training problem (3.6), assuming a parametric architecture that is differentiable in the parameter vector.This methodology can be properly viewed as a subject in nonlinear programming and iterative algorithms, and as such it can be studied independently of the approximate DP methods of this book.Thus the reader who has already some exposure to the subject may skip to the next section.The author's nonlinear programming textbook [Ber16] and the RL book [Ber19a] provide more detailed presentations.We view the training problem (3.6) as a special case of the minimization of a sum of component functionswhere each f i is a differentiable scalar function of the n-dimensional column vector y (this is the parameter vector).Thus we use the more common symbols y and m in place of r and q, respectively, and we replace the squared error terms J(x s , r) − β s 2 in the training problem (3.6) with the generic terms f i (y).The (ordinary) gradient method for problem (3.8) generates a sequence {y k } of iterates, starting from some initial guess y 0 for the minimum of the cost function f .It has the form †where γ k is a positive stepsize parameter.The incremental gradient method is similar to the ordinary gradient method, but uses the gradient of a single component of f at each iteration.It has the general formwhere i k is some index from the set {1, . . ., m}, chosen by some deterministic or randomized rule.Thus a single component function f i k is used at iteration k, with great economies in gradient calculation cost over the ordinary gradient method (3.9), particularly when m is large.This is of course a radical simplification, which involves a large approximation error, yet it performs surprisingly well!The idea is to attain faster convergence when far from the solution as we will explain shortly; see the author's books [BeT96], [Ber16], and [Ber19a] for a more detailed discussion.The method for selecting the index i k of the component to be iterated on at iteration k is important for the performance of the method.We describe three common rules , the last two of which involve randomization: ‡ (1) A cyclic order , the simplest rule, whereby the indexes are taken up in the fixed deterministic order 1, . . ., m, so that i k is equal to (k modulo m) plus 1.A contiguous block of iterations involving the components f 1 , . . ., f m in this order and exactly once is called a cycle.(2) A uniform random order , whereby the index i k chosen randomly by sampling over all indexes with a uniform distribution, independently of the past history of the algorithm.This rule may perform better than the cyclic rule in some circumstances.† We use standard calculus notation for gradients; see, e.g., [Ber16], Appendix A. In particular, ∇f (y) denotes the n-dimensional column vector whose components are the first partial derivatives ∂f (y)/∂yi of f with respect to the components y1, . . ., yn of the column vector y.‡ With these stepsize rules, the incremental gradient method is often called stochastic gradient or stochastic gradient descent method.(3) A cyclic order with random reshuffling, whereby the indexes are taken up one by one within each cycle, but their order after each cycle is reshuffled randomly (and independently of the past).This rule is used widely in practice, particularly when the number of components m is modest, for reasons to be discussed later.Note that in the cyclic cases, it is essential to include all components in a cycle; otherwise some components will be sampled more often than others, leading to a bias in the convergence process.Similarly, it is necessary to sample according to the uniform distribution in the random order case.Focusing for the moment on the cyclic rule (with or without reshuffling), we note that the motivation for the incremental gradient method is faster convergence: we hope that far from the solution, a single cycle of the method will be as effective as several (as many as m) iterations of the ordinary gradient method (think of the case where the components f i are similar in structure).Near a solution, however, the incremental method may not be as effective.In particular, we note that there are two complementary performance issues to consider in comparing incremental and nonincremental methods:(a) Progress when far from convergence.Here the incremental method can be much faster.For an extreme case take m large and all components f i identical to each other.Then an incremental iteration requires m times less computation than a classical gradient iteration, but gives exactly the same result, when the stepsize is scaled to be m times larger.While somewhat extreme, this example reflects the essential mechanism by which incremental methods can be much superior: far from the minimum a single component gradient will point to "more or less" the right direction, at least most of the time; see the following example.(b) Progress when close to convergence.Here the incremental method can be inferior.In particular, the ordinary gradient method (3.9) is convergent with a constant stepsize under reasonable assumptions, see e.g., [Ber16].However, the incremental method requires a diminishing stepsize, and its ultimate rate of convergence can be much slower.This type of behavior is illustrated in the following example, first given in the 1995 first edition of author's nonlinear programming book [Ber16], and the neurodynamic programming book [BeT96].Example 3.1.9Assume that y is a scalar, and that the problem iswhere ci and bi are given scalars with ci = 0 for all i.The minimum of each of the components fi(y) = 1 2 (ciy − bi) 2 iswhile the minimum of the least squares cost function f isIt can be seen that y * lies within the range of the component minimaand that for all y outside the range R, the gradienthas the same sign as ∇f (y) (see Fig. 3.1.8).As a result, when outside the region R, the incremental gradient methodapproaches y * at each step, provided the stepsize γ k is small enough.In fact it can be verified that it is sufficient thatHowever, for y inside the region R, the ith step of a cycle of the incremental gradient method need not make progress.It will approach y * (for small enough stepsize γ k ) only if the current point y k does not lie in the interval connecting y * i and y * .This induces an oscillatory behavior within the region R, and as a result, the incremental gradient method will typically not converge to y * unless γ k → 0. By contrast, the ordinary gradient method, which takes the formcan be verified to converge to y * for any constant stepsize γ withHowever, for y outside the region R, a full iteration of the ordinary gradient method need not make more progress towards the solution than a single step of the incremental gradient method.In other words, with comparably intelligent stepsize choices, far from the solution (outside R), a single pass through the entire set of cost components by incremental gradient is roughly as effective as m passes by ordinary gradient.The preceding example assumes that each component function f i has a minimum, so that the range of component minima is defined.In cases where the components f i have no minima, a similar phenomenon may occur, as illustrated by the following example (the idea here is that we may combine several components into a single component that has a minimum).is labeled as the "region of confusion."It is the region where the method does not have a clear direction towards the optimum.The ith step in an incremental gradient cycle is a gradient step for minimizing (c i y − b i ) 2 , so if y lies outside the region of component minima R = min i y * i , max i y * i , (labeled as the "farout region") and the stepsize is small enough, progress towards the solution y * is made.Consider the case where f is the sum of increasing and decreasing convex exponentials, i.e., fi(y) = aie b i y , y∈ ,where ai and bi are scalars with ai > 0 and bi = 0. Letand assume that I + and I − have roughly equal numbers of components.Let also y * be the minimum of m i=1 fi.Consider the incremental gradient method that given the current point, call it y k , chooses some component fi k and iterates according to the incremental gradient iterationThen it can be seen that if y k >> y * , y k+1 will be substantially closer to y * if i ∈ I + , and negligibly further away than y * if i ∈ I − .The net effect, averaged over many incremental iterations, is that if y k >> y * , an incremental gradient iteration makes roughly one half the progress of a full gradient iteration, with m times less overhead for calculating gradients.The same is true if y k << y * .On the other hand as y k gets closer to y * the advantage of incrementalism is reduced, similar to the preceding example.In fact in order for the incremental method to converge, a diminishing stepsize is necessary, which will ultimately make the convergence slower than the one of the nonincremental gradient method with a constant stepsize.The discussion of the preceding examples relies on y being one-dimensional, but in many multidimensional problems the same qualitative behavior can be observed.In particular, a pass through the ith component f i by the incremental gradient method can make progress towards the solution in the region where the component gradient ∇f i k (y k ) makes an angle less than 90 degrees with the cost function gradient ∇f (y k ).If the components f i are not "too dissimilar," this is likely to happen in a region of points that are not too close to the optimal solution set.This behavior has been verified in many practical contexts, including the training of neural networks (cf. the next section), where incremental gradient methods have been used extensively, frequently under the name backpropagation methods.The choice of the stepsize γ k plays an important role in the performance of incremental gradient methods.In practice, it is common to use a constant stepsize for a (possibly prespecified) number of iterations, then decrease the stepsize by a certain factor, and repeat, up to the point where the stepsize reaches a prespecified floor value.An alternative possibility is to use a diminishing stepsize rule of the formwhere γ, β 1 , and β 2 are some positive scalars.There are also variants of the method that use a constant stepsize throughout, and can be shown to converge to a stationary point of f under reasonable assumptions.In one type of such method the degree of incrementalism gradually diminishes as the method progresses (see [Ber97a]).Another incremental approach with similar aims, is the aggregated gradient method, which is discussed in the author's textbooks [Ber15a], [Ber16], [Ber19a].Regardless of whether a constant or a diminishing stepsize is ultimately used, the incremental method must use a much larger stepsize than the corresponding nonincremental gradient method (as much as m times larger, so that the size of the incremental gradient step is comparable to the size of the nonincremental gradient step).One possibility is to use an adaptive stepsize rule, whereby, roughly speaking, the stepsize is reduced (or increased) when the progress of the method indicates that the algorithm is (or is not) oscillating.There are formal ways to implement such stepsize rules with sound convergence properties (see [Tse98], [MYF03]).The difficulty with stepsize selection may also be addressed with diagonal scaling, i.e., using a stepsize γ k j that is different for each of the components y j of y. Second derivatives can be very useful for this purpose.In generic nonlinear programming problems of unconstrained minimization of a function f , it is common to use diagonal scaling with stepsizeswhere γ is a constant that is nearly equal 1 (the second derivatives may also be approximated by gradient difference approximations).However, in least squares training problems, this type of scaling is inconvenient because of the additive form of f as a sum of a large number of component functions:cf. Eq. (3.8).The neural network literature includes a number of practical scaling schemes, some of which have been incorporated in publicly and commercially available software; see the two influential papers, Duchi, Hazan, and Singer [DHS11], and Kingman and Ba [KiB14], as well as subsequent works that expand on the ideas of these two papers.The RL book [Ber19a] (Section 3.1.3)describes another type method that involves second derivatives and is based on Newton's method.The idea here is to write Newton's method in a format that is well suited to the additive character of the cost function f , and involves low order matrix inversion.One can then implement diagonal scaling by setting to zero the off-diagonal terms of the inverted matrices, so that the algorithm involves no matrix inversion.There is also another related algorithm, which is based on the Gauss-Newton method and the extended Kalman filter; see the author's paper [Ber96], and the books [BeT96] and [Ber16].There are several different types of neural networks that can be used for a variety of tasks, such as pattern recognition, classification, image and speech recognition, natural language processing, and others.In this section, we focus on our finite horizon DP context, and the role that neural networks can play in approximating the optimal cost-to-go functions J * k .As an example within this context, we may first use a neural network to construct an approximation to J * N −1 .Then we may use this approximation to approximate J * N −2 , and continue this process backwards in time, to obtain approximations to all the optimal cost-to-go functions J * k , k = 1, . . ., N −1, as we will discuss in more detail in Section 3.3.Throughout this section, we will focus on the type of neural network, known as a multilayer perceptron, which is the one most used at present in the RL applications discussed in this book.Naturally, there are variations that are adapted to the problem at hand.For example AlphaZero uses a specialized neural network that takes advantage of the board-like structure of chess and Go to facilitate and expedite the associated computations.To describe the use of neural networks in finite horizon DP, let us consider the typical stage k, and for convenience drop the index k; the subsequent discussion applies to each value of k separately.We consider parametric architectures J(x, v, r) of the formthat depend on two parameter vectors v and r.Our objective is to select v and r so that J(x, v, r) approximates some target cost function that can be sampled (possibly with some error).The process is to collect a training set that consists of a large number of state-cost pairs (x s , β s ), s = 1, . . ., q, and to find a function J(x, v, r) of the form (3.11) that matches the training set in a least squares sense, i.e., (v, r) minimizesWe postpone for later the question of how the training pairs (x s , β s ) are generated.† Notice the different roles of the two parameter vectors here: v parametrizes φ(x, v), which in some interpretation may be viewed as a feature vector, and r is a vector of linear weighting parameters for the components of φ(x, v).A neural network architecture provides a parametric class of functions J(x, v, r) of the form (3.11) that can be used in the optimization framework just described.The simplest type of neural network is the single layer perceptron; see Fig. 3.2.1.Here the state x is encoded as a vector of numerical values y(x) with components y 1 (x), . . ., y n (x), which is then transformed linearly as Ay(x) + b, † The least squares training problem used here is based on nonlinear regression.This is a classical method for approximating the expected value of a function with a parametric architecture, and involves a least squares fit of the architecture to simulation-generated samples of the expected value.We refer to machine learning and statistics textbooks for more discussion.where A is an m × n matrix and b is a vector in m .† This transformation is called the linear layer of the neural network.We view the components of A and b as parameters to be determined, and we group them together into the parameter vector v = (A, b).Each of the m scalar output components of the linear layer,becomes the input to a nonlinear differentiable and monotonically increasing function σ that maps scalars to scalars.A simple and popular possibility is the rectified linear unit (ReLU for short), which is simply the function max{0, ξ}, approximated by a differentiable function σ by some form of smoothing operation; for example σ(ξ) = ln(1 + e ξ ), which is illustrated in Fig. 3.2.2.Other functions, used since the early days of neural networks, have the property† The method of encoding x into the numerical vector y(x) is generally problem-dependent, but it can be critical for the success of the training process.We should note also that some of the components of y(x) could be known interesting features of x that can be designed based on problem-specific knowledge.) ξ 1 0 -1 ) ξ 1 0 -1 1 0 -1 1 0 -1 Good approximation Poor Approximation σ(ξ) = ln(1 + e ξ ) max{0, ξ} Selective Depth Lookahead Tree σ(ξ) Selective Depth Lookahead Tree σ(ξ)In what follows, we will ignore the character of the function σ (except for differentiability), and simply refer to it as a "nonlinear unit" and to the corresponding layer as a "nonlinear layer."At the outputs of the nonlinear units, we obtain the scalarsOne possible interpretation is to view φ (x, v) as features of x, which are linearly combined using weights r , = 1, . . ., m, to produce the final . . .Note that each value φ (x, v) depends on just the th row of A and the th component of b, not on the entire vector v.In some cases this motivates placing some constraints on individual components of A and b to achieve special problem-dependent "handcrafted" effects.The state encoding operation that transforms x into the neural network input y(x) can be instrumental in the success of the approximation scheme.Examples of state encodings are components of the state x, numerical representations of qualitative characteristics of x, and more generally features of x, i.e., functions of x that aim to capture "important nonlinearities" of the optimal cost-to-go function.With the latter view of state encoding, we may consider the approximation process as consisting of a feature extraction mapping, followed by a neural network with input the extracted features of x, and output the cost-to-go approximation; see Fig. 3.2.4.In a more general view of the neural network, the state encoder may involve some tunable parameters.The idea here is that with a good feature extraction mapping, the neural network need not be very complicated (may involve few nonlinear units and corresponding parameters), and may be trained more easily.This intuition is borne out by simple examples and practical experience.However, as is often the case with neural networks, it is hard to support it with a quantitative analysis.An important question is how well we can approximate the target function J * k with a neural network architecture, assuming we can choose the number of the nonlinear units m to be as large as we want.The answer to this question is quite favorable and is provided by the so-called universal approximation theorem.Roughly, the theorem says that assuming that x is an element of a Euclidean space X and y(x) ≡ x, a neural network of the form described can approximate arbitrarily closely (in an appropriate mathematical sense), over a compact subset S ⊂ X, any piecewise continuous function J : S → , provided the number m of nonlinear units is sufficiently large.For proofs of the theorem, we refer to Cybenko [Cyb89], Funahashi [Fun89], Hornik, Stinchcombe, and White [HSW89], and Leshno et al. [LLP93].For additional sources and intuitive explanations we refer to Bishop ([Bis95], pp.129-130), Jones [Jon90], and the RL textbook [Ber19a], Section 3.2.1.While the universal approximation theorem provides some assurance about the adequacy of the neural network structure, it does not predict how many nonlinear units we may need for "good" performance in a given problem.Unfortunately, this is a difficult question to even pose precisely, let alone to answer adequately.In practice, one is often reduced to trying increasingly larger values of m until one is convinced that satisfactory performance has been obtained for the task at hand.One may improve on trial-and-error schemes with more systematic hyperparameter search methods, such as Bayesian optimization, and in fact this has been used to tune the parameters of the deep network used by AlphaZero.Experience has shown that in many cases the number of required nonlinear units and corresponding dimension of A can be very large, adding significantly to the difficulty of solving the training problem.This has given rise to many suggestions for modifications of the perceptron structure.An important possibility is to concatenate multiple single layer perceptrons so that the output of the nonlinear layer of one perceptron becomes the input to the linear layer of the next, giving rise to deep neural networks, which we will discuss later.Given a set of state-cost training pairs (x s , β s ), s = 1, . . ., q, the parameters of the neural network A, b, and r are obtained by solving the problem min(3.12)Note that the cost function of this problem is generally nonconvex, so there may exist multiple local minima.In practice it is common to augment the cost function of this problem with a regularization function, such as a quadratic in the parameters A, b, and r.This is customary in least squares problems in order to make the problem easier to solve algorithmically.However, in the context of neural network training, regularization is primarily important for a different reason: it helps to avoid overfitting, which occurs when the number of parameters of the neural network is relatively large (comparable to the size of the training set).In this case a neural network model matches the training data very well but may not do as well on new data.This is a known difficulty, which is the subject of much current research, particularly in the context of deep neural networks.An important issue is to select a method to solve the training problem (3.12).While we can use any unconstrained optimization method that is based on gradients, in practice it is important to take into account the cost function structure of problem (3.12).The salient characteristic of this cost function is that it is the sum of a potentially very large number q of component functions.This makes the computation of the cost function value of the training problem and/or its gradient very costly.For this reason the incremental methods of Section 3.1.3are universally used for training.† Experience has shown that these methods can be vastly superior to their nonincremental counterparts in the context of neural network training.The implementation of the training process has benefited from experience that has been accumulated over time, and has provided guidelines for scaling, regularization, initial parameter selection, and other practical issues; we refer to books on neural networks such as Bishop [Bis95], Bishop and Bishop [BiB24], Goodfellow, Bengio, and Courville [GBC16], and Haykin [Hay08], as well as to the overview paper on deep neural network training by Sun [Sun19], and the references quoted therein.Still, incremental methods can be quite slow, and training may be a time-consuming process.Fortunately, the training is ordinarily done off-line, possibly using parallel computation, in which case computation time may not be a serious issue.Moreover, in practice the neural network training problem typically need not be solved with great accuracy.This is also supported by the Newton step view of approximation in value space, which suggests that great accuracy in the terminal cost function approximation is not critically important for good performance of the on-line play controller.An important generalization of the single layer perceptron architecture involves a concatenation of multiple layers of linear and nonlinear functions; see Fig. 3.2.5.In particular the outputs of each nonlinear layer become the inputs of the next linear layer.In some cases it may make sense to add † The incremental methods are valid for an arbitrary order of component selection within the cycle, but it is common to randomize the order at the beginning of each cycle.Also, in a variation of the basic method, we may operate on a batch of several components at each iteration, called a minibatch, rather than a single component.This has an averaging effect, which reduces the tendency of the method to oscillate and allows for the use of a larger stepsize; see the end-of-chapter references.as additional inputs some of the components of the state x or the state encoding y(x).In the early days of neural networks practitioners tended to use few nonlinear layers (say one to three).For example, Tesauro's backgammon program and its descendants have performed well with one or two nonlinear layers [PaR12].However, more recently a lot of success in certain problem domains (including image and speech processing, large language models, as well as approximate DP) has been achieved with deep neural networks, which involve a considerably larger number of layers.There are a few questions to consider here.The first has to do with the reason for having multiple nonlinear layers, when a single one is sufficient to guarantee the universal approximation property.Here are some qualitative (and somewhat speculative) explanations:(a) If we view the outputs of each nonlinear layer as features, we see that the multilayer network produces a hierarchy of features, where each set of features is a function of the preceding set of features [except for the first set of features, which is a function of the encoding y(x) of the state x].In the context of specific applications, this hierarchical structure can be exploited to specialize the role of some of the layers and to enhance some characteristics of the state.(b) Given the presence of multiple linear layers, one may consider the possibility of using matrices A with a particular sparsity pattern, or other structure that embodies special linear operations such as convolution, which may be well-matched to the training problem at hand.Moreover, when such structures are used, the training problem often becomes easier, because the number of parameters in the linear layers is drastically decreased.(c) Overparametrization (more weights than data, as in a deep neural network) helps to mitigate the detrimental effects of overfitting, and the attendant need for regularization.The explanation for this fascinating phenomenon (observed as early as the late 90s) is the subject], [ZBH21] for representative works.We finally note that the use of deep neural networks has been an important factor for the success of the AlphaGo and AlphaZero programs, as well as for large language models.New developments in hardware, software, architectural structurea, and training methodology, are likely to improve the already enormous power of deep neural networks, and to allow the use of ever larger datasets, which are becoming increasingly available.In the context of approximate DP/RL, architectures are mainly used to approximate either cost functions or policies.When a neural network is involved, the terms value network and policy network are commonly used, respectively.† In this section we will illustrate the use of value networks in finite horizon DP, while in the next section we will discuss the use of policy networks.We will also illustrate in Section 3.3.3 the combined use of policy and value networks within an approximate policy iteration context, whereby the policies and their cost functions are approximated by a policy and a value network, respectively, to generate a sequence of (approximately) improved policies.Finally, in Sections 3.3.4and 3.4.5,we will describe how approximating Q-factor or cost differences (rather than Q-factors or costs) can be beneficial within our context of approximation in value space.Let us describe a popular approach for training an approximation architecture Jk (x k , r k ) for a finite horizon DP problem.The parameter vectors r k are determined sequentially, starting from the end of the horizon, and proceeding backwards as in the DP algorithm: first r N −1 then r N −2 , and so on.The algorithm samples the state space for each stage k, and generates a large number of states x s k , s = 1, . . ., q.It then determines sequentially the parameter vectors r k to obtain a good "least squares fit" to the DP algorithm.The method can also be used in the infinite horizon case, in essentially identical form, and it is commonly called fitted value iteration.In particular, each r k is determined by generating a large number of sample states and solving a least squares problem that aims to minimize the error in satisfying the DP equation for these states at time k.At † The alternative terms critic network and actor network are also used often.In this book, we will use the terms "value network" and "policy network." the typical stage k, having obtained r k+1 , we determine r k from the least squares problemwhere x s k , i = 1, . . ., q, are the sample states that have been generated for the kth stage.Since r k+1 is assumed to be already known, the complicated minimization term in the right side of this equation is the known scalarso that r k is obtained asThe algorithm starts at stage N − 1 with the minimization2 and ends with the calculation of r 0 at k = 0.In the case of a linear architecture, where the approximate cost-to-go functions arethe least squares problem (3.14) greatly simplifies, and admits the closed form solutioncf. Eq. (3.7).For a nonlinear architecture such as a neural network, incremental gradient algorithms may be used.An important implementation issue is how to select the sample states x s k , s = 1, . . ., q, k = 0, . . ., N − 1.In practice, they are typically obtained by some form of Monte Carlo simulation, but the distribution by which they are generated is important for the success of the method.In particular, it is important that the sample states are "representative" in the sense that they are visited often under a nearly optimal policy.More precisely, the frequencies with which various states appear in the sample should be roughly proportional to the probabilities of their occurrence under an optimal policy.Aside from the issue of selection of the sampling distribution that we have just described, a difficulty with fitted value iteration arises when the horizon N is very long, since then the total number of parameters over the N stages may become excessive.In this case, however, the problem is often stationary, in the sense that the system and cost per stage do not change as time progresses.Then it may be possible to treat the problem as one with an infinite horizon and bring to bear additional methods for training approximation architectures; see the relevant discussions in Chapter 5 of the book [Ber19a].We finally note an important difficulty with the training method of this section: the calculation of each sample β s k of Eq. (3.13) requires a minimization of an expected value, which can be very time consuming.In the next section, we describe an alternative type of fitted value iteration, which uses Q-factors, and involves a simpler minimization, whereby the order of the minimization and expectation operations in Eq. (3.13) is reversed.We will now consider an alternative form of approximation in value space and fitted value iteration, which involves approximation of the optimal Q-factors of state-control pairs (x k , u k ) at time k, with no intermediate approximation of cost-to-go functions.An important characteristic of this algorithm is that it allows for a model-free computation (i.e., the use of a computer model in place of a mathematical model).We recall that the optimal Q-factors are defined by) where J * k+1 is the optimal cost-to-go function for stage k+1.Thus Q * k (x k , u k ) is the cost attained by using u k at state x k , and subsequently using an optimal policy.As noted in Section 1.3, the DP algorithm can be written asand by using this equation, we can write Eq. (3.15) in the following equivalent form that relates Q * k with Q * k+1 :This suggests that in place of the Q-factors Q * k (x k , u k ), we may use Q-factor approximations as the basis for suboptimal control.We can obtain such approximations by using methods that are similar to the ones we have considered so far.Parametric Q-factor approximations Qk (x k , u k , r k ) may involve a neural network, or a feature-based linear architecture.The feature vector may depend on just the state, or on both the state and the control.In the former case, the architecture has the formwhere r k (u k ) is a separate weight vector for each control u k .In the latter case, the architecture has the formwhere r k is a weight vector that is independent of u k .The architecture (3.17) is suitable for problems with a relatively small number of control options at each stage.In what follows, we will focus on the architecture (3.18), but the discussion, with few modifications, also applies to the architecture (3.17) and to nonlinear architectures as well.We may adapt the fitted value iteration approach of the preceding section to compute sequentially the parameter vectors r k in Q-factor parametric approximations, starting from k = N − 1.This algorithm is based on Eq. (3.16), with r k obtained by solving least squares problems similar to the ones of the cost function approximation case [cf.Eq. (3.14)].As an example, the parameters r k of the architecture (3.18) are computed sequentially by collecting sample state-control pairs (x s k , u s k ), s = 1, . . ., q, and solving the linear least squares problemswhereThus, having obtained r k+1 , we obtain r k through a least squares fit that aims to minimize the sum of the squared errors in satisfying Eq. (3.16).Note that the solution of the least squares problem (3.19) can be obtained in closed form as[cf.Eq. (3.7)].Once r k has been computed, the one-step lookahead control μk (x k ) is obtained on-line as μk (x k ) ∈ arg minwithout the need to calculate any expected value.This latter property is a primary incentive for using Q-factors in approximate DP, particularly when there are tight constraints on the amount of on-line computation that is possible in the given practical setting.The samples β s k of Eq. (3.20) involve the exact computation of an expected value.In an alternative implementation, we may replace β s k with an average of just a few samples (even a single sample) of the random variablecollected according to the probability distribution of w k .This distribution may either be known explicitly, or in a model-free situation, through a computer simulator.In particular, to implement this scheme, we only need a simulator that for any pair (x k , u k ) generates a sample of the stage cost g k (x k , u k , w k ) and the next state f k (x k , u k , w k ) according to the distribution of w k .Note that the samples of the random variable (3.22) do not require the computation of an expected value like the samples (3.13) in the cost approximation method of the preceding chapter.Moreover the samples of (3.22) involve a simpler minimization than the samples (3.13).This is an important advantage of working with Q-factors rather than state costs.Having obtained the weight vectors r 0 , . . ., r N −1 , and hence the onestep lookahead policy π = {μ 0 , . . ., μN−1 } through Eq. (3.21), a further possibility is to approximate this policy with a parametric architecture.This is approximation in policy space built on top of approximation in value space.The idea here is to simplify even further the on-line computation of the suboptimal controls by avoiding the minimization of Eq. (3.21).In this section we will briefly discuss parametric approximation methods for infinite horizon problems, based on the policy iteration (PI) method.We will focus on the finite-state version of the α-discounted problem of Section 1.4.1, and adopt notation that is more convenient for such problems.In particular, states and successor states will be denoted by i and j, respectively.Moreover the system equation will be represented by controldependent transition probabilities p ij (u) (the probability that the system will move to state j, given that it starts at state i and control u is applied).For a state-control pair (i, u), the average cost per stage is denoted by g(i, u, j).We recall that the PI algorithm in its exact form produces a sequence of stationary policies whose cost functions are progressively improving and converge in a finite number of iterations to the optimal.The corresponding convergence proof relies on the generic cost improvement property of PI, and depends on the finiteness of the state and control spaces.This proof, together with other PI-related convergence proofs, can be found in the author's textbooks [Ber17a] or [Ber19a].Let us state the exact form of the PI algorithm in terms of Q-factors, and in a form that is suitable for the use of approximations and simulationbased implementations.Given any policy µ, it generates the next policy μ with a two-step process as follows (cf.Section 1.4.1):(a) Policy evaluation: We compute the cost function J µ of µ and its associated Q-factors, which are given byp ij (u) g(i, u, j) + αJ µ (j) , i= 1, . . ., n, u ∈ U (i).Thus Q µ (i, u) is the cost of starting at i, using u at the first stage, and then using µ for the remaining stages.(b) Policy improvement : We compute the new policy μ according to μ(i) ∈ arg minLet us now describe one way to approximate the two steps of the preceding process.(a) Approximate policy evaluation: Here we introduce a parametric architecture Qµ (i, u, r) for the Q-factors of µ.We determine the value of the parameter vector r by generating (using a simulator of the system) a large number of training triplets (i s , u s , β s ), s = 1, . . ., q, and by using a least squares fit:In particular, for a given pair (i s , u s ), the scalar β s is generated by starting at i s , using u s at the first stage, and simulating a trajectory of states and controls using µ for some number k of subsequent stages.Thus, β s is a sample of Q k µ (i s , u s ), the k-stage Q-factor of µ, which in the limit as k → ∞ yields the infinite horizon Q-factor of µ.The number of stages k may be either large, or fairly small.However, in the latter case some terminal cost function approximation should be added at the end of the k-stage trajectory, to compensate for the difference Q µ (i, u) − Q k µ (i, u) , which decreases in proportion to α k , and may be large when k is small.Such a function may be obtained with additional training or from a previous iteration.(b) Approximate policy improvement : Here we compute the new policy μ according towhere r is the parameter vector obtained from the policy evaluation formula (3.23).An important alternative for approximate policy improvement, is to compute a set of pairs i s , μ(i s ) , s = 1, . . ., q, using Eq.(3.24), and fit these pairs with a policy approximation architecture (see the next section on approximation in policy space).The overall scheme then becomes policy iteration that is based on approximation in both value and policy spaces.At the end of the last policy evaluation step of PI, we have obtained a final Q-factor approximation Q(i, u, r).Then, in on-line play mode, we may apply the policy μ(i) ∈ arg mini.e., use the (would be) next policy iterate.Alternatively, we may apply the one-step lookahead policy μ(i) ∈ arg minor its multistep lookahead version.The latter alternative implements a Newton step and will likely result in substantially better performance.However, it is more time consuming, particularly if it is implemented by using a computer model and model-free simulation.Still another possibility, which also implements a Newton step, is to replace the function min u ∈U(j) Q(j, u , r) in the preceding Eq. (3.25) with an off-line trained approximation.Approximate PI in its various forms has been the subject of extensive research, both theoretical and applied.A more detailed discussion is beyond our scope, and we refer to the literature, including the DP textbook [Ber12] or the RL textbook [Ber19a], for detailed accounts.Let us provide a few comments relating to the challenges of approximate PI implementation.(a) Architectural issues: The architecture Qµ (i, u, r) may involve the use of features, and it could be linear, or it could be nonlinear such as a neural network.A major advantage of a linear feature-based architecture is that the policy evaluation (3.23) is a linear least squares problem, which admits a closed-form solution.Moreover, when linear architectures are used, there is a broader variety of approximate policy evaluation methods with solid theoretical performance guarantees, such as TD(λ), LSTD(λ), and LSPE(λ), which are not described in this book, but are discussed extensively in the literature.Generally, finding an architecture that matches well the problem at hand, and properly training it, can be a difficult and time-consuming task.(b) Exploration issues: Generating an appropriate set of training triplets (i s , u s , β s ) at the policy evaluation step poses considerable challenges.A generic difficulty has to do with inadequate exploration, the Achilles heel of approximate PI.In particular, to evaluate a policy µ, we may need to generate Q-factor samples of µ starting from states frequently visited by µ, but this may bias the simulation by underrepresenting states that are unlikely to occur under µ.As a result, the Q-factor estimates of these underrepresented states may be highly inaccurate, causing potentially serious errors in the calculation of the improved control policy μ via the policy improvement Eq. (3.24).One possibility to improve the exploration of the state space is to use a large number of initial states to form a rich and representative subset.It may then be necessary to use relatively short trajectories to keep the cost of the simulation low.However, when using short trajectories it may be important to introduce a terminal cost function approximation in the policy evaluation step in order to make the cost sample β s more accurate.There have been other related approaches to improve exploration, such as using a so-called off-policy, i.e., a policy µ other than the currently evaluated policy µ, to visit states that are unlikely to be visited using µ.See the discussions in Section 6.4 of the DP textbook [Ber12].(c) Oscillation issues: Contrary to exact PI, which is guaranteed to yield an optimal policy, approximate PI produces a sequence of policies, which are only guaranteed to lie asymptotically within a certain error bound from the optimal; see the books [BeT96], Section 6.2.2, and [Ber12], Section 2.5.Moreover, the generated policies may oscillate.By this we mean that after a few iterations, policies tend to repeat in cycles.The associated parameter vectors r may also tend to oscillate, although it is possible that there is convergence in parameter space and oscillation in policy space.This phenomenon, known as chattering, is explained in the author's survey papers [Ber10c], [Ber11b], and book [Ber12] (Section 6.4.3), and can be particularly damaging, because there is no guarantee that the policies involved in the oscillation are "good" policies, and there is often no way to verify how well they perform relative to the optimal.We note, however, that oscillations can be avoided and approximate PI can be shown to converge under special conditions, which arise in particular when an aggregation approach is used; see the approximate PI survey [Ber11b].We refer to the literature for further discussion of the preceding issues, as well as a variety of other approximate PI methods.There are also "optimistic" approximate PI methods with Q-factor approximation, and/or a few samples in between policy updates.Because of the use of Q-factors and the limited number of samples between policy updates, these schemes have the potential of on-line play implementation, but a number of difficulties must be overcome in this case, as we will explain later in this section.As an example, let us consider an extreme version of Q-factor parametric approximation that uses a single sample between policy updates.At the start of iteration k, we have the current parameter vector r k , we are at some state i k , and we have chosen a control u k .Then:(1) We simulate the next transition (i k , i k+1 ) using the transition probabilities p i k j (u k ).(2) We generate the control u k+1 with the minimization[In some schemes, to enhance exploration, u k+1 is chosen with a small probability to be a random element of U (i k+1 ) or one that is " -greedy," i.e., attains within some the minimum above.This is commonly referred to as the use of an off-policy.](3) We update the parameter vector via) where γ k is a positive stepsize, and ∇(•) denotes gradient with respect to r evaluated at the current parameter vector r k .To get a sense for the rationale of this iteration, note that if Q is a linear featurebased architecture, Q(i, u, r) = φ(i, u) r, then ∇ Q(i k , u k , r k ) is just the feature vector φ(i k , u k ), and iteration (3.27) becomesThus r k is changed in an incremental gradient direction: the one opposite to the gradient (with respect to r) of the incremental errorThe process is now repeated with r k+1 , i k+1 , and u k+1 replacing r k , i k , and u k , respectively.Extreme optimistic schemes of the type just described have received a lot of attention, in part because they admit a model-free implementation [i.e., the use of a computer simulator, which provides for each pair (i k , u k ), the next state i k+1 and corresponding cost g(i k , u k , i k+1 ) that are needed in Eq. (3.27)].They are often referred to as SARSA (State-Action-Reward-State-Action); see e.g., the books [BeT96], [BBD10], [SuB18].When Qfactor approximation is used, their behavior is very complex, their theoretical convergence properties are unclear, and there are no associated performance bounds in the literature.In practice, SARSA is more commonly used in a less extreme/optimistic form, whereby several (perhaps many) state-control-transition cost-next state samples are batched together and suitably averaged before updating the vector r k .Other variants of the method attempt to save in sampling effort by storing the generated samples in a buffer and reusing them in some randomized fashion in subsequent iterations (cf.our earlier discussion of exploration.This is also called sometimes experience replay, an idea that has been used since the early days of RL, both to save in sampling effort and to enhance exploration.The DQN (Deep Q Network) scheme, championed by DeepMind (see Mnih et al. [MKS15]), is based on this idea (the term "Deep" is a reference to DeepMind's affinity for deep neural networks, but experience replay does not depend on the use of a deep neural network architecture).Algorithms that approximate Q-factors, including SARSA and DQN, are fundamentally off-line training algorithms, primarily because their training process is long and requires the collection of many samples before reaching a stage that resembles parameter convergence.It can therefore be unreliable to use the interim approximate Q-factors for on-line decision making, particularly in an adaptive context that involves changing system parameters, thereby requiring on-line replanning.On the other hand, compared to the approximate PI method of Section 3.3.3,SARSA and DQN are far better suited for on-line implementation, because the control generation process of Eq. (3.26) can also be used to select controls on-line, thereby facilitating the combination of training and on-line control selection.To this end, it is important, among others, to make sure that the parameters r k stay at "safe" levels during the on-line control process, which can be a challenge.Still, even if this difficulty can be overcome in the context of a given problem, there are a number of other difficulties that SARSA and DQN can encounter during on-line play.(a) On-line exploration issues: The need to occasionally select controls using an off-policy in order to enhance exploration.Finding an offline policy that adequately deals with exploration in a given practical context can be a challenge.Moreover, an additional concern is that the off-policy controls may improve exploration, but may be of poor quality, and in some contexts, may induce instability.p i k+1 j (u) g(i k+1 , u, j)which is patterned after Eq. (3.26), is better in this regard, but is computationally more costly, and thus less suitable for on-line implementation.Generally speaking, the combination of off-line training and on-line play with the use of SARSA and DQN involves serious challenges.However, in some specific contexts encouraging results have been obtained, often by "manual tuning" to the problem at hand.Moreover, the methods have received a lot of attention, thanks in part to the availability of publicly available software, which also allow for a model-free implementation.In this section, we consider partial observation Markovian decision problems (POMDP) with a finite number of states and controls, and discounted additive cost over an infinite horizon.As discussed in Section 1.6.4,the optimal solution is typically intractable, so approximate DP/RL approaches must be used.In this section we focus on PI methods that are based on rollout, and approximations in policy and value space.They update a policy by using truncated rollout with that policy and a terminal cost function approximation.We focus on cost function approximation schemes, but Q-factor approximation is also possible.Because of its simulation-based rollout character, the methodology of this section depends critically on the finiteness of the control space.It can be extended to POMDP with infinite state space but finite control space, although we will not consider this possibility in this section.In particular, we assume that there are n states denoted by i ∈ {1, . . ., n} and a finite set of controls U at each state.We denote by p ij (u) and g(i, u, j) the transition probabilities and corresponding transition costs, from i to j under u ∈ U .The cost is accumulated over an infinite horizon and is discounted by α ∈ (0, 1).At each new generated state j, an observation z from a finite set Z is obtained with known probability p(z | j, u) that depends on j and the control u that was applied prior to the generation of j.The objective is to select each control optimally as a function of the prior history of observations and controls.A classical approach to this problem is to convert it to a perfect state information problem whose state is the current belief b = (b 1 , . . ., b n ), where b i is the conditional distribution of the state i given the prior history.As noted in Section 1.6.4,b is a sufficient statistic, which can serve as a substitute for the set of available observations, in the sense that optimal control can be achieved with knowledge of just b.In this section, we consider a more general form of sufficient statistic, which we call the feature state and we denote by y.We require that the feature state y subsumes the belief state b.By this we mean that b can be computed exactly knowing y.One possibility is for y to be the union of b and some identifiable characteristics of the belief state, or some compact representation of the measurement history up to the current time (such as a number of most recent measurements, or the state of a finitestate controller).We also make the additional assumption that y can be sequentially generated using a known feature estimator F (y, u, z).By this we mean that given that the current feature state is y, control u is applied, and observation z is obtained, the next feature can be exactly predicted as F (y, u, z).Clearly, since b is a sufficient statistic, the same is true for y.Thus the optimal costs achievable by the policies that depend on y and on b are the same.However, specific suboptimal schemes may become more effective with the use of the feature state y instead of just the belief state b.The optimal cost J * (y), as a function of the sufficient statistic/feature state y, is the unique solution of the corresponding Bellman equationHere we use the following notation: b y is the belief state that corresponds to feature state y, with components denoted by b y,i , i = 1, . . ., n.p(z | b y , u) is the conditional probability that the next observation will be z given the current belief state b y and control u F is the feature state estimator.In particular, F (y, u, z) is the next feature vector, when the current feature state is y, control u is applied, and observation z is obtained.The feature space reformulation of the problem can serve as the basis for approximation in value space, whereby J * is replaced in Bellman's equation by some function J after one-step or multistep lookahead.For example a one-step lookahead scheme yields the suboptimal policy μ given by μ(y) ∈ arg minIn -step lookahead schemes, J is used as terminal cost function in an -step horizon version of the original infinite horizon problem.In the standard form of a rollout algorithm, J is the cost function of some base policy.We will next discuss a rollout scheme with -step lookahead, which involves rollout truncation and terminal cost approximation.In the pure form of the rollout algorithm, the cost function approximation J is the cost function J µ of a known base policy µ, and its value J(y) = J µ (y) at any y is obtained by first extracting b from y, and then running a simulator starting from b, and using the system model, the feature generator, and µ.In the truncated form of rollout, J(y) is obtained by running the simulator of µ for a given number of steps m, and then adding a terminal cost approximation Ĵ(ȳ) for each terminal feature state ȳ that is obtained at the end of the m steps of the simulation with µ (see Fig. 3Thus the rollout policy is defined by the base policy µ, the terminal cost function approximation Ĵ, the number of steps m after which the simulated trajectory with µ is truncated, as well as the lookahead size .The choices of m and are typically made by trial and error, based on computational tractability among other considerations, while Ĵ may be chosen on the basis of problem-dependent insight or through the use of some off-line approximation method.In variants of the method, the multistep lookahead may be implemented approximately using a Monte Carlo tree search or adaptive sampling scheme.Using m-step rollout between the -step lookahead and the terminal cost approximation gives the method the character of a single PI.We will use repeated truncated rollout as the basis for constructing a PI algorithm, which we will discuss next.The rollout algorithm uses multistep lookahead and on-line simulation of the base policy to generate the rollout control at any feature state of interest.To avoid the cost of on-line simulation, we can approximate the rollout policy off-line by using some approximation architecture, which may involve a neural network.This is policy approximation built on top of the rollout scheme.To this end, we may introduce a parametric family/architecture of policies of the form μ(y, r), where r is a parameter vector.We then construct a training set that consists of a large number of sample feature state-control pairs (y s , u s ), s = 1, . . ., q, such that for each s, u s is the rollout control at feature state y s .We use this data set to obtain a parameter r by solving a corresponding classification problem, which associates each feature state y with a control μ(y, r).The parameter r defines a classifier, which given a feature state y, classifies y as requiring control μ(y, r) (see Section 3.4).We can also apply the rollout policy approximation to the context of PI.The idea is to view rollout as a single policy improvement, and to view the PI algorithm as a perpetual rollout process, which performs multiple policy improvements, using at each iteration the current policy as the base policy, and the next policy as the corresponding rollout policy.In particular, we consider a PI algorithm where at the typical iteration we have a policy µ, which we use as the base policy to generate many feature state-control sample pairs (y s , u s ), s = 1, . . ., q, where u s is the rollout control corresponding to feature state y s .We then obtain an "improved" policy μ(y, r) with an approximation architecture and a classification algorithm, as described above.The "improved" policy is then used as a base policy to generate samples of the corresponding rollout policy, which is approximated in policy space, etc.To use truncated rollout in this PI scheme, we must also provide a terminal cost approximation, which may take a variety of forms.Using zero is a simple possibility, which may work well if either the size of multistep lookahead or the length m of the rollout is relatively large.Another possibility is to use as terminal cost in the truncated rollout an approximation of the cost function of some base policy, which may be obtained with a neural network-based approximation architecture.In particular, at any policy iteration with a given base policy, once the rollout data is collected, one or two neural networks are constructed: A policy network that approximates the rollout policy, and (in the case of rollout with truncation) a value network that constructs a cost function approximation for that rollout policy.Thus, we may consider two types of methods:(a) Approximate rollout and PI with truncation, where each generated policy as well as its cost function are approximated by a policy and a value network, respectively.The cost function approximation of the current policy is used to truncate the rollout trajectories that are used to train the next policy.(b) Approximate rollout and PI without truncation, where each gener-ated policy is approximated using a policy network, but the rollout trajectories are continued up to a large maximum number of stages (enough to make the cost of the remaining stages insignificant due to discounting) or upon reaching a termination state.The advantage of this scheme is that only a policy network is needed; a value network is unnecessary since there is no rollout truncation with cost function approximation at the end.Note that as in all approximate PI schemes, the sampling of feature states used for training is subject to exploration concerns.In particular, for each policy approximation, it is important to include in the sample set {y s | s = 1, . . ., q}, a subset of feature states that are "favored" by the rollout trajectories; e.g., start from some initial subset of feature states y s and selectively add to this subset feature states that are encountered along the rollout trajectories.This is a challenging issue, which must be approached with care.An extensive case study of the methodology of this section was given in the paper by Bhattacharya et al. [BBW20], for the case of a pipeline repair problem.The implementation used there also includes the use of a partitioned state space architecture and an asynchronous distributed algorithm for off-line training; see Section 3.4.2.Let us now focus on an important alternative to computing Q-factor approximations.It is motivated by the potential benefit of approximating Q-factor differences rather than Q-factors.In this method, called advantage updating, instead of computing and comparing Q * k (x k , u k ) for all u k ∈ U k (x k ), we computeThe function A k (x k , u k ) can serve to compare controls, i.e., at state x k select μk (x k ) ∈ arg minand this can also be done when A k (x k , u k ) is approximated with a value network.Note that in the absence of approximations, selecting controls by advantage updating is clearly equivalent to selecting controls by comparing their Q-factors.By contrast, when approximation is involved, comparing advantages instead of Q-factors can be important, because the former may have a much smaller range of values than the latter.In particular, Q * k may embody sizable quantities that depend on x k but are independent of u k , and which may interfere with algorithms such as the fitted value iteration (3.19)-(3.20).Thus, when training an architecture to approximate Q * k , the training algorithm may naturally try to capture the large scale behavior of Q * k , which may be irrelevant because it may not be reflected in the Q-factor differences A k .However, with advantage updating, we may instead focus the training process on finer scale variations of Q * k , which may be all that matters.Here is an example (first given in the book [BeT96]) of what can happen when trained approximations of Q-factors are used.Consider the deterministic scalar linear systemand the quadratic cost per stagewhere δ is a very small positive constant [think of δ-discretization of a continuoustime problem involving the differential equation dx(t)/dt = u(t)].Let us focus on the stationary policy π, which applies at state x the controland view it as the base policy of a rollout algorithm.The Q-factors of π over an infinite number of stages can be calculated to be(We omit the details of this calculation, which is based on the classical analysis of linear-quadratic optimal control problems; see e.g., Section 1.5, or [Ber17a], Section 3.1.)Thus the important part of Qπ(x, u) for the purpose of rollout policy computation isHowever, when a value network is trained to approximate Qπ(x, u), the approximation will be dominated by 5x 2 4 , and the important part (3.28) will be "lost" when δ is very small.By contract, the advantage function can be calculated to beand when approximated with a value network, the approximation will be essentially unaffected by δ. ) u u u u 1 = 0 = 0 u 2 u q q u q−1 h(u) (a) (b) Category Figure 3.3.2Illustration of the idea of subtracting a baseline constant from a cost or Q-factor approximation.Here we have samples h(u 1 ), . . ., h(u q ) of a scalar function h(u) at sample points u 1 , . . ., u q , and we want to approximate h(u) with a linear function h(u, r) = ru, where r is a scalar tunable weight.We subtract a baseline constant b from the samples, and we solve the problem r ∈ arg minThe idea of advantage updating is also related to the useful technique of subtracting a suitable constant (often called a baseline) from a quantity that is estimated; see Fig. 3.3.2(in the case of advantage updating, the baselines depend on x k , but the same general idea applies).This idea can also be used in the context of the fitted value iteration method given earlier, as well as in conjunction with other simulation-based methods in RL.Example 3.1.1also points to the connection between the ideas underlying advantage updating and the rollout methods for small stage costs relative to the cost function approximation, which we discussed in Section 2.6.In both cases it is necessary to avoid including terms of disproportionate size in the target function that is being approximated.The remedy in both cases is to subtract from the target function a suitable state-dependent baseline.Let us now consider ways to approximate Q-factor differences (cf.our advantage updating discussion of the preceding section) by approximating cost function differences first.We recall here that given a base policy π = {µ 0 , . . ., µ N −1 }, the off-line computation of an approximate rollout policy π = {μ 0 , . . ., μN−1 } consists of two steps:(1) In a preliminary phase, we compute approximations Jk to the cost functions J k,π of the base policy π, possibly using simulation and a least squares fit from a parametrized class of functions.(2) Given Jk and a state x k at time k, we compute the approximate Q-factorfor all u ∈ U k (x k ), and we obtain the (approximate) rollout control μk (x k ) from the minimization μk (x k ) ∈ arg minUnfortunately, this method also suffers from the error magnification inherent in the Q-factor differencing operation.This motivates an alternative approach, called differential training, which is based on cost-to-go difference approximations.To this end, we note that to compute the rollout control μk (x k ), it is sufficient to have the differences of costs-to-gowhere µ k (x k ) is the control applied by the base policy at x k .We thus consider a function approximation approach, whereby given any two states x k+1 and xk+1 , we obtain an approximation Gk+1 (x k+1 , xk+1 ) of the cost difference (3.29).We then compute the rollout control by μk (x k ) ∈ arg min) where µ k (x k ) is the control applied by the base policy at x k .Note that the minimization (3.30) aims to simply subtract the approximate Q-factor of the base policy control µ k (x k ) from the approximate Q-factor of every other control u ∈ U k (x k ).An important point here is that the training of an approximation architecture to obtain Gk+1 can be done using any of the standard training methods, and a "differential" system, whose "states" are pairs (x k , xk ) and will be described shortly.To see this, let us denote for all k and pair of states (x k , xk )the cost function differences corresponding to the base policy π.We consider the DP equations corresponding to π, and to x k and xk :and we subtract these equations to obtainfor all (x k , xk ) and k.Therefore, G k can be viewed as the cost-to-go function for a problem involving a fixed policy (the base policy), the state (x k , xk ), the cost per stageand the system equationThus, it can be seen that any of the standard methods that can be used to train architectures that approximate J k,π , can also be used for training architectures that approximate G k .For example, one may use simulationbased methods that generate pairs of trajectories starting at the pair of initial states (x k , xk ), and generated according to Eq. (3.32) by using the base policy π.Note that a single random sequence {w 0 , . . ., w N −1 } may be used to simultaneously generate samples of G k (x k , xk ) for several triples (x k , xk , k), and in fact this may have a substantial beneficial effect.A special case of interest arises when a linear, feature-based architecture is used for the approximator Gk .In particular, let φ k be a feature extraction mapping that associates a feature vector φ k (x k ) with state x k and time k, and let Gk be of the formwhere r k is a tunable weight vector of the same dimension as φ k (x k ) and prime denotes transposition.The rollout policy is generated by μk (x k ) ∈ arg minwhich corresponds to using r k+1 φ k+1 (x k+1 ) (plus an unknown inconsequential constant) as an approximation to J k+1,π (x k+1 ).Thus, in this approach, we essentially use a linear feature-based architecture to approximate the cost functions J k,π of the base policy, but we train this architecture using the differential system (3.32) and the differential cost per stage of Eq. (3.31).This is done by selecting pairs of initial states, running in parallel the corresponding trajectories using the base policy, and subtracting the resulting trajectory costs from each other.We have focused so far on approximation in value space using parametric architectures.In this section we will discuss briefly how the cost function approximation methods of this chapter can be suitably adapted for the purpose of approximation in policy space, whereby we select the policy by using optimization over a parametric family of some form.In particular, suppose that for a given stage k, we have access to a dataset of sample state-control pairs (x s k , u s k ), s = 1, . . ., q, obtained through some unspecified process, such as rollout or problem approximation.We may then wish to "learn" this process by training the parameter vector r k of a parametric family of policies μk (x k , r k ), using least squares minimization/regression:cf. our discussion of approximation in policy space in Section 1.3.3.As we have noted in Section 3.1, in the case of a continuous control space, training of a parametric architecture for policy approximation is similar to training for a cost approximation.In the case where the control space is finite, however, it is useful to make the connection of approximation in policy space with classification; cf.Fig. 3.1.2and the discussion of Section 3.1.Classification is an important subject in machine learning.The objective is to construct an algorithm, called a classifier , which assigns a given "object" to one of a finite number of "categories" based on its "characteristics."Here we use the term "object" generically.In some cases, the classification may relate to persons or situations.In other cases, an object may represent a hypothesis, and the problem is to decide which of the hypotheses is true, based on some data.In the context of approximation in policy space, objects correspond to states, and categories correspond to controls to be applied at the different states.Thus in this case, we view each sample x s k , u s k as an object-category pair.Generally, in (multiclass) classification we assume that we have a population of objects, each belonging to one of m categories c = 1, . . ., m.We want to be able to assign a category to any object that is presented to us.Mathematically, we represent an object with a vector x (e.g., some raw description or a vector of features of the object), and we aim to construct a rule that assigns to every possible object x a unique category c.To illustrate a popular classification method, let us assume that if we draw an object x at random from this population, the conditional probability of the object being of category c is p(c | x).If we know the probabilities p(c | x), we can use a classical statistical approach, whereby we assign x to the category c * (x) that has maximal posterior probability, i.e.,More specifically, let us assume that we have a training set consisting of q object-category pairs (x s , c s ), s = 1, . . ., q, and corresponding vectorsand adopt a parametric approach.In particular, for each category c = 1, . . ., m, we approximate the probability p(c | x) with a function h(c, x, r) that is parametrized by a vector r, and optimize over r the empirical approximation to the expected squared error of Eq. (3.36).Thus we can obtain r by the least squares regression:perhaps with some quadratic regularization added.The functions h(c, x, r) may be provided for example by a feature-based architecture or a neural network.Note that each training pair (x s , c s ) is used to generate m examples for use in the regression problem (3.37): m − 1 "negative" examples of the form (x s , 0), corresponding to the m − 1 categories c = c s , and one "positive" example of the form (x s , 1), corresponding to c = c s .Note also that the incremental gradient method can be applied to the solution of this problem.The regression problem (3.37) approximates the minimization of the expected value (3.36), so we conclude that its solution h(c, x, r), c = 1, . . ., m, approximates the probabilities p(c | x).Once this solution is obtained, we may use it to classify a new object x according to the rule Estimated Object Category = c(x, r) ∈ arg max The classifier, defined by the parameter r k , is constructed by using the training set (x s k , u s k ), s = 1, . . ., q.It yields a randomized policy that consists of the probability h(u, x k , r k ) of using control u ∈ U k (x k ) at state x k .This policy is approximated by the deterministic policy μk (x k , r k ) that uses at state x k the control that maximizes over u ∈ U k (x k ) the probability h(u, x k , r k ) [cf.Eq. (3.38)].Returning to approximation in policy space, for a given training set (x s k , u s k ), s = 1, . . ., q, the classifier just described provides (approximations to) the "probabilities" of using the controls u k ∈ U k (x k ) at the states x k , so it yields a "randomized" policy h(u, x k , r k ) for stage k [once the values h(u, x k , r k ) are normalized so that, for any given x k , they add to 1]; cf.Fig. 3.4.2.In practice, this policy is usually approximated by the deterministic policy μk (x k , r k ) that uses at state x k the control of maximal probability at that state; cf.Eq. (3.38).For the simpler case of a classification problem with just two categories, say A and B, a similar formulation is to hypothesize a relation of the following form between object x and its category:where h is a given function and r is the unknown parameter vector.Given a set of q object-category pairs (x 1 , z 1 ), . . ., (x q , z q ) wherewe obtain r by the least squares regression: r ∈ arg min r q s=1 z s − h(x s , r)2 .The optimal parameter vector r is used to classify a new object with data vector x according to the ruleIn the context of DP and approximation in policy space, this classifier may be used, among others, in stopping problems where there are just two controls available at each state: stopping (i.e., moving to a termination state) and continuing (i.e., moving to some nontermination state).There are several variations of the preceding classification schemes, for which we refer to the specialized literature.Moreover, there are several commercially and publicly available software packages for solving the associated regression problems and their variants.They can be brought to bear on the problem of parametric approximation in policy space using any training set of state-control pairs, regardless of how it was obtained.We noted earlier that contrary to rollout, approximate policy iteration (PI) is fundamentally an off-line training algorithm, because for a large scale problem, it is necessary to represent the cost functions or Q-factors of the successively generated policies with an approximation architecture; cf.Section 3. Thus, in a typical implementation, approximate PI involves the successive use of value networks to represent the cost functions of the generated policies, and one-step or multistep lookahead minimization to implement policy improvement.On the other hand, it is also possible to use policy networks to approximate the results of policy improvement.In particular, we can start with a  base policy and a terminal cost approximation, and generate state-control samples of the corresponding truncated rollout policy.These samples can be used with an approximation in policy space scheme to train a policy network that approximates the truncated rollout policy.Then the cost function of the policy network can be approximated with a value network using the cost approximation methodology that we have discussed in this chapter.This value network can be used in turn as a terminal cost approximation in a truncated rollout algorithm where the previously obtained policy network can be used as a base policy.A new policy network can then be trained using samples of this rollout policy, etc.Thus a perpetual rollout scheme is obtained, which involves a sequence of value and policy networks.One may also consider approximate PI algorithms that do not use a value network at all.Indeed the value network is only used to provide the approximate cost function values of the current policy, which are needed to calculate samples of the improved policy and train the corresponding policy network.On the other hand the samples of the improved policy can also be computed by rollout, using simulation-generated cost function values of the current policy.If the rollout can be suitably implemented with simulation, the training of a value network may be unnecessary.We have noted earlier that parallelization and distributed computation can be used in several different ways in rollout and PI schemes, including Qfactor, Monte Carlo, and multiagent parallelization.State-control training pairs for the corresponding rollout policy are obtained by starting at an initial state within some subset of the partition, generating rollout trajectories using the local policy network, which are truncated once the state enters a different subset of the partition, with the corresponding terminal cost function approximation supplied by the value network of that subset.When a separate processor is used for each subset of partition, the corresponding value networks are communicated between processors.This can be done asynchronously, with each processor sharing its value network as it becomes available.In a variation of this scheme, the local policy networks may also be shared selectively among processors for selective use in the truncated rollout process.consider the use of multiple neural networks in the implementation of rollout or approximate PI.For example, when feature-based partitioning of the state space is used (cf.Example 3.1.8),we may consider a multiprocessor parallelization scheme, which involves multiple local value and/or policy networks, which operate locally within a subset of the state space partition; see Figs.Let us finally note that multiprocessor parallelization leads to the idea of an approximation architecture that involves a graph.Each node of the graph consists of a neural network and each arc connecting a pair of nodes corresponds to data transfer between the corresponding neural networks.The question of how to train such an architecture is quite complex and one may think of several alternative possibilities.For example the training may be collaborative with the exchange of training results and/or training data communicated periodically or asynchronously; see the book [Ber20a], Section 5.8.This is a sensible and common question, which stems from the mindset that neural networks have extraordinary function approximation properties.In other words, why go through the arduous and time-consuming process of on-line lookahead minimization, if we can do the same thing off-line and represent the lookahead policy with a trained policy network?In particular, we can select the policy from a suitably restricted class of policies, such as a parametric class of the form µ(x, r), where r is a parameter vector.We may then estimate r using some type of off-line training.Then the on-line computation of controls µ(x, r) can be much faster compared with on-line lookahead minimization.On the negative side, because parametrized approximations often involve substantial calculations, they are not well suited for on-line replanning.From our point of view in this book, there is another important reason why approximation in value space is needed on top of approximation in policy space: the off-line trained policy may not perform nearly as well as the corresponding one-step or multistep lookahead/rollout policy, because it lacks the extra power of the associated exact Newton step (cf.our discussion of AlphaZero and TD-Gammon in Section 1.1, and linear quadratic problems in Section 1.5).Figure 3.4.5 illustrates this fact with a one-dimensional linear-quadratic example, and compares the performance of a linear policy with its corresponding one-step lookahead policy.In this example the system equationand the quadratic cost function parameters are q = 1, r = 0.5.The optimal policy for this system and cost parameter values iswith L * ≈ −0.4, and the optimal cost function iswhere K * ≈ 1.1.We want to to explore what happens when we use a policy of the formwhere L = L * (e.g., a policy that is optimal for another system equation or cost function parameters).The cost function of µ L has the formwhere K L is obtained by using the formulas given in Section 1.5.The figure shows the quadratic cost coefficient differences K L − K * and K L − K * as a function of L, where K L and K L are the quadratic cost coefficients of µ (without one-step lookahead/Newton step) and the corresponding one-step lookahead policy μ (with one-step lookahead/Newton step).In this section we focus on infinite horizon problems and we discuss an alternative training approach for approximation in policy space, which is based on controller parameter optimization: we parametrize the policies by a vector r, and we optimize the corresponding expected cost over r.In particular, we determine r through the minimizationwhere J μ(r) (i 0 ) is the cost of the policy μ(r) starting from the initial state i 0 , and the expected value above is taken with respect to a suitable probability distribution of the initial state i 0 (cf.Fig. 3.5.1).Note that in the case where the initial state i 0 is known and fixed, the method involves just minimization of J μ(r) (i 0 ) over r.This simplifies a great deal the minimization, particularly when the problem is deterministic.Before delving into the details, it is worth reminding the reader that using an off-line trained policy without combining it with approximation in value space has the fundamental shortcoming, which we noted frequently in this book: the off-line trained policy will not perform nearly as well as a scheme that uses this policy in conjunction with lookahead minimization, because it lacks the extra power of the associated exact Newton step.where the expected value above is taken with respect to a suitable probability distribution of i 0 .Let us first consider methods that perform the minimization (3.39) by using a gradient method, and for simplicity let us assume that the initial condition i 0 is known.Thus the aim is to minimize J μ(r) (i 0 ) over r by using the gradient methodassuming that J μ(r) (i 0 ) is differentiable with respect to r.Here γ k is a positive stepsize parameter, and ∇(•) denotes gradient with respect to r evaluated at the current iterate r k .The difficulty with this method is that the gradients ∇J μ(r k ) (i 0 ) may not be explicitly available.In this case, the gradients can be approximated by finite differences of cost function values J μ(r k ) (i 0 ).Unfortunately, when the problem is stochastic, the cost function values may be computable only through Monte Carlo simulation.This may introduce a large amount of noise, so it is likely that many samples will need to be averaged in order to obtain sufficiently accurate gradients, thereby making the method inefficient.On the other hand, when the problem is deterministic, this difficulty does not appear, and the use of the gradient method (3.40) or other methods that do not rely on the use of gradients (such as coordinate descent) is facilitated.In this section we will focus on alternative and typically more efficient gradient-like methods for stochastic problems, which are based on sampling.Some popular methods of this type are based on incremental gradient ideas (cf.Section 3.1.3)and the use of randomized policies [i.e., policies that map a state i to a probability distribution over the set of controls U (i), rather than mapping onto a single control].† We discuss these gradient-like methods next.To get a sense of the general principle underlying the incremental gradient approach that uses randomization and sampling, let us digress from the DP context of this chapter, and consider the generic optimization problem min z∈Z F (z), (3.41)where Z is a subset of the m-dimensional space m , and F is some realvalued function over m .We will take the unusual step of converting this problem to the stochastic optimization problem minwhere z is viewed as a random variable, P Z is the set of probability distributions over Z, p denotes the generic distribution in P Z , and E p {•} denotes expected value with respect to p.Of course this enlarges the search space from Z to P Z , but it enhances the use of randomization schemes and simulation-based methods, even if the original problem is deterministic.Moreover, the stochastic optimization problem (3.42) may have some nice differentiability properties that are lacking in the original deterministic version (3.41); see the paper [Ber73] for an analysis of this differentiability issue under convexity assumptions on F .At this point it is not clear how the stochastic optimization problem (3.42) relates to our stochastic DP context of this chapter.We will return to this question later, but for the purpose of orientation, we note that to obtain a problem of the form (3.42), we must enlarge the set of policies to include randomized policies, mapping a state i into a probability distribution over the set of controls U (i). † The AlphaGo and AlphaZero programs (Silver et al. [SHM16], [SHS17]) also use randomized policies, and a policy adjustment scheme that involves incremental changes along "directions of improvement."However, these changes are implemented through the MCTS algorithm used by these programs, without the explicit use of a gradient (see the discussion in Section 2.4.2).Thus it may be said that the AlphaGo and AlphaZero programs involve a form of approximation in policy space (as well as approximation in value space), which bears resemblance but cannot be classified as a policy gradient method.Suppose now that we restrict attention to a subset PZ ⊂ P Z of probability distributions p(z; r) that are parametrized by some continuous parameter r, e.g., a vector in some Euclidean space.† In other words, we approximate the stochastic optimization problem (3.42) with the restricted problem min r E p(z;r) F (z) .Then we may use a gradient method for solving this problem, such aswhere ∇(•) denotes gradient with respect to r of the function in parentheses, evaluated at the current iterate r k .We will first consider an incremental version of the gradient method (3.43).This method requires that p(z; r) is differentiable with respect to r.It relies on a convenient gradient formula, sometimes referred to as the log-likelihood trick , which involves the natural logarithm of the sampling distribution.This formula is obtained by the following calculation, which is based on interchanging gradient and expected value, and using the gradient formula ∇(log p) = ∇p/p.We havewhere for any given z, ∇ log p(z; r) is the gradient with respect to r of the function log p(z; •) , evaluated at r (the gradient is assumed to exist).† To be on safe mathematical ground, we assume that p(z; r) is a discrete distribution in what follows in this section.The preceding formula suggests an incremental implementation of the gradient iteration (3.43) that approximates the expected value in the right side in Eq. (3.44) with a single sample (cf.Section 3.1.3).The typical iteration of this method is as follows.Sample-Based Gradient Method for Parametric Approximation of min z∈Z F (z) Let r k be the current parameter vector.(c) Iterate according to(3.45)The advantage of the preceding sample-based method is its simplicity and generality.It allows the use of parametric approximation for any minimization problem (well beyond DP), as long as the logarithm of the sampling distribution p(z; r) can be conveniently differentiated with respect to r, and samples of z can be obtained using the distribution p(z; r).Note that in iteration (3.45) r is adjusted along a random direction.This direction does not involve at all the gradient of F , only the gradient of the logarithm of the sampling distribution!As a result the iteration has a model-free character : we don't need to know the form of the function F as long as we have a simulator that produces the cost function value F (z) for any given z.This is also a major advantage offered by many random search methods.An important issue is the efficient computation of the sampled gradient ∇ log p(z k ; r k ) .In the context of DP, including the SSP and discounted problems that we have been dealing with, there are some specialized procedures and corresponding parametrizations to approximate this gradient conveniently.The following is an example.We consider a parametrization of randomized policies with parameter r, so the control at state i is generated according to a distribution p(u | i; r) over U (i).Then for a given r, the state-control trajectory z is a random vector with probability distribution denoted p(z; r).The cost corresponding to the trajectory z isand the problem is to minimize over r E p(z;r) F (z) .To apply the sample-based gradient method (3.45), given the current iterate r k , we must generate the sample state-control trajectory z k , according to the distribution p(z; r k ), compute the corresponding cost F (z k ), and also calculate the gradient ∇ log p(z k ; r k ) .(3.47)Let us assume that the logarithm of the randomized policy distribution p(u | i; r) is differentiable with respect to r (a soft-min policy parametrization is often recommended for this purpose).Then the logarithm that is differentiated in Eq. (3.47) can be written asand its gradient (3.47), which is needed in the iteration (3.45), is given byThis gradient involves the current randomized policy, but does not involve the transition probabilities and the costs per stage.The policy gradient method (3.45) can now be implemented with a finite horizon approximation whereby r k is changed after a finite number N of time steps [so the infinite cost and gradient sums (3.46) and (3.48) are replaced by finite sums].The method takes the formwhere z k N = (i0, u0, . . ., iN−1, uN−1) is the generated N -step trajectory, and FN (z k N ) is the corresponding cost.The initial state i0 of the trajectory is chosen randomly, with due regard to exploration issues.Policy gradient methods for other types of DP problems can be similarly developed, as well as variations involving a combination of policy and cost function approximations [e.g., replacing F (z) of Eq. (3.46) by a parametrized estimate that has smaller variance, and possibly subtracting a suitable baseline, cf. the following discussion].This leads to a class of actor-critic methods that differ from the PI-type methods that we discussed earlier in this chapter; we refer to the end-of-chapter literature for a variety of specific schemes.There are several issues to consider in the implementation of the samplebased gradient method (3.45).The first of these is that the problem solved is a randomized version of the original.If the method produces a parameter r in the limit and the distribution p(z; r) is not atomic (i.e., it is not concentrated at a single point), then a solution z ∈ Z must be extracted from p(z; r).In the SSP and discounted problems of this chapter, the subset PZ of parametric distributions typically contains the atomic distributions, while it can be shown that minimization over the set of all distributions P Z produces the same optimal value as minimization over Z (the use of randomized policies does not improve the optimal cost of the problem), so this difficulty does not arise.Another issue is how to collect the samples z k .Different methods must strike a balance between convenient implementation and a reasonable guarantee that the search space Z is sufficiently well explored.Finally, there is the issue of improving sampling efficiency.To this end, let us note a simple generalization of the gradient method (3.45), which can often improve its performance.It is based on the gradient formulawhere b is any scalar.This formula generalizes Eq. (3.44), where b = 0, and holds in view of the following calculation, which shows that the term multiplying b in Eq. (3.49) is equal to 0:where the last equality holds because z∈Z p(z; r) is identically equal to 1 and hence does not depend on r.Based on the gradient formula (3.49), we can modify the iteration (3.45) to read as follows:where b is some fixed scalar, called the baseline.Whereas the choice of b does not affect the gradient ∇ E p(z;r) F (z) [cf.Eq. (3.49)], it affects the incremental gradientwhich is used in the iteration (3.50).Thus, by optimizing the baseline b, empirically or through a calculation (see e.g., [DNP11]), we can improve the performance of the algorithm.Moreover, in the context of discounted and SSP problems, state-dependent baseline functions have been used.Ideas of cost shaping are useful within this context; we refer to the RL textbook [Ber19a] and specialized literature for further discussion.We will now consider an alternative class of incremental versions of the policy gradient method (3.43), repeated here for convenience: Moreover, these methods do not require the derivative of the sampling distribution or its logarithm.For simplicity we first consider the case where z and r are scalars, and later discuss the multidimensional case.In particular, we assume that p(z; r) is symmetric and is concentrated with probabilities p i at the points r + i and r − i , where 1 , . . ., m are some small positive scalars.Thus we have The gradient iteration (3.51) becomesLet us now consider approximation of the gradient by finite differences:We approximate the gradient iteration (3.52) bywhere i k is an index generated with probabilities that are proportional to p i k .This algorithm uses one out of the m terms of the gradient in Eq. (3.53).The extension to the case, where z and r are multidimensional, is straightforward.Here p(z; r) is a probability distribution, whereby z takes values of the form r + d, where d is a random vector that lies on the surface of the unit sphere, and (independently of d) takes scalar values according to a distribution that is symmetric around 0. The idea is that at r k , we first choose randomly a direction d k on the surface of the unit sphere, and then change r k along d k or along −d k , depending on the sign of the corresponding directional derivative.For a finite difference approximation of this iteration, we sample z k along the line {r k + d k | ∈ }, and similar to the iteration (3.54), we setwhere k is the sampled value of .Let us also discuss the case where p(z; r) is a discrete but nonsymmetric distribution, i.e., z takes values of the form r + d, where d is a random vector that lies on the surface of the unit sphere, and is a zero mean scalar.Then the analog of iteration (3.55) iswhere k is the sampled value of .Thus in this case, we still require two function values per iteration.Generally, for a symmetric sampling distribution, iteration (3.55) tends to be more accurate than iteration (3.56), and is often preferred.Algorithms of the form (3.55) and (3.56) are known as random direction methods.They use only two cost function values per iteration, and a direction d k that need not be related to the gradient of F in any way.There is some freedom in selecting d k , which could potentially be exploited in specific schemes.However, selecting the stepsize γ k and the sampling distribution for can be tricky, particularly when the values of F are noisy.The main drawback of the policy gradient methods that we have considered in this section is the risk of unreliability due to the stochastic uncertainty corrupting the calculation of the gradients, the slow convergence that is typical of gradient methods in many settings, and the presence of local minima.For this reason, methods based on random search have been considered as potentially more reliable alternatives.Viewed from a high level, random search methods are similar to policy gradient methods in that they aim at iterative cost improvement through sampling.However, they need not involve randomized policies, they are not subject to cost differentiability restrictions, and they offer some global convergence guarantees, so in principle they are not affected much by local minima.Let us consider a parametric policy optimization approach based on solving the problem min r E J μ(r) (i 0 ) , cf.Eq. (3.39).Random search methods for this problem explore the space of the parameter vector r in some randomized but intelligent fashion.There3 Schematic illustration of the cross-entropy method.At the current iterate r k , we construct an ellipsoid E k centered at r k .We generate a number of random samples within E k , and we "accept" a subset of the samples that have "low" cost.We then choose r k+1 to be the sample mean of the accepted samples, and construct a sample "covariance" matrix of the accepted samples.We then form the new ellipsoid E k+1 using this matrix and a suitably enlarged radius, and continue.Notice the resemblance with a policy gradient method: we move from r k to r k+1 in a direction of cost improvement.are several types of such methods for general optimization, and some of them have been suggested for approximate DP.We will briefly describe the cross-entropy method , which has gained considerable attention.The method, when adapted to the approximate DP context, bears resemblance to policy gradient methods, in that it generates a parameter sequence {r k } by changing r k to r k+1 along a direction of "improvement."This direction is obtained by using the policy μ(r k ) to generate randomly cost samples corresponding to a set of sample parameter values that are concentrated around r k .The current set of sample parameters are then screened: some are accepted and the rest are rejected, based on a cost improvement criterion.Then r k+1 is determined as a "central point" or as the "sample mean" in the set of accepted sample parameters, some more samples are generated randomly around r k+1 , and the process is repeated; see Fig. 3.5.3.Thus successive iterates r k are "central points" of successively better groups of samples, so in some broad sense, the random sample generation process is guided by cost improvement.This idea is shared with other popular classes of random search methods.The cross-entropy method is very simple to implement, does not suffer from the fragility of gradient-based optimization, does not involve ran-domized policies, and relies on some supportive theory.Importantly, the method does not require the calculation of gradients, and it does not require differentiability of the cost function.Moreover, it does not need a model to compute the required costs of different policies; a simulator is sufficient.Like all random search methods, the convergence rate guarantees of the cross-entropy method are limited, and its success depends on domainspecific insights and the skilled use of heuristics.However, the method relies on solid ideas and has gained a favorable reputation.In particular, it was used with impressive success in the context of the game of tetris; see Szita and Lorinz [SzL06], and Thiery and Scherrer [ThS09].There have also been reports of domain-specific successes with related random search methods; see Salimans et al. [SHC17].We refer to the end-of-chapter literature for details and examples of implementation.In this section we consider approximation in value space using a problem approximation approach that is based on aggregation.In particular, we construct a simpler and more tractable "aggregate" problem by creating special subsets of states, which we view as "aggregate states."We then solve the aggregate problem exactly by DP.This is the off-line training part of the aggregation approach, and it may be carried out with a variety of DP methods, including simulation-based value and policy iteration; we refer to the RL book [Ber19a] for a detailed account.Finally, we use the optimal cost-to-go function of the aggregate problem to construct a terminal cost approximation in a one-step or multistep lookahead approximation scheme for the original problem.Additionally, we may also use the optimal policy of the aggregate problem to construct a base policy for a truncated rollout scheme.In addition to problem approximation, aggregation is related to feature-based parametric approximation.In particular, it often produces a piecewise constant cost function approximation, which may be viewed as a linear feature-based parametrization, where the features are 0-1 membership functions; see Example 3.1.1.Aggregation can also be combined with other approximation schemes, to add a local correction to a cost function approximation J, which is already available, possibly through the use of a neural network; see the discussion of biased aggregation later in Section 3.6.7.Aggregation can be applied to both finite horizon and infinite horizon problems.In this section, we will focus primarily on the discounted infinite horizon problem.We will introduce aggregation in a simple intuitive form in Section 3.6.1,and generalize later to a more sophisticated form of featurebased aggregation, which we also discussed briefly in Example 3.1.7.States i (fine grid) (fine grid) Representative states xx (coarse grid) A relatively small number of states are viewed as representative.We define transition probabilities between pairs of aggregate states and we also define the associated expected transition costs.These specify a smaller DP problem, called the aggregate problem, which is solved exactly.The optimal cost function J * of the original problem is approximated by interpolation from the optimal costs of the representative states r * y in the aggregate problem:and is used in a one-step or multistep lookahead scheme.In this section we focus on a relatively simple form of aggregation, which involves a special subset of states, called representative.Our approach is to view these states as the states of a smaller optimal control problem, the aggregate problem, which we will formulate and solve exactly in place of the original.We will then use the optimal aggregate costs of the representative states to approximate the optimal costs of the original problem states by interpolation.In this chapter, whenever we consider a finite-state problem, we use notation that is more convenient for such a problem.In particular, states and successor states will be denoted by i and j, respectively, and the system equation is represented by control-dependent transition probabilities p ij (u); cf.Section 1.4.1.Let us describe a classical example.Example 3.6.1 (Coarse Grid Approximation)Consider a discounted problem where the state space is a grid of points i = 1, . . ., n on the plane.We introduce a coarser grid that consists of a subset A of the states/points, which we call representative and denote by x; see Fig. 3.6.1.We now wish to formulate a lower-dimensional DP problem just on the coarse grid of states.The difficulty here is that there may be positive transition probabilities pxj(u) from some representative states x to some nonrepresentative states j.To deal with this difficulty, we introduce artificial transition probabilities φjy from non-representative states j to representative states y, which we call aggregation probabilities.In particular, a transition from representative state x to a nonrepresentative state j, is followed by a transition from j to some other representative state y with probability φjy; see Fig. 3.6.2.This process involves approximation but constructs a transition mechanism for an aggregate problem whose states are just the representative ones.The transition probabilities between representative states x, y under control u ∈ U (x) and the corresponding expected transition costs areWe can solve the aggregate problem by any suitable exact DP method.Let A denote the set of representative states and let r *x denote the corresponding optimal cost of representative state x.We can then approximate the optimal cost function of the original problem with the interpolation formula J(j) = y∈A φjyr * y , j= 1, . . ., n.This function may in turn be used in a one-step or multistep lookahead scheme for approximation in value space of the original problem.Note that there is a lot of freedom in selecting the aggregation probabilities φjy.Intuitively, φjy should express a measure of proximity between j and y, e.g., φjy should be relatively large when y is geometrically close to j.For example, we could set φjy j = 1 for the representative state yj that is "closest" to j, and φjy j = 0 for all other representative states y = yj.In this case, Eq. (3.58) yields a piecewise constant cost function approximation J (the constant values are the scalars r * y of the representative states y).We will now formalize our framework for aggregation with representative states by generalizing the preceding example; see Fig. 3.6.3.We first consider the n-state version of the α-discounted problem of Section 1.4.1.We refer to this problem as the "original problem," to distinguish from the "aggregate problem," which we define next.We introduce a finite subset A of the original system states, which we call representative states, and we denote them by symbols such as x and y.We construct an aggregate problem, with state space A, and transition probabilities and transition costs defined as follows:(a) We relate the original system states j to representative states y ∈ A with aggregation probabilities φ jy ; these are scalar "weights" satisfying φ jy ≥ 0 for all y ∈ A, and y∈A φ jy = 1.x j 1 j 2 j 2 j 3x j 1x p xj1 (u) , and each of these states is associated with a convex combination of representative states using the aggregation probabilities.For example, the state j 1 is associated with  Aside from the selection of representative states, an important consideration is the choice of the aggregation probabilities.These probabilities express "similarity" or "proximity" of original to representative states (as in the case of the coarse grid Example 3.6.1),but in principle they can be arbitrary (as long as they are nonnegative and sum to 1 over y).Intuitively, φ jy may be interpreted as some measure of "strength of relation" of j to y.The vectors {φ jy | j = 1, . . ., n} may also be viewed as basis functions for a linear cost function approximation via Eq.(3.61).A special case of interest, called hard aggregation, is when for every state j, we have φ jy = 0 for all representative states y, except a single one, denoted y j , for which we have φ jy j = 1.In this case, the one-step lookahead . . . . . .The footprint sets can be used to define a bound for the error (J * − J).In particular, it can be shown thatis the maximum variation of J * within the footprint sets S y .This error bound result can be extended to the more general aggregation framework that will be given in the next section.Note the primary intuition derived from this bound: the error due to hard aggregation is small if J * varies little within each S y .For a special hard aggregation case of interest, consider the geometrical context of Example 3.6.1.There, aggregation probabilities are often based on a nearest neighbor approximation scheme, whereby each nonrepresentative state j takes the cost value of the "closest" representative state y, i.e., φ jy j = 1 if y j is the closest representative state to j.Then all states j for which a given representative state y is the closest to j (the footprint of y) are assigned equal approximate cost J(j) = r * y .The most straightforward way to solve the aggregate problem is to compute the aggregate problem transition probabilities pxy (u) [cf.Eq. (3.59)] and transition costs ĝ(x, u) [cf.Eq. (3.60)] by either an algebraic calculation or by simulation.The aggregate problem may then be solved by any one of the standard methods, such as VI or PI.This exact calculation is plausible if the number of representative states is relatively small.An alternative possibility is to use a simulation-based VI or PI method.We refer to a discussion of these methods in the author's books [Ber12], Section 6.5, and [Ber19a], Section 6.3.The idea is that a simulator for the original problem can be used to construct a simulator for the aggregate problem; cf.Fig.An important observation is that if the original problem is deterministic and hard aggregation is used, the aggregate problem is also deterministic, and can be solved by shortest-path like methods.This is true for both discounted problems and for undiscounted shortest path-type problems.In the latter case, the termination state of the original problem must be included as a representative state in the aggregate problem.However, if hard aggregation is not used, the aggregate problem will be stochastic, because of the introduction of the aggregation probabilities.Of course, once the aggregate problem is solved and the lookahead approximation J is obtained, a deterministic structure in the original problem can be exploited to facilitate the lookahead minimizations.Aggregation with representative states extends without difficulty to problems with a continuous state space, as long as the control space is finite.Then once the representative states and the aggregation probabilities have been defined, the corresponding aggregate problem is a discounted problem with finite state and control spaces, which can be solved with the standard methods.The only potential difficulty arises when the disturbance space is also infinite, in which case the calculation of the transition probabilities and expected stage costs of the aggregate problem must be obtained by some form of integration process.The case where both the state and the control spaces are continuous is somewhat more complicated, because both of these spaces must be discretized using representative state-control pairs, instead of just representative states.Suppose that we want to find the fastest route for a car to travel between two points A and B located at the opposite ends of a square with side 1000 meters, while avoiding some known obstacles.We assume a constant car speed of 1 meter per second and that the car can drive in any direction; cf.Fig. 3.6.5.Let us consider discretizing the space with a square grid (a set of representative states), and restrict the directions of motion to horizontal and vertical, so that at each stage the car moves from a grid point to one of the four closest grid points.Thus in the discretized version of the problem the car travels with a sequence of horizontal and vertical moves as indicated in the right side of Fig. 3.6.5.Is it possible to approximate the fastest route arbi-trarily closely with the optimal solution of the discretized problem, assuming a sufficiently fine grid?The answer is no!To see this note that in the discretized problem the optimal travel time is 2000 secs, regardless of how fine the discretization is.On the other hand, in the continuous space/nondiscretized problem the optimal travel time can be as little as √ 2 • 1000 secs (this corresponds to the favorable case where the straight line from A to B does not meet an obstacle).The difficulty in the preceding example is that the state space is discretized finely but the control space is not .What is needed is to introduce a fine discretization of the control space as well, through some set of "representative controls."We can deal with this situation with a suitable form of discretized aggregate problem, which when solved provides an appropriate form of cost function approximation for use with one-step lookahead.The discretized problem is a stochastic infinite horizon problem, even if the original problem is deterministic.Further discussion of this approach is outside our scope, and we refer to the sources cited at the end of the chapter.Under reasonable assumptions it is possible to show consistency, i.e., that the optimal cost function of the discretized problem converges to the optimal cost function of the original continuous spaces problem as the discretization of both the state and the control spaces becomes increasingly fine.The type of difficulty illustrated in Example 3.6.2does not arise if the state space is continuous but the control space is finite.In particular, this is true in partially observed finite spaces Markov decision problems (POMDP), which are defined over their belief space (the space of probability distributions over their states).We briefly discuss this case next.Let us consider any α-discounted DP problem, where the state space is a bounded convex subset B of a Euclidean space, such as the unit simplex, but the control space U is finite.We use b to denote the states, to emphasize the connection with belief states in POMDP and to distinguish them from x, which we will use to denote representative states.Bellman's equation is J = T J with the Bellman operator T defined byWe introduce a set of representative states {x 1 , . . ., x m } ⊂ B. We assume that the convex hull of {x 1 , . . ., x m } is equal to B, so each state b ∈ B can be expressed aswhere {φ bx i | i = 1, . . ., m} is a probability distribution:We view φ bx i as aggregation probabilities.Consider the operator T that transforms a vector r = (r x 1 , . . ., r xm ) into the vector T r with components ( T r)(x 1 ), . . ., ( T r)(x m ) defined bywhere φ f (x i ,u,w) x j are the aggregation probabilities of the state f (x i , u, w).It can then be shown that T is a contraction mapping with respect to the maximum norm (we give the proof for a similar result in the next section).Bellman's equation for an aggregate finite-state discounted DP problem whose states are x 1 , . . ., x m has the formand has a unique solution.The transitions in this problem occur as follows: from state x i under control u, we first move to f (x i , u, w) at cost g(x i , u, w), and then we move to a state x j , j = 1, . . ., m, according to the probabilities φ f (x i ,u,w) x j .The optimal costs r *x i , i = 1, . . ., m, of this problem can often be obtained by standard VI and PI methods that may or may not use simulation.We may then approximate the optimal cost function of the original problem byand reasonably expect that the optimal discretized solution converges to the optimal as the number of representative states increases.In the case where B is the belief space of an α-discounted POMDP, the representative states/beliefs and the aggregation probabilities define an aggregate problem, which is a finite-state α-discounted problem with a perfect state information structure.This problem can be solved with exact DP methods if either the aggregate transition probabilities and transition costs can be obtained analytically (in favorable cases) or if the number of representative states is small enough to allow their calculation by simulation.The aggregate problem can also be addressed with approximate DP method that we have discussed earlier, such as problem approximation/certainty equivalence approaches.It can also be addressed with a rollout method, which is suitable for an on-line implementation.We will now discuss a more general aggregation framework for the infinite horizon n-state α-discounted problem.We essentially replace the representative states x with subsets I x ⊂ {1, . . ., n} of the original state space.We introduce a finite subset A of aggregate states, which we denote by symbols such as x and y.We define: (a) A collection of disjoint subsets I x ⊂ {1, . . ., n}, x ∈ A.(b) A probability distribution over {1, . . ., n} for each x ∈ A, denoted by {d xi | i = 1, . . ., n}, and referred to the disaggregation probabilities of x.We require that the distribution corresponding to x is concentrated on the subset I x :(c) For each original system state j ∈ {1, . . ., n}, a probability distribution over A, denoted by {φ jy | y ∈ A}, and referred to as the aggregation probabilities of j.We require that φ jy = 1, for all j ∈ I y , y ∈ A.The aggregation and disaggregation probabilities specify a dynamic system involving both aggregate and original system states; cf.Fig. 3.6.6.In this system:(i) From aggregate state x, we generate an original system state i ∈ I x according to d xi .(ii) We generate transitions between original system states i and j according to p ij (u), with cost g(i, u, j).(iii) Our general aggregation framework is illustrated in Fig. 3.6.6.The sets I x are often constructed by using features, however, it is helpful to formulate our aggregation framework in a general form, and introduce features later.Note that if each set I x consists of a single state, we obtain the representative states framework of the preceding section.In this case the disaggregation distribution {d xi | i ∈ I x } is just the atomic distribution that assigns probability 1 to the unique state in I x .Consistent with the special case of representative states, the disaggregation probability d xi may be interpreted as a "measure of the relation of x and i."The aggregate problem is a DP problem with an enlarged state space that consists of two copies of the original state space {1, . . ., n} plus the set of aggregate states A. We introduce the corresponding optimal vectors J0 , J1 , and r * = {r *x | x ∈ A} where: r * x is the optimal cost-to-go from aggregate state x.J0 (i) is the optimal cost-to-go from original system state i that has just been generated from an aggregate state (left side of Fig. 3.6.6).J1 (j) is the optimal cost-to-go from original system state j that has just been generated from an original system state (right side of Fig.Note that because of the intermediate transitions to aggregate states, J0 and J1 are different.These three vectors satisfy the following three Bellman equations: The objective is to solve for the optimal costs r * x of the aggregate states in order to obtain approximate costs for the original problem through the interpolation formula J(j) =  and that H is a maximum norm contraction.Note that the composite Bellman equation (3.68) has dimension equal to the number of aggregate states, which is potentially much smaller than n.To apply the aggregation framework of this section, we may solve exactly this equation for the optimal aggregate costs r *x , x ∈ A, by simulation-based analogs of the VI and PI methods, and obtain a cost function approximation for the original problem through the interpolation formula (3.64).We will develop these methods later, but before doing so, we discuss various ways to formulate the aggregation framework, and in particular, how features can be used for this purpose.Let us consider the special case of hard aggregation, where for every state j, we have φ jy = 0 for all aggregate states y, except a single one, denoted y j , for which we have φ jy j = 1.In this case, the one-step lookahead approximation J(j) = We can show the following error bound, due to Tsitsiklis and Van Roy [TsV96]; a generalization of this error bound will be given later in this section.Proposition 3.6.1:(Error Bound for Hard Aggregation) In the case of hard aggregation, we have J * (j) − J(j) ≤ 1 − α , for all j such that j ∈ S y , y ∈ A, where is the maximum variation of the optimal cost function J * over the footprint sets S y , y ∈ A:= max y∈A max i,j∈Sy J * (i) − J * (j) .The meaning of the preceding proposition is that if the optimal cost function J * varies by at most within each set S y , the hard aggregation scheme yields a piecewise constant approximation to the optimal cost function that is within /(1 − α) of the optimal.Aside from its intuitive nature and error bound properties, hard aggregation provides a connection with another major approach for approximation in value space, the so called the projected equation approach, which we have not discussed here; see the books [Ber12] and [Ber19a].In particular, it can be shown that for a given policy, the corresponding composite Bellman equation (3.68) for approximate evaluation of µ can be viewed as a projected equation, where a projection seminorm is used that is defined by the disaggregation probabilities; see the paper by Yu and Bertsekas [YuB12] (Section 5.5), or the book [Ber12] (Exercise 6.10).Generally, the method to select the aggregate states is an important issue, for which there is no mathematical theory at present.However, in practical problems, based on intuition and problem-specific knowledge, there are usually evident choices, which may be fine-tuned by experimentation.For example, suppose that the optimal cost function J * is piecewise constant over a partition {S y | y ∈ A} of the state space {1, . . ., n}.By this we mean that for some vectorwe have J * (j) = r * y for all j ∈ S y , y ∈ A.Then from Prop.3.6.1 it follows that the hard aggregation scheme with I x = S x for all x ∈ A is exact, so r * x are the optimal costs of the aggregate states x in the aggregate problem.This suggests that in hard aggregation, the states in the footprint set S y corresponding to an aggregate state y should have roughly equal optimal cost , consistently with the error bound of Prop.3.6.1.As an extension of the preceding argument, suppose that through some special insight into the problem's structure or some preliminary calculation, we know some features of the system's state that can "predict well" its optimal cost when combined through some approximation architecture, e.g., one that is linear.Then it seems reasonable to form the set aggregate states A of a hard aggregation scheme so that the sets I y and S y consist of states with "similar features" for every y ∈ A. This is called feature-based aggregation, and was suggested in the neuro-dynamic book [BeT96], Section 3.1.2.The next section considers this possibility, and provides a way to introduce features and nonlinearities into the aggregation architecture, without compromising its other favorable aspects.Let us consider the guideline for hard aggregation that we just discussed: states i that belong to the same footprint set S y should have nearly equal optimal costs, i.e., max i,j∈Sy J * (i) − J * (j) ≈ 0, for all y ∈ A.The question now is how to select the sets S y according to this guideline.An idea that comes to mind is to use a feature mapping, i.e., a function F that maps a state i into an m-dimensional feature vector F (i); cf.Example 3.1.7.In particular, suppose that F has the property that states i with nearly equal feature vector have nearly equal optimal cost J * (i).Then we can form the sets S y by grouping together states with nearly equal feature vector.In particular, given F , we introduce a more or less regular partition of the feature space [the subset of m that consists of all possible feature vectors F (i)].The partition of the feature space induces a possibly irregular collection of subsets of the original state space.Each of these subsets is then used as the footprint of a distinct aggregate state; see Fig. 3.6.7.Note that in the resulting aggregation scheme the number of aggregate states may become very large.On the other hand, there is a significant advantage over the linear feature-based architectures of Section 3.1, which assign a single weight to each feature: in feature-based hard aggregation we are assigning a weight to each subset of the feature space partition (possibly a weight to every possible feature value, in the extreme case where each feature value is viewed by itself as a distinct set of the partition).In effect we use aggregation to construct a nonlinear (piecewise constant) feature-based architecture, which may be much more powerful than the corresponding linear architecture.The question now arises how to obtain a suitable feature vector when there is no obvious choice, based on problem-specific considerations.One possibility, discussed in the book [Ber19a] (Section 6.4), is to obtain "good" features by using a neural network.In fact any method that automatically generates features from data may be used.Here we will discuss a simple possibility.Suppose that we have obtained in some way a real-valued scoring function V (i) of the state i, which serves as an index of undesirability of state i as a starting state (smaller values of V are assigned to more desirable states, consistent with the view of V as some form of "cost" function).One possibility is to use as V an approximation of the cost function of some "good" (e.g., near-optimal) policy.Another possibility is to obtain V by problem approximation, i.e., as the cost function of some reasonable policy applied to an approximation of the original problem.Still another possibility is to obtain V by training a neural network or other architecture using samples of state-cost pairs obtained by using a software or human expert, and some supervised learning technique.Given the scoring function V , we will construct a feature mapping that groups together states i with roughly equal scores V (i).In particular, we let R x , x = 1, . . ., q, be q disjoint intervals that form a partition of the range of possible values of V [i.e., are such that for any state i, there is a unique interval R x such that V (i) ∈ R x ].We define a feature vector F (i) of the state i according tofor all i such that V (i) ∈ R x , x = 1, . . ., q.(3.71)This feature in turn defines a partition of the state space into the sets ) . . .r * q−1 r Figure 3.6.8.Hard aggregation scheme based on a single scoring function.We introduce q disjoint intervals R 1 , . . ., Rq that form a partition of the set of possible values of V , and we define a feature vector F (i) of the state i according tofor all i such that V (i) ∈ Rx, x = 1, . . ., q.This feature vector in turn defines a partition of the state space into the sets, . . ., q.The sets Ix coincide with the footprint sets Sx, and the solution of the aggregate problem yields a piecewise constant approximation of the optimal cost function of the original problem.Assuming that all the sets I x are nonempty, we thus obtain a hard aggregation scheme, where the aggregate states are x = 1, . . ., q, and the aggregation probabilities are defined by φ jx = 1 if j ∈ I x , 0 otherwise, j = 1, . . ., n, x = 1, . . ., q, (3.73) see Fig. 3.6.8.Note that the sets I x coincide with the footprint sets S x .The following proposition (due to Tsitsiklis and Van Roy [TsV96]) illustrates the important role of the quantization error , defined as δ = max x=1,...,q max i,j∈Ix V (i) − V (j) .(3.74)It represents the maximum error that can be incurred by approximating V within each set I x with a single value from its range within the subset.Its proof with additional discussion can be found in Chapter 6 of the author's RL book [Ber19a].Proposition 3.6.2:Consider the hard aggregation scheme defined by a scoring function V as described above.Assume that the variations of J * and V over the sets I 1 , . . ., I q are within a factor β ≥ 0 of each other, i.e., that J * (i) − J * (j) ≤ β V (i) − V (j) , for all i, j ∈ I x , x = 1, . . ., q.(a) We have, for all i ∈ I x , x = 1, . . ., q, where δ is the quantization error of Eq. (3.74).(b) Assume that there is no quantization error, i.e., V and J * are constant within each set I x .Then the aggregation scheme yields the optimal cost function J * exactly, i.e., J * (i) = r * x , for all i ∈ I x , x = 1, . . ., q.In this section we will introduce an extension of the preceding aggregation framework.† It involves a vector V = V (1), . . ., V (n) called the bias vector or bias function, which affects the cost structure of the aggregate problem, and biases the values of its optimal cost function towards their correct levels.When V = 0, we will obtain the aggregation scheme of Section 3.6.4.When V = 0, we will obtain a different aggregation scheme, which yields an approximation to J * that is equal to V plus a local correction; see Fig. 3.6.10.In this case the aggregate DP problem aims to provide a correction/improvement to V , which may itself be a reasonably good estimate of J * .An obvious context where biased aggregation can be used is to improve on an approximation to J * obtained using a different method, such † The aggregation framework of this section was proposed in the author's  as for example by neural network-based approximate PI, by rollout, or by problem approximation.Generally, we may speculate that if V captures a fair amount of the nonlinearity of J * , we may reduce the number of aggregate states needed for adequate performance.Let us now formulate the aggregate problem in biased aggregation.It is a discounted infinite horizon problem that is similar to the (unbiased) aggregate problem of Section 3.6.4.It involves three sets of states: two copies of the original state space, denoted I 0 and I 1 , as well as a finite set A of aggregate states, as depicted in Fig. 3.6.11.The state transitions in the aggregate problem go from a state in A to a state in I 0 , according to disaggregation probabilities, then to a state in I 1 , and then back to a state in A, according to aggregation probabilities, and the process is repeated.At state i ∈ I 0 we must choose a control u ∈ U (i), and then transition to a state j ∈ I 1 at a cost g(i, u, j) according to the original system transition probabilities p ij (u).The salient new characteristic of the biased aggregation scheme is a (possibly nonzero) cost −V (i) for transition from any aggregate state to a state i ∈ I 0 , and of a cost V (j) from a state j ∈ I 1 to any aggregate state; cf.Fig. 3.6.11.The function V is the bias function, and we will argue that V should be chosen as close as possible to J * .Moreover, for practical purposes its values at various states should be easily computable.A key insight is that biased aggregation can be viewed as unbiased aggregation applied to a modified DP problem, which is equivalent to the original DP problem in the sense that it has the same optimal policies.The modified DP problem is obtained from the original by changing its cost per stage from g(i, u, j) to g(i, u, j) − V (i) + αV (j), i,j = 1, . . ., n, u ∈ U (i).(3.75)In particular, by comparing Figs.state-control trajectories as biased aggregation applied to the original DP problem, while the incurred transition costs (from aggregate state to aggregate state) are equal.Moreover, there is a close connection between the optimal cost functions of the modified DP problem with cost per stage given by Eq. (3.75), and the original DP problem.In particular, the optimal cost function of the modified problem, call it J , satisfies the corresponding Bellman equation:p ij (u) g(i, u, j) − V (i) + αV (j) + α J(j) , i = 1, . . ., n, or equivalently J(i) + V (i) = min u∈U(i) n j=1 p ij (u) g(i, u, j) + α J(j) + V (j) , i = 1, . . ., n.By comparing this equation with the Bellman equation for the original problem, we see that the optimal cost functions of the modified and the original problems are related by J * (i) = J(i) + V (i), i= 1, . . ., n, and that the two problems have the same optimal policies.This of course assumes that the original and modified problems are solved exactly.If instead they are solved approximately using aggregation or another approximation architecture, such as a neural network, the policies obtained may be substantially different.In particular, the choice of V and the approximation architecture may affect substantially the quality of suboptimal policies obtained.To summarize, any unbiased aggregation scheme and algorithm, when applied to the modified DP problem with cost per stage given by Eq. (3.75), yields a biased aggregation scheme and algorithm for the original DP problem.Thus, we can straightforwardly transfer results, algorithms, and intuition from our earlier unbiased aggregation analysis to the biased aggregation framework, by applying them to the unbiased aggregation framework that corresponds to the modified stage cost (3.75).Moreover, we may use simulation-based algorithms for policy evaluation, policy improvement, and Q-learning for the aggregate problem, with the only requirement that the value V (i) for any state i is available when needed.Let us now discuss the distributed solution of large-scale discounted DP problems using cost function approximation, multiple agents/processors, and hard aggregation.Here we partition the original system states into aggregate states/subsets x ∈ A = {x 1 , . . ., x m }, and we envision a network of processors/agents, each updating asynchronously a detailed/exact local cost function, defined on a single aggregate state/subset.Each processor also maintains an aggregate cost for its aggregate state, which is a weighted average of the detailed cost of the (original system) states in the processor's subset, weighted by the corresponding disaggregation probabilities.These aggregate costs are communicated between processors and are used to perform the local updates.In a synchronous VI method of this type, each processor = 1, . . ., m, maintains/updates a (local) cost J(i) for every original system state i ∈ x , and an aggregate cost R( ) = i∈x d x i J(i),where d x i are the corresponding disaggregation probabilities.We generically denote by J and R the vectors with components J(i), i = 1, . . ., n, and R( ), = 1, . . ., m, respectively.These components are updated according to J k+1 (i) = min u∈U(i) and where for each original system state j, we denote by x(j) the subset to which j belongs [i.e., j ∈ x(j)].Thus the iteration (3.76) is the same as ordinary VI, except that instead of J(j), we use the aggregate costs R x(j) for the states j whose costs are updated by other processors.It is possible to show that the iteration (3.76)-(3.77)involves a supnorm contraction mapping of modulus α, so it converges to the unique solution of the system of equations in (J, R)∀ i ∈ x , = 1, . . ., m.(3.79)This follows from the fact that {d x i | i = 1, . . ., n} is a probability distribution.We may view the equations (3.79) as a set of Bellman equations for an "aggregate" DP problem, which similar to our earlier discussion, involves both the original and the aggregate system states.The difference from the Bellman equations (3.65)-(3.67) is that the mapping (3.78) involves J(j) rather than R x(j) for j ∈ x .In the algorithm (3.76)-(3.77),all processors must be updating their local costs J(i) and aggregate costs R( ) synchronously, and communicate the aggregate costs to the other processors before a new iteration may begin.This is often impractical and time-wasting.In a more practical asynchronous version of the method, the aggregate costs R( ) may be outdated to account for communication "delays" between processors.Moreover, the costs J(i) need not be updated for all i; it is sufficient that they are updated by each processor only for a (possibly empty) subset of I ,k of the aggregate state/set x .In this case, the iteration (3.76)-(3.77) is modified to take the form J k+1 (i) = min The differences k − τ ,k , = 1, . . ., m, in Eq. (3.80) may be viewed as "delays" between the current time k and the times τ ,k when the corresponding aggregate costs were computed at other processors.For convergence, it is of course essential that every i ∈ x belongs to I ,k for infinitely many k (so each cost component is updated infinitely often), and lim k→∞ τ ,k = ∞ for all = 1, . . ., m (so that processors eventually communicate more recently computed aggregate costs to other processors).The convergence of this type of method based on the sup-norm contraction property of the mapping underlying Eq.(3.79), can be established using an asynchronous convergence theory for DP developed by the author in the paper [Ber82] (see also the books [BeT89], [Ber12]).The monotonicity property is also sufficient to establish convergence, and this is useful in the convergence analysis of related aggregation algorithms for nondiscounted DP models (see the paper by Bertsekas and Yu [BeY10]).Section 3.1: Our discussion of approximation architectures, neural networks, and training has been limited, and aimed just to provide the connection with approximate DP.The literature on the subject is vast, and some of the textbooks mentioned in the references to Chapter 1 provide detailed accounts and many sources, in addition to the ones given in Sections 3.1 and 3.2.There are two broad directions of inquiry in parametric architectures:(1) The design of architectures, either in a general or a problem-specific context.(2) The training of neural networks, as well as other linear and nonlinear architectures.Research along both of these directions has been extensive and is continuing.Methods for selection of basis functions have received much attention, particularly in the context of neural network research and deep reinforcement learning (see e.g., the book by Goodfellow, Bengio, and Courville [GBC16]).For discussions that are focused outside the neural network area, see Bertsekas and Tsitsiklis [BeT96], Keller, Mannor, and Precup [KMP06], Jung and Polani [JuP07], Bertsekas and Yu [BeY09], and Bhatnagar, Borkar, and Prashanth [BBP13].Moreover, there has been considerable research on optimal feature selection within given parametric classes (see Menache,Mannor,and Shimkin [MMS05], Yu and Bertsekas [YuB09], Busoniu et al. [BBD10a], and Di Castro and Mannor [DiM10]).Incremental algorithms are the principal methods for training approximation architectures.They are supported by substantial theoretical analysis, which addresses issues of convergence, rate of convergence, stepsize selection, and component order selection.Moreover, incremental algorithms have been extended to constrained optimization settings, where the constraints are also treated incrementally, first by Nedić [Ned11], and then by several other authors: Bertsekas [Ber11a], Wang and Bertsekas [WaB15], [WaB16], Bianchi [Bia16], Iusem, Jofre, and Thompson [IJT18].It is beyond our scope to cover this analysis.The author's surveys [Ber10a] and [Ber15b], and convex optimization and nonlinear programming textbooks [Ber15a], [Ber16], collectively contain an extensive account of incremental methods, including the Kaczmarz, incremental gradient, subgradient, ag-gregated gradient, Newton, Gauss-Newton, and extended Kalman filtering methods, and give many references.The book [BeT96] and paper [BeT00] by Bertsekas and Tsitsiklis, and the survey by Bottou, Curtis, and Nocedal [BCN18] provide theoretically oriented treatments.Section 3.2: The publicly and commercially available neural network training programs incorporate heuristics for scaling and preprocessing data, stepsize selection, initialization, etc, which can be very effective in specialized problem domains.We refer to books on neural networks such as Bishop [Bis95], Goodfellow, Bengio, and Courville [GBC16], Haykin [Hay08].The recent book by Bishop and Bishop [BiB24] includes discussions of deep neural networks and transformers.Deep neural networks have created a lot of excitement in the machine learning field, in view of some high profile successes in image and speech recognition, and in RL with the AlphaGo and AlphaZero programs.One question is whether and for what classes of target functions we can enhance approximation power by increasing the number of layers while keeping the number of weights constant.For discussion, analysis, and speculation around this question, see Bengio [Ben09], Liang and Srikant [LiS16], Yarotsky [Yar17], and Daubechies et al. [DDF19].Another important research question relates to the role of overparametrization in the success of deep neural networks.With more weights than training data, the training problem has infinitely many solutions, each providing an architecture that fits the training data perfectly.The question then is how to select a solution that works well on test data (i.e., data outside the training set); see Zhang et  Section 3.3: Fitted value iteration has a long history; it was mentioned by Bellman among others.It has interesting properties, and at times exhibits pathological/unstable behavior due to accumulation of errors over a long horizon (see [Ber19a], Section 5.2).The approximate policy iteration method of Section 3.3.3has been proposed by Fern, Yoon, and Givan [FYG06], and variants have also been discussed and analyzed by several other authors.The method (with some variations) has been used to train a tetris playing computer program that performs impressively better than programs that are based on other variants of approximate policy iteration, and various other methods; see Scherrer [Sch13], Scherrer et al. [SGG15], and Gabillon, Ghavamzadeh, and Scherrer [GGS13], who also provide an analysis of the method.The RL and approximate DP books collectively describe several alternative simulationbased methods for policy evaluation; see e.g., [BeT96], [SuB18], [Ber12], Chapters 6 and 7.These include the popular temporal difference methods, which are also closely related to Galerkin approximation, a major computational approach for solving large scale equations, as first observed by Yu and Bertsekas [YuB10], and Bertsekas [Ber11c]; see also Szepesvari [Sze11].The book [Ber20a] describes distributed versions of approximate policy iteration, which are based on partitioning of the state space.The original proposal of SARSA (Section 3.3.4) is attributed to Rummery and Niranjan [RuN94], with related work presented in the papers by Peng and Williams [PeW96], and Wiering and Schmidhuber [WiS98].The ideas of the DQN algorithm attracted much attention following the paper by Mnih et al. [MKS15], which reported impressive test results on a suite of 49 classic Atari 2600 games.The rollout and approximate PI methodology for POMDP of Section 3.3.5 was described in the author's RL book [Ber19a].It was extended and tested in the paper by Bhattacharya et al. [BBW20] in the context of a challenging pipeline repair problem.Advantage updating (Section 3.3.6)was proposed by Baird [Bai93], [Bai94], and is discussed further in Section 6.6 of the neuro-dynamic programming book [BeT96].The differential training methodology (Section 3.3.7)was proposed by the author in the paper [Ber97b], and followup work was presented by Weaver and Baxter [WeB99].Generally, the challenges of implementing successfully approximate value and policy iteration schemes are quite formidable, and tend to be underestimated, because the literature naturally tends to place emphasis on success stories, and tends to underreport failures.In practice, the training difficulties, particularly exploration, must often be addressed on a caseby-case basis, and may require long and tricky parameter tuning issues, with little guarantee of ultimate success or even a diagnosis of the causes of failure.By contrast, approximation in value space with long multistep lookahead and simple terminal cost function approximation, and rollout (a single policy iteration starting from a base policy), while less ambitious, are typically much easier to implement in practice, and often attain at least some modest success rather quickly.An intermediate approach that often works well is to use truncated rollout with a terminal cost function approximation that is trained with data.Section 3.4: Classification (sometimes called "pattern classification" or "pattern recognition") is a major subject in machine learning, for which there are many approaches, an extensive literature, and an abundance of public domain and commercial software; see e.  [FPB15], and the references quoted there).While we have focused on a classification approach that makes use of least squares regression and a parametric architecture, other classification methods may also be used.For example the paper [LaP03] discusses the use of nearest neighbor schemes, support vector machines, as well as neural networks.Section 3.5: Our coverage of policy gradient, and random search methods has been limited, and aimed to provide an entry point into the field.For a detailed discussion and references on policy gradient methods, we refer to the book by Sutton and Barto [SuB18], the monographs by Deisenroth, Neumann, and Peters [DNP11], and the surveys by Peters and Schaal [PeS08], and Grondman et al. [GBL12].An influential paper in this context by Williams [Wil92] proposed among others the likelihood-ratio policy gradient method given here.The methods of [Wil92] are commonly referred to as REINFORCE in the literature (see e.g., [SuB18], Ch. 13).There are several related early works on search along randomly chosen directions (Rastrigin [Ras63], Matyas [Mat65], Aleksandrov, Sysoyev, and Shemeneva [ASS68], Rubinstein [Rub69]); see also Spall [Spa92], [Spa03], Duchi, Jordan, Wainwright, and Wibisono [DJW12], [DJW15], and Nesterov and Spokoiny [NeS17], for more modern related works.For early works on simulation-based policy gradient schemes for various DP problems, see Glynn [Gly87], [Gly90], L'Ecuyer [L'Ec91], Fu and Hu [FuH94], Jaakkola, Singh, and Jordan [JSJ95], Cao and Chen [CaC97], Cao and Wan [CaW98].There are also variants of policy gradient methods that include stabilization heuristics to guard against unusual behavior.An example is the popular proximal policy optimization (PPO) method suggested in the paper by Shulman et al. [SWD17], which also reviews earlier approaches.The challenge in the successful implementation of policy gradient methods is twofold: the difficulties with slow convergence and local minima that are inherent in gradient optimization, and the detrimental effects of simulation noise.Much work has been directed towards variations that address these difficulties, including the use of a baseline and variance reduction methods (Greensmith, Bartlett, and Baxter [GBB04], Greensmith [Gre05]), and scaling based on the so-called natural gradient (Kakade [Kak02]) or second order information (see Wang and Paschalidis [WaP17], and the references quoted there).We have not covered actor-critic methods within the policy gradient context.Such methods were introduced in the paper by Barto, Sutton, and Anderson [BSA83].The more recent works of Baxter and Bartlett [BaB01], Konda and Tsitsiklis [KoT99], [KoT03], Marbach and Tsitsiklis [MaT01], [MaT03], and Sutton et al. [SMS99] have been influential.Actor-critic algorithms that are suitable for POMDP and involve gradient estimation have been given by Yu [Yu05], and Estanjini, Li, and Paschalidis [ELP12].The cross-entropy method was initially developed in the context of rare event simulation and was later adapted for use in optimization.For textbook accounts, see Rubinstein and Kroese [RuK04], [RuK13], [RuK16], and Busoniu et al. [BBD10a], and for surveys see de Boer et al. [BKM05], and Kroese et al. [KRC13].The method was proposed for policy search in an approximate DP context by Mannor, Rubinstein, and Gat [MRG03].It was applied with success to the game of tetris by Szita and Lorinz [SzL06], and Thiery and Scherrer [ThS09].For recent analysis, see Joseph and Bhatnagar [JoB16], [JoB18].The aggregation framework with representative features was introduced in the author's DP book [Ber12], was discussed in detail in the RL textbook [Ber19a] (Chapter 6), and was further developed in the author's survey paper [Ber18b], which provides an expanded view of the methodology.Biased aggregation (Section 3.6.7)was introduced in the author's paper [Ber18c], which contains further discussion, connections with rollout algorithms, and additional methods.Distributed asynchronous aggregation (Section 3.6.8)was first proposed in the paper by Bertsekas and Yu [BeY10] (Example 2.5); see also the discussions in author's DP books [Ber12] (Section 6.5.4) and [Ber22b] (Example 1.2.11).A recent computational study, related to distributed traffic routing, was given by Vertovec and Margellos [VeM23].Aggregation may also be used as a policy evaluation method in the context of policy iteration with linear feature-based cost function approximations.Within this context, aggregation provides an alternative to temporal difference methods, such as TD(λ), LSTD(λ), and LSPE(λ), which form another major class of policy evaluation methods (not discussed in this book; see [Ber12], [Ber19a]).The aggregation and the temporal difference approaches for policy evaluation are described and compared in the author's approximate policy iteration survey paper [Ber11b].Generally speaking, aggregation methods are characterized by stronger theoretical properties, such as Bellman operator monotonicity, resilience to policy oscillations, and better error bounds.On the other hand, they are more restrictive in their use of linear approximation architectures, compared with temporal difference methods (see [Ber11b], [Ber18a]).Since this is true for all x, we also haveNeuro-Dynamic Programming Dimitri P. Bertsekas and John N. Tsitsiklis Athena Scientific, 1996 512 pp., hardcover, ISBN 1-886529-10-8 This is the first textbook that fully explains the neuro-dynamic programming/reinforcement learning methodology, a breakthrough in the practical application of neural networks and dynamic programming to complex problems of planning, optimal decision making, and intelligent control.From the review by George Cybenko for IEEE Computational Science and Engineering, May 1998: "Neuro-Dynamic Programming is a remarkable monograph that integrates a sweeping mathematical and computational landscape into a coherent body of rigorous knowledge.The topics are current, the writing is clear and to the point, the examples are comprehensive and the historical notes and comments are scholarly.""In this monograph, Bertsekas and Tsitsiklis have performed a Herculean task that will be studied and appreciated by generations to come.I strongly recommend it to scientists and engineers eager to seriously understand the mathematics and computations behind modern behavioral machine learning."Among its special features, the book:• Describes and unifies a large number of NDP methods, including several that are new• Describes new approaches to formulation and solution of important problems in stochastic optimal control, sequential decision making, and discrete optimization• Rigorously explains the mathematical principles behind NDP• Illustrates through examples and case studies the practical application of NDP to complex problems from optimal resource allocation, optimal feedback control, data communications, game playing, and combinatorial optimization This book explores the common boundary between optimal control and artificial intelligence, as it relates to reinforcement learning and simulation-based neural network methods.These are popular fields with many applications, which can provide approximate solutions to challenging sequential decision problems and large-scale dynamic programming (DP).The aim of the book is to organize coherently the broad mosaic of methods in these fields, which have a solid analytical and logical foundation, and have also proved successful in practice.The book discusses both approximation in value space and approximation in policy space.It adopts a gradual expository approach, which proceeds along four directions:• From exact DP to approximate DP: We first discuss exact DP algorithms, explain why they may be difficult to implement, and then use them as the basis for approximations.• From finite horizon to infinite horizon problems: We first discuss finite horizon exact and approximate DP methodologies, which are intuitive and mathematically simple, and then progress to infinite horizon problems.• From model-based to model-free implementations: We first discuss model-based implementations, and then we identify schemes that can be appropriately modified to work with a simulator.The mathematical style of this book is somewhat different from the one of the author's DP books, and the 1996 neuro-dynamic programming (NDP) research monograph, written jointly with John Tsitsiklis.While we provide a rigorous, albeit short, mathematical account of the theory of finite and infinite horizon DP, and some fundamental approximation methods, we rely more on intuitive explanations and less on proof-based insights.Moreover, our mathematical requirements are quite modest: calculus, a minimal use of matrix-vector algebra, and elementary probability (mathematically complicated arguments involving laws of large numbers and stochastic convergence are bypassed in favor of intuitive explanations).The book is supported by on-line video lectures and slides, as well as new research material, some of which has been covered in the present monograph.Reinforcement Learning Dimitri P. Bertsekas Athena Scientific, 2020 480 pp., hardcover, ISBN 978-1-886529-07-6This book develops in greater depth some of the methods from the author's Reinforcement Learning and Optimal Control textbook (Athena Scientific, 2019).It presents new research, relating to rollout algorithms, policy iteration, multiagent systems, partitioned architectures, and distributed asynchronous computation.The application of the methodology to challenging discrete optimization problems, such as routing, scheduling, assignment, and mixed integer programming, including the use of neural network approximations within these contexts, is also discussed.Much of the new research is inspired by the remarkable AlphaZero chess program, where policy iteration, value and policy networks, approximate lookahead minimization, and parallel computation all play an important role.Among its special features, the book:• Presents new research relating to distributed asynchronous computation, partitioned architectures, and multiagent systems, with application to challenging large scale optimization problems, such as combinatorial/discrete optimization, as well as partially observed Markov decision problems.• Describes variants of rollout and policy iteration for problems with a multiagent structure, which allow the dramatic reduction of the computational requirements for lookahead minimization.• Establishes connections of rollout algorithms and model predictive control, one of the most prominent control system design methodology.• Expands the coverage of some research areas discussed in the author's 2019 textbook Reinforcement Learning and Optimal Control.• Provides the mathematical analysis that supports the Newton step interpretations and the conclusions of the present book.The book is supported by on-line video lectures and slides, as well as new research material, some of which has been covered in the present monograph.Reinforcement Learning Dimitri P. Bertsekas Athena Scientific, 2020 480 pp., hardcover, ISBN 978-1-886529-07-6This book develops in greater depth some of the methods from the author's Reinforcement Learning and Optimal Control textbook (Athena Scientific, 2019).It presents new research, relating to rollout algorithms, policy iteration, multiagent systems, partitioned architectures, and distributed asynchronous computation.The application of the methodology to challenging discrete optimization problems, such as routing, scheduling, assignment, and mixed integer programming, including the use of neural network approximations within these contexts, is also discussed.Much of the new research is inspired by the remarkable AlphaZero chess program, where policy iteration, value and policy networks, approximate lookahead minimization, and parallel computation all play an important role.Among its special features, the book:• Presents new research relating to distributed asynchronous computation, partitioned architectures, and multiagent systems, with application to challenging large scale optimization problems, such as combinatorial/discrete optimization, as well as partially observed Markov decision problems.• Describes variants of rollout and policy iteration for problems with a multiagent structure, which allow the dramatic reduction of the computational requirements for lookahead minimization.• Establishes connections of rollout algorithms and model predictive control, one of the most prominent control system design methodology.• Expands the coverage of some research areas discussed in the author's 2019 textbook Reinforcement Learning and Optimal Control.• Provides the mathematical analysis that supports the Newton step interpretations and the conclusions of the present book.The book is supported by on-line video lectures and slides, as well as new research material, some of which has been covered in the present monograph.