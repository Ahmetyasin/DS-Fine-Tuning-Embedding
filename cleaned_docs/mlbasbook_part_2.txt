The main idea underlying GD and SGD is to approximate the objective function (5.1) locally around a current guess or approximation w (r) for the optimal weights.This local approximation is a tangent hyperplane whose normal vector is determined by the gradient ∇f (w (r) .We then obtain an updated (improved) approximation by minimizing this local approximation, i.e., by doing a GD step (5.6).The idea of advanced gradient methods [48,Ch. 8] is to exploit the information provided by the gradients ∇f w (r ) at previous iterations r = 1, . . ., r, to build an improved local approximation of f (w) around a current iterate w (r) .Figure 5. 4 indicates such an improved local approximation of f (w) which is non-linear (e.g., quadratic).These improved local approximations can be used to adapt the learning rate during the GD steps (5.6).Advanced gradient-based methods use improved local approximations to modify the gradient ∇f w (r ) to obtain an improved update direction.Figure 5.5 depicts the contours of an objective function f (w) for which the gradient ∇f w (r) points only weakly towards the optimal parameter vector w (minimizer of f (w)).The gradient history ∇f w (r ) , forr = 1, . . ., r, allows to detect such an unfavourable geometry of the objective function.Moreover, the gradient history can be used to "correct" the update direction ∇f w (r) to obtain an improved update direction towards the optimum parameter vector w.w (r−1) f (w) w (r) Figure 5.4: Advanced gradient-based methods use the gradients at previous iterations to construct an improved (non-linear) local approximation (dashed) of the objective function f (w) (solid).w (r) Figure 5.5: Advanced gradient-based methods use improved (non-linear) local approximations of the objective function f (w) to "nudge" the update direction towards the optimal parameter vector w.The update direction of plain vanilla GD (5.6) is the negative gradient −∇f w (r) .For some objective functions the negative gradient might be only weakly correlated with the straight direction from w (r) towards the optimal parameter vector ( ).Exercise 5. 1. Use Knowledge About Problem Class.Consider the space P of sequences f = (f [0], f [1], . ..) that have the following properties • for each sequence there is an index i (f ) such that f is monotone increasing for indices i ≥ i (f ) and monotone decreasing for indices i ≥ i (f )• any change point r of f , where f [r] = f [r +1] can only occur at integer multiples of 100, e.g., r = 100 or r = 300.Given a function f ∈ P and starting point r 0 our goal is to find the minimum value of min r f [r] = f [r (f ) ] as quickly as possible.Can you construct an iterative algorithm that can access a given function f only by querying the values f [r], f [r−1] and f [r+1] for any given index r.Exercise 5. 2. Maximum Eigenvalue of Flipped Product.Show that λ max AA T = λ max A T A for any given matrix A ∈ R m×n .Exercise 5. 3. Learning rate Schedule for SGD Let us learn a linear hypothesis h(x) = w T x using data points that arrive sequentially at discrete time instants t = 0, 1, . ... At time t, we gather a new data point x (r) , y (r) .The data points can be modelled as realizations of i.i.d.copies of a RV x, y .The probability distribution of the features x is a standard multivariate normal distribution N (0, I).The label of a random 2 data point is related to its features via y = w T x + ε with some fixed but unknown true parameter vector w.The additive noise ε ∼ N (0, 1) follows a standard normal distribution.We use SGD to learn the parameter vector w of a linear hypothesis, w (t+1) = w (t) − α t w (t) T x (t) − y (t) x (t) .( 5.34) with learning rate schedule α t = β/t γ .Note that we compute a new SGD iteration (5.34) for each new time instant t.What conditions on the hyper-parameters β, γ ensure that lim t→∞ w (t) = w in distribution?Exercise 5. 4. ImageNet.The "ImageNet" database contains more than 10 6 images [80].These images are labeled according to their content (e.g., does the image show a dog?) and stored as a file of size at least 4 kilobytes.We want to learn a classifier that allows to predict if an image shows a dog or not.To learn this classifier we run GD for logistic regression on a small computer that has 32 kilobytes memory and is connected to the internet with bandwidth of 1 Mbit/s.Therefore, for each single GD update (5.6) it must essentially download all images in ImageNet.How long would such a single GD update take ?Exercise 5. 5. Apple or No Apple?Consider data points being images of size of 1024 × 1024 pixels.Each image is characterized by the RGB pixel color intensities (value range 0, . . ., 255 resuling in 24 bits for each pixel), which we stack into a feature vector x ∈ R n .We assign each image the label y = 1 if it shows an apple and y = −1 if it does not show an apple.We use logistic regression to learn a linear hypothesis h(x) = w T x to classify an image according to ŷ = 1 if h(x) ≥ 0. The training set consists of m = 10 10 labeled images which are stored in the cloud.We implement the ML method on our own laptop which is connected to the internet with a bandwidth of at most 100 Mbps.Unfortunately we can only store at most five images on our computer.How long does it take at least to complete one single GD step ?Exercise 5. 6. Feature Normalization To Speed Up GD Consider the dataset with feature vectors x (1) = (100, 0) T ∈ R 2 and x (2) = (0, 1/10) T ∈ R 2 which we stack into the matrix X = (x (1) , x (2) ) T .What is the condition number of X T X? What is the condition number of X T X with the matrix X = ( x (1) , x (2) ) T constructed from the normalized feature vectors x (i) delivered by Algorithm 3.2 More precisely, a data point that is obtained as the realization of RV.Exercise 5. 7. Convergence of GD Steps Consider a differentiable objective function f (w) whose argument is a parameter vector w ∈ R n .We make no assumption about smoothness or convexity.Thus, the function f (w) might be non-convex and might also be not β smooth.However, the gradient ∇f (w) is uniformly upper bounded ∇f (w) ≤ 100 for every w.Starting from some initial vector w (0) we construct a sequence of parameter vectors using GD steps, w (r+1) = w (r) − α r ∇f w (r) .( 5.35) The learning rate α r in ( 5.35) is allowed to vary between different iterations.Can you provide sufficient conditions on the evolution of the learning rate α r , as iterations proceed, that ensure convergence of the sequence w (0) , w (1) , . . . .training error validation error baseline or benchmark (e.g., Bayes risk, existing ML methods or human performance) Figure 6.1:We can diagnose a ML method by comparing its training error with its validation error.Ideally both are on the same level as a baseline (or benchmark error level).Chapter 4 discussed ERM as a principled approach to learning a useful hypothesis out of a hypothesis space or model.In particular, ERM-based ML methods learn a hypothesis h ∈ H that incurs minimum average loss on a set of labeled data points that serve as the training set.We refer to the average loss incurred by a hypothesis on the training set as the training error.The minimum average loss achieved by a hypothesis that solves the ERM might be referred to as the training error of the overall ML method.Note that a specific ML method consists of specific design choices for the hypothesis space (or model) and loss function (see Chapter 3).ERM is sensible only if the training error of a hypothesis is an reliable approximation for its (average) loss incurred on data points outside the training set.We say that a learnt hypothesis generalizes well if the loss incurred outside the training set is not significantly larger than the average loss on the training set.Whether the training error of a hypothesis is a reliable approximation for its loss on data points outside the training set depends on both, the statistical properties of the data points and on the hypothesis space used by the ML method.Modern ML methods typically use hypothesis spaces with large effective dimension (see Section 2.2).As an example consider linear regression (see Section 3.1) with data points having a large number n of features (this setting is referred to as the high-dimensional regime).The effective dimension of the linear hypothesis space (3.1), which is used by linear regression, is equal to the number n of features.Modern technology allows to collect a huge number of features about individual data points which implies, in turn, that the effective dimension of (3.1) is large.Another example of a high-dimensional hypothesis space arises in deep learning methods using a hypothesis space are constituted by all maps represented by an ANN with billions of tunable parameters.A high-dimensional hypothesis space is very likely to contain a hypothesis that perfectly fits any given training set.Such a hypothesis achieves a very small training error but might incur a large loss when predicting the labels of a data point that is not included in training set.Thus, the (minimum) training error achieved by a hypothesis learnt by ERM can be misleading.We say that a ML method, such as linear regression using too many features, overfits the training set when it learns a hypothesis (e.g., via ERM) that has small training error but incurs much larger loss outside the training set.Section 6.1 shows that linear regression is likely to overfit a training set if the number of features of a data point exceeds the size of the training set.Section 6.2 demonstrates how to validate a learnt hypothesis by computing its average loss on data points outside the training set.We refer to the set of data points used to validate the learnt hypothesis as a validation set.If a ML method overfits the training set, it learns a hypothesis whose training error is much smaller than its validation error.We can detect if a ML method overfits by comparing its training error with its validation error (see Figure 6.1).We can use the validation error not only to detect if a ML method overfits.The validation error can also be used as a quality measure for the hypothesis space or model used by the ML method.This is analogous to the concept of a loss function that allows us to evaluate the quality of a hypothesis h ∈ H. Section 6.3 shows how to select between ML methods using different models by comparing their validation errors.Section 6.4 uses a simple probabilistic model for the data to study the relation between the training error of a learnt hypothesis and its expected loss (see (4.1)).This probabilistic analysis reveals the interplay between the data, the hypothesis space and the resulting training error and validation error of a ML method.Section 6.5 discusses the bootstrap as a simulation based alternative to the probabilistic analysis of Section 6. 4. While Section 6.4 assumes a specific probability distribution of the data points, the bootstrap does not require the specification of a probability distribution underlying the data.As indicated in Figure 6.1, for some ML applications, we might have a baseline (or benchmark) for the achievable performance of ML methods.Such a baseline might be obtained from existing ML methods, human performance levels or from a probabilistic model (see Section 6.4).Section 6.6 details how the comparison between training error, validation error and (if available) a baseline informs possible improvements for a ML method.These improvements might be obtained by collecting more data points, using more features of data points or by changing the hypothesis space (or model).Having a baseline for the expected loss, such as the Bayes risk, allows to tell if a ML method already provides satisfactory results.If the training error and the validation error of a ML method are close to the baseline, there might be little point in trying to further improve the ML method.We now have a closer look at the occurrence of overfitting in linear regression methods.As discussed in Section 3.1, linear regression methods learn a linear hypothesis h(x) = w T x which is parametrized by the parameter vector w ∈ R n .The learnt hypothesis is then used to predict the numeric label y ∈ R of a data point based on its feature vector x ∈ R n .Linear regression aims at finding a parameter vector w with minimum average squared error loss incurred on a training set D = x (1) , y (1) , . . . , x (m) , y (m) .The training set D consists of m data points x (i) , y (i) , for i = 1, . . ., m, with known label values y (i) .We stack the feature vectors x (i) and labels y (i) , respectively, of the data points in the training set into the feature matrix X = (x (1) , . . ., x (m) ) T and label vector y = (y (1) , . . ., y (m) ) T .The ERM (4.13) of linear regression is solved by any parameter vector w that solves (4.11).The (minimum) training error of the hypothesis h ( w) is obtained asw∈R n L(h (w) |D) Here, we used the orthogonal projection matrix P on the linear span span{X} = Xa : a ∈ R n ⊆ R m , of the feature matrix X = (x (1) , . . ., x (m) ) T ∈ R m×n .In many ML applications we have access to a huge number of individual features to characterize a data point.As a point in case, consider a data point which is a snapshot obtained from a modern smartphone camera.These cameras have a resolution of several megapixels.Here, we can use millions of pixel colour intensities as its features.For such applications, it is common to have more features for data points than the size of the training set, n ≥ m. (Whenever (6.2) holds, the feature vectors x (1) , . . ., x (m) ∈ R n of the data points in D are typically linearly independent.As a case in point, if the feature vectors x (1) , . . ., x (m) ∈ R n are realizations of i.i.d.RVs with a continuous probability distribution, these vectors are linearly independent with probability one [100].If the feature vectors x (1) , . . ., x (m) ∈ R n are linearly independent, the span of the feature matrix X = (x (1) , . . ., x (m) ) T coincides with R m which implies, in turn, P = I.Inserting P = I into (4.13)yieldsAs soon as the number m = |D| of training data points does not exceed the number n of features that characterize data points, there is (with probability one) a linear predictor h ( w) achieving zero training error(!).While the hypothesis h ( w) achieves zero training error, it will typically incur a non-zero average prediction error y − h ( w) (x) on data points (x, y) outside the training set (see Figure 6.2).Section 6.4 will make this statement more precise by using a probabilistic model for the data points within and outside the training set.Note that (6.3) also applies if the features x and labels y of data points are completely unrelated.Consider an ML problem with data points whose labels y and features are realizations of a RV that are statistically independent.Thus, in a very strong sense, the features x contain no information about the label of a data point.Nevertheless, as soon as the number of features exceeds the size of the training set, such that (6.2) holds, linear regression methods will learn a hypothesis with zero training error.We can easily extend the above discussion about the occurrence of overfitting in linear regression to other methods that combine linear regression with a feature map.Polynomial regression, using data points with a single feature z, combines linear regression with the feature map z → Φ(z) := z 0 , . . ., z n−1 T as discussed in Section 3. 2.It can be shown that whenever (6.2) holds and the features z (1) , . . ., z (m) of the training set are all different, the feature vectors x (1) := Φ z (1) , . . ., x (m) := Φ z (m) are linearly independent.This implies, in turn, that polynomial regression is guaranteed to find a hypothesis with zero training error whenever m ≤ n and the data points in the training set have different feature values.Figure 6.2: Polynomial regression learns a polynomial map with degree n−1 by minimizing its average loss on a training set (blue crosses).Using high-degree polynomials (large n) results in a small training error.However, the learnt high-degree polynomial performs poorly on data points outside the training set (orange dots).x (1) , y (1) x (2) , y (2) x (3) , y (3) D (train) x (4) , y (4) x (5) , y (5) D (val) Figure 6.3:We split the dataset D into two subsets, a training set D (train) and a validation set D (val) .We use the training set to learn (find) the hypothesis h with minimum empirical risk L( h|D (train) ) on the training set (4.3).We then validate h by computing its average loss L( h|D (val) ) on the validation set D (val) .The average loss L( h|D (val) ) obtained on the validation set is the validation error.Note that h depends on the training set D (train) but is completely independent of the validation set D (val) .Consider an ML method that uses ERM (4.3) to learn a hypothesis h ∈ H out of the hypothesis space H.The discussion in Section 6.1 revealed that the training error of a learnt hypothesis h can be a poor indicator for the performance of h for data points outside the training set.The hypothesis h tends to "look better" on the training set over which it has been tuned within ERM.The basic idea of validating the predictor h is simple:• first we learn a hypothesis h using ERM on a training set and • then we compute the average loss of h on data points that do not belong to the training set.Thus, validation means to compute the average loss of a hypothesis using data points that have not been used in ERM to learn that hypothesis.Assume we have access to a dataset of m data points, D = x (1) , y (1) , . . ., x (m) , y (m) .Each data point is characterized by a feature vector x (i) and a label y (iThe choice of the split ratio ρ ≈ m t /m in Algorithm 5 is often based on trial and error.We try out different choices for the split ratio and pick the one with the smallest validation error.It is difficult to make a precise statement on how to choose the split ratio which applies broadly [83].This difficulty stems from the fact that the optimal choice for ρ depends on the precise statistical properties of the data points.One approach to determine the required size of the validation set is to use a probabilistic model for the data points.The i.i.d.assumption is maybe the most widely used probabilistic model within ML.Here, we interpret data points as the realizations of i.i.d.RVs.These i.i.d.RVs have a common (joint) probability distribution p(x, y) over possible features x Algorithm 5 Validated ERM Input: model H, loss function L, dataset D = x (1) , y (1) , . . ., x (m) , y (m) ; split ratio ρ 1: randomly shuffle the data points in D 2: create the training set D (train) using the first m t = ρm data points, 1) , y (1) , . . ., x (mt) , y (mt) .3: create the validation set D (val) by the m v = m − m t remaining data points, mt+1) , y (mt+1) , . . ., x (m) , y (m) .4: learn hypothesis h via ERM on the training set,5: compute the training error6: compute the validation errorOutput: learnt hypothesis h, training error E t , validation error E v and labels y of a data point.Under the i.i.d.assumption, the validation error E v (6.6) also becomes a realization of a RV.The expectation (or mean) E{E v } of this RV is precisely the risk E{L (x, y), h } of h (see (4.1)).Within the above i.i.d.assumption, the validation error E v becomes a realization of a RV that fluctuates around its mean E{E v }.We can quantify this fluctuation using the varianceNote that the validation error is the average of the realizations L (x (i) , y (i) ), h of i.i.d.RVs.The probability distribution of the RV L (x, y), h is determined by the probability distribution p(x, y), the choice of loss function and the hypothesis h.In general, we do not know p(x, y) and, in turn, also do not know the probability distribution of L (x, y), h .If we know an upper bound U on the variance of the (random) loss L (x (i) , y (i) ), h , we can bound the variance ofWe can then, in turn, ensure that the variance σ 2 Ev of the validation error E v does not exceed a given threshold η, say η = (1/100)E 2 t , by using a validation set of sizeThe lower bound (6.7) is only useful if we can determine an upper bound U on the variance of the RV L (x, y), h where x, y is a RV with probability distribution p(x, y).An upper bound on the variance of L (x, y), h can be derived using probability theory if we know an accurate probabilistic model p(x, y) for the data points.Such a probabilistic model might be provided by application-specific scientific fields such as biology or psychology.Another option is to estimate the variance of L (x, y), h using the sample variance of the actual loss values L (x (1) , y (1) ), h , . . ., L (x (m) , y (m) ), h obtained for the dataset D.Algorithm 5 uses the most basic form of splitting a given dataset D into a training set and a validation set.Many variations and extensions of this basic splitting approach have been proposed and studied (see [32] and Section 6.5).One very popular extension of the single split into training set and validation set is known as k-fold cross-validation (k-fold CV) [58, fold 1 1) , y (1) , . . ., x (m) , y (m) Sec.7.10].We summarize k-fold CV in Algorithm 6 below.Figure 6.4 illustrates the key principle behind k-fold CV.First, we divide the entire dataset evenly into k subsets which are referred to as "folds".The learning (via ERM) and validation of a hypothesis out of a given hypothesis space H is then repeated k times.During each repetition, we use one fold as the validation set and the remaining k − 1 folds as a training set.We then average the values of the training error and validation error obtained for each repetition (fold).The average (over all k folds) validation error delivered by k-fold CV tends to better estimate the expected loss or risk (4.1) compared to the validation error obtained from a single split in Algorithm 5. Consider a dataset that consists of a relatively small number of data points.If we use a single split of this small dataset into a training set and validation set, we might be very unlucky and choose data points for the validation set which are outliers and not representative for the statistical properties of most data points.The effect of such an unlucky split is typically averaged out when using k-fold CV.The simple validation approach discussed above requires the validation set to be a good representative for the overall statistical properties of the data.This might not be the case in applications with discrete valued labels and some of the label values being very rare.We might then be interested in having a good estimate of the conditional risks E{L ((x, y), h) |y = Algorithm 6 k-fold CV ERM Input: model H, loss function L, dataset D = x (1) , y (1) , . . . , x (m) , y (m) ; number k of folds 1: randomly shuffle the data points in D (1) , y (1) , . . ., x (B) , y (B) }, . . ., D k = x ((k−1)B+1) , y ((k−1)B+1) , . . ., x (m) , y (m) } (6.8)compute the training errorcompute validation errori∈D (val) L (x (i) , y (i) ), h .(6.11)9: end for 10: compute average training and validation errorsOutput: learnt hypothesis h; average training error E t ; average validation error E v y } where y is one of the rare label values.This is more than requiring a good estimate for the risk E{L ((x, y), h)}.Consider data points characterized by a feature vector x and binary label y ∈ {−1, 1}.Assume we aim at learning a hypothesis h(x) = w T x to classify data points as ŷ = 1 if h(x) ≥ 0 while ŷ = −1 otherwise.The learning is based on a dataset D which contains only one single (!) data point with y = −1.If we then split the dataset into training and validation set, it is with high probability that the validation set does not include any data point with label value y = −1.This cannot happen when using k-fold CV since the single data point must be in one of the validation folds.However, even the applicability of k-fold CV for such an imbalanced dataset is limited since we evaluate the performance of a hypothesis h(x) using only one single data point with y = −1.The resulting validation error will be dominated by the loss of h(x) incurred on data points from the majority class (those with true label value y = 1).To learn and validate a hypothesis with imbalanced data, it might be useful to to generate synthetic data points to enlarge the minority class.This can be done using data augmentation techniques which we discuss in Section 7. 3. Another option is to choose a loss function that takes the different frequencies of label values into account.Let us illustrate this approach in what follows by an illustrative example.Consider an imbalanced dataset of size m = 100, which contains 90 data points with label y = 1 but only 10 data points with label y = −1.We might want to put more weight on wrong predictions obtained for data points from the minority class (with true label value y = −1).This can be done by using a much larger value for the loss L ((x, y = −1), h(x) = 1) than for the loss L ((x, y = 1), h(x) = −1) incurred by incorrectly predicting the label of a data point from the majority class (with true label value y = 1).Chapter 3 illustrated how many well-known ML methods are obtained by different combinations of a hypothesis space or model, loss function and data representation.While for many ML applications there is often a natural choice for the loss function and data representation, the right choice for the model is typically less obvious.We now discuss how to use the validation methods of Section 6.2 to choose between different candidate models.Consider data points characterized by a single numeric feature x ∈ R and numeric label y ∈ R. If we suspect that the relation between feature x and label y is non-linear, we might use polynomial regression which is discussed in Section 3. 2(n) poly with some maximum degree n.Different choices for the maximum degree n yield a different hypothesis space:Another ML method that learns non-linear hypothesis map is Gaussian basis regression (see Section 3.5).Here, different choices for the variance σ and shifts µ of the Gaussian basis function (3.13) result in different hypothesis spaces.For example, H (1) = H(2)Gauss with σ = 1 and µ 1 = 1 andGauss with σ = 1/10, µ 1 = 10, µ 2 = 20.Algorithm 7 summarizes a simple method to choose between different candidate models H (1) , H (2) , . . ., H (M ) .The idea is to first learn and validate a hypothesis h (l) separately for each model H (l) using Algorithm 6.For each model H (l) , we learn the hypothesis h (l) via ERM (6.4) and then compute its validation error E (l) v (6.6).We then choose the hypothesis h ( l) from those model H ( l) which resulted in the smallest validation errorThe workflow of Algorithm 7 is similar to the workflow of ERM.Remember that the idea of ERM is to learn a hypothesis out of a set of different candidates (the hypothesis space).The quality of a particular hypothesis h is measured using the (average) loss incurred on some training set.We use the same principle for model selection but on a higher level.Instead of learning a hypothesis within a hypothesis space, we choose (or learn) a hypothesis space within a set of candidate hypothesis spaces.The quality of a given hypothesis space is measured by the validation error (6.6).To determine the validation error of a hypothesis space, we first learn the hypothesis h ∈ H via ERM (6.4) on the training set.Then, we obtain the validation error as the average loss of h on the validation set.The final hypothesis h delivered by the model selection Algorithm 7 not only depends on the training set used in ERM (see (6.9)).This hypothesis h has also been chosen based on its validation error which is the average loss on the validation set in (6.11).Indeed, we compared this validation error with the validation errors of other models to pick the model H ( l) (see step 10) which contains h.Since we used the validation error (6.11) of h to learn it, we cannot use this validation error as a good indicator for the general performance of h.To estimate the general performance of the final hypothesis h delivered by Algorithm 7 we must try it out on a test set.The test set, which is constructed in step 3 of Algorithm 7, consists of data points that are neither contained in the training set (6.9) nor the validation set (6.11) used for training and validating the candidate models H (1) , . . ., H (M ) .The average loss of the final hypothesis on the test set is referred to as the test error.The test error is computed in the step 12 of Algorithm 7.Sometimes it is beneficial to use different loss functions for the training and the validationInput: list of candidate models H (1) , . . ., H (M ) , loss function L, dataset D = x (1) , y (1) , . . ., x (m) , y (m) ; number k of folds, test set fraction ρ 1: randomly shuffle the data points in D 2: determine size m := ρm of test set 3: construct a test set D (test) = x (1) , y (1) , . . ., x (m ) , y (m ) 4: construct a training set and a validation set, 11: define optimal hypothesis h = h ( l) 12: compute test error)of a hypothesis.As an example, consider logistic regression and the SVM which have been discussed in Sections 3.6 and 3.7, respectively.Both methods use the same model which is the space of linear hypothesis maps h(x) = w T x.The main difference between these two methods is in their choice for the loss function.Logistic regression minimizes the (average) logistic loss (2.12) on the training set to learn the hypothesis h (1) (x) = w (1) T x with a parameter vector w (1) .The SVM instead minimizes the (average) hinge loss (2.11) on the training set to learn the hypothesis h (2) (x) = w (2) T x with a parameter vector w (2) .It is inconvenient to compare the usefulness of the two hypotheses h (1) (x) and h (2) (x) using different loss functions to compute their validation errors.This comparison is more convenient if we instead compute the validation errors for h (1) (x) and h (2) (x) using the average 0/1 loss (2.9).Algorithm 7 requires as one of its inputs a given list of candidate models.The longer this list, the more computation is required from Algorithm 7. Sometimes it is possible to prune the list of candidate models by removing models that are very unlikely to have minimum validation error.Consider polynomial regression which uses as the model the space H (r) poly of polynomials with maximum degree r (see (3.4)).For r = 1, Hpoly is the space of polynomials with maximum degree one (which are linear maps),poly is the space of polynomials with maximum degree two, h(x) = w 3 x 2 + w 2 x + w 1 .The polynomial degree r parametrizes a nested set of models, Hpoly ⊂ H (r) poly ⊂ . . . .For each degree r, we learn a hypothesis h (r) ∈ H (r) poly with minimum average loss (training error) E (r) t on a training set (see (6.5)).To validate the learnt hypothesis h (r) , we compute its average loss (validation error) E (r) v on a validation set (see (6.6)).poly ⊂ H (5) poly since any polynomial with degree not exceeding 3 is also a polynomial with degree not exceeding 5. Therefore, the training error (6.5) obtained when minimizing over the larger model H (5) poly can only decrease but never increase compared to (6.5) using the smaller model H  t .Starting with degree r = 0, the validation error first decreases with increasing degree r.As soon as the degree r is increased beyond a critical value, the validation error starts to increase with increasing r.For very large values of r, the training error becomes almost negligible while the validation error becomes very large.In this regime, polynomial regression overfits the training set. Figure 6.6 illustrates the overfitting of polynomial regression when using a maximum degree that is too large.In particular, Figure 6.6 depicts a learnt hypothesis which is a degree 9 polynomial that fits quite well the training set, resulting in a small training error.To achieve such a low training error, requires the learnt polynomial to have an unreasonable high rate of change for feature values x ≈ 0. This results in large prediction errors for data points with feature values x ≈ 0.More Data Beats Clever Algorithms ?;More Data Beats Clever Feature Selection?A key challenge in ML is to ensure that a hypothesis that predicts well the labels on a training set (which has been used to learn that hypothesis) will also predict well the labels of data points outside the training set.We say that a ML method generalizes well if it learns a hypothesis h that performs not significantly worse on data points outside the training set.In other words, the loss incurred by h for data points outside the training set is not much larger than the average loss of h incurred on the training set.We now study the generalization of linear regression methods (see Section 3.1) using an i.i.d.assumption.In particular, we interpret data points as i.i.d.realizations of RVs that have the same distribution as a random data point z = (x, y).The feature vector x is then a realization of a standard Gaussian RV with zero mean and covariance being the identity matrix, i.e., x ∼ N (0, I).The label y of a random data point is related to its features x via a linear Gaussian model .13)We assume the noise variance σ 2 fixed and known.This is a simplifying assumption and in practice we would need to estimate the noise variance from data [24].Note that, within our probabilistic model, the error component ε in ( 6.13) is intrinsic to the data and cannot be overcome by any ML method.We highlight that the probabilistic model for the observed data points is just a modelling assumption.This assumption allows us to study some fundamental behaviour of ML methods.There are principled methods ("statistical tests") that allow to determine if a given dataset can be accurately modelled using (6.13) [62].We predict the label y from the features x using a linear hypothesis h(x) that depends only on the first l features x 1 , . . ., x l .Thus, we use the hypothesis spaceNote that each element h (w) ∈ H (l) corresponds to a particular choice of the parameter vector w ∈ R l .The model parameter l ∈ {0, . . ., n} coincides with the effective dimension of the hypothesis space H (l) .For l < n, the hypothesis space H (l) is a proper (strict) subset of the space of linear hypothesis maps (2.4) used within linear regression (see Section 3.1).Moreover, the parameter l indexes a nested sequence of models,The quality of a particular predictor h (w) ∈ H (l) is measured via the average squared error L(h (w) | D (train) ) incurred on the labeled training set D (train) = { x (1) , y (1) , . . ., x (mt) , y (mt) }. (6.15)We interpret data points in the training set D (train) as well as any other data point outside the training set as realizations of i.i.d.RVs with a common probability distribution.This common probability distribution is a multivariate normal (Gaussian) distribution,x, x (i) i.i.d. with x, x (i) ∼ N (0, I).(6.16)The labels y (i) , y are related to the features of data points via (see (6.13)) i) , and y = w T x + ε. (6.17)Here, the noise terms ε, ε (i) ∼ N (0, σ 2 ) are realizations of i.i.d.Gaussian RVs with zero mean and variance σ 2 .Chapter 4 showed that the training error L(h (w) | D (train) ) is minimized by the predictor h ( w) (x) = w T I l×n x, that uses the parameter vector w = X (l) T X (l) −1 X (l) T y. (6.18)Here we used the (restricted) feature matrix X (l) and the label vector y defined as, respectively, 1) , . . ., x (mt) ) T I n×l ∈ R mt×l , andIt will be convenient to tolerate a slight abuse of notation and denote both, the length-l vector (6.18) as well as the zero-padded parameter vector w T , 0 T T ∈ R n , by w.This allows us to write .20)We highlight that the formula (6.18) for the optimal weight vector w is only valid if the matrix X (l) T X (l) is invertible.Within our toy model (see (6.16)), this is true with probability one whenever m t ≥ l.Indeed, for m t ≥ l the truncated feature vectors I l×n x (1) , . . ., I l×n x (mt) , which are i.i.d.realizations of a Gaussian RV, are linearly independent with probability one [9,41].In what follows, we consider the case m t > l such that the formula (6.18) is valid (with probability one).The more challenging high-dimensional regime m t ≤ l will be studied in Chapter 7.The optimal parameter vector w (see (6.18)) depends on the training set D (train) via the feature matrix X (l) and label vector y (see (6.19)).Therefore, since we model the data points in the training set as realizations of RVs, the parameter vector w ( 6.18) is the realization of a RV.For each specific realization of the training set D (train) , we obtain a specific realization of the optimal parameter vector w.The probabilistic model (6.13) relates the features x of a data point to its label y via some (unknown) true parameter vector w.Intuitively, the best linear hypothesis would be h(x) = w T x with parameter vector w = w.However, in general this will not be achievable since we have to compute w based on the features x (i) and noisy labels y (i) of the data points in the training set D.The parameter vector w delivered by ERM (4.5) typically results in a non-zero estimation error ∆w := w − w. (6.21)The estimation error ( 6.21) is the realization of a RV since the learnt parameter vector w (see (6.18)) is itself a realization of a RV.The Bias and Variance Decomposition.The prediction accuracy of h ( w) , using the learnt parameter vector (6.18), depends crucially on the mean squared estimation error (MSEE)We will next decompose the MSEE E est into two components, which are referred to as a variance term and a bias term.The variance term quantifies the fluctuation of the parameter vector obtained from ERM on the training set (6.15).The bias term characterizes the systematic (or average) deviation between the true parameter vector w (see (6.13)) and the expectation of the learnt parameter vector w.Let us start with rewriting (6.22) using elementary manipulations asWe can develop the last expression further by expanding the squared Euclidean norm,The first component in (6.23) represents the (expected) variance of the learnt parameter vector w (6.18).Note that, within our probabilistic model, the training set (6.15) is the realization of a RV since it is constituted by data points that are i.i.d.realizations of RVs (see (6.16) and (6.13)).The second component in (6.23) is referred to as a bias term.The parameter vector w is computed from a randomly fluctuating training set via ( 6.18) and is therefore itself fluctuating around its expectation E w}.The bias term is the Euclidean distance between this expectation E w} and the true parameter vector w relating features and label of a data point via (6.13).The bias term B 2 and the variance V in (6.23) both depend on the model complexity parameter l but in a fundamentally different manner.The bias term B 2 typically decreases with increasing l while the variance V increases with increasing l.In particular, the bias term is given asThe bias term (6.24) is zero if and only ifThe necessary and sufficient condition (6.25) for zero bias is equivalent to h (w) ∈ H (l) .Note that the condition (6.25) depends on both, the model parameter l and the true parameter vector w.While the model parameter l is under control, the true parameter vector w is not under our control but determined by the underlying data generation process.The only way to ensure (6.25) for every possible parameter vector w in (6.13) is to use l = n, i.e., to use all available features x 1 , . . ., x n of a data point.When using the model H (l) with l < n, we cannot guarantee a zero bias term since we have no control over the true underlying parameter vector w in (6.13).In general, the bias term decreases with an increasing model size l (see Figure 6.7).We highlight that the bias term does not depend on the variance σ 2 of the noise ε in our toy model (6.13).Let us now consider the variance term in (6.23).Using the statistical independence of the features and labels of data points (see (6.13), (6.16) and (6.17)), one can show that 1By (6.16), the matrixis a realization of a (matrix-valued) RV with an inverse Wishart distribution [91].For m t > l + 1, its expectation is given as By inserting (6.27) and tr{I} = l into (6.26),The variance (6.28) typically increases with increasing model complexity l (see Figure 6.7).In contrast, the bias term (6.24) decreases with increasing l.The opposite dependence of variance and bias on the model complexity results in a biasvariance trade-off.Choosing a model (hypothesis space) with small bias will typically result in large variance and vice versa.In general, the choice of model must balance between a small variance and a small bias.Generalization.Consider a linear regression method that learns the linear hypothesis h(x) = w T x using the parameter vector (6.18).The parameter vector w T (6.18) results in a linear hypothesis with minimum training error, i.e., minimum average loss on the training set.However, the ultimate goal of ML is to find a hypothesis that predicts well the label of any data point.In particular, we want the hypothesis h(x) = w T x to generalize well to data points outside the training set.We quantify the generalization capability of h(x) = w T x by its expected prediction lossNote that E pred is a measure for the performance of a ML method and not of a specific hypothesis.Indeed, the learnt parameter vector w is not fixed but depends on the data points in the training set.These data points are modelled as realizations of i.i.d.RVs and, in turn, the learnt parameter vector w becomes a realization of a RV.Thus, in some sense, the expected prediction loss (6.29) characterizes the overall ML method that reads in a training set and delivers (learn) a linear hypothesis with parameter vector w (6.18).In contrast, the risk (4.1) introduced in Chapter 4 characterizes the performance of a specific (fixed) hypothesis h without taking into account a learning process that delivered h based on data.Let us now relate the expected prediction loss (6.29) of the linear hypothesis h(x) = w T x to the bias and variance of ( 6.18),Here, step (a) uses the law of iterated expectation (see, e.g., [9]).Step (b) uses that the feature vector x of a "new" data point is a realization of a RV which is statistically independent of the data points in the training set D (train) .We also used our assumption that x is the realization of a RV with zero mean and covariance matrix E{xx T } = I (see (6.16)).According to (6.30), the average (expected) prediction error E pred is the sum of three components: (i) the bias B 2 , (ii) the variance V and (iii) the noise variance σ 2 .Figure 6.7 illustrates the typical dependency of the bias and variance on the model ( 6.14), which is parametrized by the model complexity l.Note that the model complexity parameter l in ( 6.14) coincides with the effective model dimension d eff H (l) (see Section 2.2).The bias and variance, whose sum is the estimation error E est , can be influenced by varying the model complexity l which is a design parameter.The noise variance σ 2 is the intrinsic accuracy limit of our toy model (6.13) and is not under the control of the ML engineer.It is impossible for any ML method (no matter how computationally expensive) to achieve, on average, a prediction error smaller than the noise variance σ 2 .Carefully note that this statement only applies if the data points arising in a ML application can be (reasonably well) modelled as realizations of i.i.d.RVs.We highlight that our statistical analysis, resulting the formulas for bias (6.24), variance (6.28) and the average prediction error (6.30), applies only if the observed data points can be well modelled using the probabilistic model specified by (6.13), (6.16) and (6.17).The validity of this probabilistic model can to be verified by principled statistical model validation techniques [159,146].Section 6.5 discusses a fundamentally different approach to analyzing the statistical properties of a ML method.Instead of a probabilistic model, this approach uses random sampling techniques to synthesize i.i.d.copies of given (small) data points.We can approximate the expectation of some relevant quantity, such as the loss L x, y , h , using an average over synthetic data [58].The qualitative behaviour of estimation error in Figure 6.7 depends on the definition for the model complexity.Our concept of effective dimension (see Section 2.2) coincides with most other notions of model complexity for the linear hypothesis space (6.14).However, for more complicated models such as deep nets it is often not obvious how effective dimension is related to more tangible quantities such as the total number of tunable weights or the number of artificial neurons.Indeed, the effective dimension might also depend on the specific learning algorithm such as SGD.Therefore, for deep nets, if we would plot estimation error against the number of tunable weights we might observe a behaviour of estimation error fundamentally different from the shape in Figure 6. 7. One example for such un-intuitive behaviour is known as "double descent phenomena" [8].6.5 The Bootstrap basic idea of bootstrap: use histogram of dataset as the underlying probability distribution; generate new data points by random sampling (with replacement) from that distribution.Consider learning a hypothesis h ∈ H by minimizing the average loss incurred on a dataset D = { x (1) , y (1) , . . ., x (m) , y (m) }.The data points x (i)) , y (i) are modelled as realizations of i.i.d.RVs.Let use denote the (common) probability distribution of these RVs by p(x, y).If we interpret the data points x (i)) , y (i) as realizations of RVs, also the learnt hypothesis h is a realization of a RV.Indeed, the hypothesis h is obtained by solving an optimization problem (4.3) that involves realizations of RVs.The bootstrap is a method for estimating (parameters of) the probability distribution p( h) [58].Section 6.4 used a probabilistic model for data points to derive (the parameters of) the probability distribution p( h).Note that the analysis in Section 6.4 only applies to the specific probabilistic model (6.16), (6.17).In contrast, the bootstrap can be used for data points drawn from an arbitrary probability distribution.The core idea behind the bootstrap is to use the histogram p(z) of the data points in D to generate B new datasets D (1) , . . ., D (B) .Each dataset is constructed such that is has the same size as the original dataset D. For each dataset D (b) , we solve a separate ERM (4.3) to obtain the hypothesis h (b) .The hypothesis h (b) is a realization of a RV whose distribution is determined by the histogram p(z) as well as the hypothesis space and the loss function used in the ERM (4.3).diagnose ML methods by comparing training error with validation error and (if available) some baseline; baseline can be obtained via the Bayes risk when using a probabilistic model (such as the i.i.d.assumption) or human performance or the performance of existing ML methods ("experts" in regret framework)In what follows, we tacitly assume that data points can (to a good approximation) be interpreted as realizations of i.i.d.RVs (see Section 2. 1.4).This "i.i.d.assumption" underlies ERM (4.3) as the guiding principle for learning a hypothesis with small risk (4.1).This assumption also motivates to use the average loss (6.6) on a validation set as an estimate for the risk.More fundamentally, we need the i.i.d.assumption to define the concept of risk as a measure for how well a hypothesis predicts the labels of arbitrary data points.Consider a ML method which uses Algorithm 5 (or Algorithm 6) to learn and validate the hypothesis h ∈ H.Besides the learnt hypothesis h, these algorithms also deliver the training error E t and the validation error E v .As we will see shortly, we can diagnose ML methods to some extent just by comparing training with validation errors.This diagnosis is further enabled if we know a baseline E (ref) .One important source for a baseline E (ref) are probabilistic models for the data points (see Section 6.4).Given a probabilistic model, which specifies the probability distribution p(x, y) of the features and label of data points, we can compute the minimum achievable risk (4.1).Indeed, the minimum achievable risk is precisely the expected loss of the Bayes estimator h(x) of the label y, given the features x of a data point.The Bayes estimator h(x) is fully determined by the probability distribution p(x, y) of the features and label of a (random) data point [85,Chapter 4].When using the squared error loss (2.8) to define the risk (4.1), the Bayes estimator is given by the posterior mean h(x) = E{y|x}.A further potential source for a baseline E (ref) is an existing, but for some reason unsuitable, ML method.This existing ML method might be computationally too expensive to be used for the ML application at end.However, we might still use its statistical properties as a benchmark.TheWe might also use the performance of human experts as a baseline.If we want to develop a ML method that detects certain type of skin cancers from images of the skin, a benchmark might be the current classification accuracy achieved by experienced dermatologists [36].We can diagnose a ML method by comparing the training error E t with the validation error E v and (if available) the benchmark E (ref) .The training error is on the same level as the validation error and the benchmark error.There is not much to improve here since the validation error is already on the desired error level.Moreover, the training error is not much smaller than the validation error which indicates that there is no overfitting.It seems we have obtained a ML method that achieves the benchmark error level.The validation error is significantly larger than the training error.It seems that the ERM (4.3) results in a hypothesis h that overfits the training set.The loss incurred by h on data points outside the training set, such as those in the validation set, is significantly worse.This is an indicator for overfitting which can be addressed either by reducing the effective dimension of the hypothesis space or by increasing the size of the training set.To reduce the effective dimension of the hypothesis space we have different options depending on the used model.We might use a small number of features in a linear model (3.1), a smaller maximum depth of decision trees (Section 3.10) or a fewer layers in an ANN (Section 3.11).One very elegant means for reducing the effective dimension of a hypothesis space is by limiting the number of GD steps used in gradient-based methods.This optimization based shrinking of a hypothesis space is referred to as early stopping.More generally, we can reduce the effective dimension of a hypothesis space via regularization techniques (see Chapter 7).The training error is on the same level as the validation error and both are significantly larger than the baseline.Since the training error is not much smaller than the validation error, the learnt hypothesis seems to not overfit the training set.However, the training error achieved by the learnt hypothesis is significantly larger than the benchmark error level.There can be several reasons for this to happen.First, it might be that the hypothesis space used by the ML method is too small, i.e., it does not include a hypothesis that provides a good approximation for the relation between features and label of a data point.The remedy for this situation is to use a larger hypothesis space, e.g., by including more features in a linear model, using higher polynomial degrees in polynomial regression, using deeper decision trees or having larger ANNs (deep nets).Another reason for the training error being too large is that the optimization algorithm used to solve ERM (4.3) is not working properly.When using gradient-based methods (see Section 5.4) to solve ERM, one reason for E t E (ref) could be that the learning rate α in the GD step (5.6) is chosen too small or too large (see Figure 5.3-(b)).This can be solved by adjusting the learning rate by trying out several different values and using the one resulting in the smallest training error.Another option is derive optimal values for the learning rate based on a probabilistic model for how the data points are generated.One example for such a probabilistic model is the i.i.d.assumption that has been used in Section 6.4 to analyze linear regression methods.The training error is significantly larger than the validation error (see Exercise 6.2).The idea of ERM (4.3) is to approximate the risk (4.1) of a hypothesis by its average loss on a training set D = {(x (i) , y (i) )} m i=1 .The mathematical underpinning for this approximation is the law of large numbers which characterizes the average of (realizations of) i.i.d.RVs.The quality and usefulness of this approximation depends on the validity of two conditions.First, the data points used for computing the average loss should be such that they would be typically obtained as realizations of i.i.d.RVs with a common probability distribution.Second, the number of data points used for computing the average loss must be sufficiently large.Whenever the data points behave different than the the realizations of i.i.d.RVs or if the size of the training set or validation set is too small, the interpretation (and comparison) of the training error and the validation error of a learnt hypothesis becomes more difficult.As an extreme case, it might then be that the validation error consists of data points for which every hypothesis incurs small average loss.Here, we might try to increase the size of the validation set by collecting more labeled data points or by using data augmentation (see Section 7.3).If the size of training set and validation set are large but we still obtain E t E v , one should verify if data points in these sets conform to the i.i.d.assumption.There are principled statistical test for the validity of the i.i.d.assumption for a given dataset (see [88] and references therein).x of the (zero-mean) feature, the variance σ 2x of the (zero-mean) label and the covariance between feature and label of a random data point.How many data points do we need to include in a validation set such that with probability of at least 0.8 the validation error of a given hypothesis h does not deviate by more than 20 percent from its expected loss?Exercise 6. 4. Too many Features?Consider data points that are characterized by n = 1000 numeric features x 1 , . . ., x n ∈ R and a numeric label y ∈ R. We want to learn a linear hypothesis map h(x) = w T x for predicting the label of a data point based on its features.Could it be beneficial to constrain the learnt hypothesis by requiring it to only depend on the first 5 features of a data point?Exercise 6. 5. Benchmark via Probability Theory.Consider data points that are characterized with single numeric feature x and label y.We model the feature and label of a data point as i.i.d.realizations of a Gaussian random vector z ∼ N (0, C) with zero mean and covariance matrix C. The optimal hypothesis h(x) to predict the label y given the feature x is the conditional expectation of the (unobserved) label y given the (observed) feature x.How is the expected squared error loss of this optimal hypothesis (which is the Bayes estimator) related to the covariance matrix C of the Gaussian random vector z.Keywords: Data Augmentation.Robustness.Semi-Supervised Learning.Transfer Learning.Multitask Learning.label y feature x (x (1) , y (1) ) (x (2) , y (2) ) h(x)   7.1 illustrates a typical scenario for a modern ML method which uses a large hypothesis space.This large hypothesis space includes highly non-linear maps which can perfectly resemble any dataset of modest size.However, there might be non-linear maps for which a small training error does not guarantee accurate predictions for the labels of data points outside the training set.Chapter 6 discussed validation techniques to verify if a hypothesis with small training error will predict also well the labels of data points outside the training set.These validation techniques, including Algorithm 5 and Algorithm 6, probe the hypothesis h ∈ H delivered by ERM on a validation set.The validation set consists of data points which have not been used in the training set of ERM (4.3).The validation error, which is the average loss of the hypothesis on the data points in the validation set, serves as an estimate for the average error or risk (4.1) of the hypothesis h.This chapter discusses regularization as an alternative to validation techniques.In contrast to validation, regularization techniques do not require having a separate validation set which is not used for the ERM (4.3).This makes regularization attractive for applications where obtaining a separate validation set is difficult or costly (where labelled data is scarce).Instead of probing a hypothesis h on a validation set, regularization techniques estimate (or approximate) the loss increase when applying h to data points outside the training set.The loss increase is estimated by adding a regularization term to the training error in ERM (4.3).Section 7.1 discusses the resulting regularized ERM, which we will refer to as SRM.It turns out that the SRM is equivalent to ERM using a smaller (pruned) hypothesis space.The amount of pruning depends on the weight of the regularization term relative to the training error.For an increasing weight of the regularization term, we obtain a stronger pruning resulting in a smaller effective hypothesis space.Section 7.2 constructs regularization terms by requiring the resulting ML method to be robust against (small) random perturbations of the data points in a training set.Here, we replace each data point of a training set by the realization of a RV that fluctuates around this data point.This construction allows to interpret regularization as a (implicit) form of data augmentation.Section 7.3 discusses data augmentation methods as a simulation-based implementation of regularization.Data augmentation adds a certain number of perturbed copies to each data point in the training set.One way to construct perturbed copies of a data point is to add the realization of a RV to its features.Section 7.4 analyzes the effect of regularization for linear regression using a simple probabilistic model for data points.This analysis parallels our previous study of the validation error of linear regression in Section 6. 4. Similar to Section 6.4, we reveal a trade-off between the bias and variance of the hypothesis learnt by regularized linear regression.This trade-off was traced out by a discrete model parameter (the effective dimension) in Section 6. 4. In contrast, regularization offers a continuous trade-off between bias and variance via a continuous regularization parameter.Semi-supervised learning (SSL) uses (large amounts of) unlabeled data points to support the learning of a hypothesis from (a small number of) labeled data points [21].Section 7.5 discusses SSL methods that use the statistical properties of unlabeled data points to construct useful regularization terms.These regularization terms are then used in SRM with a (typically small) set of labeled data points.Multitask learning exploits similarities between different but similar learning tasks [19].We can formally define a learning task by a particular choice for the loss function (see Section 2.3) .The primary role of a loss function is to score the quality of a hypothesis map.However, the loss function also encapsulates the choice for the label of a data point.For learning tasks defined for a single underlying data generation process it is reasonable to assume that the same subset of features is relevant for those learning tasks.One example for a ML application involving several similar learning tasks is multi-label classification (see Section 2. 1.2).Each individual label of a data point represents a separate learning task.Section 7.6 shows how multitask learning can be implemented using regularization methods.The loss incurred in different learning tasks serves mutual regularization terms in a joint SRM for all learning tasks.Section 7.7 shows how regularization can be used for transfer learning.Like multitask learning also transfer learning exploits relations between different but similar learning tasks.In contrast to multitask learning, which jointly solves the individual learning tasks, transfer learning solves the learning tasks sequentially.One example of transfer learning is to fine tune a pre-trained model.A pre-trained model can be obtained via ERM (4.3) in a ("source") learning task for which we have a large amount of labeled training data.The fine-tuning is then obtained via ERM (4.3) in the ("target") learning task of interest for which we might have only a small amount of labeled training data.Section 2.2 defined the effective dimension d eff (H) of a hypothesis space H as the maximum number of data points that can be perfectly fit by some hypothesis h ∈ H.As soon as the effective dimension of the hypothesis space in (4.3) exceeds the number m of training data points, we can find a hypothesis that perfectly fits the training data.However, a hypothesis that perfectly fits the training data might deliver poor predictions for data points outside the training set (see Figure 7.1).Modern ML methods typically use a hypothesis space with large effective dimension [151,17].Two well-known examples for such methods is linear regression (see Section 3.1) using a large number of features and deep learning with ANNs using a large number (billions) of artificial neurons (see Section 3.11).The effective dimension of these methods can be easily on the order of billions (10 9 ) if not larger [126].To avoid overfitting during the naive use of ERM ( 4.3) we would require a training set containing at least as many data points as the effective dimension of the hypothesis space.However, in practice we often do not have access to a training set consisting of billions of labeled data points.The challenge is typically in the labelling process which often requires human labour.It seems natural to combat overfitting of a ML method by pruning its hypothesis space H.We prune H by removing some of the hypothesis in H to obtain the smaller hypothesis space H ⊂ H.We then replace ERM (4.3) with the restricted (or pruned) ERM h = argmin h∈H L(h|D) with pruned hypothesis space H ⊂ H.(7.1)The effective dimension of the pruned hypothesis space H is typically much smaller than the effective dimension of the original (large) hypothesis space H, d eff (H ) d eff (H).For a given size m of the training set, the risk of overfitting in (7.1) is much smaller than the risk of overfitting in (4.3).Let us illustrate the idea of pruning for linear regression using the hypothesis space (3.1) constituted by linear maps h(x) = w T x.The effective dimension of (3.1) is equal to the number of features, d eff (H) = n.The hypothesis space H might be too large if we use a large number n of features, leading to overfitting.We prune (3.1) by retaining only linear hypothesis maps h(x) = w T x whose weight vector w satisfies w 3 = w 4 = . . .= w n = 0.Thus, the hypothesis space H is constituted by all linear maps that only depend on the first two featuresPruning the hypothesis space is a special case of a more general strategy which some authors refer to as SRM [145].The idea behind SRM is to modify the training error in ERM (4.3) to favour a hypothesis that is more smooth or regular.By steering ML methods towards learning a smooth hypothesis, we make it less sensitive, or more robust, to small perturbations of data points in the training set.Section 7.2 discusses the intimate relation between the robustness (against perturbations of the data points in the training set) of a ML method and its ability to generalize to data points outside the training set.We measure the smoothness of a hypothesis using a regularizer R(h) ∈ R + .The value R(h) is a quantitative measure for the non-smoothness or irregularity of the hypothesis h.The (design) choice for the regularizer depends on the precise definition of what is meant by regularity or variation of a hypothesis.Section 7.3 discusses how a particular choice for the regularizer R(h) arises naturally from a probabilistic model for data points.We obtain SRM by adding the scaled regularizer λR(h) to the ERM (4.3) ,h∈HWe can interpret the penalty term λR(h) in (7.2) as an estimate (or approximation) for the increase, relative to the training error on D, of the average loss of a hypothesis h when applied to data points outside D. Another interpretation of the term λR(h) will be discussed in Section 7. 3.The regularization parameter λ allows us to trade between a small training error L(h (w) |D) and small regularization term R(h), which enforces smoothness or regularity of h.If we choose a large value for λ, we heavily punish any "irregular" hypothesis h with large R(h) (see (7.2)).Thus, increasing the value of λ results in the solution (minimizer) of ( 7.2) having smaller R(h).On the other hand, choosing a small value for λ in (7.2) puts more emphasis on obtaining a hypothesis h incurring a small training error.For the extreme case λ = 0, the SRM (7.2) reduces to ERM (4.3).The pruning approach (7.1) is intimately related to the SRM (7.2).They are, in a certain sense, dual to each other.First, note that (7.2) reduces to the pruning approach (7.1) when using the regularizer R(h) = 0 for all h ∈ H , and R(h) = ∞ otherwise, in (7.2).In the other direction, for many important choices for the regularizer R(h), there is a restriction λ = 0 H (λ) ⊂ H such that the solutions of (7.1) and ( 7.2) coincide (see Figure 7.2).The relation between the optimization problems (7.1) and ( 7.2) can be made precise using the theory of convex duality (see [14,Ch. 5] and [10]).For a hypothesis space H whose elements h ∈ H are parametrized by a parameter vector w ∈ R n , we can rewrite SRM (7.2) asFor the particular choice of squared error loss (2.8), linear hypothesis space (3.1) and regularizer R(w) = w 2 2 , SRM (7.3) specializes toThe special case (7.4) of SRM ( 7.3) is known as ridge regression [58].Ridge regression (7.4) is equivalent to (see [10,Ch. 5])with the restricted hypothesis spaceFor any given value λ of the regularization parameter in (7.4), there is a number C(λ) such that solutions of (7.4) coincide with the solutions of (7.5).Thus, ridge regression (7.4) is equivalent to linear regression with a pruned version H (λ) of the linear hypothesis space (3.1).The size of the pruned hypothesis space H (λ) (7.6) varies continuously with λ.Another popular special case of ERM (7.3) is obtained for the regularizer R(w) = w 1 and known as the Lasso [59]Ridge regression (7.4) and the Lasso (7.7) have fundamentally different computational and statistical properties.Ridge regression (7.4) uses a smooth and convex objective function that can be minimized using efficient GD methods.The objective function of Lasso (7.7) is also convex but non-smooth and therefore requires more advanced optimization methods.The increased computational complexity of Lasso (7.7) comes at the benefit of typically delivering a hypothesis with a smaller expected loss than those obtained from ridge regression [17,59].Section 7.1 motivates regularization as a soft variant of model selection.Indeed, the regularization term in SRM (7.2) is equivalent to ERM (7.1) using a pruned hypothesis space.We now discuss an alternative view on regularization as a means to make ML methods robust.The ML methods discussed in Chapter 4 rest on the idealizing assumption that we have access to the true label values and feature values of labeled data points (that form a training set).These methods learn a hypothesis h ∈ H with minimum average loss (training error) incurred for data points in the training set.In practice, the acquisition of label and feature values might be prone to errors.These errors might stem from the measurement device itself (hardware failures or thermal noise in electronic devices) or might be due to human mistakes such as labelling errors.Let us assume for the sake of exposition that the label values y (i) in the training set are accurate but that the features x (i) are a perturbed version of the true features of the ith data point.Thus, instead of having observed the data point x (i) , y (i) we could have equally well observed the data point x (i) + ε, y (i) in the training set.Here, we have modelled the perturbations in the features using a RV ε.The probability distribution of the perturbation ε is a design parameter that controls robustness properties of the overall ML method.We will study a particular choice for this distribution in Section 7. 3.A robust ML method should learn a hypothesis that incurs a small loss not only for a specific data point x (i) , y (i) but also for perturbed data points x (i) + ε, y (i) .Therefore, it seems natural to replace the loss L x (i) , y (i) , h , incurred on the ith data point in the training set, with the expectationThe expectation (7.8) is computed using the probability distribution of the perturbation ε.We will show in Section 7.3 that minimizing the average of the expectation (7.8), for i = 1, . . ., m, is equivalent to the SRM (7.2).Using the expected loss (7.8) is not the only possible approach to make a ML method robust.Another approach to make a ML method robust is known as bootstrap aggregation (bagging).The idea of bagging is to use the bootstrap method (see Section 6.5 and [58,Ch. 8]) to construct a finite number of perturbed copies D (1) , . . ., D (B) of the original training set D.We then learn (e.g, using ERM) a separate hypothesis h (b) for each perturbed copy D (b) , b = 1, . . ., B. This results in a whole ensemble of different hypotheses h (b) which might even belong to different hypothesis spaces.For example, one the hypothesis h (1) could be a linear map (see Section 3.1) and the hypothesis h (2) could be obtained from an ANN (see Section 3.11).The final hypothesis delivered by bagging is obtained by combining or aggregating (e.g., using the average) the predictions h (b) x delivered by each hypothesis h (b) , for b = 1, . . ., B in the ensemble.The ML method referred to as random forest uses bagging to learn an ensemble of decision trees (see Chapter 3.10).The individual predictions obtained from the different decision trees forming a random forest are then combined (e.g., using an average for numeric labels or a majority vote for finite-valued labels), to obtain a final prediction [58].ML methods using ERM (4.3) are prone to overfitting as soon as the effective dimension of the hypothesis space H exceeds the number m of data points in the training set.Section 6.3 and Section 7.1 approached this by modifying either the model or the loss function by adding a regularization term.Both approaches prune the hypothesis space H underlying a ML method to reduce the effective dimension d eff (H).Model selection does this reduction in a discrete fashion while regularization implements a soft "shrinking" of the hypothesis space.Instead of trying to reduce the effective dimension we could also try to increase the number m of data points in the training set used for ERM (4.3).We now discuss how to synthetically generate new labeled data points by exploiting statistical symmetries of data.The data arising in many ML applications exhibit intrinsic symmetries and invariances at least in some approximation.The rotated image of a cat still shows a cat.The temperature measurement taken at a given location will be similar to another measurement taken 10 milliseconds later.Data augmentation exploits such symmetries and invariances to augment the raw data with additional synthetic data.Let us illustrate data augmentation using an application that involves data points characterized by features x ∈ R n and number labels y ∈ R. We assume that the data generating process is such that data points with close feature values have the same label.Equivalently, this assumption is requiring the resulting ML method to be robust against small perturbations of the feature values (see Section 7.2).This suggests to augment a data point x, y by several synthetic data points x + ε (1) , y , . . ., x + ε (B) , y , (with ε (1) , . . ., ε (B) being realizations of i.i.d.random vectors with the same probability distribution p(ε).Given a (raw) dataset D = x (1) , y (1) , . . ., x (m) , y (m) } we denote the associated aug-mented dataset by 1) , y (1) , . . ., x (1,B) , y (1) ,x (2,1) , y (2) , . . ., x (2,B) , y (2) , . . . 1) , y (m) , . . ., x (m,B) , y (m) }.(7.10)The size of the augmented dataset D is m = B × m.For a sufficiently large augmentation parameter B, the augmented sample size m is larger than the effective dimension n of the hypothesis space H.We then learn a hypothesis via ERM on the augmented dataset,h∈HWe can interpret data-augmented ERM (7.11) as a data-driven form of regularization (see Section 7.1): For each data point x (i) , y (i) ∈ D in D, we replace the corresponding loss L (x (i) , y (i) ), h with the average loss (1/B) B b=1 L (x (i) + ε (b) , y (i) ), h .This average loss is computed over the augmented data points arising from x (i) , y (i) ∈ D.The actual implementation of (7.11) require to first generate B realizations ε (b) ∈ R n of i.i.d.RVs with common probability distribution p(ε).This generation might be computationally costly for large values of B or n.However, when using a large augmentation parameter B, we might use the approximationThis approximation is made precise by a key result of probability theory, known as the law of large numbers.We obtain an instance of ERM by inserting (7.12) into (7.11),The usefulness of (7.13) as an approximation to the augmented ERM (7.11) depends on the difficulty of computing the expectation E L (x (i) + ε, y (i) ), h .The complexity of computing this expectation depends on the choice of loss function and the choice for the probability distribution p(ε).Let us study (7.13) for the special case linear regression with squared error loss (2.8) and linear hypothesis space (3.1),We use perturbations ε drawn from a multivariate normal distribution with zero mean and covariance matrixWe develop (7.14) further by usingThe identity (7.16) uses that the data points x (i) , y (i) are fixed and known (deterministic) while ε is a zero-mean random vector.Combining (7.16) with (7.14), .17)where the last step used E ε 2= nσ 2 .Inserting (7.17) into (7.14),We have obtained (7.18) as an approximation of the augmented ERM (7.11) for the special case of squared error loss (2.8) and the linear hypothesis space (3.1).This approximation uses the law of large numbers (7.12) and becomes more accurate for increasing augmentation parameter B.Note that (7.18) is nothing but ridge regression (7.4) using the regularization parameter λ = nσ 2 .Thus, we can interpret ridge regression as implicit data augmentation (7.10) by applying random perturbations (7.9) to the feature vectors in the original training set D.The regularizer R(w) = w 2 2 in (7.18) arose naturally from the specific choice for the probability distribution (7.15) of the random perturbation ε (i) in (7.9) and using the squared error loss.Other choices for this probability distribution or the loss function result in different regularizers.Augmenting data points with random perturbations distributed according (7.15) treat the features of a data point independently.For application domains that generate data points with highly correlated features it might be useful to augment data points using random perturbations ε (see (7.9)) distributed asThe covariance matrix C of the perturbation ε can be chosen using domain expertise or estimated (see Section 7.5).Inserting the distribution (7.19) into (7.13),Note that (7.20) reduces to ordinary ridge regression (7.18) for the choice C = σ 2 I.The goal of this section is to develop a better understanding for the effect of the regularization term in SRM (7.3).We will analyze the solutions of ridge regression (7.4) which is the special case of SRM using the linear hypothesis space (3.1) and squared error loss (2.8).Using the feature matrix X = x (1) , . . ., x (m) T and label vector y = (y (1) , . . ., y (m) ) T , we can rewrite (7.4) more compactly asThe solution of (7.21) is given by [134, Sec.3.3.]For λ = 0, (7.22) reduces to the formula (6.18) for the optimal weights in linear regression (see (7.4) and (4.5)).Note that for λ > 0, the formula (7.22) is always valid, even when X T X is singular (not invertible).For λ > 0 the optimization problem (7.21) (and (7.4)) has the unique solution (7.22).To study the statistical properties of the predictor h ( w (λ) ) (x) = w (λ) T x (see (7.22)) we use the probabilistic toy model (6.13), (6.16) and ( 6.17) that we used already in Section 6. 4. We interpret the training data D (train) = {(x (i) , y (i) )} m i=1 as realizations of i.i.d.RVs whose distribution is defined by (6.13), (6.16) and (6.17).We can then define the average prediction error of ridge regression asAs shown in Section 6.4, the errorpred is the sum of three components: the bias, the variance and the noise variance σ 2 (see (6.30)).The bias of w (λ) isFor sufficiently large size m of the training set, we can use the approximationsuch that (7.24) can be approximated asLet us compare the (approximate) bias term (7.26) of ridge regression with the bias term (6.24) of ordinary linear regression (which is the extreme case of ridge regression with λ = 0).The bias term (7.26) increases with increasing regularization parameter λ in ridge regression (7.4).Sometimes the increase in bias is outweighed by the reduction in variance.The variance typically decreases with increasing λ as shown next.bias of w (λ) variance of w (λ) regularization parameter λ The variance of ridge regression (7.4) satisfiesInserting the approximation (7.25) into (7.27),According to (7.28), the variance of w (λ) decreases with increasing regularization parameter λ of ridge regression (7.4).This is the opposite behaviour as observed for the bias (7.26), which increases with increasing λ.By comparing the variance approximation (7.28) with the variance (6.28) of linear regression suggests to interpret the ratio n/(1+λ) 2 as an effective number of features used by ridge regression.Increasing the regularization parameter λ decreases the effective number of features.Figure 7.3 illustrates the trade-off between the bias B 2 (7.26) of ridge regression, which increases for increasing λ, and the variance V (7.28) which decreases with increasing λ.Note that we have seen another example for a bias-variance trade-off in Section 6. 4.This trade-off was traced out by a discrete (model complexity) parameter l ∈ {1, 2, . ..} (see (6.14)).In stark contrast to discrete model selection, the bias-variance trade-off for ridge regression is traced out by the continuous regularization parameter λ ∈ R + .The main statistical effect of the regularization term in ridge regression is to balance the bias with the variance to minimize the average prediction error of the learnt hypothesis.There is also a computational effect or adding a regularization term.Roughly speaking, the regularization term serves as a pre-conditioning of the optimization problem and, in turn, reduces the computational complexity of solving ridge regression (7.21).The objective function in (7.21) is a smooth (infinitely often differentiable) convex function.We can therefore use GD to solve (7.21) efficiently (see Chapter 5).Algorithm 8 summarizes the application of GD to (7.21).The computational complexity of Algorithm 8 depends crucially on the number of GD iterations required to reach a sufficiently small neighbourhood of the solutions to (7.21).Adding the regularization term λ w 2 2 to the objective function of linear regression speeds up GD.To verify this claim, we first rewrite (7.21) as the quadratic problem .29)This is similar to the quadratic optimization problem (4.9) underlying linear regression but with a different matrix Q.The computational complexity (number of iterations) required by GD (see (5.6)) to solve (7.29) up to a prescribed accuracy depends crucially on the condition number κ(Q) ≥ 1 of the psd matrix Q [68].The smaller the condition number κ(Q), the fewer iterations are required by GD.We refer to a matrix with a small condition number as being "well-conditioned".The condition number of the matrix Q in (7.29) is given byAccording to (7.30), the condition number κ(Q) tends to one for increasing regularization parameter λ,Thus, the number of required GD iterations in Algorithm 8 decreases with increasing regularization parameter λ.Consider the task of predicting the numeric label y of a data point z = x, y based on its feature vector x = x 1 , . . ., x n T ∈ R n .At our disposal are two datasets D (u) and D (l) .Input: dataset D = {(x (i) , y (i) )} m i=1 ; GD learning rate α > 0. Initialize:set w (0) := 0; set iteration counter r := 0 1: repeat 2:r := r + 1 (increase iteration counter) 3:)) 4: until stopping criterion met Output: w (r) (which approximates w (λ) in (7.21))For each datapoint in D (u) we only know the feature vector.We therefore refer to D (u) as "unlabelled data".For each datapoint in D (l) we know both, the feature vector x and the label y.We therefore refer to D (l) as "labeled data".SSL methods exploit the information provided by unlabelled data D (u) to support the learning of a hypothesis based on minimizing its empirical risk on the labelled (training) data D (l) .The success of SSL methods depends on the statistical properties of the data generated within a given application domain.Loosely speaking, the information provided by the probability distribution of the features must be relevant for the ultimate task of predicting the label y from the the features x [21].Let us design a SSL method, summarized in Algorithm 9 below, using the data augmentation perspective from Section 7. 3. The idea is to augment the (small) labeled dataset D (l) by adding random perturbations fo the features of each data point in D (l) .This is reasonable for applications where features are subject to inherent measurement or modelling errors.Given a data point with vector x we could have equally well observed a feature vector x + ε with some small random perturbation ε ∼ N (0, C).To estimate the covariance matrix matrix C, we use the sample covariance matrix of the feature vectors in the (large) unlabelled dataset D (u) .We then learn a hypothesis using the augmented (regularized) ERM (7.20).Consider a specific learning task of finding a hypothesis h with minimum (expected) loss L ((x, y), h).Note that the loss incurred by h for a specific data point depends on the definition for the label of a data point.We can obtain different learning tasks for the same data points by using different choices or definitions for the label of a data point.Multitask learning exploits the similarities between different learning tasks to jointly solve them.Let us next discuss a simple example of a multitask learning problem.Algorithm 9 A Semi-Supervised Learning Algorithmx (i) .(7.32)2: compute (e.g. using GD) .33)Consider a data point z representing a hand-drawing that is collected via the online game https://quickdraw.withgoogle.com/.The features of a data point are the pixel intensities of the bitmap which is used to store the hand-drawing.As label we could use the fact if a hand-drawing shows an apple or not.This results in the learning task T (1) .Another choice for the label of a hand-drawing could be the fact if a hand-drawing shows a fruit at all or not.This results in another learning task T (2) which is similar but different from the task T (1) .The idea of multitask learning is that a reasonable hypothesis h for some learning task should also do well for a similar learning task.Thus, we can use the loss incurred on similar learning tasks as a regularization term for learning a hypothesis for the learning task at hand.Algorithm 10 is a straightforward implementation of this idea for a given dataset that gives rise to T related learning tasks T (1) , . . ., T (T ) .For each individual learning task T (t ) it uses the loss on the remaining learning tasks T (t) , with t = t , as regularization term in (7.34).The applicability of Algorithm 10 is somewhat limited as it aims at finding a single hypothesis that does well for all T learning tasks simultaneously.For certain application domains it might be more reasonable to not learn a single hypothesis for all learning tasks but to learn a separate hypothesis h (t) for each learning task t = 1, . . ., T .However, these separate hypothesis maps typically might still share some structural similarities. 1We can Algorithm 10 A Multitask Learning Algorithm Input: dataset D = {z (1) , . . ., z (m) }; T learning tasks with loss functions L (1) , . . ., L (T ) , hypothesis space H 1: learn a hypothesis h viaOutput: hypothesis h enforce different notion of similarity between hypothesis maps h (t) by adding a regularization term to the individual loss functions of the learning tasks.Algorithm 11 generalizes Algorithms 10 by learning a separate hypothesis for each task t while requiring these hypotheses to be structurally similar.The structural (dis-)similarity between the hypothesis maps is measured by a regularization term R in (7.35).Input: dataset D = {z (1) , . . ., z (m) } with T associated learning tasks with loss functions L (1) , . . ., L (T ) , hypothesis space H 1: learn a hypothesis h via h (1) , . . ., h (T ) := argmin h (1) ,...,h (T ) ∈H T t=1 m i=1 L (t) z (i) , h (t) + λR h (1) , . . ., h (T ) .(7.35)Output: hypotheses h (1) , . . ., h (T )Regularization is also instrumental for transfer learning to capitalize on synergies between different related learning tasks [108,61].Transfer learning is enabled by constructing regularization terms for a learning task by using the result of a previous learning task.While multitask learning methods solve many related learning tasks simultaneously, transfer learning methods operate in a more sequential fashion.Let us illustrate the idea of transfer learning using two learning tasks which differ significantly in their intrinsic difficulty.Informally, we consider a learning task to be easy if sparse [34].we can easily gather large amounts of labeled (training) data for that task.Consider the learning task T (1) of predicting whether an image shows a cat or not.For this learning task we can easily gather a large training set D (1) using via image collections of animals.Another (related) learning task T (2) is to predict whether an image shows a cat of a particular breed, with a particular body height and with a specific age.The learning task T (2) is more dificult than T (1) since we have only a very limited amount of cat images for which we know the particular breed, body height and precise age of the depicted cat.Exercise 7.1.Ridge Regression is a Quadratic Problem.Consider the linear hypothesis space consisting of linear maps parameterized by weights w.We try to find the best linear map by minimizing the regularized average squared error loss (empirical risk) incurred on a training set D := (x (1) , y (1) ), (x (2) , y (2) ), . . ., (x (m) , y (m) ) .Ridge reression augments the average squared error loss on D by the regularizer w 2 , yielding the following learning problemIs it possible to rewrite the objective function f (w) as a convex quadratic function We could learn such a hypothesis by two approaches.The first approach is to split the dataset into a training set and a validation set.Then we consider all models that consists of linear hypotheses with weight vectors having at most two non-zero weights.Each of these models corresponds to a different subset of two weights that might be non-zero.Find the model resulting in the smallest validation errors (see Algorithm 5).Compute the average loss of the resulting optimal linear hypothesis on some data points that have neither been used in the training set nor the validation set.Compare this average loss ("test error") with the average loss obtained on the same data points by the hypothesis learnt by ridge regression (7.4).Chapter 8x (3) x (4) x (2) x (1) x (5) x (6) x (7) x gx r Figure 8.1:Each circle represents an image which is characterized by its average redness x r and average greenness x g .The i-th image is depicted by a circle located at the pointT ∈ R 2 .It seems that the images can be grouped into two clusters.So far we focused on ML methods that use the ERM principle and lean a hypothesis by minimizing the discrepancy between its predictions and the true labels on a training set.These methods are referred to as supervised methods as they require labeled data points for which the true label values have been determined by some human (who serves as a "supervisor").This and the following chapter discus ML methods which do not require to know the label of any data point.These methods are often referred to as "unsupervised" since they do not require a "supervisor" to provide the label values for any data point.One important family of unsupervised ML methods aim at clustering a given set of data points such as those depicted in Figure 8. 1.The basic idea of clustering is to decompose a set of data points into few subsets or clusters that consist of similar data points.For the dataset in Figure 8.1 it seems reasonable to define two clusters, one cluster x (1) , x (5) , x (6) , x (7) and a second cluster x (2) , x (3) , x (4) , x (8) .Formally, clustering methods learn a hypothesis that assign each data point either to precisely one cluster (see Section 8.1) or several clusters with different degrees of belonging (see Section 8.2).Different clustering methods use different measures for the similarity between data points.For data points characterized by (numeric) Euclidean feature vectors, the similarity between data points can be naturally defined in terms of the Euclidean distance between feature vectors.Section 8.3 discusses clustering methods that use notions of similarity that are not based on a Euclidean space.There is a strong conceptual link between clustering methods and the classification methods discussed in Chapter 3.Both type of methods learn a hypothesis that reads in the features of a data point and delivers a prediction for some quantity of interest.In classification methods, this quantity of interest is some generic label of a data point.For clustering methods, this quantity of interest for a data point is the cluster assignment (for hard clustering) of the degree of belonging (for soft clustering).A main difference between clustering and classification is that clustering methods do not require the true label (cluster assignment or degree of belonging) of a single data point.Classification methods learn a good hypothesis via minimizing their average loss incurred on a training set of labeled data points.In contrast, clustering methods do not have access to a single labeled data point.To find the correct labels (cluster assignments), clustering methods must rely solely on the intrinsic geometry of the data points.We will see that clustering methods use this intrinsic geometry to determine an empirical risk incurred by a candidate hypothesis (which maps data points to cluster assignments or degree of belonging).Thus, much like classification methods, also clustering implement the generic ERM principle (see Chapter 4) to find a good hypothesis (clustering).This chapter discusses two main flavours of clustering methods:• hard clustering (see Section 8.1)• and soft clustering methods (see Section 8.2).Hard clustering methods learn a hypothesis h that reads in the feature vector x of a data point and delivers a predicted cluster assignment ŷ = h(x) ∈ {1, . . ., k}.Thus, assigns each data point to one single cluster.Section 8.1 will discuss one of the most widely-used hard clustering algorithms which is known as k-means.In contrast to hard clustering methods, soft clustering methods assign each data point to several clusters with varying degree of belonging.These methods learn a hypothesis that delivers a vector ŷ = ŷ1 , . . ., ŷk T with entry ŷc ∈ [0, 1] being the predicted degree by which the data point belongs to the c-th cluster.Hard clustering is an extreme case of soft clustering where we enforce each degree of belonging to take only values in {0, 1}.Moreover, hard clustering requires that for each data point only of the corresponding degree of belonging (one for each cluster) is non-zero.The main focus of this chapter is on methods that require data points being represented by numeric feature vectors (see Sections 8.1 and 8.2).These methods define the similarity between data points using the Euclidean distance between their feature vectors.Some applications generate data points for which it is not obvious how to obtain numeric feature vectors such that their Euclidean distances reflect the similarity between data points.It is then desirable to use a more flexible notion of similarity which does not require to determine (useful) numeric feature vectors of data points.Maybe the most fundamental concept to represent similarities between data points is a similarity graph.The nodes of the similarity graph are the individual data points of a dataset.Similar data points are connected by edges (links) that might be assigned some weight that quantities the amount of similarity.Section 8.3 discusses clustering methods that use a graph to represent similarities between data points.Consider a dataset D which consists of m data points that are indexed by i = 1, . . ., m.The data points are characterized via their numeric feature vectors x (i) ∈ R n , for i = 1, . . ., m.It will be convenient for the following discussion if we identify a data point with its feature vector.In particular, we refer by x (i) to the i-th data point.Hard clustering methods decompose (or cluster) the dataset into a given number k of different clusters C (1) , . . ., C (k) .These methods assign each data point x (i) to one and only one cluster C (c) with the cluster index c ∈ {1, . . ., k}.Let us define for each data point its label y (i) ∈ {1, . . ., k} as the index of the cluster to which the ith data point actually belongs to.The c-th cluster consists of all data points with y (i) = c, C (c) := i ∈ {1, . . ., m} :We can interpret hard clustering methods as ML methods that compute predictions ŷ(i) for the ("correct") cluster assignments y (i) .The predicted cluster assignments result in the predicted clustersWe now discuss a hard clustering method which is known as k-means.This method does not require the knowledge of the label or (true) cluster assignment y (i) for any data point in D. This method computes predicted cluster assignments ŷ(i) based solely from the intrinsic geometry of the feature vectors x (i) ∈ R n for all i = 1, . . ., m.Since it does not require any labeled data points, k-means is often referred to as being an unsupervised method.However, note that k-means requires the number k of clusters to be given as an input (or hyper-) parameter.The k-means method represents the c-th cluster C (c) by a representative feature vector µ (c) ∈ R n .It seems reasonable to assign data points in D to clusters C (c) such that they are well concentrated around the cluster representatives µ (c) .We make this informal requirement precise by defining the clustering errorNote that the clustering error L (8.3) depends on both, the cluster assignments ŷ(i) , which define the cluster (8.2), and the cluster representatives µ (c) , for c = 1, . . ., k.Finding the optimal cluster means {µ (c) } k c=1 and cluster assignments {ŷ (i) } m i=1 that minimize the clustering error (8.3) is computationally challenging.The difficulty stems from the fact that the clustering error is a non-convex function of the cluster means and assignments.While jointly optimizing the cluster means and assignments is hard, separately optimizing either the cluster means for given assignments or vice-versa is easy.In what follows, we present simple closed-form solutions for these sub-problems.The k-means method simply combines these solutions in an alternating fashion.It can be shown that for given predictions (cluster assignments) ŷ(i) , the clustering error (8.3) is minimized by setting the cluster representatives equal to the cluster means [12]To evaluate (8.4) we need to know the predicted cluster assignments ŷ(i) .The crux is that the optimal predictions ŷ(i) , in the sense of minimizing clustering error (8.3), depend themselves on the choice for the cluster representatives µ (c) .In particular, for given cluster representative µ (c) with c = 1, . . ., k, the clustering error is minimized by the cluster assignments ŷ(i) ∈ argmin c∈{1,...,k}Here, we denote by argmin c ∈{1,...,k}x (i) − µ (c ) the set of all cluster indices c ∈ {1, . . ., k} such that x (i) − µ (c) = min c ∈{1,...,k} x (i) − µ (c ) .Note that (8.5) assigns the ith datapoint to those cluster C (c) whose cluster mean µ (c) is nearest (in Euclidean distance) to x (i) .Thus, if we knew the optimal cluster representatives, we could predict the cluster assignments using (8.5).However, we do not know the optimal cluster representatives unless we have found good predictions for the cluster assignments ŷ(i) (see (8.4)).To recap: We have characterized the optimal choice (8.4) for the cluster representatives for given cluster assignments and the optimal choice (8.5) for the cluster assignments for given cluster representatives.It seems natural, starting from some initial guess for the cluster representatives, to alternate between the cluster assignment update (8.5) and the update (8.4) for the cluster means.This alternating optimization strategy is illustrated in Figure 8.2 and summarized in Algorithm 12.Note that Algorithm 12, which is maybe the most basic variant of k-means, simply alternates between the two updates (8.4)  "k-Means" until some stopping criterion is satisfied.Algorithm 12 requires the specification of the number k of clusters and initial choices for the cluster means µ (c) , for c = 1, . . ., k.Those quantities are hyper-parameters that must be tuned to the specific geometry of the given dataset D. This tuning can be based on probabilistic models for the dataset and its cluster structure (see Section 2. 1.4 and [81, 150]).Alternatively, if Algorithm 12 is used as pre-processing within an overall supervised ML method (see Chapter 3), the validation error (see Section 6.3) of the overall method might guide the choice of the number k of clusters.Choosing Number of Clusters.The choice for the number k of clusters typically depends on the role of the clustering method within an overall ML application.If the clustering method serves as a pre-processing for a supervised ML problem, we could try out different values of the number k and determine, for each choice k, the corresponding validation error.We then pick the value of k which results in the smallest validation error.If the clustering method is mainly used as a tool for data visualization, we might prefer a small number of clusters.The choice for the number k of clusters can also be guided by the so-called "elbow-method".Here, we run the k-means Algorithm 12 for several different choices of k.For each value of k, Algorithm 12 delivers a clustering with clustering error(update cluster assignments) (8.6)3:for each cluster c = 1, . . ., k dox (i) (update cluster means) (8.7)4: until stopping criterion is met2Output: cluster means µ (c) , for c = 1, . . ., k, cluster assignments ŷ(i) ∈ {1, . . ., k}, for i = 1, . . ., m, final clustering errorWe then plot the minimum empirical error E (k) as a function of the number k of clusters.Figure 8.3 depicts an example for such a plot which typically starts with a steep decrease for increasing k and then flattening out for larger values of k.Note that for k ≥ m we can achieve zero clustering error since each datapoint x (i) can be assigned to a separate cluster C (c) whose mean coincides with that datapoint, x (i) = µ (c) .Cluster-Means Initialization.We briefly mention some popular strategies for choosing the initial cluster means in Algorithm 12.One option is to initialize the cluster means with realizations of i.i.d.random vectors whose probability distribution is matched to the dataset D = {x (i) } m i=1 (see Section 3.12).For example, we could use a multivariate normal distribution N (x; µ, Σ) with the sample mean µ = (1/m) m i=1 x (i) and the sample covariance T .Alternatively, we could choose the initial cluster means µ (c) by selecting k different data points x (i) from D. This selection process might combine random choices with an optimization of the distances between cluster means [3].Finally, the cluster means might also be chosen by evenly partitioning the principal component of the dataset (see Chapter 9).Interpretation as ERM.For a practical implementation of Algorithm 12 we need to decide when to stop updating the cluster means and assignments (see (8.6) and (8.7)).To this end it is useful to interpret Algorithm 12 as a method for iteratively minimizing the  Another main difference between Algorithm 12 and most classification methods is the choice for the empirical risk used to evaluate the quality or usefulness of a given hypothesis h(•).Classification methods typically use an average loss over labeled data points in a training set as empirical risk.In contrast, Algorithm 12 uses the clustering error (8.3) as a form of empirical risk.Consider a hypothesis that resembles the cluster assignments ŷ(i) obtained after completing an iteration in Algorithm 12, ŷ(i) = h x (i) .Then we can rewrite the resulting clustering error achieved after this iteration asNote that the i-th summand in (8.8) depends on the entire dataset D and not only on (the features of) the i-th data point x (i) .Some Practicalities.For a practical implementation of Algorithm 12 we need to fix three issues.• Issue 1 ("tie-breaking"): We need to specify what to do if several different cluster indices c ∈ {1, . . ., k} achieve the minimum value in the cluster assignment update (8.6) during step 2.• Issue 2 ("empty cluster"): The cluster assignment update (8.6) in step 3 of Algorithm 12 might result in a cluster c with no datapoints associated with it, |{i : ŷ(i) = c}| = 0.For such a cluster c, the update (8.7) is not well-defined.• Issue 3 ("stopping criterion"): We need to specify a criterion used in step 4 of Algorithm 12 to decide when to stop iterating.Algorithm 13 is obtained from Algorithm 12 by fixing those three issues [51].Step 3 of Algorithm 13 solves the first issue mentioned above ("tie breaking"), arising when there are several cluster clusters whose means have minimum distance to a data point x (i) , by assigning x (i) to the cluster with smallest cluster index (see (8.9)).Step 4 of Algorithm 13 resolves the "empty cluster" issue by computing the variables b (c) ∈ {0, 1} for c = 1, . . ., k.The variable b (c) indicates if the cluster with index c is active (b (c) = 1) or the cluster c is inactive (b (c) = 0).The cluster c is defined to be inactive if there are no data points assigned to it during the preceding cluster assignment step (8.9).The cluster activity indicators b (c) allows to restrict the cluster mean updates (8.10)only to the clusters c with at least one data point x (i) .To obtain a stopping criterion, step 7 Algorithm 13 monitors the clustering error E r incurred by the cluster means and assignments obtained after r iterations.Algorithm 13 continues updating cluster assignments (8.9) and cluster means (8.10) as long as the decrease is above a given threshold ε ≥ 0.Algorithm 13 "k-Means II" (slight variation of "Fixed Point Algorithm" in [51]) for all c = 1, . . ., k with b (c) = 1,x (i) (update cluster means) (8.10)r := r + 1 (increment iteration counter)7:))8: until r > 1 and E r−1 − E r ≤ ε (check for sufficient decrease in clustering error)(compute final clustering error)Output: cluster assignments ŷ(i) ∈ {1, . . ., k}, cluster means µ (c) , clustering error E (k) .For Algorithm 13 to be useful we must ensure that the stopping criterion is met within a finite number of iterations.In other words, we must ensure that the clustering error decrease can be made arbitrarily small within a sufficiently large (but finite) number of iterations.To this end, it is useful to represent Algorithm 13 as a fixed-point iterationThe operator P, which depends on the dataset D, reads in a list of cluster assignments and delivers an improved list of cluster assignments aiming at reducing the associated clustering error (8.3).Each iteration of Algorithm 13 updates the cluster assignments ŷ(i) by applying the operator P. Representing Algorithm 13 as a fixed-point iteration (8.11) allows for an elegant proof of the convergence of Algorithm 13 within a finite number of iterations (even for ε = 0) [51, Thm.2]. Figure 8.4 depicts the evolution of the cluster assignments and cluster means during the iterations Algorithm 13.Each subplot corresponds to one iteration of Algorithm 13 and depicts the cluster means before that iteration and the clustering assignments (via the marker symbols) after the corresponding iteration.In particular, the upper left subplot depicts the cluster means before the first iteration (which are the initial cluster means) and the cluster assignments obtained after the first iteration of Algorithm 13.Consider running Algorithm 13 with tolerance ε = 0 (see step 8) such that the iterations are continued until there is no decrease in the clustering error E (r) (see step 7 of Algorithm 13).As discussed above, Algorithm 13 will terminate after a finite number of iterations.Moreover, for ε = 0, the delivered cluster assignments ŷ(i) m i=1 are fully determined by the delivered clustered means µ (c) k c=1 , ŷ(i) = min{ argmin c ∈{1,...,k}Indeed, if (8.12) does not hold one can show the final iteration r would still decrease the clustering error and the stopping criterion in step 8 would not be met.If cluster assignments and cluster means satisfy the condition (8.12), we can rewrite the clustering error (8.3) as a function of the cluster means solely,Even for cluster assignments and cluster means that do not satisfy (8.12), we can still use (8.13) to lower bound the clustering error (8.3),Algorithm 13 iteratively improves the cluster means in order to minimize (8.13).Ideally, we would like Algorithm 13 to deliver cluster means that achieve the global minimum of (8.13) (see Figure 8.5).However, for some combination of dataset D and initial cluster means, Algorithm 13 delivers cluster means that form only a local optimum of L µ (c) k c=1 |D which is strictly worse (larger) than its global optimum (see Figure 8.5).The tendency of Algorithm 13 to get trapped around a local minimum of (8.13) depends on the initial choice for cluster means.It is therefore useful to repeat Algorithm 13 several times, with each repetition using a different initial choice for the cluster means.We then pick the cluster assignments {ŷ (i) } m i=1 obtained for the repetition that resulted in the smallest clustering error E (k) (see step 9).Consider a dataset D = {x (1) , . . ., x (m) } that we wish to group into a given number of k different clusters.The hard clustering methods discussed in Section 8.1 deliver cluster assignments ŷ(i) , for i = 1, . . ., m.The cluster assignment ŷ(i) is the index of the cluster to which the ith data point x (i) is assigned to.These cluster assignments ŷ provide rather coarse-grained information.Two data points x (i) , x (i ) might be assigned to the same cluster c although their distances to the cluster mean µ (c) might differ significantly.Intuitively, these two data points have a different degree of belonging to the cluster c.For some clustering applications it is desirable to quantify the degree by which a data point belongs to a cluster.Soft clustering methods use a continues range, such as the closed interval [0, 1], of possible values for the degree of belonging.In contrast, hard clustering methods use only two possible values for the degree of belonging to a specific cluster, either "full belonging" or no "belonging at all".While hard clustering methods assign a given data point to precisely one cluster, soft clustering methods typically assign a data point to several different clusters with non-zero degree of belonging.This chapter discusses soft clustering methods that compute, for each data point x (i) in the dataset D, a vectorWe can interpret the entry ŷ(i) c ∈ [0, 1] as the degree by which the data point x (i) belongs to the cluster C (c) .For ŷ(i) c ≈ 1, we are quite confident in the data point x (i) belonging to cluster C (c) .In contrast, for ŷ(i) c ≈ 0, we are quite confident that the data point x (i) is outside the cluster C (c) .A widely used soft clustering method uses a probabilistic model for the data points D = {x (i) } m i=1 .Within this model, each cluster C (c) , for c = 1, . . ., k, is represented by a multivariate normal distribution [9] NThe probability distribution (8.14) is parametrized by a cluster-specific mean vector µ (c) and an (invertible) cluster-specific covariance matrix Σ (c) . 1 Let us interpret a specific data point x (i) as a realization drawn from the probability distribution (8.14) of a specific cluster c (i) ,We can think of c (i) as the true index of the cluster to which the data point x (i) belongs to.The variable c (i) selects the cluster distributions (8.14) from which the feature vector x (i) has been generated (drawn).We will therefore refer to the variable c (i) as the (true) cluster assignment for the ith data point.Similar to the feature vectors x (i) we also interpret the cluster assignments c (i) , for i = 1, . . ., m as realizations of i.i.d.RVs.In contrast to the feature vectors x (i) , we do not observe (know) the true cluster indices c (i) .After all, the goal of soft clustering is to estimate the cluster indices c (i) .We obtain a soft clustering method by estimating the cluster indices c (i) based solely on the data points in D. To compute these estimates we assume that the (true) cluster indices c (i) are realizations of i.i.d.RVs with the common probability distribution (or probability mass function)The (prior) probabilities p c , for c = 1, . . ., k, are either assumed known or estimated from data [85,9].The choice for the probabilities p c could reflect some prior knowledge about different sizes of the clusters.For example, if cluster C (1) is known to be larger than cluster C (2) , we might choose the prior probabilities such that p 1 > p 2 .The probabilistic model given by (8.15), (8.16) is referred to as a a GMM.As its name suggests, within a GMM the common marginal distribution for the feature vectors x (i) , for i = 1, . . ., m, is a (additive) mixture of multivariate normal (Gaussian) distributions,.(8.17)As already mentioned, the cluster assignments c (i) are hidden (unobserved) RVs.We thus have to infer or estimate these variables from the observed data points x (i) which realizations or i.i.d.RVs with the common distribution (8.17).The GMM (see (8.15) and (8.16)) lends naturally to a rigorous definition for the degree y (i) c by which data point x (i) belongs to cluster c. 2 Let us define the label value y (i) c as the "a-posteriori" probability of the cluster assignment c (i) being equal to c ∈ {1, . . ., k}:By their very definition (8.18), the degrees of belonging yc always sum to one,We emphasize that we use the conditional cluster probability (8.18), conditioned on the dataset D, for defining the degree of belonging y (i) c .This is reasonable since the degree of belonging y (i) c depends on the overall (cluster) geometry of the dataset D. The definition (8.18) for the label values (degree of belongings) y)).Since we do not know these parameters beforehand we cannot evaluate the conditional probability in (8.18).A principled approach to solve this problem is to evaluate (8.18) with the true GMM parameters replaced by some estimates { µ (c) , Σ (c) , pc } k c=1 .Plugging in the GMM parameter estimates into (8.18)provides us with predictions ŷ(i)for the degrees of belonging.However, to compute the GMM parameter estimates we would have already needed the degrees of belonging y (i) c .This situation is similar to hard clustering where ultime goals is to jointly optimize cluster means and assignments (see Section 8.1).Similar to the spirit of Algorithm 12 for hard clustering, we solve the above dilemma of soft clustering by an alternating optimization scheme.This scheme, which is illustrated in Figure 8.6, alternates between updating (optimizing) the predicted degrees of belonging (or soft cluster assignments) ŷ(i) c , for i = 1, . . ., m and c = 1, . . ., k, given the current GMM parameter estimates { µ (c) , Σ (c) , pc } k c=1 and then updating (optimizing) these GMM parameter estimates based on the updated predictions ŷ(i) c .We summarize the resulting soft clustering method in Algorithm 14.Each iteration of Algorithm 14 consists of an update 2 Remember that the degree of belonging y (i) c is considered as the (unknown) label value of a data point.The choice or definition for the labels of data points is a design choice.In particular, we can define the labels of data points using a hypothetical probabilistic model such as the GMM.Starting from an initial guess or estimate for the cluster parameters, the soft cluster assignments and cluster parameters are updated (improved) in an alternating fashion.(8.22) for the degrees of belonging followed by an update (step 3) for the GMM parameters.To analyze Algorithm 14 it is helpful to interpret (the features of) data points x (i) as realizations of i.i.d.RVs distributed according to a GMM (8.15)- (8.16).We can then understand Algorithm 14 as a method for estimating the GMM parameters based on observing realizations drawn from the GMM (8.15)- (8.16).We can estimate the parameters of a probability distribution using the maximum likelihood method (see Section 3.12 and [76,85]).As its name suggests, maximum likelihood methods estimate the GMM parameters by maximizing the probability (density) p D; {µ (c) , Σ (c) , p c } k c=1 (8.20) of actually observing the data points in the dataset D.It can be shown that Algorithm 14 is an instance of a generic approximate maximum likelihood technique referred to as expectation maximization (EM) (see [58,Chap. 8.5] for more details).In particular, each iteration of Algorithm 14 updates the GMM parameter estimates such that the corresponding probability density (8.20) does not decrease [157].If we denote the GMM parameter estimate obtained after r iterations of Algorithm 14 by θ (r) [58, Sec. 8. 5p D; θ (r+1) ≥ p D; θ (r) (8.21)As for Algorithm 12, we can also interpret Algorithm 14 as an instance of the ERM principle discussed in Chapter 4. Indeed, maximizing the probability density (8.20) is equivalentΣ (3) Figure 8.7:The GMM (8.15), (8.16) results in a probability distribution (8.17) for (feature vectors of) data points which is a weighted sum of multivariate normal distributions N (µ (c) , Σ (c) ).The weight of the c-th component is the cluster probability p(c (i) = c).Algorithm 14 "A Soft-Clustering Algorithm" [12] Input: dataset D = {x (i) } m i=1 ; number k of clusters, initial GMM parameter estimates { µ (c) , Σ (c) , pc } k c=1 1: repeat 2:for each i = 1, . . ., m and c = 1, . . ., k, update degrees of belongingfor each c ∈ {1, . . ., k}, update GMM parameter estimates:x (i) − µ (c) T (cluster covariance matrix)4: until stopping criterion met Output: predicted degrees of belonging y (i) = (ŷ The monotone decrease (8.24) in the empirical risk (8.23) achieved by the iterations of Algorithm 14 naturally lends to a stopping criterion.Let E r denote the empirical risk (8.23) achieved by the GMM parameter estimates θ (r) obtained after r iterations in Algorithm 14. Algorithm 14 stops iterating as soon as the decrease E r−1 − E r achieved by the r-th iteration of Algorithm 14 falls below a given (positive) threshold ε > 0.Similar to Algorithm 12, also Algorithm 14 might get trapped in local minima of the underlying empirical risk.The GMM parameters delivered by Algorithm 14 might only be a local minimum of (8.23) but not the global minimum (see Figure 8.5 for the analogous situation in hard clustering).As for hard clustering Algorithm 12, we typically repeat Algorithm 14 several times.During each repetition of Algorithm 14, we use a different (randomly chosen) initialization for the GMM parameter estimates θ = { µ (c) , Σ (c) , pc } k c=1 .Each repetition of Algorithm 14 results in a potentially different set of GMM parameter estimates and degrees of belongings ŷ(i) c .We then use the results for that repetition that achieves the smallest empirical risk (8.23).Let us point out an interesting link between soft clustering methods based on GMM (see Algorithm 14) and hard clustering with k-means (see Algorithm 12).Consider the GMM (8.15) with prescribed cluster covariance matriceswith some given variance σ 2 > 0. We assume the cluster covariance matrices in the GMM to be given by (8.25) and therefore can replace the covariance matrix updates in Algorithm 14 with the assignment Σ (c) := σ 2 I.It can be verified easily that for sufficiently small variance σ 2 in (8.25), the update (8.22) tends to enforce ŷ(i) c ∈ {0, 1}.In other words, each data pointx (i) becomes then effectively associated with exactly one single cluster c whose cluster mean µ (c) is nearest to x (i) .For σ 2 → 0, the soft clustering update (8.22) in Algorithm 14 reduces to the (hard) cluster assignment update (8.6) in k-means Algorithm 12.We can interpret Algorithm 12 as an extreme case of Algorithm 14 that is obtained by fixing the covariance matrices in the GMM to σ 2 I with a sufficiently small σ 2 .Combining GMM with linear regression.Let us sketch how Algorithm 14 could be combined with linear regression methods (see Section 3.1).The idea is to first compute the degree of belongings to the clusters for each data point.We then learn separate linear predictors for each cluster using the degree of belongings as weights for the individual loss terms in the training error.To predict the label of a new data point, we first compute the predictions obtained for each cluster-specific linear hypothesis.These cluster-specific predictions are then averaged using the degree of belongings for the new data point as weights.The clustering methods discussed in Sections 8.1 and 8.2 can only be applied to data points which are characterized by numeric feature vectors.These methods define the similarity between data points using the Euclidean distance between the feature vectors of these data points.As illustrated in Figure 8.8, these methods can only produce "Euclidean shaped" clusters that are contained either within hyper-spheres (Algorithm 12) or hyper-ellipsoids (Algorithm 14).µ (1) (a) Some applications generate data points for which the construction of useful numeric features is difficult.Even if we can easily obtain numeric features for data points, the Euclidean distances between them might not reflect the actual similarities between data points.As a case in point, consider data points representing text documents.We could use the histogram of a pre-specified list (dictionary) of words as numeric features for a text document.In general, a small Euclidean distance between histograms of text documents does not imply that the text documents have similar meanings.Moreover, clusters of similar text documents might have highly complicated shapes in the space of feature vectors that cannot be grouped within hyper-ellipsoids.For datasets with such "non-Euclidean" cluster shapes, k-means or GMM are not suitable as clustering methods.We should then replace the Euclidean distance between feature vectors with another concept to determine or measure the similarity between data points.Connectivity-based clustering methods do not require any numeric features of data points.These methods cluster data points based on explicitly specifying for any two different data points if they are similar and to what extent.A convenient mathematical tool to represent similarities between the data points of a dataset D is a weighted undirected graph G = V, E .We refer to this graph as the similarity graph of the dataset D (see Figure 8.9).The nodes V in this similarity graph G represent data points in D and the undirected edges connect nodes that represent similar data points.The extent of similarity is represented by the weights W i,i for each edge {i, i } ∈ E.Given a similarity graph G of a dataset, connectivity-based clustering methods determine clusters as subsets of nodes that are well connected within the cluster but weakly connected between different clusters.Different concepts for quantifying the connectivity between nodes in a graph yield different clustering methods:• Spectral clustering methods use eigenvectors of a graph Laplacian matrix to measure the connectivity between nodes [148,106].• Flow-based clustering methods measure the connectivity between two nodes via the amount of flow that can be routed between them [73].We can use connectivity measures between nodes of an empirical graph to construct meaningful numerical feature vectors ("embeddings") for these nodes.These feature vectors can then be fed into the hard-clustering Algorithm 13 or the soft clustering Algorithm 14 (see Figure 8.9).The algorithm density-based spatial clustering of applications with noise (DBSCAN) considers two data points i, i as connected if one of them (say i) is a core node and the other node (i ) can be reached via a sequence (path) of connected core nodes i (1) , . . ., i (r) , with {i, i (1) }, {i (1) , i (2) }, . . ., {i (r) , i } ∈ E.DBSCAN considers a node to be a core node if it has a sufficiently large number of neighbours [35].The minimum number of neighbours required for a node to be considered a core node is a hyper-parameter of DBSCAN.When DBSCAN is applied to data points with numeric feature vectors, it defines two data points as connected if the Euclidean distance between their feature vectors does not exceed a given threshold ε (see Figure 8.10).In contrast to k-means and GMM, DBSCAN does not require the number of clusters to be specified.The number of clusters is determined automatically by DBSCAN and depends on its hyper-parameters.DBSCAN also performs an implicit outlier detection.The outliers delivered by DBSCAN are those data points which do not belong to the same cluster as any other data point.x (1) (a)x (1) x (2) x (3) x (4) x (5)x (6) x (7) x (8) x 1x 2 (b) x (1) x (2) < εFigure 8.10: DBSCAN assigns two data points to the same cluster if they are reachable.Two data points x (i) , x (i ) are reachable if there is a path of data points from x (i ) to x (i) .This path consists of a sequence of data points that are within a distance of ε.Moreover, each data point on this path must be a core point which has at least a given number of neighbouring data points within the distance ε.In applications it might be beneficial to combine clustering methods with supervised methods such as linear regression.As a point in case, consider a dataset that consists of data points obtained from two different data generation processes (such as two weather observation stations, one located in Helsinki and the other one in Lapland).Let us denote the data points generated by one process by D (1) and the other one by D (2) .Each data point is characterized by several numeric features and a numeric label.While there would be an accurate linear hypothesis for predicting the label of data points in D (1) and another linear hypothesis for D (2) these two are very different.Formally, this approach amounts to using the cluster assignment of a data points as an additional feature.We could also use the degree of belonging delivered by soft clustering methods as additional features.We could try to use clustering methods to assign any given data point to the corresponding data generation process.If we are lucky, the resulting clusters resemble (approximately) the two data generation processes D (1) and D (2) .Once we have successfully clustered the data points, we can learn a separate (tailored) hypothesis for each cluster.More generally, we can use the predicted cluster assignments obtained from the methods of Section 8.1 -8.3 as additional features for each data point.Let us illustrate the above ideas by combining Algorithm 12 with linear regression.We first group data points into a given number k of clusters and then learn separate linear predictors h (c) (x) = w (c) T x for each cluster c = 1, . . ., k.To predict the label of a new data point with features x, we first assign to the cluster c with the nearest cluster mean.We then use the linear predictor h (c ) assigned to cluster c to compute the predicted label ŷ = h (c ) (x).Consider m = 10000 data points x (1) , . . ., x (m) which are represented by two numeric features.We apply k-means to cluster the data set into k = 5 clusters.How many bits do we need to store the resulting cluster assignments?Feature Learning "Solving Problems By Changing the Viewpoint."Figure 9.1: Dimensionality reduction methods aim at finding a map h which maximally compresses the raw data while still allowing to accurately reconstruct the original datapoint from a small number of features x 1 , . . ., x n .Chapter 2 defined features as those properties of a data point that can be measured or computed easily.Sometimes the choice of features follows naturally from the available hardand software.For example, we might use the numeric measurement z ∈ R delivered by a sensing device as a feature.However, we could augment this single feature with new features such as the powers z 2 and z 3 or adding a constant z +5.Each of these computations produces a new feature.Which of these additional features are most useful?Feature learning methods automate the choice of finding good features.These methods learn a hypothesis map that reads in some representation of a data point and transforms it to a set of features.Feature learning methods differ in the precise format of the original data representation as well as the format of the delivered features.This chapter mainly discusses feature learning methods that require data points being represented by n numeric raw features and deliver a set of n new numeric features.We will denote the raw features and the learnt new features by z = z 1 , . . ., z n T ∈ R n and x = x 1 , . . ., x n T ∈ R n , respectively.Many ML application domains generate data points for which can access a huge number of raw features.Consider data points being snapshots generated by a smartphone.It seems natural to use the pixel colour intensities as the raw features of the snapshot.Since modern smartphone have "Megapixel cameras", the pixel intensities would provide us with millions of raw features.It might seem a good idea to use as many raw features of a data point as possible since more features should offer more information about a data point and its label y.There are, however, two pitfalls in using an unnecessarily large number of features.The first one is a computational pitfall and the second one is a statistical pitfall.Computationally, using very long feature vectors x ∈ R n (with n being billions), might result in prohibitive computational resource requirements (bandwidth, storage, time) of the resulting ML method.Statistically, using a large number of features makes the resulting ML methods more prone to overfitting.For example, linear regression will typically overfit when using feature vectors x ∈ R n whose length n exceeds the number m of labeled data points used for training (see Chapter 7).Both from a computational and a statistical perspective, it is beneficial to use only the maximum necessary amount of features.The challenge is to select those features which carry most of the relevant information required for the prediction of the label y.Finding the most relevant features out of a huge number of raw features is the goal of dimensionality reduction methods.Dimensionality reduction methods form an important sub-class of feature learning methods.These methods learn a hypothesis h(z) that maps a long raw feature vector z ∈ R n to a new (short) feature vector x ∈ R n with n n .Beside avoiding overfitting and coping with limited computational resources, dimensionality reduction can also be useful for data visualization.Indeed, if the resulting feature vector has length n = 2, we depict data points in the two-dimensional plane in form of a scatterplot.We will discuss the basic idea underlying dimensionality reduction methods in Section 9.1.Section 9.2 presents one particular example of a dimensionality reduction method that computes relevant features by a linear transformation of the raw feature vector.Section 9.4 discusses a method for dimensionality reduction that exploits the availability of labelled data points.Section 9.6 shows how randomness can be used to obtain computationally cheap dimensionality reduction .Most of this chapter discusses dimensionality reduction methods that determine a small number of relevant features from a large set of raw features.However, sometimes it might be useful to go the opposite direction.There are applications where it might be beneficial to construct a large (even infinite) number of new features from a small set of raw features.Section 9.7 will showcase how computing additional features can help to improve the prediction accuracy of ML methods.The efficiency of ML methods depends crucially on the choice of features that are used to characterize data points.Ideally we would like to have a small number of highly relevant features to characterize data points.If we use too many features we risk to waste computations on exploring irrelevant features.If we use too few features we might not have enough information to predict the label of a data point.For a given number n of features, dimensionality reduction methods aim at learning an (in a certain sense) optimal map from the data point to a feature vector of length n.Figure 9.1 illustrates the basic idea of dimensionality reduction methods.Their goal is to learn (or find) a "compression" map h(•) : R n → R n that transforms a (long) raw feature vector z ∈ R n to a (short) feature vector x = (x 1 , . . ., x n ) T := h(z) (typically n n ).The new feature vector x = h(z) serves as a compressed representation (or code) for the original feature vector z.We can reconstruct the raw feature vector using a reconstruction map r(•) : R n → R n .The reconstructed raw features z := r(x) = r(h(z)) will typically differ from the original raw feature vector z.In general, we obtain a non-zero reconstruction error zDimensionality reduction methods learn a compression map h(•) such that the reconstruction error (9.1) is minimized.In particular, for a dataset D = z (1) , . . ., z (m) , we measure the quality of a pair of compression map h and reconstruction map r by the average reconstruction errorHere, L z, r h z (i) denotes a loss function that is used to measure the reconstruction error r h z (i) z −z.Different choices for the loss function in (9.2) result in different dimensionality reduction methods.One widely-used choice for the loss is the squared Euclidean normPractical dimensionality reduction methods have only finite computational resources.Any practical method must therefore restrict the set of possible compression and reconstruction maps to small subsets H and H * , respectively.These subsets are the hypothesis spaces for the compression map h ∈ H and the reconstruction map r ∈ H * .Feature learning methods differ in their choice for these hypothesis spaces.Dimensionality reduction methods learn a compression map by solving= argminWe can interpret (9.4) as a (typically non-linear) approximation problem.The optimal compression map h is such that the reconstructionr( h(z)), with a suitably chosen reconstruction map r, approximates the original raw feature vector z with optimal accuracy.Note that we use a single compression map h(•) and a single reconstruction map r(•) for all data points in the dataset D.We obtain variety of dimensionality methods by using different choices for the hypothesis spaces H, H * and loss function in (9.4).Section 9.2 discusses a method that solves (9.4) for H, H * constituted by linear maps and the loss (9.3).Deep autoencoders are another family of dimensionality reduction methods that solve (9.4) with H, H * constituted by non-linear maps that are represented by deep neural networks [48,Ch. 14].We now consider the special case of dimensionality reduction where the compression and reconstruction map are required to be linear maps.Consider a data point which is characterized by a (typically very long) raw feature vector z = z 1 , . . ., z n T ∈ R n of length n .The length n of the raw feature vector might be easily of the order of millions.To obtain a small set of relevant features x = x 1 , . . ., x n T ∈ R n , we apply a linear transformation to the raw feature vector, x = Wz.(9.5)Here, the "compression" matrix W ∈ R n×n maps (in a linear fashion) the (long) raw feature vector z ∈ R n to the (shorter) feature vector x ∈ R n .It is reasonable to choose the compression matrix W ∈ R n×n in (9.5) such that the resulting features x ∈ R n allow to approximate the original data point z ∈ R n as accurate as possible.We can approximate (or recover) the data point z ∈ R n back from the features x by applying a reconstruction operator R ∈ R n ×n , which is chosen such that z ≈ Rx The approximation error L W, R | D resulting when (9.6) is applied to each data point in a datasetOne can verify that the approximation error L W, R | D can only by minimal if the compression matrix W is of the formwith n orthonormal vectors u (j) , for j = 1, . . ., n.The vectors u (j) are the eigenvectors corresponding to the n largest eigenvalues of the sample covariance matrixHere we used the data matrix Z = z (1) , . . ., z (m) T ∈ R m×n . 1 It can be verified easily, using the definition (9.9), that the matrix Q is psd.As a psd matrix, Q has an eigenvalue1 Some authors define the data matrix as Z = z (1) , . . ., z (m) T ∈ R m×D using "centered" data points z (i) − m obtained by subtracting the average m = (1/m) m i=1 z (i) .with real-valued eigenvalues λ 1 ≥ λ 2 ≥ . . .≥ λ n ≥ 0 and orthonormal eigenvectors {u (j) } n j=1 .The feature vectors x (i) are obtained by applying the compression matrix W PCA (9.8) to the raw feature vectors z (i) .We refer to the entries of the learnt feature vector x (i) = W PCA z (i) (see (9.8)) as the principal components (PC) of the raw feature vectors z (i) .Algorithm 15 summarizes the overall procedure of determining the compression matrix (9.8) and computing the learnt feature vectors x (i) .This procedure is known as PCA.The length Algorithm 15 PCA; number n of PCs.1: compute the EVD (9.10) to obtain orthonormal eigenvectors u (1) , . . ., u (n ) corresponding to (decreasingly ordered) eigenvalues4: compute approximation error L (PCA) = n j=n+1 λ j (see (9.11)).Output: x (i) , for i = 1, . . ., m, and the approximation error L (PCA) .n ∈ {0, . . ., n ) of the delivered feature vectors x (i) , for i = 1, . . ., m, is an input (or hyper-) parameter of Algorithm 15.Two extreme cases are n = 0 (maximum compression) and n = n (no compression).We finally note that the choice for the orthonormal eigenvectors in (9.8) might not be unique.Depending on the sample covariance matrix Q, there might be different sets of orthonormal vectors that correspond to the same eigenvalue of Q.Thus, for a given length n of the new feature vectors, there might be several different matrices W that achieve the same (optimal) reconstruction error L (PCA) .Computationally, Algorithm 15 essentially amounts to an EVD of the sample covariance matrix Q (9.9).The EVD of Q provides not only the optimal compression matrix W PCA but also the measure L (PCA) for the information loss incurred by replacing the original data points z (i) ∈ R n with the shorter feature vector x (i) ∈ R n .We quantify this information loss by the approximation error obtained when using the compression matrix W PCA (and corresponding reconstruction matrix R opt = W T PCA ), .11)As depicted in Figure 9.2, the approximation error L (PCA) decreases with increasing number n of PCs used for the new features (9.5).For the extreme case n = 0, where we completely ignore the raw feature vectors z (i) , the optimal reconstruction error is 2 .The other extreme case n = n allows to use the raw features directly as the new features x (i) = z (i) .This extreme case means no compression at all, and trivially results in a zero reconstruction error L (PCA) = 0.One important use case of PCA is as a pre-processing step within an overall ML problem such as linear regression (see Section 3.1).As discussed in Chapter 7, linear regression methods are prone to overfitting whenever the data points are characterized by raw feature vectors z whose length n exceeds the number m of labeled data points used in ERM.One simple but powerful strategy to avoid overfitting is to preprocess the raw features z (i) ∈ R n , for i = 1, . . ., m by applying PCA.Indeed, PCA Algorithm 15 delivers feature vectors x (i) ∈ R n of prescribed length n.Thus, choosing the parameter n such that n < m will typically prevent the follow-up linear regression method from overfitting.There are several aspects which can guide the choice for the number n of PCs to be used as features.• To generate data visualizations we might use either n = 2 or n = 3.• We should choose n sufficiently small such that the overall ML method fits the available computational resources.• Consider using PCA as a pre-processing for linear regression (see Section 3.1).In particular, we can use the learnt feature vectors x (i) delivered by PCA as the feature vectors of data points in plain linear regression methods.To avoid overfitting, we should choose n < m (see Chapter 7).• Choose n large enough such that the resulting approximation error L (PCA) is reasonably small (see Figure 9.2).If we use PCA with n = 2, we obtain feature vectors x (i) = W PCA z (i) (see (9.5)) which can be depicted as points in a scatterplot (see Section 2. 1.3).As an example, consider data points z (i) obtained from historic recordings of Bitcoin statistics.Each data point z (i) ∈ R n is a vector of length n = 6.It is difficult to visualise points in an Euclidean space R n of dimension n > 2. Therefore, we apply PCA with n = 2 which results in feature vectors x (i) ∈ R 2 .These new feature vectors (of length 2) can be depicted conveniently as the scatterplot in Figure 9.3.We now briefly discuss variants and extensions of the basic PCA method.• Kernel PCA [58,Ch.14.5.4]:The PCA method is most effective if the raw feature vectors of data points are concentrated around a n-dimensional linear subspace of R n .Kernel PCA extends PCA to data points that are located near a low-dimensional −8,000 −6,000 −4,000 −2,000 2,000 4,000 6,000T whose entries are the first two PCs of the Bitcoin statistics z (i) of the i-th day.manifold which might be highly non-linear.This is achieved by applying PCA to transformed feature vectors instead of the original raw feature vectors.Kernel PCA first applies a (typically non-linear) feature map φ to the raw feature vectors z (i) (see Section 3.9) and applies PCA to the transformed feature vectors φ z (i) , for i = 1, . . ., m.• Robust PCA [156]: The basic PCA Algorithm 15 is sensitive to outliers, i.e., a small number of data points with significantly different statistical properties than the bulk of data points.This sensitivity might be attributed to the properties of the squared Euclidean norm (9.3) which is used in PCA to measure the reconstruction error (9.1).We have seen in Chapter 3 that linear regression (see Section 3.1 and 3.3) can be made robust against outliers by replacing the squared error loss with another loss function.In a similar spirit, robust PCA replaces the squared Euclidean norm with another norm that is less sensitive to having very large reconstruction errors (9.1) for a small number of data points (which are outliers).• Sparse PCA [58, Ch. 14.5.5]:The basic PCA method transforms the raw feature vector z (i) of a data point to a new (shorter) feature vector x (i) .In general each entry x (i) j of the new feature vector will depend on each entry of the raw feature vector z (i) .More precisely, the new feature x (i) j depends on all raw features z (i) j for which the corresponding entry W j,j of the matrix W = W PCA (9.8) is non-zero.For most datasets, all entries of the matrix W PCA will typically be non-zero.In some applications of linear dimensionality reduction we would like to construct new features that depend only on a small subset of raw features.Equivalently we would like to learn a linear compression map W (9.5) such that each row of W contains only few non-zero entries.To this end, sparse PCA enforces the rows of the compression matrix W to contain only a small number of non-zero entries.This enforcement can be implement either using additional constraints on W or by adding a penalty term to the reconstruction error (9.7).• Probabilistic PCA [118,138]: We have motivated PCA as a method for learning an optimal linear compression map (matrix) (9.5) such that the compressed feature vectors allows to linearly reconstruct the original raw feature vector with minimum reconstruction error (9.7).Another interpretation of PCA is that of a method that learns a subspace of R n that best fits the distribution of the raw feature vectors z (i) , for i = 1, . . ., m.This optimal subspace is precisely the subspace spanned by the rows of W PCA (9.8).Probabilistic PCA (PPCA) interprets the raw feature vectors z (i) as realizations of i.i.d.RVs.These realizations are modelled asHere, W ∈ R n×n is some unknown matrix with orthonormal rows.The rows of W span the subspace around which the raw features are concentrated.The vectors x (i) in (9.12) are realizations of i.i.d.RVs whose common probability distribution is N (0, I).The vectors ε (i) are realizations of i.i.d.RVs whose common probability distribution is N (0, σ 2 I) with some fixed but unknown variance σ 2 .Note that W and σ 2 parametrize the joint probability distribution of the feature vectors z (i) via (9.12).PPCA amounts to maximum likelihood estimation (see Section 3.12) of the parameters W and σ 2 .This maximum likelihood estimation problem can be solved using computationally efficient estimation techniques such as EM [138, Appendix B].The implementation of PPCA via EM also offers a principled approach to handle missing data.Roughly speaking, the EM method allows to use the probabilistic model (9.12) to estimate missing raw features [138, Sec.4.1].We have motivated dimensionality reduction methods as transformations of (very long) raw feature vectors to a new (shorter) feature vector x such that it allows to reconstruct the raw features z with minimum reconstruction error (9.1).To make this requirement precise we need to define a measure for the size of the reconstruction error and specify the class of possible reconstruction maps.PCA uses the squared Euclidean norm (9.7) to measure the reconstruction error and only allows for linear reconstruction maps (9.6).Alternatively, we can view dimensionality reduction as the generation of new feature vectors x (i) that maintain the intrinsic geometry of the data points with their raw feature vectors z (i) .Different dimensionality reduction methods use different concepts for characterizing the "intrinsic geometry" of data points.PCA defines the intrinsic geometry of data points using the squared Euclidean distances between feature vectors.Indeed, PCA produces feature vectors x (i) such that for data points whose raw feature vectors have small squared Euclidean distance, also the new feature vectors x (i) will have small squared Euclidean distance.Some application domains generate data points for which the Euclidean distances between raw feature vectors does not reflect the intrinsic geometry of data points.As a point in case, consider data points representing scientific articles which can be characterized by the relative frequencies of words from some given set of relevant words (dictionary).A small Euclidean distance between the resulting raw feature vectors typically does not imply that the corresponding text documents are similar.Instead, the similarity between two articles might depend on the number of authors that are contained in author lists of both papers.We can represent the similarities between all articles using a similarity graph whose nodes represent data points which are connected by an edge (link) if they are similar (see Figure 8.9).Consider a dataset D = z (1) , . . ., z (m) whose intrinsic geometry is characterized by an unweighted similarity graph G = V := {1, . . ., m}, E .The node i ∈ V represents the i-th data point z (i) .Two nodes are connected by an undirected edge if the corresponding data points are similar.We would like to find short feature vectors x (i) , for i = 1, . . ., m, such that two data points i, i , whose feature vectors x (i) , x (i ) have small Euclidean distance, are well-connected to each other.This informal requirement must be made precise by a measure for how well two nodes of an undirected graph are connected.We refer the reader to literature on network theory for an overview and details of various connectivity measures [103].Let us discuss a simple but powerful technique to map the nodes i ∈ V of an undirected graph G to (short) feature vectors x (i) ∈ R n .This map is such that the Euclidean distances between the feature vectors of two nodes reflect their connectivity within G.This technique uses the Laplacian matrix L ∈ R m×m which is defined for an undirected graph G (with node set V = {1, . . ., m}) element-wiseHere, d (i) := {i : {i, i } ∈ E} denotes the number of neighbours (the degree) of node i ∈ V.It can be shown that the Laplacian matrix L is psd [148, Proposition 1].Therefore we can find a set of orthonormal eigenvectorswith corresponding (ordered in a non-decreasing fashion) eigenvaluesFor a given number n, we construct the feature vectorfor the ith data point.Here, we used the entries of of the first n eigenvectors (9.14).It can be shown that the Euclidean distances between the feature vectors x (i) , for i = 1, . . ., m, reflect the connectivities between data points i = 1, . . ., m in the similarity graph G.For a more precise statement of this informal claim we refer to the excellent tutorial [148].To summarize, we can construct numeric feature vectors for (non-numeric ) data points via the eigenvectors of the Laplacian matrix of a similarity graph for the data points.Algorithm 16 summarizes this feature learning method which requires as its input a similarity graph for the data points and the desired number n of numeric features.Note that Algorithm 16 does not make any use of the Euclidean distances between raw feature vectors and uses solely the similarity graph G to determine the intrinsic geometry of D.Algorithm 16 Feature Learning for Non-Numeric Datai=1 ; similarity graph G; number n of features to be constructed for each data point.1: construct the Laplacian matrix L of the similarity graph (see (9.13))2: compute EVD of L to obtain n orthonormal eigenvectors (9.14) corresponding to the smallest eigenvalues of L 3: for each data point i, construct feature vectorOutput: x (i) , for i = 1, . . ., mWe have discussed PCA as a linear dimensionality reduction method.PCA learns a compression matrix that maps raw features z (i) of data points to new (much shorter) feature vectors x (i) .The feature vectors x (i) determined by PCA depend solely on the raw feature vectors z (i) of a given dataset D. In particular, PCA determines the compression matrix such that the new features allow for a linear reconstruction (9.6) with minimum reconstruction error (9.7).For some application domains we might not only have access to raw feature vectors but also to the label values y (i) of the data points in D. Indeed, dimensionality reduction methods might be used as pre-processing step within a regression or classification problem that involves a labeled training set.However, in its basic form, PCA (see Algorithm 15) does not allow to exploit the information provided by available labels y (i) of data points z (i) .For some datasets, PCA might deliver feature vectors that are not very relevant for the overall task of predicting the label of a data point.Let us now discuss a modification of PCA that exploits the information provided by available labels of the data points.The idea is to learn a linear construction map (matrix) W such that the new feature vectors x (i) = Wz (i) allow to predict the label y (i) as good as possible.We restrict the prediction to be linear,with some weight vector r ∈ R n .While PCA is motivated by minimizing the reconstruction error (9.1), we now aim at minimizing the prediction error ŷ(i) − y (i) .In particular, we assess the usefulness of a given pair of construction map W and predictor r (see (9.16)), using the empirical risk(9.17)to guide the learning of a compressing matrix W and corresponding linear predictor weights r ((9.16)).The optimal matrix W that minimizes the empirical risk (9.17) can be obtained via the EVD (9.10) of the sample covariance matrix Q (9.9).Note that we have used the EVD of Q already for PCA in Section 9.2 (see (9.8)).Remember that PCA uses the n eigenvectors u (1) , . . ., u (n) corresponding to the n largest eigenvalues of Q.In contrast, to minimize (9.17), we need to use a different set of eigenvectors in the rows of W in general.To find the right set of n eigenvectors, we need the sample cross-correlation vectorThe entry q j of the vector q estimates the correlation between the raw feature z (i) j and the label y (i) .We then define the index set S := {j 1 , . . ., j n } such that q j 2 /λ j ≥ q j 2 /λ j for any j ∈ S, j ∈ {1, . . ., n } / ∈ S. (The set S is constituted by the indices j ∈ {1, . . ., n } of n largest numbers q j 2 /λ j .It can then be shown that the rows of the optimal compression matrix W (minimizing (9.17)) are the eigenvectors u (j) with j ∈ S. We summarize the overall feature learning method in Algorithm 17.The main focus of this section is on regression problems that involve data points with numeric labels (e.g., from the label space Y = R).Given the raw features and labels of the data points in D, Algorithm 17 determines new feature vectors x (i) that allow to linearly predict a numeric label with minimum squared error.A similar approach can be used for classification problems involving data points with a finite label space Y. Linear (or Fisher) discriminant analysis aims at constructing a compression matrix W such that the learnt Algorithm 17 Linear Feature Learning for Labeled Data Input: dataset z (1) , y (1) , . . ., z (m) , y (m) with raw features z (i) ∈ R n and numeric labels y (i) ∈ R ; length n of new feature vectors.1: compute EVD (9.10) of the sample covariance matrix (9.9) to obtain orthonormal eigenvectors u (1) , . . ., u (n ) corresponding to (decreasingly ordered) eigenvalues λ 1 ≥ λ 2 ≥ . . .≥ λ n ≥ 0 2: compute the sample cross-correlation vector (9.18) and, in turn, the sequence3: determine indices j 1 , . . ., j n of n largest elements in (9.20)4: construct compression matrix W := u (j 1 ) , . . ., u (jn) T ∈ R n×n 5: compute feature vectorOutput: x (i) , for i = 1, . . ., m, and compression matrix W.features x = Wz of a data point allow to predict its label y as accurately as possible [58].Many important application domains of ML involve sensitive data that is subject to data protection law [149].Consider a health-care provider (such as a hospital) holding a large database of patient records.From a ML perspective this databases is nothing but a (typically large) set of data points representing individual patients.The data points are characterized by many features including personal identifiers (name, social security number), bio-physical parameters as well as examination results .We could apply ML to learn a predictor for the risk of particular disease given the features of a data point.Given large patient databases, the ML methods might not be implemented locally at the hospital but using cloud computing.However, data protection requirements might prohibit the transfer of raw patient records that allow to match individuals with bio-physical properties.In this case we might apply feature learning methods to construct new features for each patient such that they allow to learn an accurate hypothesis for predicting a disease but do not allow to identify sensitive properties of the patient such as its name or a social security number.Let us formalize the above application by characterizing each data point (patient in the hospital database) using raw feature vector z (i) ∈ R n and a sensitive numeric property π (i) .We would like to find a compression map W such that the resulting features x (i) = Wz (i) do not allow to accurately predict the sensitive property π (i) .The prediction of the sensitive property is restricted to be a linear π(i) := r T x (i) with some weight vector r.Similar to Section 9.4 we want to find a compression matrix W that transforms, in a linear fashion, the raw feature vector z ∈ R n to a new feature vector x ∈ R n .However the design criterion for the optimal compression matrix W was different in Section 9.4 where the new feature vectors should allow for an accurate linear prediction of the label.In contrast, here we want to construct feature vectors such that there is no accurate linear predictor of the sensitive property π (i) .As in Section 9.4, the optimal compression matrix W is given row-wise by the eigenvectors of the sample covariance matrix (9.9).However, the choice of which eigenvectors to use is different and based on the entries of the sample cross-correlation vectorWe summarize the construction of the optimal privacy-preserving compression matrix and corresponding new feature vectors in Algorithm 18.Input: dataset z (1) , y (1) , . . ., z (m) , y (m) ; each data point characterized by raw features z (i) ∈ R n and (numeric) sensitive property π (i) ∈ R; number n of new features.1: compute the EVD (9.10) of the sample covariance matrix (9.9) to obtain orthonormal eigenvectors u (1) , . . ., u (n ) corresponding to (decreasingly ordered) eigenvalues λ 1 ≥ λ 2 ≥ . . .≥ λ n ≥ 0 2: compute the sample cross-correlation vector (9.21) and, in turn, the sequence3: determine indices j 1 , . . ., j n of n smallest elements in (9.22) 4: construct compression matrix W := u (j 1 ) , . . ., u (jn) T ∈ R n×n 5: compute feature vectorOutput: feature vectors x (i) , for i = 1, . . ., m, and compression matrix W.Algorithm 18 learns a map W to extract privacy-preserving features out of the raw feature vector of a data point.These new features are privacy-preserving as they do not allow to accurately predict (in a linear fashion) a sensitive property π of the data point.Another formalization for the preservation of privacy can be obtained using informationtheoretic concepts.This information-theoretic approach interprets data points, their feature vector and sensitive property, as realizations of RVs.It is then possible to use the mutual information between new features x and the sensitive (private) property π as an optimization criterion for learning a compression map h (Section 9.1).The resulting feature learning method (referred to as privacy-funnel) differs from Algorithm 18 not only in the optimization criterion for the compression map but also in that it allows it to be non-linear [89,128].Note that PCA uses an EVD of the sample covariance matrix Q (9.9).The computational complexity (e.g., measured by number of multiplications and additions) for computing this EVD is lower bounded by n min{n 2 , m 2 } [46,139].This computational complexity can be prohibitive for ML applications with n and m being of the order of millions or even billions.There is a computationally cheap alternative to PCA (Algorithm 15) for finding a useful compression matrix W in (9.5).This alternative is to construct the compression matrix W entry-wise W j,j := a j,j with a j,j ∼ p(a).The matrix entries (9.23) are realizations a i,j of i.i.d.RVs with some common probability distribution p(a).Different choices for the probability distribution p(a) have been studied in the literature [39].The Bernoulli distribution is used to obtain a compression matrix with binary entries.Another popular choice for p(a) is the multivariate normal (Gaussian) distribution.Consider data points whose raw feature vectors z are located near a s-dimensional subspace of R n .The feature vectors x obtained via (9.5) using a random matrix (9.23) allows to reconstruct the raw feature vectors z with high probability whenever n ≥ Cs log n .(The constant C depends on the maximum tolerated reconstruction error η (such that z − z 2 2 ≤ η for any data point) and the probability that the features x (see (9.23)) allow for a maximum reconstruction error η [39, Theorem 9. 27.].The focus of this chapter is on dimensionality reduction methods that learn a feature map delivering new feature vectors which are (significantly) shorter than the raw feature vectors.However, it might sometimes be beneficial to learn a feature map that delivers new feature vectors which are longer than the raw feature vectors.We have already discussed two examples for such feature learning methods in Sections 3.2 and 3.9.Polynomial regression maps a single raw feature z to a feature vector containing the powers of the raw feature z.This allows to use apply linear predictor maps to the new feature vectors to obtain predictions that depend non-linearly on the raw feature z.Kernel methods might even use a feature map that delivers feature vectors belonging to an infinite-dimensional Hilbert space [125].Mapping raw feature vectors into higher-dimensional (or even infinite-dimensional) spaces might be useful if the intrinsic geometry of the data points is simpler when looked at in the higher-dimensional space.Consider a binary classification problem where data points are highly inter-winded in the original feature space (see Figure 3.7).Loosely speaking, mapping into higher-dimensional feature space might "flatten-out" a non-linear decision boundary between data points.We can then apply linear classifiers to the higher-dimensional features to achieve accurate predictions.Exercise 9.1.Computational Burden of Many Features.Discuss the computational complexity of linear regression.How much computation do we need to compute the linear predictor that minimizes the average squared error on a training set?Exercise 9.2.Power Iteration.The key computational step of PCA amounts to an EVD of the psd matrix (9.9).Consider an arbitrary initial vector u (r) and the sequence obtained by iterating u (r+1) := Qu (r) / Qu (r) .(What (if any) conditions on the initialization u (r) ensure that the sequence u (r) converges to the eigenvector u (1) of Q that corresponds to its largest eigenvalue λ 1 ?Exercise 9. 3. Linear Classifiers with High-Dimensional Features.Consider a training set D consisting of m = 10 10 labeled data points z (1) , y (1) , . . ., z (m) , y (m) with raw feature vectors z (i) ∈ R 4000 and binary labels y (i) ∈ {−1, 1}.Assume we have used a feature learning method to obtain the new features x (i) ∈ {0, 1} n with n = m and such that the only non-zero entry of x (i) is xCan you find a linear classifier that perfectly classifies the training set?Chapter 10The successful deployment of ML methods depends on their transparency or explainability.We formalize the notion of an explanation and its effect using a simple probabilistic model in Section 10.1.Roughly speaking, an explanation is any artefact.such as a list of relevant features or a reference data point from a training set, that conveys information about a ML method and its predictions.Put differently, explaining a ML method should reduce the uncertainty (of a human end-user) about its predictions.Explainable ML is umbrella term for techniques that make ML method transparent or explainable.Providing explanations for the predictions of a ML method is particularly important when these predictions inform decision making [23].It is increasingly becoming a legal requirement to provide explanations for automated decision making systems [53].Even for applications where predictions are not directly used to inform far-reaching decisions, providing explanations is important.The human end users have an intrinsic desire for explanations that resolve the uncertainty about the prediction.This is known as the "need for closure" in psychology [29,75].Beside legal and psychological requirements, providing explanations for predictions might also be useful for validating and verifying ML methods.Indeed, the explanations of ML methods (and its predictions) can point the user (which might be a "domain expert") to incorrect modelling assumptions used by the ML method [37].Explainable ML is challenging since explanations must be tailored (personalized) to human end-users with varying backgrounds and in different contexts [87].The user background includes the formal education as well as the individual digital literacy.Some users might have received university-level education in ML, while other users might have no relevant formal training (such as an undergraduate course in linear algebra).Linear regression with few features might be perfectly interpretable for the first group but be considered a "black box" for the latter.To enable tailored explanations we need to model the user background as relevant for understanding the ML predictions.This chapter discusses explainable ML methods that have access to some user signal or feedback for some data points.Such a user signal might be obtained in various ways, including answers to surveys or bio-physical measurements collected via wearables or medical diagnostics.The user signal is used to determine (to some extent) the end-user background and, in turn, to tailor the delivered explanations for this end-user.Existing explainable ML methods can be roughly divided into two categories.The first category is referred to as "model-agnostic" [23]).Model-agnostic methods do not require knowledge of the detailed work principles of a ML method.These methods do not require knowledge of the hypothesis space used by a ML method but learn how to explain its predictions by observing them on a training set [22].A second category of explainable ML methods, sometimes referred to as "white-box" methods [23], uses ML methods that are considered as intrinsically explainable.The intrinsic explainability of a ML method depends crucially on its choice for the hypothesis space (see Section 2.2).This chapter discusses one recent method from each of the two explainable ML categories [72,69].The common theme of both methods is the use of information-theoretic concepts to measure the usefulness of explanations [27].Section 10.1 discusses a recently proposed model-agnostic approach to explainable ML that constructs tailored explanations for the predictions of a given ML method [72].This approach does not require any details about the internal mechanism of a ML method whose predictions are to be explained.Rather, this approach only requires a (sufficiently large) training set of data points for which the predictions of the ML method are known.To tailor the explanations to a particular user, we use the values of a user (feedback) signal provided for the data points in the training set.Roughly speaking, the explanations are chosen such that they maximally reduce the "surprise" or uncertainty that the user has about the predictions of the ML method.Section 10.2 discusses an example for a ML method that uses a hypothesis space that is intrinsically explainable [69].We construct an explainable hypothesis space by appropriate pruning of a given hypothesis space such as linear maps (see Section 3.1) or non-linear maps represented by either an ANN (see Section 3.11) or decision trees (see Section 3.10).This pruning is implemented via adding a regularization term to ERM (4.3), resulting in an instance of SRM (7.2) which we refer to as explainable empirical risk minimization (EERM).The regularization term favours hypotheses that are explainable to a user.Similar to the method in Section 10.1, the explainability of a map is quantified by information theoretic quantities.For example, if the original hypothesis space is the set of linear maps using a large number of features, the regularization term might favour maps that depend only on few features that are interpretable.Hence, we can interpret EERM as a feature learning method that aims at learning relevant and interpretable features (see Chapter 9).Consider a ML application involving data points with features x = x 1 , . . ., x n T ∈ R n and label y ∈ R. We use ML method that reads in some labelled data pointsx (1) , y (1) , x (2) , y (2) , . . ., x (m) , y (m) , (and learns a hypothesisThe precise working principle of this ML method for how to learn this hypothesis h is not relevant in what follows.user u consumig prediction ŷ ML method prediction ŷ explanation e The learnt predictor h(x) is applied to the features of a data point to obtain the predicted label ŷ := h(x).The prediction ŷ is then delivered to a human end-user (see Figure 10.1).Depending on the ML application, this end-user might be a streaming service subscriber [47], a dermatologist [36] or a city planner [158].Human users of ML methods often have some conception or model for the relation between features x and label y of a data point.This intrinsic model might vary significantly between users with different (social or educational) background.We will model the user understanding of a data point by a "user summary" u ∈ R. The summary is obtained by a (possibly stochastic) map from the features x of a data point.For ease of exposition, we focus on summaries obtained by a deterministic map u(•) : R n → R : x → u := u(x).(10.3)However, the resulting explainable ML method can be extended to user feedback u modelled as a stochastic maps.In this case, the user feedback u is characterized by a probability distribution p(u|x).The user feedback u is determined by the features x of a data point.We might think of the value u for a specific data point as a signal that reflects how the human end-user interprets (or perceives) the data point, given her knowledge (including formal education) and the context of the ML application.We do not assume any knowledge about the details for how the signal value u is formed for a specific data point.In particular, we do not know any properties of the map u(•) : x → u.The above approach is quite flexible as it allows for very different forms of user summaries.The user summary could be the prediction obtained from a simplified model, such as linear regression using few features that the user anticipates as being relevant.Another example for a user summary u could be a higher-level feature, such as eye spacing in facial pictures, that the user considers relevant [67].Note that, since we allow for an arbitrary map in (10.3), the user summary u(x) obtained for a random data point with features x might be correlated with the prediction ŷ = h(x).As an extreme case, consider a very knowledgable user that is able to predict the label of any data point from its features as well as the ML method itself.In this case, the maps (10.2) and ( 10.3) might be nearly identical.However, in general the predictions delivered by the learnt hypothesis (10.2) will be different from the user summary u(x).We formalize the act of explaining a prediction ŷ = h(x) as presenting some additional quantity e to the user (see Figure 10.1).This explanation e can be any artefact that helps the user to understand the prediction ŷ, given her understanding u of the data point.Loosely speaking, the aim of providing explanation e is to reduce the uncertainty of the user u about the prediction ŷ [75].For the sake of exposition, we construct explanations e that are obtained via a determin-istic map e(•) : R n → R : x → e := e(x), (10.4) from the features x of a data point.However, the explainable ML methods in this chapter can be generalized without difficulty to handle explanations obtained from a stochastic map.In the end, we only require the specification of the conditional probability distribution p(e|x).The explanation e (10.4) depends only on the features x but not explicitly on the prediction ŷ.However, our method for constructing the map (10.4) takes into account the properties of the predictor map h(x) (10.2).In particular, Algorithm 19 below requires as input the predicted labels ŷ(i) for a set of data points (that serve as a training set for our method).To obtain comprehensible explanations that can be computed efficiently, we must typically restrict the space of possible explanations to a small subset F of maps (10.4).This is conceptually similar to the restriction of the space of possible predictor functions in a ML method to a small subset of maps which is known as the hypothesis space.In what follows, we model data points as realizations of i.i.d.RVs with common (joint) probability distribution p(x, y) of features and label (see Section 2. 1.4).Modelling the data points as realizations of RVs implies that the user summary u, prediction ŷ and explanation e are also realizations of RVs.The joint distribution p(u, ŷ, e, x, y) conforms with the Bayesian network [111] depicted in Figure 10.2.Indeed, p(u, ŷ, e, x, y) = p(u|x) • p(e|x) • p(ŷ|x) • p(x, y).(10.5)We measure the amount of additional information provided by an explanation e for a prediction ŷ to some user u via the conditional mutual information (MI) [27, Ch.The conditional MI I(e; ŷ|u) can also be interpreted as a measure for the amount by which the explanation e reduces the uncertainty about the prediction ŷ which is delivered to some user u.Providing the explanation e serves the apparent human need to understand observed phenomena, such as the predictions from a ML method [75].A simple probabilistic graphical model (a Bayesian network [84,79]) for explainable ML.We interpret data points (with features x and label y) along with the user summary u, e and predicted label ŷ as realizations of RVs.These RVs satisfy conditional independence relations encoded by the directed links of the graph [79].Given the data point, the predicted label ŷ, the explanation e and the user summary u are conditionally independent.This conditional independence is trivial if all these quantities are obtained from deterministic maps applied to the features x of the data point.Capturing the effect of an explanation using the probabilistic model (10.6) offers a principled approach to computing an optimal explanation e.We require the optimal explanation e * to maximize the conditional MI (10.6) between the explanation e and the prediction ŷ conditioned on the user summary u of the data point.Formally, an optimal explanation e * solves I(e * ; ŷ|u) = sup e∈F I(e; ŷ|u).(The choice for the subset F of valid explanations offers a trade-off between comprehensibility, informativeness and computational cost incurred by an explanation e * (solving (10.7)).The maximization problem (10.7) for obtaining optimal explanations is similar to the approach in [22].However, while [22] uses the unconditional MI between explanation and prediction, (10.7) uses the conditional MI given the user summary u.Therefore, (10.7) delivers personalized explanations that are tailored to the user who is characterized by the summary u.It is important to note that the construction (10.7) allows for many different forms of explanations.An explanation could be a subset features of a data point (see [116] and Section 10. 1.2).More generally, explanations could be obtained from simple local statistics (averages) of features that are considered related, such as nearby pixels in an image or consecutive amplitude values of an audio signal.Instead of individual features, carefully chosen data points from a training set can also serve as an explanation [92,117].Let us illustrate the concept of optimal explanations (10.7) using linear regression.We model the features x as a realization of a multivariate normal random vector with zero mean and covariance matrix C x ,x ∼ N (0, C x ).(10.8)The predictor and the user summary are linear functions ŷ := w T x, and u := v T x. (10.9)We construct explanations via subsets of individual features x j that are considered most relevant for a user to understand the prediction ŷ (see [98,Definition 2] and [97]).Thus, we consider explanations of the form e := {x j } j∈E with some subset E ⊆ {1, . . ., n}.(The complexity of an explanation e is measured by the number |E| of features that contribute to it.We limit the complexity of explanations by a fixed (small) sparsity level, |E| ≤ s( n).( 10.11) Modelling the feature vector x as Gaussian (10.8) implies that the prediction ŷ and user summary u obtained from (10.9) is jointly Gaussian for a given E (10.4).Basic properties of multivariate normal distributions [27,Ch. 8], allow to develop (10.Here, σ 2 ŷ|u denotes the conditional of the prediction ŷ, conditioned on the user summary u.Similarly, σ 2 ŷ|u,E denotes the conditional variance of ŷ, conditioned on the user summary u and the subset {x j } j∈E of features.The last step in (10.12) follows from the fact that ŷ is a scalar random variable.The first component of the final expression of (10.12) does not depend on the index set E used to construct the explanation e (see (10.10)).Therefore, the optimal choice for E solvesThe maximization (10.13) is equivalent toIn order to solve (10.14), we relate the conditional variance σ 2 ŷ|u,E to a particular decomposition ŷ = ηu + j∈EFor an optimal choice of the coefficients η and β j , the variance of the error term in (10.15) is given by σ 2 ŷ|u,E .Indeed,Inserting (10.29) into (10.14),an optimal choice E (of feature) for the explanation of prediction ŷ to user u is obtained from .18)An optimal subset E opt of features defining the explanation e (10.10) is obtained from any solution β opt of ( 10.18) viaSection 10.1.2uses the probabilistic model (10.8) to construct optimal explanations via the (support of the) solutions β opt of the sparse linear regression problem (10.18).To obtain a practical algorithm for computing (approximately) optimal explanations (10.19), we approximate the expectation (10.18) using an average over the training set x (i) , ŷ(i) , u (i) , for i = 1, . . ., m.This resulting method for computing personalized explanations is summarized in Algorithm 19.Input: explanation complexity s, training set x (i) , ŷ(i) , u (i) for i = 1, . . ., m 1: compute β by solving(10.20)Output: feature set E := supp β Algorithm 19 is interactive in the sense that the user has to provide a feedback signal u (i) for the data points with features x (i) .Based on the user feedback u (i) , for i = 1, . . ., m, Algorithm 19 learns an optimal subset E of features (10.10) that are used for the explanation of predictions.The sparse regression problem (10.20) becomes intractable for large feature length n.However, if the features are weakly correlated with each other and the user summary u, the solutions of (10.20) can be found by efficient convex optimization methods.One popular method to (approximately) solve sparse regression (10.20) is the Lasso (see Section 3.4),There is large body of work that studies the choice of Lasso parameter λ in (10.21) such that solutions (10.21) coincide with the solutions of (10.20) (see [59,144] and references therein).The proper choice for λ typically requires knowledge of statistical properties of data.If such a probabilistic model is not available, the choice of λ can be guided by simple validation techniques (see Section 6.2).Section 7.1 discussed SRM (7.1) as a method for pruning the hypothesis space H used in ERM (4.3).This pruning is implemented either via a (hard) constraint as in (7.1) or by adding a regularization term to the training error as in (7.2).The idea of SRM is to avoid (prune away) hypothesis that perform good on the training set but poorly outside, i.e., they do not generalize well.Here, we will use another criterion for steering the pruning and construction of regularization terms.In particular, we use the (intrinsic) explainability of a hypotheses map as a regularization term.To make the notion of explainability precise we use again the probabilistic model of Section 10. 1.1.We interpret data points as realizations of i.i.d.RVs with common (joint) probability distribution p(x, y) of features x and label y.A quantitative measure the intrinsic explainability of a hypothesis h ∈ H is the conditional (differential) entropy [27,Ch. 2 and 8] H(ŷ|u) := −E log p(ŷ|u) .( 10.22) The conditional entropy (10.22) indicates the uncertainty about the prediction ŷ, given the user summary û = u(x).Smaller values H(ŷ; u) correspond to smaller levels of uncertainty in the predictions ŷ that is experienced by user u.We obtain EERM by requiring a sufficiently small conditional entropy (10.22) of a hypothesis, ĥ ∈ argminThe random variable ŷ = h(x) in the constraint of (10.23) is obtained by applying the predictor map h ∈ H to the features.The constraint H(ŷ|û) ≤ η in (10.23) enforces the learnt hypothesis ĥ to be sufficiently explainable in the sense that the conditional entropy H( ĥ|û) ≤ η does not exceed a prescribed level η.Let us now consider the special case of EERM (10.23) for the linear hypothesis space h (w) (x) := w T x with some parameter vector w ∈ R n .(Moreover, we assume that the features x of a data point and its user summary u are jointly Gaussian with mean zero and covariance matrix C,x T , û T ∼ N (0, C). (10.25)Under the assumptions (10.24) and (10.25) (see [27,Ch. 8]),Here, we used the conditional variance σ 2 ŷ|û of ŷ given the random user summary u u(x).Inserting ( 10.26)  The identity (10.29) relates the conditional variance σ 2 ŷ|û to the minimum mean squared error that can be achieved by estimating ŷ using a linear estimator ηû with some η ∈ R. Inserting ( 10.29) and ( 10. 24) into (10.28),ĥ ∈ argminThe inequality constraint in (10.30) is convex [14, Ch. 4.2.].For squared error loss, the objective function L(h (w) ) is also convex.Thus, for linear least squares regression, we can reformulate (10.30) as an equivalent (dual) unconstrained problem [14,Ch. 5] ĥ ∈ argmin Output: weights w of explainable linear hypothesis Identify the matrix Q ∈ R n×n and the vector q ∈ R n .k-fold cross-validation (k-fold CV) k-fold cross-validation is a method for learning and validating a hypothesis using given dataset.This method first divides the dataset evenly into k subsets or "folds" and then executes k repetitions of training and validation.Each repetition uses a different fold as the validation set and the remaining k − 1 folds as a training set.The final output is the average of the validation errors obtained from the k repetitions.159-162k-means The k-means algorithm is a hard clustering method which assigns each data points to pecisely one out of k different clusters.The method iteratively updates this assignment in order to minimize the average distance between data points in their nearest cluster mean (centre).14, 202, 203, 206, 207, 210, 217-220, 222, 256, 258 accuracy Consider data points characterized by features x ∈ x and a categorical label y which takes on values from a finite label space Y.The accuracy of a hypothesis h : X → Y, when applied to the data points in a dataset D = x (1) , y (1) , . . ., x (m) , y (m) is then defined as 1 − (1/m) m i=1 L x (i) , y (i) , h using the 0/1 loss (2.9) 63, 80, 107 activation function Each artificial neuron within an ANN consists of an activation function that maps the inputs of the neuron to a single output value.In general, an activation function is a non-linear map of the weighted sum of neuron inputs (this weighted sum is the activation of the neuron).dataset With a slight abuse of notation we use the terms "dataset" or "set of data points" to refer to an indexed list of data points z (1) , . . .,. Thus, there is a first data point z (1) , a second data point z (2) and so on.Strictly speaking a dataset is a list and not a set [56].By using indexed lists of data points we avoid some of the challenges arising in concept of an abstract set.[12,58,153].Expectation maximization delivers an approximation to the maximum likelihood estimate for the model parameters w. 215, 232, 269 expert ML aims at learning a hypothesis h that accurately predicts the label of a data point based on its features.We measure the prediction error using some loss function.Ideally we want to find a hypothesis that incurs minimum loss.One approach to make this goal precise is to use the i.i.d.assumption and use the resulting Bayes risk as the benchmark level for the (average) loss of a hypothesis.Alternatively we might know a reference or benchmark hypothesis h which might be obtained by some existing ML method.We can then compare the loss incurred by h with the loss incurred by h .Such a reference or baseline hypothesis h is referred to as an expert.Note that an expert might deliver very poor predictions.We typically compare against many different experts and aim at incurring not much more loss than the best among those experts (this is known as regret minimization) [20,60].first 69, 70, 260, 270 explainability We define the (subjective) explainability of a ML method as the level of predictability (or lack of uncertainty) in its predictions delivered to a specific human user.Quantitative measures for the (subjective) explainability can be obtained via probabilistic models for the data fed into the ML method [72,69].34, 242-244, 251, 253 explainable empirical risk minimization An instance of structural risk minimization that adds a regularization term to the training error in ERM.The regularization term is chosen to favour hypotheses that are intrinsically explainable for a user.243, 244, 251, 252 explainable machine learning Explainable ML methods aim at complementing its predictions with some form of explanation for how each prediction has been obtained.3,23,34,242,243,[245][246][247] feature map A map that transforms the original features of a data point into new features.The so-obtained new features might preferable over the oginal features for several reasons.For example, the shape of datasets might become simpler in the new feature space, allowing to use linear models in the new features.Another reason could be that the number of new features is much smaller which is preferable in terms of avoiding overfitting.If the feature map delivers two new feautres, we can depict data points in a scatterplot.54,55,84,88,155,240 feature matrix Conisder a dataset D with m data points, each of them characterized by the features x (i) , i = 1, . . ., m.The feature matrix X of D is constructed by stacking, colum-wise, the features of the data points into a matrix X = x (1) , . . ., x (m) of size m × n. first 119 feature space The feature space of a given ML application or method is constituted by all potential values that the feature vector of a data point can take on.Within this book the most frequently used choice for the feature space is the Euclidean space R n with dimension n being the number of individual features of a data point.41-43, 55, 94, 98, 103, 263 features Features are those properties of a data point that can be measured or computed in an automated fashion.For example, if a data point is a bitmap image, then we could use the red-green-blue intensities of its pixels as features.Some widely used synonyms for the term feature are "covariate","explanatory variable", "independent variable", "input (variable)", "predictor (variable)" or "regressor" [52,30,38].However, this book makes consequent use of the term features for low-level properties of data points that can be measured easily.8,35,37,38,43,55,60,61,82,87,90,95,96,101,119,129,130,140,195,222,254,257,261,265,266,268,272 federated learning (FL) Federated learning is an umbrella term for ML methods that train models in a collaborative fashion using decentralized data and computation.25,114 Finnish Meteorological Institute The Finnish Meteorological Institute is a government agency responsible for gathering and reporting weather data in Finland. 16,17,25,73,126 Gaussian mixture model Gaussian mixture models (GMM) are a family of probabilistic models for data points.Within a GMM, the feature vector x of a data point is terpreted as being drawn from one out of k different multivariate normal (Gaussian) distributions indexed by c = 1, . . ., k.The probability that the feature vector x is drawn from the c-th Gaussian distribution is denoted p c .The GMM is parametrized by the probability p c of x being drawn from the c-th Gaussian distribution as well as the mean vectors µ (c) and covariance matrices Σ (c) for c = 1, . . . , k. 123, 213-220, 256, 258 gradient For a real-valued function f : R n → : w → f (w), a vector a such that lim w→w f (w)− f (w )+a T (w−w ) w−w = 0 is referred to as the gradient of f at w .If such a vector exists it is denoted ∇f (w ) or ∇f (w) w . 11, 33, 53, 65, 131, 132, 134, 138, 141, 144, 147, 148, 258, 271, 272 gradient descent (GD) Gradient descent is an iterative method for finding the minimum of a differentiable function f (w).64, 65, 119, 120, 131, 133-150, 176, 177, 186, 194-196, 265, 272 gradient-based method Gradient-based methods are iterative algorithms for finding the minimum (or maximum) of a differentiable objective function of a parameter vector.These algorithms construct a sequence of approximations to an optimal parameter vector whose function value is minimal (or maximal).As their name indicates, gradientbased methods use the gradients of the objective function evaluated during previous iterations to construct a new (hopefully) improved approximation of an optimal parameter vector.3,9,33,58,59,64,94,95,112,115,116,122,131,132,134,135,147,148,176,177,272 hard clustering Hard clustering refers to the task of partitioning a given set of data points into (few) non-overlapping clusters.Each data point is assigned to one specific cluster.201-203, 212, 217, 222 high-dimensional regime A ML method or problem belongs to the high-dimensional regime if the effective dimension of the model is larger than the number of available (labeled) data points.For example, linear regression belongs to the high-dimensional regime whenever the number n of features used to characterize data points is larger than the number of data points in the training set.Another example for the highdimensional regime are deep learning methods that use a hypothesis space generated by a ANN with much more tunable weights than the number of data points in training set.The recent field of high-dimensional statistics uses probability theory to analyze ML methods in the high-dimensional regime [151,17].87,152,169 Hilbert space A Hilbert space is a linear vector space that is equipped with an inner product between pairs of vectors.One important example for a Hilbert space is the Euclidean spaces R n , for some dimension n, which consists of Euclidean vectors u = u 1 , . . ., u n T along with the inner product u T v. 240hinge loss Consider a data point that is characterized by a feature vector x ∈ R n and a binary label y ∈ {−1, 1}.The hinge loss incurred by a specific hypothesis h is defined as (2.11).A regularized variant of the hinge loss is used by the SVM to learn a linear classifier with maximum margin between the two classes (see Figure 3 hypothesis A map (or function) h : X → Y from the feature space X to the label space Y.Given a data point with features x we use a hypothesis map h to estimate (or approximate) the label y using the predicted label ŷ = h(x).ML is about learning (or finding) a hypothesis map h such that y ≈ h(x) for any data point.18, 19, 31, 48, 49, 58, 60-63, 78, 80, 87, 90, 93, 94, 98, 107, 119, 128-130, 138, 151, 152, 167, 177, 181, 183, 184, 196, 197, 201, 254, 256, 259, 266, 268, 272, 273 hypothesis space Every practical ML method uses a specific hypothesis space (or model) H.The hypothesis space of a ML method is a subset of all possible maps from the feature space to label space.The design choice of the hypothesis space should take into account available computational resources and statistical aspects.If the computational infrastructure allows for efficient matrix operations and we expect a linear relation between feature values and label, a resonable first candidate for the hypothesis space is the space of linear maps (2.4).2, 3, 12, 17, 18, 20, 31, 35, 36, 49-58, 66, 71, 74, learning rate Consider an iterative method for finding or learning a good choice for a hypothesis.Such an iterative method repeats similar (update) steps that adjust or modify the current choice for the hypothesis to obtain an improved hypothesis.A prime example for such an iterative learning method is GD and its variants (see 5).We refer by learning rate to any parameter of an iterative learning method that controls the extent by which the current hypothesis might be modified or improved in each iteration.A prime example for such a parameter is the step size used in GD.Within this book we use the term learning rate mostly as a synonym for the step size of (a variant of) GD 9,[134][135][136][137][138][139][140]142,[144][145][146][147][148][149][150]177,195,272 learning task A learning tasks consists of a specific choice for a collection of data points (e.g., all images stored in a particular database), their features and labels. 182, 196, 197, 273 least absolute deviation regression Least absolute deviation regression uses the average of the absolute precondition errors to find a linear hypothesis.115least absolute shrinkage and selection operator (Lasso) The least absolute shrinkage and selection operator (Lasso) is an instance of SRM for learning the weights w of a linear map h(x) = w T x.The Lasso minimizes the sum consisting of an average squared error loss (as in linear regression) and the scaled 1 norm of the weight vector w.87, 250 linear classifier A classifier h(x) maps the feature vector x ∈ R n of a data point to a predicted label ŷ ∈ Y out of a finite set of label values Y.We can characterize such a classifier equivalently by the decision regions R a , for every possible label value a ∈ Y.Linear classifiers are such that the boundaries between the regions R a are hyperplanes in R n .21, 53, 54, 94-97, 100, 141, 263 linear regression Linear regression aims at learning a linear hypothesis map to predict a numeric label based on numeric features of a data point.The quality of a linear hypothesis map is typically measured using the average squared error loss incurred on a set of labeled data points (the training set).2, 15, 33, 36, 45, 56, 78, 82, 84-89, 92, 96, 99, 104, 105, 107, 108, 111, 114-117, 119, 120, 126, 127, 129-132, 134, 137-139, 142, 152-155, 167, 168, 172, 177, 178, 182, 183, 186, 192-195, 218, 221, 224, 229-231, 240, 242, 245, 248, 249, 252, 265 logistic loss Consider a data point that is characterized by the features x and a binary y ∈ {−1, 1}.We use a hypothesis h to predict the label y solely from the features x.The logistic loss incurred by a specific hypothesis h is defined as (2.12).59,64,65,79,80,90,94,95,107,141,165,267 logistic regression Logistic regression aims at learning a linear hypothesis map to predict a binary label based on numeric features of a data point.The quality of a linear hypothesis map (classifier) is measured using its average logistic loss on some labeled data points (the training set).45, 53, 64, 89, 90, 92, 94-96, 99, 100, 104, 105, 107, 108, 120, 124, 125, 131, 132, 134, 137, 141, 142, 149, 165 loss With a slight abuse of language, we use the term loss either for loss function itself or for its value for a specific pair of data point and hypothesis . 2, 10, 19, 22, 32, 33, 35, 58-60, 64, 65, 79, 82, 87, 93, 95, 111, 115, 122, 127, 128, 130, 132, 138, 151-153, 157, 162, 167, 177, 181, 184, 189, 254, 255, 270-273 loss function A loss function is a map L : X × Y × H → R + : x, y , h → L ((x, y), h) which assigns a pair consisting of a data point, with features x and label y, and a hypothesis h ∈ H the non-negative real number L ((x, y), h).The loss value L ((x, y), h) quantifies the discrepancy between the true label y and the predicted label h(x).Smaller (closer to zero) values L ((x, y), h) mean a smaller discrepancy between predicted label and true label of a data point.Figure 2.12 depicts a loss function for a given data point, with features x and label y, as a function of the hypothesis h ∈ H. 2, 3, 18-20, 25, 30-33, 35, 36, 45, 53, 57-61, 63-65, 68, 78, 81, 82, 85, 92-96, 102, 105, 111, 113, 115, 116, 127, 128, 132, 133, 151, 152, 162, 164, 165, 175, 182, 197, 231, 260, 266, 267, 271 maximum Given a set of real numbers, the maximum is the largest of those numbers.74 maximum likelihood Consider data points D = z (1) , . . ., z (m) } that are interpreted as realizations of i.i.d.RVs with a common probability distribution p(z; w) which depends on a parameter vector w ∈ W ⊆ R n .Maximum likelihood methods aim at finding a parameter vector w such that the probability (density) p(D; w) = m i=1 p(z (i) ; w) of observing the data is maximized.Thus, the maximum likelihood estimator is obtained as a solution to the optimization problem max w∈W p(D; w).48,58,91,92,102,103,123,215,260 mean The of a real-valued random variable.47 mean squared estimation error Consider a ML method that uses a parametrized hypothesis space.For a given training set, whose data points are interpreted as realizations of RVs, the ML method learns the parameters incurring the estimation error ∆w.The mean squared estimation error is defined as the expectation E ∆w 2 of the squared Euclidean norm of the estimation error.170, 172 metric A metric refers to a loss function that is used solely for the final performance evaluation of a learnt hypothesis.The metric is typically a loss function that has a "natural" interpretation (such as the 0/1 loss (2.9)) but is not a good choice to guide the learning process, e.g., via ERM.For ERM, we typically prefer loss functions that depend smoothly on the (parameters of the) hypothesis.that allow to choose between different hypothesis maps.For example, the linear model H := {h : h(x) = w 1 x + w 2 } consists of all hypothesis maps h(x) = w 1 x + w 2 with a particular choice for the parameters w 1 , w 2 .Another example of parameters are the weights assigned to the connections of an ANN.17, 152positive semi-definite A symmetric matrix Q = Q T ∈ R n×n is referred to as positive semi-definite if x T Qx ≥ 0 for every vector x ∈ R n .6,10,103,136,140,194,227,234,240,256 prediction A prediction is an estimate or approximation for some quantity of interest.ML revolves around learning or finding a hypothesis map h that reads in the features x of a data point and delivers a prediction y := h(x) for its label y. probability distribution The data generated in some ML applications can be reasonably well modelled as realizations of a RV.The overall statistical properties (or intrinsic structure) of such data are then governed by the probability distribution of this RV.We use the term probability distribution in a highly informal manner and mean the collection of probabilities assigned to different values or value ranges of a RV.The probability distribution of a binary RV y ∈ {0, 1} is fully specified by the probabilities p(y = 0) and p(y = 1) = 1 − p(y = 0) .The probability distribution of a realvalued RV x ∈ R might be specified by a probability density function p(x) such that p(x ∈ [a, b]) ≈ p(a)|b−a|.In the most general case, a probability distribution is defined by a probability measure [50,11].22, 25, 26, 47, 56, 58, 63, 66-68, 95, 110-113, 122, 123, 125, 128, 129, 145, 148, 153, 154, 157, 159, 168, 174, 175, 177, 178, 189, 255, 256, 260, 264, 266, 268, 269, 271 random forest A random forest is a set (ensemble) of different decision trees.Each of these decision trees is obtained by fitting a perturbed copy of the original dataset.187 random variable (RV) A random variable is a mapping from a probability space P to a value space [11].The probability space, whose element are elementary events, is equipped with a probability measure that assigns a proability to subsets of P. A binary risk Consider a hypothesis h that is used to predict the label y of a data point based on its features x. measure the quality of a particular prediction using a loss function L ((x, y), h).If we interpret data points as realizations of i.i.d.RVs, also the L ((x, y), h) becomes the realization of a RV.Using such an i.i.d.assumption allows to define the risk of a hypothesis as the expected loss E L ((x, y), h) .Note that the risk of h depends on both, the specific choice for the loss function and the common probability distribution of the data points.step size Many ML methods use iterative optimization methods (such as gradient-based methods) to construct a sequence of increasingly accurate hypothesis maps h (1) , h (2) , . ... The rth iteration of such an algorithm starts from the current hypothesis h (r) and tries to modify to obtain an improved hypothesis h (r+1) .Iterative algorithms often use a step size (hyper-) parameter.The step size controls the amount by which a single iteration can change or modify the current hypothesis.Since the overall goal of such iteration ML methods is to learn a (approximately) optimal hypothesis we refer to a step size parameter also as a learning rate.subgradient descent Subgradient descent is a generalization of GD that is obtained by using sub-gradients (instead of gradients) to construct local approximations of an objective function such as the empirical risk L h (w) D as a function of the parameters w of a hypothesis h (w) .65support vector machine A binary classification method for learning a linear map that maximally separates data points the two classes in the feature space ("maximum margin").Maximizing this separation is equivalent to minimizing a regularized variant of the hinge loss (2.11).training set A set of data points that is used in ERM to learn a hypothesis ĥ.The average loss of ĥ on the training set is referred to as the training error.The comparison between training error and validation error of ĥ allows to diagnose ML methods and informs how to improve them (e.g., using a different hypothesis space or collecting more data points).9, 10, 22, 24, 25, 30, 31, 34, 51, 61, 62, 68, 73, 76, 79, 86, 87, 90, 94, 95, 103, 104, 107, 109-114, 116, 118-120, 122, 125-130, 137, 138, 141, 145, 146, 149, 151-161, 163-170, 172, 173, 176-178, 180, 181, 183, 184, 186-188, 190, 192, 198-200 Vapnik-Chervonenkis (VC) dimension The VC dimension of an infinite hypothesis space is a widely-used measure for its size.We refer to [126] for a precise definition of VC dimension as well as a discussion of its basic properties and use in ML. 56