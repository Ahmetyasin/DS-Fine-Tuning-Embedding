As mentioned in the previous section, in the transfer function-noise model in Eq. (6.11) x t and N t are assumed to be independent.Moreover, we will assume that the noise N t can be represented by an ARIMA(p, d, q) model, (6.13)where {𝜀 t } is white noise with E(𝜀 t ) = 0. Hence the transfer function-noise model can be written asAfter rearranging Eq. (6.14), we have=𝛿 * (B)𝜀 t (6.15) 𝛿 * (B)y t = w * (B)x t−b + 𝜃 * (B)𝜀 t orIgnoring the terms involving x t , Eq. (6.16) is the ARMA representation of the response y t .Due to the addition of x t , the model in Eq. (6.16) is also called an ARMAX model.Hence the transfer function-noise model as given in Eq. ( 6. 16) can be interpreted as an ARMA model for the response with the additional exogenous factor x t .For the bivariate time series (x t , y t ), we define the cross-covariance function as 𝛾 xy (t, s) = Cov(x t , y s ) (6.17)CROSS-CORRELATION FUNCTIONAssuming that (x t , y t ) is (weakly) stationary, we have E(x t ) = 𝜇 x , constant for all t E(y t ) = 𝜇 y , constant for all t Cov(x t , x t+j ) = 𝛾 x (j), depends only on j Cov(y t , y t+j ) = 𝛾 y (j), depends only on j and Cov(x t , y t+j ) = 𝛾 xy (j), depends only on j for j = 0, ±1, ±2, … Hence the cross-correlation function (CCF) is defined as 𝜌 xy (j) = corr(x t , y t+j ) = 𝛾 xy (j) √ 𝛾 x (0)𝛾 y (0) for j = 0, ±1, ±2, … (6.18)It should be noted that 𝜌 xy (j) ≠ 𝜌 xy (−j) but 𝜌 xy (j) = 𝜌 yx (−j).We then define the correlation matrix at lag j as For a given sample of N observations, the sample cross covariance is estimated from Similarly, the sample cross correlations are estimated from r xy (j) = ρxy (j) = γxy (j) √ γx (0)γ y (0)for j = 0, ±1, ±2, … (6.22)whereSampling properties such as the mean and variance of the sample CCF are quite complicated.For a few special cases, however, we have the following.1.For large data sets, E(r xy (j)) ≈ 𝜌 xy (j) but the variance is still complicated.2. If x t and y t are autocorrelated but un(cross)correlated at all lags, that is, 𝜌 xy (j) = 0, we then have E(r xy (j)) ≈ 0 and var(r xy (j)) ≈ (1∕N) ∑ ∞ i=−∞ 𝜌 x (i)𝜌 y (i). 3.If 𝜌 xy (j) = 0 for all lags j but also x t is white noise, that is, 𝜌 x (j) = 0 for j ≠ 0, then we have var(r xy (j)) ≈ 1∕N for j = 0, ±1, ±2, …. 4. If 𝜌 xy (j) = 0 for all lags j but also both x t and y t are white noise, then we have corr(r xy (i), r xy (j)) ≈ 0 for i ≠ j.In this section, we will discuss the issues regarding the specification of the model order in a transfer function-noise model.Further discussion can be found in Kulahci (2006a,b, 2011).We will first consider the general form of the transfer function-noise model with time delay given as(6.23)MODEL SPECIFICATIONThe six-step model specification process is outlined next.Step 1. Obtaining the preliminary estimates of the coefficients in v(B).One approach is to assume that the coefficients in v(B) are zero except for the first k lags:We can then attempt to obtain the initial estimates for v 1 , v 2 , … , v k through ordinary least squares.However, this approach can lead to highly inaccurate estimates as x t may have strong autocorrelation.Therefore a method called prewhitening of the input is generally preferred.For the transfer function-noise model in Eq. (6.23), suppose that x t follows an ARIMA model asx t = 𝜃 x (B)𝛼 t , (6.24)where 𝛼 t is white noise with variance 𝜎 2 𝛼 .Equivalently, we have (6.25)In this notation, 𝜃 x (B) −1 𝜑 x (B) can be seen as a filter that when applied to x t generates a white noise time series, hence the name "prewhitening."When we apply this filter to the transfer function-noise model in Eq. (6.23), we obtainThe cross covariance between the filtered series 𝛼 t and 𝛽 t is given byNote that Cov(𝛼 t , N * t+j ) = 0 since x t and N t are assumed to be independent.From Eq. (6.27), we have 𝛾 𝛼𝛽 = v j 𝜎 2 𝛼 and hencewhere 𝜌 𝛼𝛽 (j) = corr(𝛼 t , 𝛽 t+j ) is the CCF between 𝛼 t and 𝛽 t .So through the sample estimates we can obtain the initial estimates for the v j : vj = r 𝛼𝛽 (j) σ𝛽 σ𝛼 .(6.29) Equation (6.29) implies that there is a simple relationship between the impulse response function, v(B), and the cross-correlation function of the prewhitened response and input series.Hence the estimation of the coefficients in v(B) is possible through this relationship as summarized in Eq. (6.29).A similar relationship exists when the response and the input are not prewhitened (see Box et al., 2008).However, the calculations become fairly complicated when the series are not prewhitened.Therefore we strongly recommend the use of prewhitening in model identification and estimation of transfer function-noise models.Moreover, since 𝛼 t is white noise, the variance of r 𝛼𝛽 (j) is relatively easier to obtain than that of r xy (j).In fact, from the special case 3 in the previous section, we have Var[r 𝛼𝛽 (j)] ≈ 1 N , (6.30)if 𝜌 𝛼𝛽 (j) = 0 for all lags j.We can then use ±2∕√ N as the approximate 95% confidence interval to judge the significance of r 𝛼𝛽 (j).Step 2. Specifications of the orders r and s.Once the initial estimates of the v j from Eq. (6.29) are obtained, we can use them to specify the orders r and s inThe specification of the orders r and s can be accomplished by plotting the v j .In Figure 6.2, we have an example of the plot of the initial estimates for the v j in which we can see that v0 ≈ 0, implying that there might be a time delay (i.e., b = 1).However, for j > 1, we have an exponential decay pattern, suggesting that we may have r = 1, which implies v j − 𝛿v j−1 = 0 for j > 1 and s = 0.Hence for this example, our initial attempt in specifying the order of the transfer function noise model will be (6.31)Caution: In model specification, one should be acutely aware of overparameterization as for an arbitrary 𝜂, the model in Eq. (6.31) can also be written as(6.32)But the parameters in Eq. (6.32) are not identifiable, since 𝜂 can arbitrarily take any value.Step 3. Obtain the estimates of 𝛿 i and w i .From δ(B)v(B) = ŵ(B), we can recursively estimate 𝛿 i and w i using Eq.(6.12),with v b = w 0 and v j = 0 for j < b.Hence for the example in Step 2, we haveStep 4. Model the noise.Once the initial estimates of the model parameters are obtained, the estimated noise can be obtained asx t− b, (6.33)To obtain the estimated noise, we define ŷt = ( ŵ(B)∕ δ(B))x t− b.We can then calculate ŷt recursively.To model the estimated noise, we observe its ACF and PACF and determine the orders of the ARIMA model,Step 5. Fitting the overall model.Steps 1 through 4 provide us with the model specifications and the initial estimates of the parameters in the transfer function-noise model,The final estimates of the model parameters are then obtained by a nonlinear model fit.Model selection criteria such as AIC and BIC can be used to pick the "best" model among competing models.Step 6. Model adequacy checks.At this step, we check the validity of the two assumptions in the fitted model:1.The assumption that the noise 𝜀 t is white noise requires the examination of the residuals εt .We perform the usual checks through analysis of the sample ACF and PACF of the residuals.2. We should also check the independence between 𝜀 t and x t .For that, we observe the sample cross-correlation function between εt and xt .Alternatively, we can examine r α ε(j), where 𝛼 t = θx (B) −1 φx (B)x t .Under the assumption the model is adequate, r α ε(j) will have 0 mean, 1∕√ N standard deviation, and be independent for different lags j.Hence we can use ±2∕N as the limit to check the independence assumption.Example 6.2 In a chemical process it is expected that changes in temperature affect viscosity, a key quality characteristic.It is therefore of great importance to learn more about this relationship.The data are collected every 10 seconds and given in Table 6.2 (Note that for each variable the data are centered by subtracting the respective averages).Figure 6.3 shows the time series plots of the two variables.Since the data are taken in time and at frequent intervals, we expect the variables to exhibit some autocorrelation and decide to fit a transfer function-noise model following the steps provided earlier.Step 1. Obtaining the preliminary estimates of the coefficients in v (B) In this step we use the prewhitening method.First we fit an ARIMA model to the temperature.Since the time series plot in Figure 6.3 shows that the process is changing around a constant mean and has a constant variance, we will assume that it is stationary.Sample ACF and PACF plots in Figure 6.4 suggest that an AR(1) model should be used to fit the temperature data.Table 6.3 shows that φ ≅ 0.73.The sample ACF and PACF plots in Figure 6.5 as well as the additional residuals plots in Figure 6.6 reveal that no autocorrelation is left in the data and the model gives a reasonable fit.Hence we defineWe then compute the sample cross-correlation of 𝛼 t and 𝛽 t , r 𝛼𝛽 given in Figure 6.7.Since the cross correlation at lags 0, 1 and 2 do not seem to be significant, we conclude that there is a delay of 3 lags (30 seconds) in the system, that is, b = 3 .Step 2. Specifications of the orders r and s.To identify the pattern in Figures 6.7 and 6.8, we can refer back to Table 6.1.From the examples of impulse response functions given in that table, we may conclude the denominator of the transfer function is a second order polynomial in B. That is, r = 2 so we have 1 − 𝛿 2 B − 𝛿 2 , B 2 for the denominator.For the numerator, it seems that s = 0 or w 0 would be appropriate.Hence our tentative impulse response function is the followingStep 3. Obtain the estimates of the 𝛿 i and w i .To obtain the estimates of 𝛿 i and w i , we refer back to Eq. (6.12) which implies that we have We then define Nt = y t − ŷt .Figures 6.9 and 6.10 show the time series plot of Nt and its sample ACF/PACF plots respectively which indicate an AR model.Note that partial autocorrelation at lag 3 is borderline significant.However when an AR(3) model is fitted, both 𝜙 2 and 𝜙 3 are found to be insignificant.Therefore AR(1) model is considered to be the appropriate model.The parameter estimates for the AR(1) model for Nt are given in Table 6.4.Diagnostic checks of the residuals through sample ACF and PACF plots in Figure 6.11 and residuals plots in Figure 6.12 imply that we have a good fit.MODEL SPECIFICATION Note that we do not necessarily need the coefficients estimates as they will be re-estimated in the next step.Thus at this step all we need is a sensible model for Nt to put into the overall model.Step 5. Fitting the overall model.Step 4, we have the tentative overall model asThe calculations that were made so far could have been performed practically in any statistical package.However as we mentioned at the beginning of the Chapter, unfortunately only a few software packages have the capability to fit the overall transfer function-noise model described above.In the following we provide the output from JMP with which such a model can be fitted.At the end of the chapter, we also provide the R code that can be used to fit the transfer function-noise model.JMP output for the overall transfer function-noise model is provided in Table 6.5.The estimated coefficients are ŵ0 = 1.3276, δ1 = 0.3414, δ2 = 0.2667, φ1 = 0.8295, and they are all significant.Step 6. Model adequacy checksThe sample ACF and PACF of the residuals provided in Table 6.5 show no indication of leftover autocorrelation.We further check the cross correlation function between 𝛼 t = (1 − 0.73B)x t and the residuals as given in Figure 6.13.There is a borderline significant cross correlation at lag 5. However we believe that it is at this point safe to claim that the current fitted model is adequate.Example 6.2 illustrates transfer function modeling with a single input series where both the input and output time series were stationary.It is often necessary to incorporate multiple input time series into the model.A simple generalization of the single-input transfer function is to form an additive model for the inputs, sayx j,t−b j (6.35)where each input series has a transfer function representation including a potential delay.This is an appropriate approach so long as the input series are uncorrelated with each other.If any of the original series are nonstationary, then differencing may be required.In general, differencing of a higher order than one may be required, and inputs and outputs need not be identically differenced.We now present an example from Montgomery and Weatherby (1980) where two inputs are used in the model.Example 6.3 Montgomery and Weatherby (1980) present an example of modeling the output viscosity of a chemical process as a function of two inputs, the incoming raw material viscosity x 1,t and the reaction temperature x 2,t .Readings are recorded hourly.Figure 6.14 is a plot of the last 100 readings.All three variables appear to be nonstationary.Standard univariate ARIMA modeling techniques indicate that the input raw material viscosity can be modeled by an ARIMA(0, 1, 2) or IMA(1,2) processwhich is then used to prewhiten the output final viscosity.Similarly, an IMA(1,1) modelwas used to prewhiten the temperature input.Table 6.6 contains the impulse response functions between the prewhitened inputs and outputs.Both impulse response functions in for the temperature input.This is consistent with a tentative model identification of r = 1, s = 0, b = 2 for the transfer function relating initial and final viscosities and r = 1, s = 0, b = 3 for the transfer function relating temperature and final viscosity.The preliminary parameter estimates for these models are The noise time series is then computed fromThe sample ACF and PACF of the noise series indicate that it can be modeled asTherefore, the combined transfer function plus noise model isThe estimates of the parameters in this model are shown in Table 6.7 along with other model summary statistics.The t-test statistics indicate that all model parameters are different from zero.Residual checks did not reveal any problems with model adequacy.The chi-square statistic for the first 25 residual autocorrelations was 10.412.The first 20 cross-correlations between the residuals and the prewhitened input viscosity produced a chi-square test statistic of 11.028 and the first 20 cross-correlations between the residuals and the prewhitened input temperature produced a chi-square test statistic of 15.109.These chi-square Notice that a complex relationship between two input variables and one output has been modeled with only five parameters and the small off-diagonal elements in the covariance matrix above imply that these parameter estimates are essentially uncorrelated.In this section we discuss making 𝜏-step-ahead forecasts using the transfer function-noise model in Eq. (6.23).We can rearrange Eq. (6.23) and rewrite it in the difference equation form as (6.37)Then at time t + 𝜏, we will have (6.38)where r is the order of 𝛿(B), p * is the order of 𝜑(B)(= 𝜙(B)(1 − B) d ), and s is the order of 𝜔(B), and q is the order of 𝜃(B).The 𝜏-step ahead MSE forecasts are obtained fromNote that the MA terms will vanish for 𝜏 > q + r.We obtain Eq. (6.39) using(6.40) Equation (6.40) implies that the relationship between x t and y t is unidirectional and that xt (l) is the forecast from the univariate ARIMA model,So forecasts ŷt+1 (t), ŷt+2 (t), … can be computed recursively from Eqs. (6.39) and (6.40).The variance of the forecast errors can be obtained from the infinite MA representations for x t and N t given asHence the infinite MA form of the transfer function-noise model is given asThus the minimum MSE forecast can be represented asand the 𝜏-step-ahead forecast error isAs we can see in Eq. (6.45), the forecast error has two components that are assumed to be independent: forecast errors in forecasting x t ,The forecast variance is simply the sum of the two variances:To check the effect of adding x t in the model when forecasting, it may be appealing to compare the forecast errors between the transfer functionnoise model and the univariate ARIMA model for y t .Let the forecast error variances for the former and the latter be denoted by 𝜎 2 TFN (𝜏) and 𝜎 2 UM (𝜏), respectively.We may then considerThis quantity is expected to go down significantly if the introduction of x t were indeed appropriate.Example 6.4 Suppose we need to make forecasts for the next minute (6 observations) for the viscosity data in Example 6.2.We first consider the final model suggested in Example 6.2After some rearrangement, we haveFrom Eq. (6.38), we have the 𝜏-step ahead prediction aswhereHence for the current and past response and input values, we can use the actual data.For the future response and input values we will instead use their respective forecasts.To forecast the input variable x t , we will use the AR(1) model, (l − 0.73B)x t = 𝛼 t from Example 6.2.As for the error estimates, we can use the residuals from the transfer function-noise model or for b ≥ 1, the one-step-ahead forecast errors for the current and past values of the errors, and set the error estimates equal to zero for future values.We can obtain the variance of the prediction error from Eq. (6.45).The estimates of 𝜎 2 𝛼 and 𝜎 2 𝜀 in Eq. ( 6.45) can be obtained from the univariate AR(1) model for x t , and the transfer function-noise model from Example 6.2, respectively.Hence for this example we have σ2 𝛼 = 0.0102 and σ2 𝜀 = 0.0128.The coefficients in v * (B) and 𝜓(B) can be calculated fromHence the estimates of the coefficients in v * (B) and 𝜓(B) can be obtained by using the estimates of the parameters given in Example 6.2.Note that FIGURE 6.15The time series plots of the actual and 1-to 6-step ahead forecasts for the viscosity data.for up to 6-step-ahead forecasts, from Eq. (6.45), we will only need to calculate v * 0 , v * 1 and v * 2 .The time series plot of the forecasts is given in Figure 6.15 together with the approximate 95% prediction limits calculate.by±2 σ(𝜏).For comparison purposes we fit a univariate ARIMA model for y t .Following the model identification procedure given in Chapter 5, an AR(3) model is deemed a good fit.The estimated standard deviations of the prediction error for the transfer function-noise model and the univariate model are given in Table 6.8.It can be seen that adding the exogenous variable x t helps to reduce the prediction error standard deviation.In some cases, the response y t can be affected by a known event that happens at a specific time such as fiscal policy changes, introduction of new regulatory laws, or switching suppliers.Since these interventions do not have to be quantitative variables, we can represent them with indicator variables.Consider, for example, the transfer function-noise model as the following:where 𝜉 (T) t is a deterministic indicator variable, taking only the values 0 and 1 to indicate nonoccurrence and occurrence of some event.The model in Eq. (6.48) is called the intervention model.Note that this model has only one intervention event.Generalization of this model with several intervention events is also possible.The most common indicator variables are the pulse and step variables,andwhere T is a specified occurrence time of the intervention event.The transfer function operator v(B) = w(B)∕𝛿(B) in Eq. (6.48) usually has a fairly simple and intuitive form.1. We will first consider the pulse indicator variable.We will further assume a simple transfer function-noise model asAfter rearranging Eq. (6.51), we have2. For the step indicator variable with the same transfer function-noise model as in the previous case, we haveSolving the difference equationIn intervention analysis, one of the things we could be interested in may be how permanent the effect of the event will be.Generally, for y t = (w(B)∕𝛿(B))𝜉 (T) t with stable 𝛿(B), if the intervention event is a pulse, we will then have a transient (short-lived) effect.On the other hand, if the intervention event is a step, we will have a permanent effect.In general, depending on the form of the transfer function, there are many possible responses to the step and pulse inputs.Table 6.9 displays the output  Example 6.5 The weekly sales data of a cereal brand for the last two years are given in Table 6.10.As can be seen from Figure 6.16, the sales were showing a steady increase during most of the two-year period.At the end of the summer of the second year (Week 88), the rival company introduced a similar product into the market.Using intervention analysis, we want to study whether that had an effect on the sales.For that, we will first fit an ARIMA model to the preintervention data from Week 1 to Week 87.The sample ACF and PACF of the data for that time period in Figure 6.17 show that the process is nonstationary.The sample ACF and PACF of the first difference given in Figure 6.18 suggest that an ARIMA(0,1,1) model is appropriate.Then the intervention model has the following form:where This means that for the intervention analysis we assume that the competition simply slows down (or reverses) the rate of increase in the sales.To fit the model we use the transfer function model option in JMP with S (88) t as the input.The output in Table 6.11 shows that there was indeed a significant effect on sales due to the introduction of a similar product in the market.The coefficient estimate ŵ0 = −2369.9further suggests that if no appropriate action is taken, the sales will most likely continue to go down.The natural logarithm of monthly electric energy consumption in megawatt hours (MWh) for a regional utility from January 1951 to April 1977 is shown in Figure 6.19.The original data exhibited considerable inequality  of variance over this time period and the natural logarithm stabilizes the variance.In November of 1973 the Arab oil embargo affected the supply of petroleum products to the United States.Following this event, it is hypothesized that the rate of growth of electricity consumption is smaller than in the pre-embargo period.Montgomery and Weatherby (1980) tested this hypothesis using an intervention model.Although the embargo took effect in November of 1973, we will assume that first impact of this was not felt until the following month, December, 1973.Therefore, the period from January of 1951 until November of 1973 is assumed to have no intervention effect.These 275 months are analyzed to produce a univariate ARIMA model for the noise model.Both regular and seasonal differencing are required to achieve stationary and a multiplicative (0, 1, 2) × (0, 1, 1) 12 was fit to the pre-intervention data.The model isTo develop the intervention, it is necessary to hypothesize the effect that the oil embargo may have had on electricity consumption.Perhaps the simplest scenario in which the level of the consumption time series is hypothesized to be permanently changed by a constant amount.Logarithms can be thought of as percent change, so this is equivalent to hypothesizing that the intervention effect is a change in the percent growth of electricity consumption.Thus the input series would be a step functionThe intervention model is thenwhere Θ 12 is the seasonal MA parameter at lag 12. Table 6.12 gives the model parameter estimates and the corresponding 95% confidence intervals.Since the 95% confidence intervals do not contain zero, we conclude that all model parameters are statistically significant.The residual standard error for this model is 0.029195.Since the model was built to the natural logarithm of electricity consumption, the standard error has a simple, direct interpretation: namely, the standard deviation of one-step-ahead forecast errors is 2.9195 percent of the level of the time series.It is also possible to draw conclusions about the intervention effect.This effect is a level change of magnitude ω0 = −0.07303,expressed in the natural logarithm metric.The estimate of the intervention effect in the original MWh metric is e ω0 = e −0.07303 = 0.9296.That is, the postintervention level of electricity consumption is 92.96 percent of the preintervention level.The effect of the Arab oil embargo has been to reduce the increase in electricity consumption by 7.04 percent.This is a statistically significant effect.In this example, there are 275 pre-intervention observations and 41 post-intervention observations.Generally, we would like to have as many observations as possible in the post-intervention period to ensure that the power of the test for the intervention effect is high.However, an extremely long post-intervention period may allow other unidentified factors to affect the output, leading to potential confounding of effects.The ability of the procedure to detect an intervention effect is a function of the number of preand post-intervention observations, the size of the intervention effect, the form of the noise model, and the parameter values of the process.In many cases, however, an intervention effect can be identified with relatively short record of post-intervention observations.It is interesting to consider alternative hypotheses regarding the impact of the Arab oil embargo.For example, it may be more reasonable to suspect that the effect of the oil embargo is not to cause an immediate level change in electricity consumption, but a gradual one.This would suggest a modelThe results of fitting this model to the data are shown in Table 6.13.Note that the 95% confidence interval for 𝛿 1 includes zero, implying that we can In some problems there may be multiple intervention effects.Generally, one indicator variable must be used for each intervention effect.For example, suppose that in the electricity consumption example, we think that the oil embargo had two separate effects: the initial impact beginning in month 276, and a second impact beginning three months later.The intervention model to incorporate these effects iswhere.In this model the parameters 𝜔 10 and 𝜔 20 represent the initial and secondary effects of the oil embargo and 𝜔 10 + 𝜔 20 represents the long-term total impact.There have been many other interesting applications of intervention analysis.For some very good examples, see the following references:r Box and Tiao (1975) investigate the effects on ozone (O 3 ) concentration in downtown Los Angeles of a new law that restricted the amount of reactive hydrocarbons in locally sold gasoline, regulations that mandated automobile engine design changes, and the diversion of traffic by opening of the Golden State Freeway.They showed that these interventions did indeed lead to reductions in ozone levels.r Wichern and Jones (1977) analyzed the impact of the endorsement by the American Dental Association of Crest toothpaste as an effective aid in reducing cavities on the market shares of Crest and Colgate toothpaste.The endorsement led to a significant increase in market share for Crest.See Bisgaard and Kulahci (2011) for a detailed analysis of that example.r Atkins (1979) used intervention analysis to investigate the effect of compulsory automobile insurance, a company strike, and a change in insurance companies' policies on the number of highway accidents on freeways in British Columbia.r Izenman and Zabell (1981) study the effect of the 9 November, 1965, blackout in New York City that resulted from a widespread power failure, on the birth rate nine months later.An article in The New York Times in August 1966 noted that births were up, but subsequent medical and demographic articles appeared with conflicting statements.Using the weekly birth rate from 1961 to 1966, the authors show that there is no statistically significant increase in the birth rate.r Ledolter and Chan (1996) used intervention analysis to study the effect of a speed change on rural interstate highways in Iowa on the occurrence of traffic accidents.Another important application of intervention analysis is in the detection of time series outliers.Time series observations are often influenced by external disruptive events, such as strikes, social/political events, economic crises, or wars and civil disturbances.The consequences of these events are observations that are not consistent with the other observations in the time series.These inconsistent observations are called outliers.In addition to the external events identified above, outliers can also be caused by more mundane forces, such as data recording or transmission errors.Outliers can have a very disruptive effect on model identification, parameter estimation, and forecasting, so it is important to be able to detect their presence so that they can be removed.Intervention analysis can be useful for this.There are two kinds of time series outliers: additive outliers and innovation outliers.An additive outlier affects only the level of the t * observation, while an innovation outlier affects all observations y t * , y t * +1 , y t * +2 ,… beyond time t * where the original outlier effect occurred.An additive outlier can be modeled aswheret is an indicator time series defined asAn innovation outlier is modeled asWhen the timing of the outlier is known, it is relatively straightforward to fit the intervention model.Then the presence of the outlier can be tested by comparing the estimate of the parameter 𝜔, say, ω, to its standard error.When the timing of the outlier is not known, an iterative procedure is required.This procedure is described in Box, Jenkins, and Reinsel (1994) and in Wei (2006).The iterative procedure is capable of identifying multiple outliers in the time series.Example 6.2 The data for this example are in the array called vistemp.data of which the two columns represent the viscosity and the temperature respectively.Below we first start with the prewhitening step.par(mfrow=c(2,1),oma=c(0,0,0,0)) plot (xt,type="o",pch=16,cex=.5,xlab='Time',ylab=expression (italic(x[italic(t)]))) plot (yt,type="o",pch=16,cex=.5,xlab='Time',ylab= expression (italic(y[italic(t)]))) We now fit the following transfer function-noise modelFor that we will use the "arimax" function in TSA package.# 4-in-1 plot of the residuals par(mfrow=c(2,2),oma=c(0,0,0,0)) qqnorm (res.visc.tf,datax=TRUE,pch=16,xlab='Residual',main=") qqline(res.visc.tf,datax=TRUE) plot(fit.visc.tf,res.visc.tf,pch=16,xlab='FittedFor forecasting we use the formula given in the example.Before we proceed, we first make forecasts for x(t) based on the AR(1) model.We will only make 6-step-ahead forecasts for x(t) even if not all of them are needed due to delay.To make the recursive calculations given in the example simpler, we will simply extend the xt, yt and residuals vectors as the following.xt. new<-c(xt,xt.ar1.forecast$mean)res.tf.new<-c(rep(0,3),res.visc.tf,rep(0,tau))yt.new<-c(yt,rep(0,tau)) #Note that 3 0's are added to the beginning to compensate for the #misalignment between xt and the residuals of transfer function #noise model due to the delay of 3 lags.Last 6 0's are added #since the future values of the error are assumed to be 0.We now get the parameter estimates for the transfer function-noise modelThe forecasts are then obtained using: To calculate the prediction limits, we need to first calculate the estimate of forecast error variance given in (5.83).To estimate 𝜓 0 through 𝜓 5 we use the formula given in (5.46).psi.yt<-vector() psi.yt[1:4]<-c(0,0,0,1) sig2.yt<-yt.ar3$sigma2for (i in 5: We can see that adding the exogenous variable x(t) helps to reduce the prediction error variance by half.To plot the forecasts and the prediction limits, we have plot(yt.new[1:T],type="p",pch=16,cex=.5,xlab='Time',ylab='Viscosity',xlim=c(1,110)) lines (101:106,yt.new[101:106],col="grey40")lines(101:106, yt.new[101:106]+2*sqrt(sig2.tfn)) lines(101:106, yt.new[101:106]-2*sqrt(sig2.tfn)) legend(20,-.4,c("y(t)","Forecast","95%LPL","95% UPL"), pch=c(16, NA, NA,NA),lwd=c(NA,.5,.5,.5),cex=.55,col=c("black","grey40","black","black")) The series appears to be non-stationary.We try the first differences par(mfrow=c(1,2),oma=c(0,0,0,0)) acf(diff(yt.sales[1:87],1),lag.max=25,type="correlation",main="ACF for First Differences \nWeeks 1-87") acf(diff(yt.sales[1:87],1),lag.max=25,type="partial",main="PACF for First Differences \nWeeks 1-87",ylab="PACF") ARIMA(0,0,1) model for the first differences seems to be appropriate.We now fit the transfer function-noise model with the step input.First we define the step indicator variable.transfer=list(c(0,0)), include.mean= FALSE) #Note that we adjusted the step function for the differencing we # did on the yt.sales sales.tfSeries: diff(yt.sales)ARIMA( 0Find initial estimates of the parameters of the transfer function model for the situation in Exercise 6.1.An input and output time series consists of 200 observations.The prewhitened input series is well modeled by an MA(1) model y t = 0.8𝛼 t−1 + 𝛼 t .We have estimated σ𝛼 = 0.4 and σ𝛽 = 0.6.The estimated cross-correlation function between the prewhitened input and output time series is shown below.Lag, j 0 1 2 3 4 5 6 7 8 9 10 r 𝛼𝛽 (j) 0.01 0.55 0.40 0.28 0.20 0.07 0.02 0.01 −0.02 0.01 −0.01 a. Find the approximate standard error of the cross-correlation function.Which spikes on the cross-correlation function appear to be significant?b.Estimate the impulse response function.Tentatively identify the form of the transfer function model.Find initial estimates of the parameters of the transfer function model for the situation in Exercise 6.3.Write the equations that must be solved in order to obtain initial estimates of the parameters in a transfer function model with b = 2, r = 1, and s = 0.Write the equations that must be solved in order to obtain initial estimates of the parameters in a transfer function model with b = 2, r = 2, and s = 1.Write the equations that must be solved in order to obtain initial estimates of the parameters in a transfer function model with b = 2, r = 1, and s = 1.Consider a transfer function model with b = 2, r = 1, and s = 0.Assume that the noise model is AR(1).Find the forecasts in terms of the transfer function and noise model parameters.Consider the transfer function model in Exercise 6.8 with b = 2, r = 1, and s = 0. Now assume that the noise model is AR(2).Find the forecasts in terms of the transfer function and noise model parameters.What difference does this noise model make on the forecasts?Consider the transfer function model in the Exercise 6.8 with b = 2, r = 1, and s = 0. Now assume that the noise model is MA(1).Find the forecasts in terms of the transfer function and noise model parameters.What difference does this noise model make on the forecasts?Consider the transfer function modelFind the forecasts that are generated from this model.6.12 Sketch a graph of the impulse response function for the following transfer function:x t .6.13 Sketch a graph of the impulse response function for the following transfer function:6.14 Sketch a graph of the impulse response function for the following transfer function:6. 15 Box, Jenkins, and Reinsel (1994) fit a transfer function model to data from a gas furnace.The input variable is the volume of methane entering the chamber in cubic feet per minute and the output is the concentration of carbon dioxide emitted.The transfer function model iswhere the input and output variables are measured every nine seconds.a.What are the values of b, s,and r for this model?b.What is the form of the ARIMA model for the errors?c.If the methane input was increased, how long would it take before the carbon dioxide concentration in the output is impacted?a. Plot the data.b.There is an apparent outlier in the data.Use intervention analysis to investigate the presence of this outlier.E6.2 provides 100 observations on a time series.These data represent weekly shipments of a product.a. Plot the data.b.Note that there is an apparent increase in the level of the time series at about observation 80. Management suspects that this increase in shipments may be due to a strike at a competitor's plant.Build an appropriate intervention model for these data.Do you think that the impact of this intervention is likely to be permanent?Fit a transfer function model to these data using both the number of licensed drivers and the annual unemployment rate as the input time series.Compare this two-input transfer function model to a univariate ARIMA model for the annual fatalities data, and to the two univariate transfer function models from Exercises 6.24 and 6.25.CHAPTER 7I always avoid prophesying beforehand, because it is a much better policy to prophesy after the event has already taken place.SIR WINSTON CHURCHILL, British Prime MinisterIn many forecasting problems, it may be the case that there are more than just one variable to consider.Attempting to model each variable individually may at times work.However, in these situations, it is often the case that these variables are somehow cross-correlated, and that structure can be effectively taken advantage of in forecasting.In the previous chapter we explored this for the "unidirectional" case, where it is assumed that certain inputs have impact on the variable of interest but not the other way around.Multivariate time series models involve several variables that are not only serially but also cross-correlated.As in the univariate case, multivariate or vector ARIMA models can often be successfully used in forecasting multivariate time series.Many of the concepts we have seen in Chapter 5will be directly applicable in the multivariate case as well.We will first start with the property of stationarity.Suppose that the vector time series Y t = (y 1t , y 2t , … , y mt ) consists of m univariate time series.Then Y t with finite first and second order moments is said to be weakly stationary ifNote that the diagonal elements of 𝚪(s) give the autocovariance function of the individual time series, 𝛾 ii (s).Similarly, the autocorrelation matrix is given bywhich can also be obtained by definingWe then haveWe can further show that 𝚪(s) = 𝚪(−s) ′ and 𝛒(s) = 𝛒(−s) ′ .The stationary vector time series can be represented with a vector ARMA model given bywhere, and 𝛆 t represents the sequence of independent random vectors with E(𝛆 t ) = 0 and Cov(𝛆 t ) = 𝚺.Since the random vectors are independent, we have Γ 𝜀 (s) = 0 for all s ≠ 0.The process Y t in Eq. ( 7.4) is stationary if the roots ofare all greater than one in absolute value.Then the process Y t is also said to have infinite MA representation given aswhere] = 0 are greater than unity in absolute value the process Y t in Eq. ( 7.4) is invertible.To illustrate the vector ARMA model given in Eq. ( 7.4), consider the bivariate ARMA(1,1) model withandHence the model can be written asAs in the univariate case, if nonstationarity is present, through an appropriate degree of differencing a stationary vector time series may be achieved.Hence the vector ARIMA model can be represented aswhereHowever, the degree of differencing is usually quite complicated and has to be handled with care (Reinsel (1997)).The identification of the vector ARIMA model can indeed be fairly difficult.Therefore in the next section we will concentrate on the more commonly used and intuitively appealing vector autoregressive models.For a more general discussion see Reinsel (1997), Lütkepohl (2005), Tiao and Box (1981), Tiao and Tsay (1989), Tsay (1989), and Tjostheim and Paulsen (1982).The vector AR(p) model is given byFor a stationary vector AR process, the infinite MA representation is given aswherewith 𝛆 t−s , 𝛆 t−s−1 , …, which are not correlated with 𝛆 t .Moreover, we also haveAs in the univariate case, the Yule-Walker equations can be obtained from the first p equations asThe model parameters in 𝚽 and 𝚺 can be estimated from Eqs. (7.11) and (7.12).For the VAR(p), the autocorrelation matrix in Eq. ( 7.3) will exhibit a decaying behavior following a mixture of exponential decay and damped sinusoid.The autocovariance matrix for VAR(1) is given aswhere V = diag{𝛾 11 (0), 𝛾 22 (0), … , 𝛾 mm (0)}.The eigenvalues of 𝚽 determine the behavior of the autocorrelation matrix.In fact, if the eigenvalues of 𝚽 are real and/or complex conjugates, the behavior will be a mixture of the exponential decay and damped sinusoid, respectively.The pressure readings at two ends of an industrial furnace are taken every 10 minutes and given in Table 7.1.It is expected the individual time series are not only autocorrelated but also cross-correlated.Therefore it is decided to fit a multivariate time series model to this data.The time series plots of the data are given in Figure 7.1.To identify the model we consider the sample ACF plots as well as the cross correlation of the time series given in Figure 7.2.These plots exhibit an exponential decay pattern, suggesting that an autoregressive model may be appropriate.It is further conjectured that a VAR(1) or VAR(2) model may provide a good fit.Another approach to model identification would be to fit ARIMA models to the individual time series and consider the cross correlation of the residuals.For that, we fit an AR(1) model to both time series.The cross-correlation plot of the residuals given in Figure 7.3 further suggests that the VAR(1) model may indeed provide an appropriate fit.Using the SAS ARIMA procedure given inCross correlation function for RESI1, RESI2The sample ACF plot for: (a) the pressure readings at the front end of the furnace, y 1 ; (b) the pressure readings at the back end of the furnace, y 2 ; (c) the cross correlation between y 1 and y 2 ; and (d) the cross correlation between the residuals from the AR(1) model for front pressure and the residuals from the AR(1) model for back pressure.In this section we give a brief introduction to an approach to forecasting based on the state space model.This is a very general approach that can include regression models and ARIMA models.It can also incorporate a Bayesian approach to forecasting and models with time-varying coefficients.State space models are based on the Markov property, which implies the independence of the future of a process from its past, given the present system state.In this type of system, the state of the process at the current time contains all of the past information that is required to predict future process behavior.We will let the system state at time t be   represented by the state vector X t .The elements of this vector are not necessarily observed.A state space model consists of two equations: an observation or measurement equation that describes how time series observations are produced from the state vector, and a state or system equation that describes how the state vector evolves through time.We will write these two equations as The state space model does not look like any of the time series models we have studied previously.However, we can put many of these models in the state space form.This is illustrated in the following two examples.Example 7.3 Consider an AR(1) model, which we have previously written asIn this case we let X t = y t and a t = 𝜀 t and write the state equation asand the observation equation isIn the AR(1) model the state vector consists of previous consecutive observations of the time series y t .Any ARIMA model can be written in the state space form.Refer to Brockwell and Davis (1991).Example 7.4 Now let us consider a regression model with one predictor variable and AR(1) errors.We will write this model aswhere p t is the predictor variable and 𝜀 t is the AR(1) error term.To write this in state space form, define the state vector asThe vector h t and the matrix A areand the state space representation of this model becomesMultiplying these equations out will produce the time series regression model with one predictor and AR(1) errors.The state space formulation does not admit any new forecasting techniques.Consequently, it does not produce better forecasts than any of the other methods.The state space approach does admit a Bayesian formulation of the problem, in which the model parameters have a prior distribution that represents our degree of belief concerning the values of these coefficients.Then after some history of the process (observation) becomes available, this prior distribution is updated into a posterior distribution.Another formulation allows the coefficients in the regression model to vary through time.The state space formulation does allow a common mathematical framework to be used for model development.It also permits relatively easy generalization of many models.This has some advantages for researchers.It also would allow common computer software to be employed for making forecasts from a variety of techniques.This could have some practical appeal to forecasters.In the standard regression and time series models we have covered so far, many diagnostic checks were based on the assumptions that we imposed on the errors: independent, identically distributed with zero mean, and constant variance.Our main concern has mostly been about the independence of the errors.The constant variance assumption is often taken as a given.In many practical cases and particularly in finance, it is fairly common to observe the violation of this assumption.Figure 7.6, for example, shows the S&P500 Index (weekly close) from 1995 to 1998.Most of the 1990s enjoyed a bull market up until toward the end when the dotcom bubble burst.The worrisome market resulted in high volatility (i.e., increasing variance).A linear trend model, an exponential smoother, or even an ARIMA model would have failed to capture this phenomenon, as all assume constant variance of the errors.This will in turn result in 9 / 2 9 / 1 9 9 7 6 / 9 / 1 9 9 7 2 / 1 8 / 1 9 9 7 1 0 / 2 8 / 1 9 9 6 7 / 8 / 1 9 9 6 3 / 1 8 / 1 9 9 6 1 1 / 2 7 / 1 9 9 5 8 / 7 / 1 9 9 5 4 / 1 7 / 1 9 9 5 1 / 3 / 1 9 9 5 the underestimation of the standard errors calculated using OLS and will lead to erroneous conclusions.There are different ways of dealing with this situation.For example, if the changes in the variance at certain time intervals are known, weighted regression can be employed.However, it is often the case that these changes are unknown to the analyst.Moreover, it is usually of great value to the analyst to know why, when, and how these changes in the variance occur.Hence, if possible, modeling these changes (i.e., the variance) can be quite beneficial.Consider, for example, the simple AR(p) model from Chapter 5 given aswhere e t is the uncorrelated, zero mean noise with changing variance.Please note that we used e t to distinguish it from our general white noise error 𝜀 t .Since we let the variance of e t change in time, one approach is to model e 2 t as an AR(l) model aswhere a t is a white noise sequence with zero mean and constant variance 𝜎 2 a .In this notation e t is said to follow an autoregressive conditional heteroskedastic process of order l, ARCH(l).To check for a need for an ARCH model, once the ARIMA or regression model is fitted, not only the standard residual analysis and diagnostics checks have to be performed but also some serial dependence checks for e 2 t should be made.To further generalize the ARCH model, we will consider the alternative representation originally proposed by Engle (1982).Let us assume that the error can be represented aswhere w t is independent and identically distributed with mean 0 and variance 1, andHence the conditional variance of e t isVar(eWe can also argue that the current conditional variance should also depend on the previous conditional variances asIn this notation, the error term e t is said to follow a generalized autoregressive conditional heteroskedastic process of orders k and l, GARCH(k, l), proposed by Bollerslev (1986).In Eq. ( 7.22) the model for conditional variance resembles an ARMA model.However, it should be noted that the model in Eq. (7.22) is not a proper ARMA model, as this would have required a white noise error term with a constant variance for the MA part.But none of the terms on the right-hand side of the equation possess this property.For further details, see Hamilton (1994), Bollerslev et al. (1992), and Degiannakis and Xekalaki (2004).Further extensions of ARCH models also exist for various specifications of v t in Eq. (7.22); for example, Integrated GARCH (I-GARCH) by Engle and Bollerslev (1986), Exponential GARCH (E-GARCH) by Nelson (1991), Nonlinear GARCH by Glosten et al. (1993), and GARCH for multivariate data by Engle and Kroner (1993).But they are beyond the scope of this book.For a brief overview of these models, see Hamilton (1994).Example 7.5 Consider the weekly closing values for the S&P500 Index from 1995 to 1998 given in Table 7.4.Figure 7.6 shows that the data exhibits nonstationarity.But before taking the first difference of the data, we decided to take the log transformation of the data first.As observed in Chapters 2 and 3, the log transformation is sometimes used for financial data when we are interested, for example, in the rate of change or percentage changes in the price of a stock.For further details, see Granger and Newbold (1986).The time series plot of the first differences of the log of the S&P500 Index is given in Figure 7.7, which shows that while the mean seems to be stable around 0, the changes in the variance are worrisome.The ACF and PACF plots of the first difference given in Figure 7.8 suggest that, except for some borderline significant ACF values at seemingly arbitrary lags, there is no autocorrelation left in the data.As in the case of the Dow Jones Index in Chapter 5, this suggests that the S&P500 Index follows a random walk process.However, the time series plot of the differences does not exhibit a constant variance behavior.For that, we consider the ACF and PACF of the squared differences given in Figure 7.9, which suggests that an AR(3) model can be used.Thus we fit the ARCH(3) model for the variance using the AUTOREG procedure in SAS given in Table 7.5.The SAS output in 511 9 / 2 9 / 1 9 9 7 6 / 9 / 1 9 9 7 2 / 1 8 / 1 9 9 7 1 0 / 2 8 / 1 9 9 6 7 / 8 / 1 9 9 6 3 / 1 8 / 1 9 9 6 1 1 / 2 7 / 1 9 9 5 8 / 7 / 1 9 9 5 4 / 1 7 / 1 9 9 5 1 / 3 / 1 9 9 5 Autocorrelation function for wt^2 (with 5% significance limits for the autocorrelations) Partial autocorrelation function for wt^2 (with 5% significance limits for the partial autocorrelations) FIGURE 7.9 ACF and PACF plots of the square of the first difference of the log transformation of the weekly close for S&P500 Index from 1995 to 1998.There are other studies on financial indices also yielding the ARCH(3) model for the variance, for example, Bodurtha and Mark (1991) and Attanasio (1991).In fact, successful implementations of reasonably simple, low-order ARCH/GARCH models have been reported in various research studies; see, for example, French et al. (1987).Throughout this book we have stressed the concept that a forecast should almost always be more than a point estimate of the value of some future event.A prediction interval should accompany most point forecasts, because the PI will give the decision maker some idea about the inherent variability of the forecast and the likely forecast error that could be experienced.Most of the forecasting techniques in this book have been presented showing how both point forecasts and PIs are obtained.A PI can be thought of as an estimate of the percentiles of the distribution of the forecast variable.Typically, a PI is obtained by forecasting the mean and then adding appropriate multiples of the standard deviation of forecast error to the estimate of the mean.In this section we present and illustrate a different method that directly smoothes the percentiles of the distribution of the forecast variable.Suppose that the forecast variable y t has a probability distribution f (y).We will assume that the variable y t is either stationary or is changing slowly with time.Therefore a model for y t that is correct at least locally isLet the observations on y t be classified into a finite number of bins, where the bins are defined with limitsThe n bins should be defined so that they do not overlap; that is, each observation can be classified into one and only one bin.The bins do not have to be of equal width.In fact, there may be situations where bins may be defined with unequal width to obtain more information about specific percentiles that are of interest.Typically, 10 ≤ n ≤ 20 bins are used.Let p k be the probability that the variable y t falls in the bin defined by the limits B k−1 and B k .That is,∑ k j=1 p j .Now let us consider estimating the probabilities.Write the probabilities as an n × 1 vector p defined asLet the estimate of the vector p at time period T beNote that if we wanted to estimate the percentile of the distribution of y t corresponding to B k at time period T we could do this by calculating ∑ k j=1 pj (T).We will use an exponential smoothing procedure to compute the estimated probabilities in the vector p(T).Suppose that we are at the end of time period t and the current observation y T is known.Let u k (T) be an indicator variable defined as follows:So the indicator variable u k (T) is equal to unity if the observation y T in period T falls in the kth bin.Note that ∑ T t=1 u k (t) is the total number of observations that fell in the kth bin during the time periods t = 1, 2, … , T. Define the n × 1 observation vector u(T) asThis vector will have n -1 elements equal to zero and one element equal to unity.The exponential smoothing procedure for revising the probabilities pk (T − 1) given that we have a new observation y T iswhere 0 < 𝜆 < 1 is the smoothing constant.In vector form, Eq. (7.23) for updating the probabilities isThis smoothing procedure produces an unbiased estimate of the probabilities p k .Furthermore, because u k (T) is a Bernoulli random variable with parameter p k , the variance of pk (T) isStarting estimates or initial values of the probabilities at time T = 0 are required.These starting values pk (0), k = 1, 2, … , n could be subjective estimates or they could be obtained from an analysis of historical data.The estimated probabilities can be used to obtain estimates of specific percentiles of the distribution of the variable y t .One way to do this would be to estimate the cumulative probability distribution of y t at time T as follows:The values of the cumulative distribution could be plotted on a graph with F(y) on the vertical axis and y on the horizontal axis and the points connected by a smooth curve.Then to obtain an estimate of any specific percentile, say, F1−𝛾 = 1 − 𝛾, all you would need to do is determine the value of y on the horizontal axis corresponding to the desired percentile 1 − 𝛾 on the vertical axis.For example, to find the 95th percentile of the distribution of y, find the value of y on the horizontal axis that corresponds to 0.95 on the vertical axis.This can also be done mathematically.If the desired percentile 1 − 𝛾 exactly matches one of the bin limits so that F(B k ) = 1 − 𝛾, then the solution is easy and the desired percentile estimate is F1−𝛾 = B k .However, if the desired percentile 1 − 𝛾 is between two of the bin limits, say, F(B k−1 ) < 1 − 𝛾 < F(B k ), then interpolation is required.A linear interpolation formula isIn the extreme tails of the distribution or in cases where the bins are very wide, it may be desirable to use a nonlinear interpolation scheme.Example 7.6 A financial institution is interested in forecasting the number of new automobile loan applications generated each week by a particular business channel.The information in Table 7.7 is known at the end of week T − 1.The next-to-last column of this table is the cumulative distribution of loan applications at the end of week T − 1.This cumulative distribution is shown in Figure 7.10.Suppose that 74 loan applications are received during the current week, T. This number of loan applications fall into the eighth bin (k = 7 in Table 7.7).Therefore we can construct the observation vector u(T) as follows:Therefore the new cumulative distribution of loan applications is found by summing the cumulative probabilities in pk (T − 1):These cumulative probabilities are also listed in the last column of Table 7.7.The graph of the updated cumulative distribution is shown in Figure 7.11.Now suppose that we want to find the number of loan applications that corresponds to a particular percentile of this distribution.If this percentile corresponds exactly to one of the cumulative probabilities, such as the 67.6 th percentile, the problem is easy.From the last column of percentile does not correspond to one of the cumulative probabilities in the last column of Table 7.7, we will need to interpolate using Eq.(7.24).For instance, if we want the 75th percentile, we would use Eq. ( 7.24) as follows: Therefore, in about three of every four weeks, we would expect to have approximately 86 or fewer loan applications from this loan channel.Readers have been sure to notice that any time series can be modeled and forecast using several methods.For example, it is not at all unusual to find that the time series y t , t = 1, 2, …, which contains a trend (say), can be forecast by both an exponential smoothing approach and an ARIMA model.In such situations, it seems inefficient to use one forecast and ignore all of the information in the other.It turns out that the forecasts from the two methods can be combined to produce a forecast that is superior in terms of forecast error than either forecast alone.For a review paper on the combination of forecasts, see Clemen (1989).Bates and Granger (1969) suggested using a linear combination of the two forecasts.Let ŷ1,T+𝜏 (T) and ŷ2,T+𝜏 (T) be the forecasts from two different methods at the end of time period T for some future period T + 𝜏 for the time series y t .The combined forecast iswhere k 1 and k 2 are weights.If these weights are chosen properly, the combined forecast ŷc T+𝜏 can have some nice properties.Let the two individual forecasts be unbiased.Then we should choose k 2 = 1 − k 1 so that the combined forecast will also be unbiased.Let k = k 1 so that the combined forecast isLet the error from the combined forecast be e c T+𝜏 (T) = y T+𝜏 − ŷc T+𝜏 (T).The variance of this forecast error iswhere e 1,T+𝜏 (T) and e 2s,T+𝜏 (T) are the forecast errors in period T + 𝜏 for the two individual forecasting methods, 𝜎 2 1 and 𝜎 2 2 are the variances of the individual forecast errors for the two forecasting methods, and 𝜌 is the correlation between the two individual forecast errors.A good combined forecast would be one that minimizes the variance of the combined forecast error.If we choose the weight k equal tothis will minimize the variance of the combined forecast error.By choosing this value for the weight, the minimum variance of the combined forecast error is equal toand this minimum variance of the combined forecast error is less than or equal to the minimum of the variance of the forecast errors of the twoIt turns out that the variance of the combined forecast error depends on the correlation coefficient.Let 𝜎 2 1 be the smaller of the two individual forecast error variances.Then we have the following:Clearly, we would be happiest if the two forecasting methods have forecast errors with large negative correlation.The best possible case is when the two individual forecasting methods produce forecast errors that are perfectly negatively correlated.However, even if the two individual forecasting methods have forecast errors that are positively correlated, the combined forecast will still be superior to the individual forecasts, provided that 𝜌 ≠ 𝜎 1 ∕𝜎 2 .Example 7.7 Suppose that two forecasting methods can be used for a time series, and that the two variances of the forecast errors are 𝜎 2 1 = 20 and 𝜎 2 2 = 40.If the correlation coefficient 𝜌 = −0.6,then we can calculate the optimum value of the weight from Eq. (7.27) as follows: Forecasting method one, which has the smallest individual forecast error variance, receives about 1.5 times the weight of forecasting method two.COMBINING FORECASTS TO IMPROVE PREDICTION PERFORMANCEThe variance of the forecast error for the combined forecast is computed from Eq. (7.28):Min Var [ e c T+𝜏 (T)This is a considerable reduction in the variance of forecast error.If the correlation had been positive instead of negative, thenIn this situation, there is very little improvement in the forecast error resulting from the combination of forecasts.Newbold and Granger (1974) have extended this technique to the combination of n forecasts.Let ŷi,T+𝜏 (T), i = 1, 2, … , n be the n unbiased 522 SURVEY OF OTHER FORECASTING METHODS forecasts at the end of period T for some future period T + 𝜏 for the time series y t .The combined forecast iswhereis the vector of weights, and ŷc T+𝜏 (T) is a vector of the individual forecasts.We require that all of the weights 0 ≤ k i ≤ 1 and ∑ n i=1 k i = 1.The variance of the forecast error is minimized if the weights are chosen aswhere 𝚺 T+𝜏 (T) is the covariance matrix of the lead 𝜏 forecast errors given by ∑] is a vector of ones, and e T+𝜏 (T) = y T+𝜏 1 − ŷT+𝜏 (T) is a vector of the individual forecast errors.The elements of the covariance matrix are usually unknown and will need to be estimated.This can be done by straightforward methods for estimating variances and covariances (refer to Chapter 2).It may also be desirable to regularly update the estimates of the covariance matrix so that these quantities reflect current forecasting performance.Newbold and Granger (1974) suggested several methods for doing this, and Montgomery, Johnson, and Gardiner (1990) investigate several of these methods.They report that a smoothing approach for updating the elements of the covariance matrix seems to work well in practice.Suppose that you wish to forecast the unemployment level of the state in which you live.One way to do this would be to forecast this quantity directly, using the time series of current and previous unemployment data, plus any other predictors that you think are relevant.Another way to do this would be to forecast unemployment at a substate level (say, by county and/or metropolitan area), and then to obtain the state level forecast by summing up the forecasts for each substate region.Thus individual forecasts of a collection of subseries are aggregated to form the forecasts of the quantity of interest.If the substate level forecasts are useful in their own right (as they probably are), this second approach seems very useful.However, there is another way to do this.First, forecast the state level unemployment and then disaggregate this forecast into the individual substate level regional forecasts.This disaggregation could be accomplished by multiplying the state level forecasts by a series of indices that reflect the proportion of total statewide unemployment that is accounted for by each region at the substate level.These indices also evolve with time, so it will be necessary to forecast them as well as part of a complete system.This problem is sometimes referred to as the top-down versus bottomup forecasting problem.In many, if not most, of these problems, we are interested in both forecasts for the top level quantity (the aggregate time series) and forecasts for the bottom level time series that are the components of the aggregate.This leads to an obvious question: is it better to forecast the aggregate or top level quantity directly and then disaggregate, or to forecast the individual components directly and then aggregate them to form the forecast of the total?In other words, is it better to forecast from the top down or from the bottom up?The literature in statistical forecasting, business forecasting and econometrics, and time series analysis suggests that this question is far from settled at either the theoretical or empirical levels.Sometimes the aggregate quantity is more accurate than the disaggregated components, and sometimes the aggregate quantity is subject to less measurement error.It may be more complete and timely as well, and these aspects of the problem should encourage those who consider forecasting the aggregate quantity and then disaggregating.On the other hand, sometimes the bottom level data is easier to obtain and is at least thought to be more timely and accurate, and this would suggest that a bottom-up approach would be superior to the top-down approach.In any specific practical application it will be difficult to argue on theoretical grounds what the correct approach should be.Therefore, in most situations, this question will have to be settled empirically by trying both approaches.With modern computer software for time series analysis and forecasting, this is not difficult.However, in conducting such a study it is a good idea to have an adequate amount of data for identifying and fitting the time series models for both the top level series and the bottom level series, and a reasonable amount of data for testing the two approaches.Obviously, data splitting should be done here, and the data used for model building should not be used for investigating forecasting model performance.Once an approach is determined, the forecasts should be carefully monitored over time to make sure that the dynamics of the problem have not changed, and that the top-down approach that was found to be optimal in testing (say) is now no longer as effective as the bottom-up approach.The methods for monitoring forecasting model performance presented in Chapter 2 are useful in this regard.There are some results available about the effect of adding time series together.This is a special case of a more general problem called temporal aggregation, in which several time series may be combined as, for instance, when monthly data are aggregated to form quarterly data.For example, suppose that we have a top level time series Y t that is the sum of two independent time series y 1,t and y 2,t , and let us assume that both of the bottom level time series are moving average (MA) processes of orders q 1 and q 2 , respectively.So, using the notation for ARIMA models introduced in Chapter 5, we havewhere a t and b t are independent white noise processes.Now let q be the maximum of q 1 and q 2 .The autocorrelation function for the top level time series Y t must be zero for all of the lags beyond q.This means that there is a representation of the top level time series as an MA processwhere u t is white noise.This moving average process has the same order as the higher order bottom level time series.Now consider the general ARIMA(p 1 , d, q 1 ) modeland suppose that we are interested in the sum of two time series z t = y t + w t .A practical situation where this occurs, in addition to the top-down versus bottom-up problem, is when the time series y t we are interested in cannot be observed directly and w t represents added noise due to measurement error.We want to know something about the nature of the sum of the two series, z t .The sum can be written asAssume that the time series w t can be represented as a stationary ARMA(p 2 , 0, q 2 ) modelwhere b t is white noise independent of a t .Then the top level time series isThe term on the left-hand side is a polynomial of order P = p 1 + p 2 , the first term on the right-hand side is a polynomial of order q 1 + p 2 , and the second term on the right-hand side is a polynomial of order p 1 + q 2 + d.Let Q be the maximum of q 1 + p 2 and p 1 + q 2 + d.Then the top level time series is an ARIMA(P, d, Q) model, say,where u t is white noise.Example 7.8 Suppose that we have a time series that is represented by an IMA(1, 1) model, and to this time series is added white noise.This could be a situation where measurements on a periodic sample of some characteristic in the output of a chemical process are made with a laboratory procedure, and the laboratory procedure has some built-in measurement error, represented by the white noise.Suppose that the underlying IMA(1, 1) model isLet D t be the first difference of the observed time series z t = y t + w t , where w t is white noise:The autocovariances of the differenced series areBecause the autocovariances at and beyond lag 2 are zero, we know that the observed time series will be IMA(1, 1).In general, we could write this aswhere the parameter 𝜃 * is unknown.However, we can find 𝜃 * easily.The autocovariances of the first differences of this observed time series areNow all we have to do is to equate the autocovariances for this observed series in terms of the parameter 𝜃 * with the autocovariances of the time series D t and we can solve for 𝜃 * and 𝜎 2 u .This gives the following:Suppose that 𝜎 2 a = 2 and 𝜎 2 w = 1.Then it turns out that the solution is 𝜃 * = 0.4 and 𝜎 2 u = 4.50.Adding the measurement error from the laboratory procedure to the original sample property has inflated the variability of the observed value rather considerably over the original variability that was present in the sample property.Neural networks, or more accurately artificial neural networks, have been motivated by the recognition that the human brain processes information in a way that is fundamentally different from the typical digital computer.The neuron is the basic structural element and informationprocessing module of the brain.A typical human brain has an enormous number of them (approximately 10 billion neurons in the cortex and 60 trillion synapses or connections between them) arranged in a highly complex, nonlinear, and parallel structure.Consequently, the human brain is a very efficient structure for information processing, learning, and reasoning.An artificial neural network is a structure that is designed to solve certain types of problems by attempting to emulate the way the human brain would solve the problem.The general form of a neural network is a "black-box" type of model that is often used to model high-dimensional, nonlinear data.In the forecasting environment, neural networks are sometimes used to solve prediction problems instead of using a formal model NEURAL NETWORKS AND FORECASTING 527 building approach or development of the underlying knowledge of the system that would be required to develop an analytical forecasting procedure.If it was a successful approach that might be satisfactory.For example, a company might want to forecast demand for its products.If a neural network procedure can do this quickly and accurately, the company may have little interest in developing a specific analytical forecasting model to do it.Hill et al. (1994) is a basic reference on artificial neural networks and forecasting.Multilayer feedforward artificial neural networks are multivariate statistical models used to relate p predictor variables x 1 , x 2 , … , x p to one or more output or response variables y.In a forecasting application, the inputs could be explanatory variables such as would be used in a regression model, and they could be previous values of the outcome or response variable (lagged variables).The model has several layers, each consisting of either the original or some constructed variables.The most common structure involves three layers: the inputs, which are the original predictors; the hidden layer, comprised of a set of constructed variables; and the output layer, made up of the responses.Each variable in a layer is called a node.Figure 7.12 shows a typical three-layer artificial neural network for forecasting the output variable y in terms of several predictors.A node takes as its input a transformed linear combination of the outputs from the nodes in the layer below it.Then it sends as an output a transformation of itself that becomes one of the inputs to one or more nodes on the next layer.The transformation functions are usually either sigmoidal (S shaped) or linear and are usually called activation functions or transferx 2,t-1Hidden layer Output layer y t FIGURE 7.12 Artificial neural network with one hidden layer.functions.Let each of the k hidden layer nodes a u be a linear combination of the input variables:where the w 1ju are unknown parameters that must be estimated (called weights) and 𝜃 u is a parameter that plays the role of an intercept in linear regression (this parameter is sometimes called the bias node).Each node is transformed by the activation function g( ).Much of the neural networks literature refers to these activation functions notationally as 𝜎 u because of their S shape (the use of 𝜎 is an unfortunate choice of notation so far as statisticians are concerned).Let the output of node a u be denoted by z u = g(a u ).Now we form a linear combination of these outputs, say, b = ∑ k u=1 w uev z u .Finally, the output response or the predicted value for y is a transformation of the b, say, y = g(b), where g(b) is the activation function for the response.The response variable y is a transformed linear combination of the original predictors.For the hidden layer, the activation function is often chosen to be either a logistic function or a hyperbolic tangent function.The choice of activation function for the output layer often depends on the nature of the response variable.If the response is bounded or dichotomous, the output activation function is usually taken to be sigmoidal, while if it is continuous, an identity function is often used.The neural network model is a very flexible form containing many parameters, and it is this feature that gives a neural network a nearly universal approximation property.That is, it will fit many historical data sets very well.However, the parameters in the underlying model must be estimated (parameter estimation is called "training" in the neural network literature), and there are a lot of them.The usual approach is to estimate the parameters by minimizing the overall residual sum of squares taken over all responses and all observations.This is a nonlinear least squares problem, and a variety of algorithms can be used to solve it.Often a procedure called backpropagation (which is a variation of steepest descent) is used, although derivative-based gradient methods have also been employed.As in any nonlinear estimation procedure, starting values for the parameters must be specified in order to use these algorithms.It is customary to standardize all the input variables, so small essentially random values are chosen for the starting values.With so many parameters involved in a complex nonlinear function, there is considerable danger of overfitting.That is, a neural network will provide a nearly perfect fit to a set of historical or "training" data, but it will often predict new data very poorly.Overfitting is a familiar problem to statisticians trained in empirical model building.The neural network community has developed various methods for dealing with this problem, such as reducing the number of unknown parameters (this is called "optimal brain surgery"), stopping the parameter estimation process before complete convergence and using cross-validation to determine the number of iterations to use, and adding a penalty function to the residual sum of squares that increases as a function of the sum of the squares of the parameter estimates.There are also many different strategies for choosing the number of layers and number of neurons and the form of the activation functions.This is usually referred to as choosing the network architecture.Crossvalidation can be used to select the number of nodes in the hidden layer.Artificial neural networks are an active area of research and application in many fields, particularly for the analysis of large, complex, highly nonlinear problems.The overfitting issue is frequently overlooked by many users and even the advocates of neural networks, and because many members of the neural network community do not have sound training in empirical model building, they often do not appreciate the difficulties overfitting may cause.Furthermore, many computer programs for implementing neural networks do not handle the overfitting problem particularly well.Studies of the ability of neural networks to predict future values of a time series that were not used in parameter estimation (fitting) have been, in many cases, disappointing.Our view is that neural networks are a complement to the familiar statistical tools of forecasting, and they might be one of the approaches you should consider, but they are not a replacement for them.This book has been focused on the analysis and modeling of time series in the time domain.This is a natural way to develop models, since time series all are observed as a function of time.However, there is another approach to describing and analyzing time series that uses a frequency domain approach.This approach consists of using the Fourier representation of a time series, given bywhere f k = k∕T.This model is named after J.B.J Fourier, an 18 th century French mathematician, who claimed that any periodic function could be represented as a series of harmonically related sinusoids.Other contributors to Fourier analysis include Euler, D. Bernoulli, Laplace, Lagrange, and Dirichlet.The original work of Fourier was focused on phenomena in continuous time, such as vibrating strings, and there are still many such applications today from such diverse fields as geophysics, oceanography, atmospheric science, astronomy, and many disciplines of engineering.However, the key ideas carry over to discrete time series.We confine our discussion to stationary discrete time series.Computing the constants a k and b k turns out to be quite simple:These coefficients are combined to form a periodogramThe periodogram is then usually smoothed and scaled to produce the spectrum or a spectral density function.The spectral density function is just the Fourier transform of the autocorrelation function, so it conveys similar information as is found in the autocorrelations.However, sometimes the spectral density is easier to interpret than the autocorrelation function because adjacent sample autocorrelations can be highly correlated while estimates of the spectrum at adjacent frequencies are approximately independent.Generally, if the frequency k∕T is important then I(f k ) will be large, and if the frequency k∕T is not important then I(f k ) will be small.It can be shown thatwhere 𝜎 2 is the variance of the time series.Thus the spectrum decomposes the variance of the time series into individual components, each of which is associated with a particular frequency.So we can think of spectral analysisFrequencyThe spectrum of a white noise process.as an analysis of variance technique.It decomposes the variability in the time series by frequency.It is helpful to know what the spectrum looks like for some simple ARMA models.If the time series white noise (uncorrelated observations with constant variance 𝜎 2 ), it can be shown that the spectrum is a horizontal straight line as shown in Figure 7.13.This means that the contribution to the variance at all frequencies is equal.A logical use for the spectrum is to calculate it for the residuals from a time series model and see if the spectrum is reasonably flat.Now consider the AR(1) process.The shape of the spectrum depends on the value of the AR(1) parameter 𝜙.When 𝜙 > 0, which results in a positively autocorrelated time series, the spectrum is dominated by lowfrequency components.These low-frequency or long-period components result in a relative smooth time series.When 𝜙 < 0, the time series is negatively autocorrelated, and the time series has a more ragged or volatile appearance.This produces a spectrum dominated by high-frequency or short-period components.Examples are shown in Figure 7.14.The spectrum of the MA(1) process is shown in Figure 7.15.When the MA(1) parameter is positive, that is, when 𝜃 > 0, the time series is negatively autocorrelated and has a more volatile appearance.Thus the spectrum is dominated by higher frequencies.When the MA(1) parameter is negative (𝜃 > 0), the time series is negatively autocorrelated and has a smoother appearance.This results in a spectrum that is dominated by low frequencies.The spectrum of seasonal processes will exhibit peaks at the harmonically related seasonal frequencies.For example, consider the simple seasonal model with period 12, as might be used to represent monthly data with an annual seasonal cycle:(1 − 𝜙 * B 12 )y t = 𝜀 t If 𝜙 * is positive, the spectrum will exhibit peaks at frequencies 0 and 2𝜋kt∕12, k = 1, 2, 3, 4, 5, 6. Figure 7.16 shows the spectrum.16 The spectrum of the seasonal (1 − 𝜙 * B 12 )y t = 𝜀 t process.Fisher's Kappa statistic tests the null hypothesis that the values in the series are drawn from a normal distribution with variance 1 against the alternative hypothesis that the series has some periodic component.The test statistic kappa (𝜅) is the ratio of the maximum value of the periodogram, I(f k ), to its average value.The probability of observing a larger Kappa if the null hypothesis is true is given bywhere k is the observed value of the kappa statistic, q = T∕2 if T is even and q = (T − 1)∕2 if T is odd.The null hypothesis is rejected if the computed probability is less that the desired significance level.There is also a Kolmogorov-Smirnov test due to Bartlett that compares the normalized cumulative periodogram to the cumulative distribution function of the uniform distribution on the interval (0, 1).The test statistic equals the maximum absolute difference of the cumulative periodogram and the uniform CDF.If this quantity exceeds a∕ √ q, then we should reject the hypothesis that the series comes from a normal distribution.The values a = 1.36 and a = 1.63 correspond to significance levels of 0.05 and 0.01, respectively.In general, we have found it difficult to determine the exact form of an ARIMA model purely from examination of the spectrum.The autocorrelation and partial autocorrelation functions are almost always more useful and easier to interpret.However, the spectrum is a complimentary tool and should always be considered as a useful supplement to the ACF and PACF.For further reading on spectral analysis and its many applications, see Jenkins and Watts (1969), Percival and Walden (1992), and Priestley (1991).Example 7.9 JMP can be used to compute and display the spectrum for time series.We will illustrate the JMP output using the monthly US beverage product shipments.These data are shown originally in Figure 1.5 and are in Appendix Table B.These data were also analyzed in Chapter 2 to illustrate decomposition techniques.Figure 7.17 presents the JMP output, including a time series plot, the sample ACF, PACF, and variogram, and the spectral density function.Notice that there is a prominent peak in the spectral density at frequency 0.0833 that corresponds to a seasonal period of 12 months.The JMP output also provides the Fisher kappa statistic and the P-value indicates that there is at least one periodic component.The Bartlett Kolmogorov-Smirnov test statistic is also significant at the 0.01 level indicating that the data do not come from a normal distribution.In many forecasting problems there is little or no historical information available at the time initial forecasts are required.Consequently, the initial forecasts must be based on subjective considerations.As information becomes available, this subjective information can be modified in light of actual data.An example of this is forecasting demand for seasonal clothing, which, because of style obsolescence, has a relatively short life.In this industry a common practice is to, at the start of the season, make a forecast of total sales for a product during the season and then as the season progresses the original forecast can be modified taking into account actual sales.Bayesian methods can be useful in problems of this general type.The original subjective estimates of the forecast are translated into subjective estimates of the forecasting model parameters.Then Bayesian methods are used to update these parameter estimates when information in the form of time series data becomes available.This section gives a brief overview of the Bayesian approach to parameter estimation and demonstrates the methodology with a simple time series model.The method of parameter estimation makes use of the Bayes' theorem.Let y be a random variable with probability density function that is characterized by an unknown parameter 𝜃.We write this density as f (y|𝜃) to show that the distribution depends on 𝜃.Assume that 𝜃 is a random variable with probability distribution h 0 (𝜃) which is called the prior distribution for 𝜃.The prior distribution summarizes the subjective information that we have about 𝜃, and the treatment of 𝜃 as a random variable is the major difference between Bayesian and classical methods of estimation.If we are relatively confident about the value of 𝜃 we should choose prior distribution with a small variance and if we are relatively uncertain about the value of 𝜃 we should choose prior distribution with a large variance.In a time series or forecasting scenario, the random variable y is a sequence of observations, say y 1 , y 2 , … , y T .The new estimate of the parameter 𝜃 will be in the form of a revised distribution, h 1 (𝜃|y), called the posterior distribution for 𝜃.Using Bayes' theorem the posterior distribution iswhere f (y|𝜃) is usually called the likelihood of y, given the value of 𝜃, and g(y) is the unconditional distribution of y averaged over all 𝜃.If the parameter 𝜃 is a discrete random variable then the integral in Eq. ( 7.34) should be replaced by a summation sign.This equation basically blends the observed information with the prior information to obtain a revised description of the uncertainty in the value of 𝜃 in the form of a posterior probability distribution.The Bayes' estimator of 𝜃, which we will denote by 𝜃 * , is defined as the expected value of the posterior distribution:Typically we would use 𝜃 * as the estimate of 𝜃 in the forecasting model.In some cases, it turns out that 𝜃 * is optimal in the sense of minimizing the variance of forecast error.We will illustrate the procedure with a relatively simple example.Suppose that y is normally distributed with mean 𝜇 and variance 𝜎 2 y ; that is,We will assume that 𝜎 2 y is known.The prior distribution for 𝜇 is assumed to be normal with mean 𝜇 ′ and variance 𝜎 2 ′ 𝜇 :The posterior distribution of 𝜇 given the observation y iswhich is a normal distribution with mean and variancerespectively.Refer to Winkler (2003), Raiffa and Schlaifer (1961), Berger (1985), and West and Harrison (1997) for more details of Bayesian statistical inference and decision making and additional examples.Now let us consider a simple time series model, the constant process, defined in Eq. (4.1) aswhere 𝜇 is the unknown mean and the random component is 𝜀 t , which we will assume to have a normal distribution with mean zero and known variance 𝜎 2 y Consequently, we are assuming that the observation in any period t has a normal distribution, saySince the variance 𝜎 2 y is known, the problem is to estimate 𝜇.Suppose that at the start of the forecasting process, time t = 0, we estimate the mean demand rate to be 𝜇 ′ and the variance 𝜎 2 ′ 𝜇 captures the uncertainty in this estimate.So the prior distribution for 𝜇 is the normal priorAfter one period, the observation y 1 is known.The estimate 𝜇 ′ and the variance 𝜎 2 ′ 𝜇 can now be updated using the results obtained above for a normal sampling process and a normal prior:whereAt the end of period 2, when the next observation y 2 becomes available, the Bayesian updating process transforms h 1 (𝜇|y 1 ) into h 2 (𝜇|y 1, y 2 ) in the following way:Here the old posterior h 1 is now used as a prior and combined with the likelihood of y 2 to obtain the new posterior distribution of 𝜇 at the end of period 2. Using our previous results, we now haveand))where ȳ = (y 1 + y 2 )∕2.It is easy to show that h 2 (𝜇|y 1 , y 2 ) = h 2 (𝜇|ȳ); that is, the same posterior distribution is obtained using the sample average ȳ as from using y 1 and y 2 sequentially because the sample average is a sufficient statistic for estimating the mean 𝜇.In general, we can show that after observing y T , we can calculate the posterior distribution aswherewhere ȳ = (y 1 + y 2 + ⋯ + y T )∕T.The Bayes estimator of 𝜇 after T periods is 𝜇 * (T) = 𝜇 ′′ (T).We can write this aswhere r = 𝜎 2 y ∕𝜎 2 ′ 𝜇 .Consequently, the Bayes estimator of 𝜇 is just a weighted average of the sample mean and the initial subjective estimate 𝜇 ′ .The Bayes estimator can be written in a recursive form aswhere (7.36) shows that the estimate of the mean in period T is updated at each period by a form that is similar to first-order exponential smoothing.However, notice that the smoothing factor 𝜆(T) is a function of T, and it becomes smaller as T increases.Furthermore, since 𝜎 2 ′′ 𝜇 (T) = 𝜆(T)𝜎 2 y , the uncertainty in the estimate of the mean decreases to zero as time T becomes large.Also, the weight given to the prior estimate of the mean decreases as T becomes large.Eventually, as more data becomes available, a permanent forecasting procedure could be adopted, perhaps involving exponential smoothing.This estimator is optimal in the sense that it minimizes the variance of forecast error even if the process is not normally distributed.We assumed that the variance of the demand process was known, or at least a reasonable estimate of it was available.Uncertainty in the value of this parameter could be handled by also treating it as a random variable.Then the prior distribution would be a joint distribution that would reflect the uncertainty in both the mean and the variance.The Bayesian updating process in this case is considerably more complicated than in the knownvariance case.Details of the procedure and some useful advice on choosing a prior are in Raiffa and Schlaifer (1961).Once the prior has been determined, the forecasting process is relatively straightforward.For a constant process, the forecasting equation isusing the Bayes estimate as the current estimate of the mean.Our uncertainty in the estimate of the mean is just the posterior variance.So the variance of the forecast isand the variance of forecast error isThe variance of forecast error is independent of the lead time in the Bayesian case for a constant process.If we assume that y and 𝜇 are normally distributed, then we can use Eq.(3.38) to find a 100(1-𝛼)% prediction interval on the forecast V[ŷ T+𝜏 (T)] as follows:where Z 𝛼∕2 is the usual 𝛼∕2 percentage point of the standard normal distribution.Example 7.10 Suppose that we are forecasting weekly demand for a new product.We think that demand is normally distributed, and that at least in the short run that a constant model is appropriate.There is no useful historical information, but a reasonable prior distribution for 𝜇 is N(100, 25) and 𝜎 2 y is estimated to be 150.At time period T = 0 the forecast for period 1 isThe variance of forecast error is 150 + 25 = 175, so a 96% prediction interval for y 1 is 100 ± 1.96 √ Suppose the actual demand experienced in period 1 is y 1 = 86.We can use Eq.(7.37) to update the estimate of the mean.First, calculate r = 𝜎 2 y ∕𝜎 2 ′ 𝜇 = 150∕25 = 6 and 𝜆( 1 So the forecast for period 3 isand the updated variance estimate isTherefore the 96% prediction interval for y 3 is 97.5 ± 1.96 150 + 18.8 or [72.0, 123.0]This procedure would be continued until it seems appropriate to change to a more permanent forecasting procedure.For example, a change to first-order exponential smoothing could be made when 𝜆(T) drop to a target level, say 0.05 < 𝜆(T) < 0.1.Then after sufficient data has been observed, an appropriate time series model could be fit to the data.Over the last 35 years there has been considerable information accumulated about forecasting techniques and how these methods are applied in a wide variety of settings.Despite the development of excellent analytical techniques, many business organizations still rely on judgment forecasts by their marketing, sales, and managerial/executive teams.The empirical evidence regarding judgment forecasts is that they are not as successful as statistically based forecasts.There are some fields, such as financial investments, where there is considerable strong evidence that this is so.There are a number of reasons why we would expect judgment forecasts to be inferior to statistical methods.Inconsistency, or changing one's mind for no compelling or obvious reason, is a significant source of judgment forecast errors.Formalizing the forecasting process through the use of analytical methods is one approach to eliminating inconsistency as a source of error.Formal decision rules that predict the variables of interest using relatively few inputs invariably predict better than humans, because humans are inconsistent over time in their choice of input factors to consider, and how to weight them.Letting more recent events dominate one's thinking, instead of weighting current and previous experience more evenly, is another source of judgment forecast errors.If these recent events are essentially random in nature, they can have undue impact on current forecasts.A good forecasting system will certainly monitor and evaluate recent events and experiences, but will only incorporate them into the forecasts if there is sufficient evidence to indicate that they represent real effects.Mistaking correlation for causality can also be a problem.This is the belief that two (or more) variables are related in a causal manner and taking action on this, when the variables exhibit only a correlation between them.It is not difficult to find correlative relationships; any two variables that are monotonically related will exhibit strong correlation.So company sales may appear to be related to some factor that over a short time period is moving synchronously with sales, but relying on this as a causal relationship will lead to problems.The statistical significance of patterns and relationships does not necessarily imply a cause-and-effect relationship.Judgment forecasts are often dominated by optimistic thinking.Most humans are naturally optimistic.An executive wants sales for the product line to increase because his/her bonus may depend on the results.A product manager wants his/her product to be successful.Sometimes bonus payouts are made for exceeding sales goals, and this can lead to unrealistically low forecasts, which in turn are used to set the goals.However, unrealistic forecasts, whether too high or too low, always result in problems downstream in the organization where forecast errors have meaningful impact on efficiency, effectiveness, and bottom-line results.Humans are notorious for underestimating variability.Judgment forecasts rarely incorporate uncertainty in any formal way and, as a result, often underestimate its magnitude and impact.A judgment forecaster often completely fails to express any uncertainty in his/her forecast.Because all forecasts are wrong, one must have some understanding of the magnitude of forecast errors.Furthermore, planning for appropriate actions in the face of likely forecast error should be part of the decision-making process that is driven by the forecast.Statistical forecasting methods can be accompanied by prediction intervals.In our view, every forecast should be accompanied by a PI that adequately expresses for the decision maker how much uncertainty is associated with the point forecast.In general, both the users of forecasts (decision makers) and the preparers (forecasters) have reasonably good awareness of many of the basic analytical forecasting techniques, such as exponential smoothing and regression-based methods.They are less familiar with time series models such as the ARIMA model, transfer function models, and other more sophisticated methods.Decision makers are often unsatisfied with subjective and judgment methods and want better forecasts.They often feel that analytical methods can be helpful in this regard.This leads to a discussion of expectations.What kind of results can one reasonably expect to obtain from analytical forecasting methods?By results, we mean forecast errors.Obviously, the results that a specific forecaster obtains are going to depend on the specific situation: what variables are being forecast, the availability and quality of data, the methods that can be applied to the problem, and the tools and expertise that are available.However, because there have been many surveys of both forecasters and users of forecasts, as well as forecast competitions (e.g., see Makridakis et al. (1993)) where many different techniques have been applied in headto-head challenges, some broad conclusions can be drawn.In general, exponential smoothing type methods, including Winters' method, typically experience mean absolute prediction errors ranging from 10% to 15% for lead-one forecasts.As the lead time increases, the prediction error increases, with mean absolute prediction errors typically in the 17-25% range at lead times of six periods.At 12 period lead times, the mean absolute prediction error can range from 18% to 45%.More sophisticated time series models such as ARIMA models are not usually much better, with the mean absolute prediction error ranging from about 10% for lead-one forecasts, to about 17% for lead-six forecasts, and up to 25% for 12 period lead times.This probably accounts for some of the dissatisfaction that forecasters often express with the more sophisticated techniques; they can be much harder to use, but they do not have substantial payback in terms of reducing forecasting errors.Regression methods often produce mean absolute prediction errors ranging from 12% to 18% for lead-one forecasts.As the lead time increases, the prediction error increases, with mean absolute prediction errors typically in the 17-20% range for six period lead times.At 12 period lead times, the mean absolute prediction error can range from 20% to 25%.Seasonal time series are often easier to predict than nonseasonal ones, because seasonal patterns are relatively stable through time, and relatively simple methods such as Winters' method and seasonal adjustment procedures typically work very well as forecasting techniques.Interestingly, seasonal adjustment techniques are not used nearly as widely as we would expect, given their relatively good performance.When forecasting is done well in an organization, it is typically done by a group of individuals who have some training and experience in the techniques, have access to the right information, and have an opportunity to see how the forecasts are used.If higher levels of management routinely intervene in the process and use their judgment to modify the forecasts, it is highly desirable if the forecast preparers can interact with these managers to learn why the original forecasts require modification.Unfortunately, in many organizations, forecasting is done in an informal way, and the forecasters are often marketing or sales personnel, or market researchers, for whom forecasting is only a (sometimes small) part of their responsibilities.There is often a great deal of turnover in these positions, and so no long-term experience base or continuity builds up.The lack of a formal, organized process is often a big part of the reason why forecasting is not as successful as it should be.Any evaluation of a forecasting effort in an organization should consider at least the following questions:1. What methods are being used?Are the methods appropriate to organizational needs, when planning horizons and other business issues are taken into account?Is there an opportunity to use more than one forecasting procedure?Could forecasts be combined to improve results? 2. Are the forecasting methods being used correctly?3. Is an appropriate set of data being used in preparing the forecasts?Is data quality an issue?Are the underlying assumptions of the methods employed satisfied at least well enough for the methods to be successful?4. Is uncertainty being addressed adequately?Are prediction intervals used as part of the forecast report?Do forecast users understand the PIs? 5. Does the forecasting system take economic/market forces into account?Is there an ability to capitalize on current events, natural forces, and swings in customer preferences and tastes?6.Is forecasting separate from planning?Very often the forecast is really just a plan or schedule.For example, it may reflect a production plan, not a forecast of what we could realistically expect to sell (i.e., demand).Many individuals do not understand the difference between a forecast and a plan.In the short-to-medium term, most businesses can benefit by taking advantage of the relative stability of seasonal patterns and the inertia present in most time series of interest.These are the methods we have focused on in this book.Example 7.11 The data for this example are in the array called pressure .data of which the two columns represent the viscosity and the temperature, respectively.To model the multivariate data we use the "VAR" function in R package "vars."But we first start with time series, acf, pacf, ccf plots as suggested in the example library(vars) pf<-pressure.data[,1]pb<-pressure.data[,2]plot (pf,type="o",pch=16,cex=.5,xlab='Time',ylab='Pressure',ylim= c(4,25)) lines (pb, type="o",pch=15,cex=.5, col="grey40") legend(1,7,c("Variable","Front","Back"), pch=c (NA,16,15),lwd=c(NA,.5,.5),cex=.55,col=c("black","black","grey40")) res.pf<-as.vector(residuals(arima(pf,order=c(1,0,0))))res.pb<-as.vector(residuals(arima(pb,order=c(1,0,0))))par(mfrow=c(2,2),oma=c(0,0,0,0)) acf(pf,lag.max=25,type="correlation",main="ACF for Front Pressure") acf(pb, lag.max=25, type="correlation",main="PACF for Back Pressure",ylab="PACF") We now fit a VAR(1) model to the data using VAR function: Note that there is also a VARselect function that will automatically select the best p order of the VAR(p) model.In this case we tried p upto 5.> VARselect(pressure.data,lag.max=5)The output shows that VAR(1) was indeed the right choice.We now plot the residuals.plot(residuals(pressure.var1)[,1],type="p",pch=15,cex=.5,xlab='Time', ylab='Residuals',ylim=c(-3,3)) points(residuals(pressure.var1)[,2],pch=1,cex=.5, col="grey40") legend(100,3,c("Residuals","Front","Back"), pch=c(NA,15,1),lwd=c(NA,.5,.5),cex=.55,col=c("black","black","grey40"))Show that an MA(1) model can be written in state space form.Consider the information on weekly spare part demand shown in Table E7.1.Suppose that 74 requests for 55 parts are received during the current week, T. Find the new cumulative distribution of demand.Use 𝜆 = 0.1.What is your forecast of the 70th percentile of the demand distribution?7.4 Consider the information on weekly luxury car rentals shown in Table E7.2.Suppose that 37 requests for rentals are received during the current week, T. Find the new cumulative distribution of demand.Use 𝜆 = 0.1.What is your forecast of the 90th percentile of the demand distribution?7.5 Rework Exercise 7.3 using 𝜆 = 0.4.How much difference does changing the value of the smoothing parameter make in your estimate of the 70th percentile of the demand distribution?7.6 Rework Exercise 7.4 using 𝜆 = 0.4.How much difference does changing the value of the smoothing parameter make in your estimate of the 70th percentile of the demand distribution?Suppose that two forecasting methods can be used for a time series, and that the two variances of the forecast errors are 𝜎 2 1 = 10 and 𝜎 2 2 = 25.If the correlation coefficient 𝜌 = −0.75,calculate the optimum value of the weight used to optimally combine the two individual forecasts.What is the variance of the combined forecast?7.8 Suppose that two forecasting methods can be used for a time series, and that the two variances of the forecast errors are 𝜎 2 1 = 15 and 𝜎 2 2 = 20.If the correlation coefficient 𝜌 = −0.4,calculate the optimum value of the weight used to optimally combine the two individual forecasts.What is the variance of the combined forecast?7.9 Suppose that two forecasting methods can be used for a time series, and that the two variances of the forecast errors are 𝜎 2 1 = 8 and 𝜎 2 2 = 16.If the correlation coefficient 𝜌 = −0.3,calculate the optimum value of the weight used to optimally combine the two individual forecasts.What is the variance of the combined forecast?7.10 Suppose that two forecasting methods can be used for a time series, and that the two variances of the forecast errors are 𝜎 2 1 = 1 and 𝜎 2 2 = 8.If the correlation coefficient 𝜌 = −0.65,calculate the optimum value of the weight used to optimally combine the two individual forecasts.What is the variance of the combined forecast?7.11 Rework Exercise 7.8 assuming that 𝜌 = 0.4.What effect does changing the sign of the correlation coefficient have on the weight used to optimally combine the two forecasts?What is the variance of the combined forecast?7.12 Rework Exercise 7.9 assuming that 𝜌 = 0.3.What effect does changing the sign of the correlation coefficient have on the weight used to optimally combine the two forecasts?What is the variance of the combined forecast?7.13 Suppose that there are three lead-one forecasts available for a time series, and the covariance matrix of the three forecasts is as follows:Find the optimum weights for combining these three forecasts.What is the variance of the combined forecast?7.14 Suppose that there are three lead-one forecasts available for a time series, and the covariance matrix of the three forecasts is as follows:Find the optimum weights for combining these three forecasts.What is the variance of the combined forecast?7.15 Table E7.3 presents 25 forecast errors for two different forecasting techniques applied to the same time series.Is it possible to combine the two forecasts to improve the forecast errors?What is the optimum weight for combining the forecasts?What is the variance of the combined forecast?7. 16 Show that when combining two forecasts, if the correlation between the two sets of forecast errors is 𝜌 = 𝜎 1 ∕𝜎 2 , then Min Var [e c T+𝜏 (T)] = 𝜎 2 1 , where 𝜎 2 1 is the smaller of the two forecast error variances.7.17 Show that when combining two forecasts, if the correlation between the two sets of forecast errors is 𝜌 = 0, then Var [e c T+𝜏 (T)] = 𝜎 2 1 𝜎 2 2 ∕(𝜎 2 1 + 𝜎 2 2 ).7.18 Let y t be an IMA(1, 1) time series with parameter 𝜃 = 0.4.Suppose that this time series is observed with an additive white noise error.a. What is the model form of the observed error?b.Find the parameters of the observed time series, assuming that the variances of the errors in the original time series and the white noise are equal.Show that an AR(1) time series that is observed with an additive white noise error is an ARMA(1, 1) process.2005.Develop an appropriate multivariate time series model for the gas, electricity, and oil time series.Reconsider the data on heating fuel in Table E7.4.Suppose that you are interested in forecasting the aggregate series (the Total column in Table E7.4).One way to do this is to forecast the total directly.Another way is to forecast the individual component series and sum the forecasts of the components to obtain a forecast for the total.Investigate these approaches for this data and report on your conclusions.Reconsider the data on heating fuel in Table E7.4.Suppose that you are interested in forecasting the four individual components series (the Gas, Electricity, Oil, and Other Types columns in Table E7.4).One way to do this is to forecast the individual time series directly.Another way is to forecast the total and obtain forecasts of the individual component series by decomposing the forecast for the totals into component parts.Investigate these approaches for this data and report on your conclusions.E7.5 contains data on property crimes reported to the police in the United States.Both the number of property crimes and the crime rate per 100,000 individuals are shown.Using the data on the number of crimes reported, develop an appropriate multivariate time series model for the burglary, larceny-theft, and motor vehicle theft time series.7.28 Repeat Exercise 7.27 using the property crime rate data.Compare the models obtained using the number of crimes reported versus the crime rate.Reconsider the data on property crimes in Table E7.5.Suppose that you are interested in forecasting the aggregate crime rate series.One way to do this is to forecast the total directly.Another way is to forecast the individual component series and sum the forecasts of the components to obtain a forecast for the total.Investigate these approaches for this data and report on your conclusions.Reconsider the data on property crimes in Table E7.5.Suppose that you are interested in forecasting the four individual component series (the Burglary, Larceny-Theft, and Motor Vehicle Theft columns in Table E7.5).One way to do this is to forecast the individual time series directly.Another way is to forecast the totalThe mean 𝜆 of the demand distribution is assumed to be a random variable with a gamma distributionwhere a and b are parameters having subjectively determined values.In the week following the establishment of this prior distribution d parts were demanded.What is the posterior distribution of 𝜆?APPENDIX A STATISTICAL TABLES            17 39.25 39.30 39.33 39.36 39.37 39.39 39.40 39.41 39.43 39.45 39.46 39.46 39.47 39.48 39.49 39.50 3 17.44 16.04 15.44 15.10 14.88 14.73 14.62 14.54 14.47 14.42 14.34 14.25 14.17 14.12                 Week 1997 1998 1999 2000 2001 2002 2003 2004 2005 2006 2007 2008   Week 1997 1998 1999 2000 2001 2002 2003 2004 2005 2006 2007 2008            http://www.census.gov/econ/currentdata/.INTRODUCTION TO R r a well-developed, simple and effective programming language which includes conditionals, loops, user-defined recursive functions and input and output facilities.The term "environment" is intended to characterize it as a fully planned and coherent system, rather than an incremental accretion of very specific and inflexible tools, as is frequently the case with other data analysis software.R, like S, is designed around a true computer language, and it allows users to add additional functionality by defining new functions.Much of the system is itself written in the R dialect of S, which makes it easy for users to follow the algorithmic choices made.For computationally-intensive tasks, C, C++ and Fortran code can be linked and called at run time.Advanced users can write C code to manipulate R objects directly.Many users think of R as a statistics system.We prefer to think of it of an environment within which statistical techniques are implemented.R can be extended (easily) via packages.There are about eight packages supplied with the R distribution and many more are available through the CRAN family of Internet sites covering a very wide range of modern statistics.In this second edition of our book, we decided to provide the R-code for most of the examples at the end of the chapters.The codes are generated with the novice R user in mind and we therefore tried to keep them simple and easy to understand, sometimes without taking full advantage of more sophisticated options available in R. We nonetheless believe that they offer readers the possibility to immediately apply the techniques covered in the chapters with the data provided at the end of the book or with their own data.This after all we believe is the best way to learn time series analysis and forecasting.R can be downloaded from the R project webpage mentioned above.Although there are some generic built-in functions such as mean() to calculate the sample mean or lm() to fit a linear model, R provides the flexibility of writing your own functions as in C++ or Matlab.In fact one of the main advantages of R is its ever-growing user community, who openly shares the new functions they wrote in terms of "packages."Each new package has to be installed and loaded from "Packages" option in order to be able to use its contents.We provide the basic commands in R below.Data entry can be done manually using c() function such as > temp<-c(75.5,76.3,72.4,75.7,78.6)Now the vector temp contains 5 elements that can displayed using However for large data sets, importing the data from an ASCII file, for example, a .txtfile, is preferred.If, for example, each entry of temp represents the average temperature on a weekday and is stored in a file named temperature.txt,the data can then be imported to R using read.table()function as > temp<-read.table("temperature.txt",header=TRUE,Sep="")This command will assign the contents of temperature.txtfile into the data frame "temp."It assumes that the first row of the file contains the names of the individual variables in the file, for example, in this case "Day" and "Temperature" and the data are space delimited.Also note that the command further assumes that the file is in the working directory, which can be changed using the File option.Otherwise the full directory has to be specified, for example, if the file is in C:/Rcoding directory, read.table("C:/Rcoding/temperature.txt",header=T,sep=",")Now that the data are imported, we can start using built-in function such as the sample mean and the log transform of the temperature by > mean(temp$Temperature) [1] 75.7 > log(temp$Temperature) [1] 4.324133 4.334673 4.282206 4.326778 4.364372 One can also write user-defined functions to analyze the data.As we mentioned earlier, for most basic statistical functions there already exists packages containing the functions that would serve the desired purpose.Some basic examples of functions are provided in the R-code of the examples.As indicated in the R project's webpage: "One of R's strengths is the ease with which well-designed publication-quality plots can be produced, including mathematical symbols and formulae where needed.Great care has been taken over the defaults for the minor design choices in graphics, but the user retains full control."In order to show the flexibility of plotting options in R, in the examples we provide the code for different plots for time series data and residual analysis with various options to make the plots look very similar to the ones generated by the commercial software packages used in the chapters.Exporting the output or new data can be done through write.table()function.In order to create, for example, a new data frame by appending to the original data frame the log transform of the temperature and export the new data frame into a .txtfile, the following commands can be usedDATA SETS FOR EXERCISESThroughout the book, we often refer to commercial statistical software packages such as JMP and Minitab when discussing the examples.These software packages indeed provide an effective option particularly for the undergraduate level students and novice statisticians with their pull-down menus and various built-in statistical functions and routines.However there is also a growing community of practitioners and academicians who prefer to use R, an extremely powerful and freely available statistical software package that can be downloaded from http://www.r-project.org/.According to this webpage,DATA SETS FOR EXERCISESThroughout the book, we often refer to commercial statistical software packages such as JMP and Minitab when discussing the examples.These software packages indeed provide an effective option particularly for the undergraduate level students and novice statisticians with their pull-down menus and various built-in statistical functions and routines.However there is also a growing community of practitioners and academicians who prefer to use R, an extremely powerful and freely available statistical software package that can be downloaded from http://www.r-project.org/.According to this webpage,