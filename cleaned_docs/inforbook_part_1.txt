The fundamental problem of communication is that of reproducing at one point either exactly or approximately a message selected at another point.(Claude Shannon, 1948) In the first half of this book we study how to measure information content; we learn how to compress data; and we learn how to communicate perfectly over imperfect communication channels.We start by getting a feeling for this last problem.Some examples of noisy communication channels are:• an analogue telephone line, over which two modems communicate digital modem phone line modeminformation;• the radio communication link from Galileo, the Jupiter-orbiting space- • a disk drive.The last example shows that communication doesn't have to involve information going from one place to another.When we write a file on a disk drive, we'll read it off in the same location -but at a later time.These channels are noisy.A telephone line suffers from cross-talk with other lines; the hardware in the line distorts and adds noise to the transmitted signal.The deep space network that listens to Galileo's puny transmitter receives background radiation from terrestrial and cosmic sources.DNA is subject to mutations and damage.A disk drive, which writes a binary digit (a one or zero, also known as a bit) by aligning a patch of magnetic material in one of two orientations, may later fail to read out the stored binary digit: the patch of material might spontaneously flip magnetization, or a glitch of background noise might cause the reading circuit to report the wrong value for the binary digit, or the writing head might not induce the magnetization in the first place because of interference from neighbouring bits.In all these cases, if we transmit data, e.g., a string of bits, over the channel, there is some probability that the received message will not be identical to the 1 -Introduction to Information Theory transmitted message.We would prefer to have a communication channel for which this probability was zero -or so close to zero that for practical purposes it is indistinguishable from zero.Let's consider a noisy disk drive that transmits each bit correctly with probability (1−f ) and incorrectly with probability f .This model communication channel is known as the binary symmetric channel (figure 1.4).The noise level, the probability that a bit is flipped, is f .As an example, let's imagine that f = 0.1, that is, ten per cent of the bits are flipped (figure 1.5).A useful disk drive would flip no bits at all in its entire lifetime.If we expect to read and write a gigabyte per day for ten years, we require a bit error probability of the order of 10 −15 , or smaller.There are two approaches to this goal.The physical solution is to improve the physical characteristics of the communication channel to reduce its error probability.We could improve our disk drive by 1. using more reliable components in its circuitry; 2. evacuating the air from the disk enclosure so as to eliminate the turbulence that perturbs the reading head from the track;3. using a larger magnetic patch to represent each bit; or 4. using higher-power signals or cooling the circuitry in order to reduce thermal noise.These physical modifications typically increase the cost of the communication channel.Information theory and coding theory offer an alternative (and much more exciting) approach: we accept the given noisy channel as it is and add communication systems to it so that we can detect and correct the errors introduced by the channel.As shown in figure 1.6, we add an encoder before the channel and a decoder after it.The encoder encodes the source message s into a transmitted message t, adding redundancy to the original message in some way.The channel adds noise to the transmitted message, yielding a received message r.The decoder uses the known redundancy introduced by the encoding system to infer both the original signal s and the added noise..6.The 'system' solution for achieving reliable communication over a noisy channel.The encoding system introduces systematic redundancy into the transmitted vector t.The decoding system uses this known redundancy to deduce from the received vector r both the original source vector and the noise introduced by the channel.Whereas physical solutions give incremental channel improvements only at an ever-increasing cost, system solutions can turn noisy channels into reliable communication channels with the only cost being a computational requirement at the encoder and decoder.Information theory is concerned with the theoretical limitations and potentials of such systems.'What is the best error-correcting performance we could achieve?'Coding theory is concerned with the creation of practical encoding and decoding systems.We now consider examples of encoding and decoding systems.What is the simplest way to add useful redundancy to a transmission?[To make the rules of the game clear: we want to be able to detect and correct errors; and retransmission is not an option.We get only one chance to encode, transmit, and decode.]A straightforward idea is to repeat every bit of the message a prearranged number of times -for example, three times, as shown in table 1.7.We call this repetition code 'R 3 '.Transmitted sequence sequence s t 0 000 1 111Table 1.7.The repetition code R 3 .Imagine that we transmit the source message s = 0 0 1 0 1 1 0 over a binary symmetric channel with noise level f = 0.1 using this repetition code.We can describe the channel as 'adding' a sparse noise vector n to the transmitted vector -adding in modulo 2 arithmetic, i.e., the binary algebra in which 1+1=0.A possible noise vector n and received vector r = t + n are shown in figure 1.8.s 0 0 1 0 1 1 0 t 0 0 0 0 0 0 1 1 1 0 0 0 1 1 1 1 1 1 0 0 0 n 0 0 0 0 0 1 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 r 0 0 0 0 0 1 1 1 1 0 0 0 0 1 0 1 1 1 0 0 0Algorithm 1.9.Majority-vote decoding algorithm for R 3 .Also shown are the likelihood ratios (1.23), assuming the channel is a binary symmetric channel; γ ≡ (1 − f )/f .At the risk of explaining the obvious, let's prove this result.The optimal decoding decision (optimal in the sense of having the smallest probability of being wrong) is to find which value of s is most probable, given r.Consider the decoding of a single bit s, which was encoded as t(s) and gave rise to three received bits r = r 1 r 2 r 3 .By Bayes' theorem, the posterior probability of s is P (s | r 1 r 2 r 3 ) = P (r 1 r 2 r 3 | s)P (s) P (r 1 r 2 r 3 ) .(1.18)We can spell out the posterior probability of the two alternatives thus:P (s = 1 | r 1 r 2 r 3 ) = P (r 1 r 2 r 3 | s = 1)P (s = 1) P (r 1 r 2 r 3 ) ;(1.19) P (s = 0 | r 1 r 2 r 3 ) = P (r 1 r 2 r 3 | s = 0)P (s = 0) P (r 1 r 2 r 3 ) .(1.20)This posterior probability is determined by two factors: the prior probability P (s), and the data-dependent term P (r 1 r 2 r 3 | s), which is called the likelihood of s.The normalizing constant P (r 1 r 2 r 3 ) needn't be computed when finding the optimal decoding decision, which is to guess ŝ = 0 if P (s = 0 | r) > P (s = 1 | r), and ŝ = 1 otherwise.To find P (s = 0 | r) and P (s = 1 | r), we must make an assumption about the prior probabilities of the two hypotheses s = 0 and s = 1, and we must make an assumption about the probability of r given s.We assume that the prior probabilities are equal: P (s = 0) = P (s = 1) = 0.5; then maximizing the posterior probability P (s | r) is equivalent to maximizing the likelihood P (r | s).And we assume that the channel is a binary symmetric channel with noise level f < 0.5, so that the likelihood iswhere N = 3 is the number of transmitted bits in the block we are considering, andThus the likelihood ratio for the two hypotheses is(1.23) each factor P (rn|tn(1)) P (rn|tn(0)) equals (1−f )Thus the majority-vote decoder shown in algorithm 1.9 is the optimal decoder if we assume that the channel is a binary symmetric channel and that the two possible source messages 0 and 1 have equal prior probability.We now apply the majority vote decoder to the received vector of figure 1.8.The first three received bits are all 0, so we decode this triplet as a 0. In the second triplet of figure 1.8, there are two 0s and one 1, so we decode this triplet as a 0 -which in this case corrects the error.Not all errors are corrected, however.If we are unlucky and two errors fall in a single block, as in the fifth triplet of figure 1.8, then the decoding rule gets the wrong answer, as shown in figure 1.10.p.16] Show that the error probability is reduced by the use of The exercise's rating, e.g.'[2 ]', indicates its difficulty: '1' exercises are the easiest.Exercises that are accompanied by a marginal rat are especially recommended.If a solution or partial solution is provided, the page is indicated after the difficulty rating; for example, this exercise's solution is on page 16.R 3 by computing the error probability of this code for a binary symmetric channel with noise level f .The error probability is dominated by the probability that two bits in a block of three are flipped, which scales as f 2 .In the case of the binary symmetric channel with f = 0.1, the R 3 code has a probability of error, after decoding, of p b 0.03 per bit. Figure 1.11 shows the result of transmitting a binary image over a binary symmetric channel using the repetition code.source bits over a binary symmetric channel with f = 10% using a repetition code and the majority vote decoding algorithm.The probability of decoded bit error has fallen to about 3%; the rate has fallen to 1/3.Copyright Cambridge University Press 2003.On-screen viewing permitted.Printing not permitted.http://www.cambridge.org/0521642981You can buy this book for 30 pounds or $50.See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links.We would like the rate to be large and p b to be small.The repetition code R 3 has therefore reduced the probability of error, as desired.Yet we have lost something: our rate of information transfer has fallen by a factor of three.So if we use a repetition code to communicate data over a telephone line, it will reduce the error frequency, but it will also reduce our communication rate.We will have to pay three times as much for each phone call.Similarly, we would need three of the original noisy gigabyte disk drives in order to create a one-gigabyte disk drive with p b = 0.03.Can we push the error probability lower, to the values required for a sellable disk drive -10 −15 ?We could achieve lower error probabilities by using repetition codes with more repetitions.p.16] (a) Show that the probability of error of R N , the repetition code with N repetitions, is (1.24) for odd N .(b) Assuming f = 0.1, which of the terms in this sum is the biggest?How much bigger is it than the second-biggest term?A block code is a rule for converting a sequence of source bits s, of length K, say, into a transmitted sequence t of length N bits.To add redundancy, we make N greater than K.In a linear block code, the extra N − K bits are linear functions of the original K bits; these extra bits are called parity-check bits.An example of a linear block code is the (7, 4) Hamming code, which transmits N = 7 bits for every K = 4 source bits.The encoding operation for the code is shown pictorially in figure 1.13.We arrange the seven transmitted bits in three intersecting circles.The first four transmitted bits, t 1 t 2 t 3 t 4 , are set equal to the four source bits, s 1 s 2 s 3 s 4 .The parity-check bits t 5 t 6 t 7 are set so that the parity within each circle is even: the first parity-check bit is the parity of the first three source bits (that is, it is 0 if the sum of those bits is even, and 1 if the sum is odd); the second is the parity of the last three; and the third parity bit is the parity of source bits one, three and four.As an example, figure 1.13b shows the transmitted codeword for the case s = 1000.Table 1.14 shows the codewords generated by each of the 2 4 = sixteen settings of the four source bits.These codewords have the special property that any pair differ from each other in at least three bits.Any pair of codewords differ from each other in at least three bits.Because the Hamming code is a linear code, it can be written compactly in terms of matrices as follows.The transmitted codeword t is obtained from the source sequence s by a linear operation,where G is the generator matrix of the code, .26)and the encoding operation (1.25) uses modulo-2 arithmetic (1 + 1 = 0, 0 + 1 = 1, etc.).In the encoding operation (1.25)I have assumed that s and t are column vectors.If instead they are row vectors, then this equation is replaced by 1 0 0 0 1 0 1 0 1 0 0 1 1 0 0 0 1 0 1 1 1 0 0 0 1 0 1 1     .(1.28) I find it easier to relate to the right-multiplication (1.25) than the left-multiplication (1.27).Many coding theory texts use the left-multiplying conventions (1.27-1.28),however.The rows of the generator matrix (1.28) can be viewed as defining four basis vectors lying in a seven-dimensional binary space.The sixteen codewords are obtained by making all possible linear combinations of these vectors.Decoding the (7, 4) Hamming codeWhen we invent a more complex encoder s → t, the task of decoding the received vector r becomes less straightforward.Remember that any of the bits may have been flipped, including the parity bits.If we assume that the channel is a binary symmetric channel and that all source vectors are equiprobable, then the optimal decoder identifies the source vector s whose encoding t(s) differs from the received vector r in the fewest bits.[Refer to the likelihood function (1.23) to see why this is so.]We could solve the decoding problem by measuring how far r is from each of the sixteen codewords in table 1.14, then picking the closest.Is there a more efficient way of finding the most probable source vector?For the (7, 4) Hamming code there is a pictorial solution to the decoding problem, based on the encoding picture, figure 1.13.As a first example, let's assume the transmission was t = 1000101 and the noise flips the second bit, so the received vector is r = 1000101 ⊕ 0100000 = 1100101.We write the received vector into the three circles as shown in figure 1.15a, and look at each of the three circles to see whether its parity is even.The circles whose parity is not even are shown by dashed lines in figure 1.15b.The decoding task is to find the smallest set of flipped bits that can account for these violations of the parity rules.[The pattern of violations of the parity checks is called the syndrome, and can be written as a binary vector -for example, in figure 1.15b, the syndrome is z = (1, 1, 0), because the first two circles are 'unhappy' (parity 1) and the third circle is 'happy' (parity 0).]To solve the decoding task, we ask the question: can we find a unique bit that lies inside all the 'unhappy' circles and outside all the 'happy' circles?If so, the flipping of that bit would account for the observed syndrome.In the case shown in figure 1.15b, the bit r 2 lies inside the two unhappy circles and outside the happy circle; no other single bit has this property, so r 2 is the only single bit capable of explaining the syndrome.Let's work through a couple more examples.Figure 1.15c shows what happens if one of the parity bits, t 5 , is flipped by the noise.Just one of the checks is violated.Only r 5 lies inside this unhappy circle and outside the other two happy circles, so r 5 is identified as the only single bit capable of explaining the syndrome.If the central bit r 3 is received flipped, figure 1.15d shows that all three checks are violated; only r 3 lies inside all three circles, so r 3 is identified as the suspect bit.(a).In (b,c,d,e), the received vector is shown, assuming that the transmitted vector was as in figure 1.13b and the bits labelled by were flipped.The violated parity checks are highlighted by dashed circles.One of the seven bits is the most probable suspect to account for each 'syndrome', i.e., each pattern of violated and satisfied parity checks.In examples (b), (c), and (d), the most probable suspect is the one bit that was flipped.In example (e), two bits have been flipped, s 3 and t 7 .The most probable suspect is r 2 , marked by a circle in (e ), which shows the output of the decoding algorithm.Syndrome z 000 001 010 011 100 101 110 111Unflip this bit none r 7 r 6 r 4 r 5 r 1 r 2 r 3Algorithm 1.16.Actions taken by the optimal decoder for the (7, 4) Hamming code, assuming a binary symmetric channel with small noise level f .The syndrome vector z lists whether each parity check is violated (1) or satisfied (0), going through the checks in the order of the bits r 5 , r 6 , and r 7 .If you try flipping any one of the seven bits, you'll find that a different syndrome is obtained in each case -seven non-zero syndromes, one for each bit.There is only one other syndrome, the all-zero syndrome.So if the channel is a binary symmetric channel with a small noise level f , the optimal decoder unflips at most one bit, depending on the syndrome, as shown in algorithm 1.16.Each syndrome could have been caused by other noise patterns too, but any other noise pattern that has the same syndrome must be less probable because it involves a larger number of noise events.What happens if the noise actually flips more than one bit? Figure 1.15e shows the situation when two bits, r 3 and r 7 , are received flipped.The syndrome, 110, makes us suspect the single bit r 2 ; so our optimal decoding algorithm flips this bit, giving a decoded pattern with three errors as shown in figure 1.15e .If we use the optimal decoding algorithm, any two-bit error pattern will lead to a decoded seven-bit vector that contains three errors.We can also describe the decoding problem for a linear code in terms of matrices.The first four received bits, r 1 r 2 r 3 r 4 , purport to be the four source bits; and the received bits r 5 r 6 r 7 purport to be the parities of the source bits, as defined by the generator matrix G.We evaluate the three parity-check bits for the received bits, r 1 r 2 r 3 r 4 , and see whether they match the three received bits, r 5 r 6 r 7 .The differences (modulo 2) between these two triplets are called the syndrome of the received vector.If the syndrome is zero -if all three parity checks are happy -then the received vector is a codeword, and the most probable decoding is source bits over a binary symmetric channel with f = 10% using a (7, 4) Hamming code.The probability of decoded bit error is about 7%.given by reading out its first four bits.If the syndrome is non-zero, then the noise sequence for this block was non-zero, and the syndrome is our pointer to the most probable error pattern.The computation of the syndrome vector is a linear operation.If we define the 3 × 4 matrix P such that the matrix of equation (1.26) iswhere I 4 is the 4 × 4 identity matrix, then the syndrome vector is z = Hr, where the parity-check matrix H is given by H = −P I 3 ; in modulo 2 arithmetic, −1 ≡ 1, so1 1 1 0 1 0 0 0 1 1 1 0 1 0 1 0 1 1 0 0 1   .(1.30)All the codewords t = G T s of the code satisfy(1.31)Exercise 1.4.[1 ] Prove that this is so by evaluating the 3 × 4 matrix HG T .Since the received vector r is given by r = G T s + n, the syndrome-decoding problem is to find the most probable noise vector n satisfying the equation(1.32)A decoding algorithm that solves this problem is called a maximum-likelihood decoder.We will discuss decoding problems like this in later chapters.Summary of the (7, 4) Hamming code's propertiesEvery possible received vector of length 7 bits is either a codeword, or it's one flip away from a codeword.Since there are three parity constraints, each of which might or might not be violated, there are 2 × 2 × 2 = 8 distinct syndromes.They can be divided into seven non-zero syndromes -one for each of the one-bit error patternsand the all-zero syndrome, corresponding to the zero-noise case.The optimal decoder takes no action if the syndrome is zero, otherwise it uses this mapping of non-zero syndromes onto one-bit error patterns to unflip the suspect bit.There is a decoding error if the four decoded bits ŝ1 , ŝ2 , ŝ3 , ŝ4 do not all match the source bits s 1 , s 2 , s 3 , s 4 .The probability of block error p B is the probability that one or more of the decoded bits in one block fail to match the corresponding source bits, p B = P (ŝ = s).(1.33)The probability of bit error p b is the average probability that a decoded bit fails to match the corresponding source bit,(1.34)In the case of the Hamming code, a decoding error will occur whenever the noise has flipped more than one bit in a block of seven.The probability of block error is thus the probability that two or more bits are flipped in a block.This probability scales as O(f 2 ), as did the probability of error for the repetition code R 3 .But notice that the Hamming code communicates at a greater rate, R = 4/7.Figure 1.17 shows a binary image transmitted over a binary symmetric channel using the (7, 4) Hamming code.About 7% of the decoded bits are in error.Notice that the errors are correlated: often two or three successive decoded bits are flipped.Exercise 1.5. [1 ]This exercise and the next three refer to the (7, 4) Hamming code.p.17] (a) Calculate the probability of block error p B of the (7, 4) Hamming code as a function of the noise level f and show that to leading order it goes as 21f 2 .(b) [3 ] Show that to leading order the probability of bit error p b goes as 9f 2 .p.19] Find some noise vectors that give the all-zero syndrome (that is, noise vectors that leave all the parity checks unviolated).How many such noise vectors are there?Exercise 1.8. [2 ]I asserted above that a block decoding error will result whenever two or more bits are flipped in a single block.Show that this is indeed so.[In principle, there might be error patterns that, after decoding, led only to the corruption of the parity bits, with no source bits incorrectly decoded.]Summary of codes' performances It also shows the performance of a family of linear block codes that are generalizations of Hamming codes, called BCH codes.This figure shows that we can, using linear block codes, achieve better performance than repetition codes; but the asymptotic situation still looks grim.p.19] Design an error-correcting code and a decoding algorithm for it, estimate its probability of error, and add it to figure 1.18.[Don't worry if you find it difficult to make a code better than the Hamming code, or if you find it difficult to find a good decoder for your code; that's the point of this exercise.]p.20]A (7, 4) Hamming code can correct any one error; might there be a (14, 8) code that can correct any two errors?Optional extra: Does the answer to this question depend on whether the code is linear or nonlinear?p.21] Design an error-correcting code, other than a repetition code, that can correct any two errors in a block of size N .There seems to be a trade-off between the decoded bit-error probability p b (which we would like to reduce) and the rate R (which we would like to keep large).How can this trade-off be characterized?What points in the (R, p b ) plane are achievable?This question was addressed by Claude Shannon in his pioneering paper of 1948, in which he both created the field of information theory and solved most of its fundamental problems.At that time there was a widespread belief that the boundary between achievable and nonachievable points in the (R, p b ) plane was a curve passing through the origin (R, p b ) = (0, 0); if this were so, then, in order to achieve a vanishingly small error probability p b , one would have to reduce the rate correspondingly close to zero.'No pain, no gain.'However, Shannon proved the remarkable result that the boundary be- The solid curve shows the Shannon limit on achievable values of (R, p b ) for the binary symmetric channel with f = 0.1.Rates up to R = C are achievable with arbitrarily small p b .The points show the performance of some textbook codes, as in figure 1.18.The equation defining the Shannon limit (the solid curve) is R = C/(1 − H 2 (p b )), where C and H 2 are defined in equation (1.35).binary symmetric channel with noise level f isthe channel we were discussing earlier with noise level f = 0.1 has capacity C 0.53.Let us consider what this means in terms of noisy disk drives.The repetition code R 3 could communicate over this channel with p b = 0.03 at a rate R = 1/3.Thus we know how to build a single gigabyte disk drive with p b = 0.03 from three noisy gigabyte disk drives.We also know how to make a single gigabyte disk drive with p b 10 −15 from sixty noisy one-gigabyte drives (exercise 1.3, p.8).And now Shannon passes by, notices us juggling with disk drives and codes and says:'What performance are you trying to achieve? 10 −15 ?You don't need sixty disk drives -you can get that performance with just two disk drives (since 1/2 is less than 0.53).And if you want p b = 10 −18 or 10 −24 or anything, you can get there with two disk drives too!'[Strictly, the above statements might not be quite right, since, as we shall see, Shannon proved his noisy-channel coding theorem by studying sequences of block codes with ever-increasing blocklengths, and the required blocklength might be bigger than a gigabyte (the size of our disk drive), in which case, Shannon might say 'well, you can't do it with those tiny disk drives, but if you had two noisy terabyte drives, you could make a single high-quality terabyte drive from them'.]The (7, 4) Hamming CodeBy including three parity-check bits in a block of 7 bits it is possible to detect and correct any single bit error in each block.Information can be communicated over a noisy channel at a non-zero rate with arbitrarily small error probability. 1 -Introduction to Information TheoryInformation theory addresses both the limitations and the possibilities of communication.The noisy-channel coding theorem, which we will prove in Chapter 10, asserts both that reliable communication at any rate beyond the capacity is impossible, and that reliable communication at all rates up to capacity is possible.The next few chapters lay the foundations for this result by discussing how to measure information content and the intimately related topic of data compression.p.21] Consider the repetition code R 9 .One way of viewing this code is as a concatenation of R 3 with R 3 .We first encode the source stream with R 3 , then encode the resulting output with R 3 .We could call this code 'R 2 3 '.This idea motivates an alternative decoding algorithm, in which we decode the bits three at a time using the decoder for R 3 ; then decode the decoded bits from that first decoder using the decoder for R 3 .Evaluate the probability of error for this decoder and compare it with the probability of error for the optimal decoder for R 9 .Do the concatenated encoder and decoder for R 2 3 have advantages over those for R 9 ?1.6 Solutions Solution to exercise 1.2 (p.7).An error is made by R 3 if two or more bits are flipped in a block of three.So the error probability of R 3 is a sum of two terms: the probability that all three bits are flipped, f 3 ; and the probability that exactly two bits are flipped, 3f 2 (1 − f ).[If these expressions are not obvious, see example 1.1 (p.1): the expressions are P (r = 3 | f, N = 3) and P (r = 2 | f, N = 3).](1.36)This probability is dominated for small f by the term 3f 2 .See exercise 2.38 (p.39) for further discussion of this problem.Solution to exercise 1.3 (p.8).The probability of error for the repetition code R N is dominated by the probability that N/2 bits are flipped, which goes (for odd N ) as Notation: N/2 denotes the smallest integer greater than or equal to N/2.(1.37)The term N K can be approximated using the binary entropy function: (1.38) where this approximation introduces an error of order √ N -as shown in equation (1.17).So(1.39)Setting this equal to the required value of 10 −15 we find N 2 log 10 −15 log 4f (1−f ) = 68.This answer is a little out because the approximation we used overestimated A slightly more careful answer (short of explicit computation) goes as follows.Taking the approximation for N K to the next order, we find: .40)This approximation can be proved from an accurate version of Stirling's approximation (1.12), or by considering the binomial distribution with p = 1/2 and notingwhere σ = N/4, from which equation (1.40) follows.The distinction between N/2 and N/2 is not important in this term since N K has a maximum at K = N/2.Then the probability of error (for odd N ) is to leading orderThe equation p b = 10 −15 can be written In equation (1.44), the logarithms can be taken to any base, as long as it's the same base throughout.In equation (1.45),I use base 10. (N − 1)/2 log 10 −15 + logwhich may be solved for N iteratively, the first iteration starting from N1 = 68:( N2 − 1)/2 −15 + 1.7 −0.44 = 29.9⇒ N2 60.9.(1.45)This answer is found to be stable, so N 61 is the blocklength at which p b 10 −15 .Solution to exercise 1.6 (p.13).(a) The probability of block error of the Hamming code is a sum of six terms -the probabilities that 2, 3, 4, 5, 6, or 7 errors occur in one block.(1.46)To leading order, this goes as(1.47) (b) The probability of bit error of the Hamming code is smaller than the probability of block error because a block error rarely corrupts all bits in the decoded block.The leading-order behaviour is found by considering the outcome in the most probable case where the noise vector has weight two.The decoder will erroneously flip a third bit, so that the modified received vector (of length 7) differs in three bits from the transmitted vector.That means, if we average over all seven bits, the probability that a randomly chosen bit is flipped is 3/7 times the block error probability, to leading order.Now, what we really care about is the probability that 1 -Introduction to Information Theory a source bit is flipped.Are parity bits or source bits more likely to be among these three flipped bits, or are all seven bits equally likely to be corrupted when the noise vector has weight two?The Hamming code is in fact completely symmetric in the protection it affords to the seven bits (assuming a binary symmetric channel).[This symmetry can be proved by showing that the role of a parity bit can be exchanged with a source bit and the resulting code is still a (7, 4) Hamming code; see below.]The probability that any one bit ends up corrupted is the same for all seven bits.So the probability of bit error (for the source bits) is simply three sevenths of the probability of block error.(1.48)Symmetry of the Hamming (7, 4) codeTo prove that the (7, 4) code protects all bits equally, we start from the paritycheck matrix(1.49)The symmetry among the seven transmitted bits will be easiest to see if we reorder the seven bits using the permutation (t 1 t 2 t 3 t 4 t 5 t 6 t 7 ) → (t 5 t 2 t 3 t 4 t 1 t 6 t 7 ).Then we can rewrite H thus:Now, if we take any two parity constraints that t satisfies and add them together, we get another parity constraint.For example, row 1 asserts t 5 + t 2 + t 3 + t 1 = even, and row 2 asserts t 2 + t 3 + t 4 + t 6 = even, and the sum of these two constraints iswe can drop the terms 2t 2 and 2t 3 , since they are even whatever t 2 and t 3 are; thus we have derived the parity constraint t 5 + t 1 + t 4 + t 6 = even, which we can if we wish add into the parity-check matrix as a fourth row.[The set of vectors satisfying Ht = 0 will not be changed.]We thus defineThe fourth row is the sum (modulo two) of the top two rows.Notice that the second, third, and fourth rows are all cyclic shifts of the top row.If, having added the fourth redundant constraint, we drop the first constraint, we obtain a new parity-check matrix H ,which still satisfies H t = 0 for all codewords, and which looks just like the starting H in (1.50), except that all the columns have shifted along one 1.6:Solutions to the right, and the rightmost column has reappeared at the left (a cyclic permutation of the columns).This establishes the symmetry among the seven bits.Iterating the above procedure five more times, we can make a total of seven different H matrices for the same original code, each of which assigns each bit to a different role.We may also construct the super-redundant seven-row parity-check matrix for the code,.(1.54)This matrix is 'redundant' in the sense that the space spanned by its rows is only three-dimensional, not seven.This matrix is also a cyclic matrix.Every row is a cyclic permutation of the top row.Cyclic codes: if there is an ordering of the bits t 1 . . .t N such that a linear code has a cyclic parity-check matrix, then the code is called a cyclic code.The codewords of such a code also have cyclic properties: any cyclic permutation of a codeword is a codeword.For example, the Hamming (7, 4) code, with its bits ordered as above, consists of all seven cyclic shifts of the codewords 1110100 and 1011000, and the codewords 0000000 and 1111111.Cyclic codes are a cornerstone of the algebraic approach to error-correcting codes.We won't use them again in this book, however, as they have been superceded by sparse-graph codes (Part VI).Solution to exercise 1.7 (p.13).There are fifteen non-zero noise vectors which give the all-zero syndrome; these are precisely the fifteen non-zero codewords of the Hamming code.Notice that because the Hamming code is linear , the sum of any two codewords is a codeword.Solution to exercise 1.9 (p.14).When answering this question, you will probably find that it is easier to invent new codes than to find optimal decoders for them.There are many ways to design codes, and what follows is just one possible train of thought.We make a linear block code that is similar to the (7, 4) Hamming code, but bigger.Many codes can be conveniently expressed in terms of graphs.In figure 1.13, we introduced a pictorial representation of the (7, 4) Hamming code.If we replace that figure's big circles, each of which shows that the parity of four particular bits is even, by a 'parity-check node' that is connected to the four bits, then we obtain the representation of the (7, 4) Hamming code by a bipartite graph as shown in figure 1.20.The 7 circles are the 7 transmitted bits.The 3 squares are the parity-check nodes (not to be confused with the 3 parity-check bits, which are the three most peripheral circles).The graph is a 'bipartite' graph because its nodes fall into two classes -bits and checks 1 -Introduction to Information Theory -and there are edges only between nodes in different classes.The graph and the code's parity-check matrix (1.30) are simply related to each other: each parity-check node corresponds to a row of H and each bit node corresponds to a column of H; for every 1 in H, there is an edge between the corresponding pair of nodes.Having noticed this connection between linear codes and graphs, one way to invent linear codes is simply to think of a bipartite graph.For example, a pretty bipartite graph can be obtained from a dodecahedron by calling the vertices of the dodecahedron the parity-check nodes, and putting a transmitted bit on each edge in the dodecahedron.This construction defines a parity- check matrix in which every column has weight 2 and every row has weight 3.[The weight of a binary vector is the number of 1s it contains.]This code has N = 30 bits, and it appears to have M apparent = 20 paritycheck constraints.Actually, there are only M = 19 independent constraints; the 20th constraint is redundant (that is, if 19 constraints are satisfied, then the 20th is automatically satisfied); so the number of source bits is K = N − M = 11.The code is a (30, 11) code.It is hard to find a decoding algorithm for this code, but we can estimate its probability of error by finding its lowest-weight codewords.If we flip all the bits surrounding one face of the original dodecahedron, then all the parity checks will be satisfied; so the code has 12 codewords of weight 5, one for each face.Since the lowest-weight codewords have weight 5, we say that the code has distance d = 5; the (7, 4) Hamming code had distance 3 and could correct all single bit-flip errors.A code with distance 5 can correct all double bit-flip errors, but there are some triple bit-flip errors that it cannot correct.So the error probability of this code, assuming a binary symmetric channel, will be dominated, at least for low noise levels f , by a term of order f 3 , perhaps something likeOf course, there is no obligation to make codes whose graphs can be represented on a plane, as this one can; the best linear codes, which have simple graphical descriptions, have graphs that are more tangled, as illustrated by the tiny (16, 4) code of figure 1.22.Furthermore, there is no reason for sticking to linear codes; indeed some nonlinear codes -codes whose codewords cannot be defined by a linear equation like Ht = 0 -have very good properties.But the encoding and decoding of a nonlinear code are even trickier tasks.Solution to exercise 1.10 (p.14).First let's assume we are making a linear code and decoding it with syndrome decoding.If there are N transmitted bits, then the number of possible error patterns of weight up to two is(1.56)For N = 14, that's 91 + 14 + 1 = 106 patterns.Now, every distinguishable error pattern must give rise to a distinct syndrome; and the syndrome is a list of M bits, so the maximum possible number of syndromes is 2 M .For a (14, 8) code, M = 6, so there are at most 2 6 = 64 syndromes.The number of possible error patterns of weight up to two, 106, is bigger than the number of syndromes, 64, so we can immediately rule out the possibility that there is a (14, 8) code that is 2-error-correcting.The same counting argument works fine for nonlinear codes too.When the decoder receives r = t + n, his aim is to deduce both t and n from r.If it is the case that the sender can select any transmission t from a code of size S t , and the channel can select any noise vector from a set of size S n , and those two selections can be recovered from the received bit string r, which is one of at most 2 N possible strings, then it must be the case that(1.57)So, for a (N, K) two-error-correcting code, whether linear or nonlinear,(1.58)Solution to exercise 1.11 (p.14).There are various strategies for making codes that can correct multiple errors, and I strongly recommend you think out one or two of them for yourself.If your approach uses a linear code, e.g., one with a collection of M parity checks, it is helpful to bear in mind the counting argument given in the previous exercise, in order to anticipate how many parity checks, M , you might need.Examples of codes that can correct any two errors are the (30, 11) dodecahedron code on page 20, and the (15, 6) pentagonful code to be introduced on p.221.Further simple ideas for making codes that can correct multiple errors from codes that can correct only one error are discussed in section 13.7.Solution to exercise 1.12 (p.16).The probability of error of R 2 3 is, to leading order,whereas the probability of error of R 9 is dominated by the probability of five flips,The R 2 3 decoding procedure is therefore suboptimal, since there are noise vectors of weight four that cause it to make a decoding error.It has the advantage, however, of requiring smaller computational resources: only memorization of three bits, and counting up to three, rather than counting up to nine.This simple code illustrates an important concept.Concatenated codes are widely used in practice because concatenation allows large codes to be implemented using simple encoding and decoding hardware.Some of the best known practical codes are concatenated codes.This chapter, and its sibling, Chapter 8, devote some time to notation.Just as the White Knight distinguished between the song, the name of the song, and what the name of the song was called (Carroll, 1998), we will sometimes need to be careful to distinguish between a random variable, the value of the random variable, and the proposition that asserts that the random variable has a particular value.In any particular chapter, however, I will use the most simple and friendly notation possible, at the risk of upsetting pure-minded readers.For example, if something is 'true with probability 1', I will usually simply say that it is 'true'.An ensemble X is a triple (x, A X , P X ), where the outcome x is the value of a random variable, which takes on one of a set of possible values, A X = {a 1 , a 2 , . . ., a i , . . ., a I }, having probabilities P X = {p 1 , p 2 , . . ., p I }, with P (x = a i ) = p i , p i ≥ 0 and a i ∈A X P (x = a i ) = 1.The name A is mnemonic for 'alphabet'.One example of an ensemble is a letter that is randomly selected from an English document.This ensemble is shown in figure 2.1.There are twenty-seven possible letters: a-z, and a space character '-'.Probability distribution over the 27 outcomes for a randomly selected letter in an English language document (estimated from The Frequently Asked Questions Manual for Linux ).The picture shows the probabilities by the areas of white squares.Abbreviations.Briefer notation will sometimes be used.For example, P (x = a i ) may be written as P (a i ) or P (x).Probability of a subset.If T is a subset of A X then:For example, if we define V to be vowels from figure 2.1, V = {a, e, i, o, u}, then P (V ) = 0.06 + 0.09 + 0.06 + 0.07 + 0.03 = 0.31.(2.2)A joint ensemble XY is an ensemble in which each outcome is an ordered pair x, y with x ∈ A X = {a 1 , . . ., a I } and y ∈ A Y = {b 1 , . . ., b J }.We call P (x, y) the joint probability of x and y.Commas are optional when writing ordered pairs, so xy ⇔ x, y.N.B.In a joint ensemble XY the two variables are not necessarily independent.The probability distribution over the 27×27 possible bigrams xy in an English language document, The Frequently Asked Questions Manual for Linux.Marginal probability.We can obtain the marginal probability P (x) from the joint probability P (x, y) by summation:Similarly, using briefer notation, the marginal probability of y is:(2.4)We pronounce P (x = a i | y = b j ) 'the probability that x equals a i , given y equals b j '.Example 2.1.An example of a joint ensemble is the ordered pair XY consisting of two successive letters in an English document.The possible outcomes are ordered pairs such as aa, ab, ac, and zz; of these, we might expect ab and ac to be more probable than aa and zz.An estimate of the joint probability distribution for two neighbouring characters is shown graphically in figure 2.2.This joint ensemble has the special property that its two marginal distributions, P (x) and P (y), are identical.They are both equal to the monogram distribution shown in figure 2.1.From this joint ensemble P (x, y) we can obtain conditional distributions, P (y | x) and P (x | y), by normalizing the rows and columns, respectively (figure 2.3).The probability P (y | x = q) is the probability distribution of the second letter given that the first letter is a q.As you can see in figure 2.3a, the two most probable values for the second letter y given that the first letter x is q are u and -.(The space is common after q because the source document makes heavy use of the word FAQ.)The probability P (x | y = u) is the probability distribution of the first letter x given that the second letter y is a u.As you can see in figure 2.3b the two most probable values for x given y = u are n and o.Rather than writing down the joint probability directly, we often define an ensemble in terms of a collection of conditional probabilities.The following rules of probability theory will be useful.(H denotes assumptions on which the probabilities are based.)Product rule -obtained from the definition of conditional probability:(2.6)This rule is also known as the chain rule.Sum rule -a rewriting of the marginal probability definition:(2.8)Bayes' theorem -obtained from the product rule:.(2.10) Independence.Two random variables X and Y are independent (sometimes written X⊥Y ) if and only if I said that we often define an ensemble in terms of a collection of conditional probabilities.The following example illustrates this idea.Example 2.3.Jo has a test for a nasty disease.We denote Jo's state of health by the variable a and the test result by b. a = 1 Jo has the disease a = 0 Jo does not have the disease.(2.12)The result of the test is either 'positive' (b = 1) or 'negative' (b = 0); the test is 95% reliable: in 95% of cases of people who really have the disease, a positive result is returned, and in 95% of cases of people who do not have the disease, a negative result is obtained.The final piece of background information is that 1% of people of Jo's age and background have the disease.OK -Jo has the test, and the result is positive.What is the probability that Jo has the disease?Solution.We write down all the provided probabilities.The test reliability specifies the conditional probability of b given a: Jo has received a positive result b = 1 and is interested in how plausible it is that she has the disease (i.e., that a = 1).The man in the street might be duped by the statement 'the test is 95% reliable, so Jo's positive result implies that there is a 95% chance that Jo has the disease', but this is incorrect.The correct solution to an inference problem is found using Bayes' theorem.(2.17) = 0.16.(2.18)So in spite of the positive result, the probability that Jo has the disease is only 16%. 2Probabilities can be used in two ways.Probabilities can describe frequencies of outcomes in random experiments, but giving noncircular definitions of the terms 'frequency' and 'random' is a challenge -what does it mean to say that the frequency of a tossed coin's Box 2.4.The Cox axioms.If a set of beliefs satisfy these axioms then they can be mapped onto probabilities satisfying P (false) = 0, P (true) = 1, 0 ≤ P (x) ≤ 1, and the rules of probability:P (x) = 1 − P (x), and P (x, y) = P (x | y)P (y).Notation.Let 'the degree of belief in proposition x' be denoted by B(x).The negation of x (not-x) is written x.The degree of belief in a conditional proposition, 'x, assuming proposition y to be true', is represented by B(x | y).Axiom 1. Degrees of belief can be ordered; if B(x) is 'greater' than B(y), and B(y) is 'greater' than B(z), then B(x) is 'greater' than B(z).[Consequence: beliefs can be mapped onto real numbers.]Axiom 2. The degree of belief in a proposition x and its negation x are related.There is a function f such thatAxiom 3. The degree of belief in a conjunction of propositions x, y (x and y) is related to the degree of belief in the conditional proposition x | y and the degree of belief in the proposition y.There is a function g such thatcoming up heads is 1/ 2? If we say that this frequency is the average fraction of heads in long sequences, we have to define 'average'; and it is hard to define 'average' without using a word synonymous to probability!I will not attempt to cut this philosophical knot.Probabilities can also be used, more generally, to describe degrees of belief in propositions that do not involve random variables -for example 'the probability that Mr. S. was the murderer of Mrs. S., given the evidence' (he either was or wasn't, and it's the jury's job to assess how probable it is that he was); 'the probability that Thomas Jefferson had a child by one of his slaves'; 'the probability that Shakespeare's plays were written by Francis Bacon'; or, to pick a modern-day example, 'the probability that a particular signature on a particular cheque is genuine'.The man in the street is happy to use probabilities in both these ways, but some books on probability restrict probabilities to refer only to frequencies of outcomes in repeatable random experiments.Nevertheless, degrees of belief can be mapped onto probabilities if they satisfy simple consistency rules known as the Cox axioms (Cox, 1946) (figure 2.4).Thus probabilities can be used to describe assumptions, and to describe inferences given those assumptions.The rules of probability ensure that if two people make the same assumptions and receive the same data then they will draw identical conclusions.This more general use of probability to quantify beliefs is known as the Bayesian viewpoint.It is also known as the subjective interpretation of probability, since the probabilities depend on assumptions.Advocates of a Bayesian approach to data modelling and pattern recognition do not view this subjectivity as a defect, since in their view, you cannot do inference without making assumptions.In this book it will from time to time be taken for granted that a Bayesian approach makes sense, but the reader is warned that this is not yet a globally held view -the field of statistics was dominated for most of the 20th century by non-Bayesian methods in which probabilities are allowed to describe only random variables.The big difference between the two approaches is thatBayesians also use probabilities to describe inferences.Probability calculations often fall into one of two categories: forward probability and inverse probability.Here is an example of a forward probability problem:p.40]An urn contains K balls, of which B are black and W = K − B are white.Fred draws a ball at random from the urn and replaces it, N times.(a) What is the probability distribution of the number of times a black ball is drawn, n B ?(b) What is the expectation of n B ?What is the variance of n B ?What is the standard deviation of n B ? Give numerical answers for the cases N = 5 and N = 400, when B = 2 and K = 10.Forward probability problems involve a generative model that describes a process that is assumed to give rise to some data; the task is to compute the probability distribution or expectation of some quantity that depends on the data.Here is another example of a forward probability problem:p.40]An urn contains K balls, of which B are black and W = K − B are white.We define the fraction f B ≡ B/K.Fred draws N times from the urn, exactly as in exercise 2.4, obtaining n B blacks, and computes the quantityWhat is the expectation of z?In the case N = 5 and f B = 1/5, what is the probability distribution of z? What is the probability that z < 1? [Hint: compare z with the quantities computed in the previous exercise.]Like forward probability problems, inverse probability problems involve a generative model of a process, but instead of computing the probability distribution of some quantity produced by the process, we compute the conditional probability of one or more of the unobserved variables in the process, given the observed variables.This invariably requires the use of Bayes' theorem.Example 2.6.There are eleven urns labelled by u ∈ {0, 1, 2, . . ., 10}, each containing ten balls.Urn u contains u black balls and 10 − u white balls.Fred selects an urn u at random and draws N times with replacement from that urn, obtaining n B blacks and N − n B whites.Fred's friend, Bill, looks on.If after N = 10 draws n B = 3 blacks have been drawn, what is the probability that the urn Fred is using is urn u, from Bill's point of view?(Bill doesn't know the value of u.)Solution.The joint probability distribution of the random variables u and n B can be written(2.20)From the joint probability of u and n B , we can obtain the conditional distribution of u given n B : The marginal probability of u is P (u) = 1 11 for all u.You wrote down the probability of n B given u and N , P (n B | u, N ), when you solved exercise 2.4 (p.27).[You are doing the highly recommended exercises, aren't you?]If we define f u ≡ u/10 thenWhat about the denominator, P (n B | N )?This is the marginal probability of n B , which we can obtain using the sum rule:So the conditional probability of u given n B is This conditional distribution can be found by normalizing column 3 of figure 2.5 and is shown in figure 2.6.The normalizing constant, the marginal probability of n B , is P (n B = 3 | N = 10) = 0.083.The posterior probability (2.26) is correct for all u, including the end-points u = 0 and u = 10, where f u = 0 and f u = 1 respectively.The posterior probability that u = 0 given n B = 3 is equal to zero, because if Fred were drawing from urn 0 it would be impossible for any black balls to be drawn.The posterior probability that u = 10 is also zero, because there are no white balls in that urn.The other hypotheses u = 1, u = 2, . . .u = 9 all have non-zero posterior probability.2In inverse probability problems it is convenient to give names to the probabilities appearing in Bayes' theorem.In equation (2.25), we call the marginal probability P (u) the prior probability of u, andNever say 'the likelihood of the data'.Always say 'the likelihood of the parameters'.The likelihood function is not a probability distribution.(If you want to mention the data that a likelihood function is associated with, you may say 'the likelihood of the parameters given the data'.)The conditional probability P (u | n B , N ) is called the posterior probability of u given n B .The normalizing constant P (n B | N ) has no u-dependence so its value is not important if we simply wish to evaluate the relative probabilities of the alternative hypotheses u.However, in most data-modelling problems of any complexity, this quantity becomes important, and it is given various names: P (n B | N ) is known as the evidence or the marginal likelihood.If θ denotes the unknown parameters, D denotes the data, and H denotes the overall hypothesis space, the general equation:is written:(2.28)Example 2.6 (continued).Assuming again that Bill has observed n B = 3 blacks in N = 10 draws, let Fred draw another ball from the same urn.What is the probability that the next drawn ball is a black?[You should make use of the posterior probabilities in figure 2.6.]Solution.By the sum rule,(2.29) Since the balls are drawn with replacement from the chosen urn, the probability P (ball N+1 is black | u, n B , N ) is just f u = u/10, whatever n B and N are.So(2.30)Using the values of P (u | n B , N ) given in figure 2.6 we obtainComment.Notice the difference between this prediction obtained using probability theory, and the widespread practice in statistics of making predictions by first selecting the most plausible hypothesis (which here would be that the urn is urn u = 3) and then making the predictions assuming that hypothesis to be true (which would give a probability of 0.3 that the next ball is black).The correct prediction is the one that takes into account the uncertainty by marginalizing over the possible values of the hypothesis u.Marginalization here leads to slightly more moderate, less extreme predictions.Now consider the following exercise, which has the character of a simple scientific investigation.Example 2.7.Bill tosses a bent coin N times, obtaining a sequence of heads and tails.We assume that the coin has a probability f H of coming up heads; we do not know f H .If n H heads have occurred in N tosses, what is the probability distribution of f H ? (For example, N might be 10, and n H might be 3; or, after a lot more tossing, we might have N = 300 and n H = 29.)What is the probability that the N +1th outcome will be a head, given n H heads in N tosses?Unlike example 2.6 (p.27), this problem has a subjective element.Given a restricted definition of probability that says 'probabilities are the frequencies of random variables', this example is different from the eleven-urns example.Whereas the urn u was a random variable, the bias f H of the coin would not normally be called a random variable.It is just a fixed but unknown parameter that we are interested in.Yet don't the two examples 2.6 and 2.7 seem to have an essential similarity?[Especially when N = 10 and n H = 3!]To solve example 2.7, we have to make an assumption about what the bias of the coin f H might be.This prior probability distribution over f H , P (f H ),Here P (f ) denotes a probability density, rather than a probability distribution.corresponds to the prior over u in the eleven-urns problem.In that example, the helpful problem definition specified P (u).In real life, we have to make assumptions in order to assign priors; these assumptions will be subjective, and our answers will depend on them.Exactly the same can be said for the other probabilities in our generative model too.We are assuming, for example, that the balls are drawn from an urn independently; but could there not be correlations in the sequence because Fred's ball-drawing action is not perfectly random?Indeed there could be, so the likelihood function that we use depends on assumptions too.In real data modelling problems, priors are subjective and so are likelihoods.We are now using P () to denote probability densities over continuous variables as well as probabilities over discrete variables and probabilities of logical propositions.The probability that a continuous variable v lies between values a and b (where b > a) is defined to be b a dv P (v).P (v)dv is dimensionless.The density P (v) is a dimensional quantity, having dimensions inverse to the dimensions of v -in contrast to discrete probabilities, which are dimensionless.Don't be surprised to see probability densities greater than 1.This is normal, and nothing is wrong, as long as b a dv P (v) ≤ 1 for any interval (a, b).Conditional and joint probability densities are defined in just the same way as conditional and joint probabilities.Exercise 2.8. [2 ]Assuming a uniform prior on f H , P (f H ) = 1, solve the problem posed in example 2.7 (p.30).Sketch the posterior distribution of f H and compute the probability that the N +1th outcome will be a head, for You will find the beta integral useful:You may also find it instructive to look back at example 2.6 (p.27) and equation (2.31).People sometimes confuse assigning a prior distribution to an unknown parameter such as f H with making an initial guess of the value of the parameter.But the prior over f H , P (f H ), is not a simple statement like 'initially, I would guess f H = 1/ 2'.The prior is a probability density over f H which specifies the prior degree of belief that f H lies in any interval (f, f + δf ).It may well be the case that our prior for f H is symmetric about 1/ 2, so that the mean of f H under the prior is 1/ 2. In this case, the predictive distribution for the first toss x 1 would indeed be(2.33)But the prediction for subsequent tosses will depend on the whole prior distribution, not just its mean.Consider the following task.Example 2.9.Write a computer program capable of compressing binary files like this one: 0000000000000000000010010001000000100000010000000000000000000000000000000000001010000000000000110000 1000000000010000100000000010000000000000000000000100000000000000000100000000011000001000000011000100 0000000001001000000000010001000000000000000011000000000000000000000000000010000000000000000100000000The string shown contains n 1 = 29 1s and n 0 = 271 0s.Intuitively, compression works by taking advantage of the predictability of a file.In this case, the source of the file appears more likely to emit 0s than 1s.A data compression program that compresses this file must, implicitly or explicitly, be addressing the question 'What is the probability that the next character in this file is a 1?' Do you think this problem is similar in character to example 2.7 (p.30)?I do.One of the themes of this book is that data compression and data modelling are one and the same, and that they should both be addressed, like the urn of example 2.6, using inverse probability.Example 2.9 is solved in Chapter 6.Please solve the following two exercises.contains three balls: two black, and one white.One of the urns is selected at random and one ball is drawn.The ball is black.What is the probability that the selected urn is urn A?Example 2.11.Urn A contains five balls: one black, two white, one green and ... one pink; urn B contains five hundred balls: two hundred black, one hundred white, 50 yellow, 40 cyan, 30 sienna, 25 green, 25 silver, 20 gold, and 10 purple.[One fifth of A's balls are black; two-fifths of B's are black.]One of the urns is selected at random and one ball is drawn.The ball is black.What is the probability that the urn is urn A? 2 -Probability, Entropy, and Inference What do you notice about your solutions?Does each answer depend on the detailed contents of each urn?The details of the other possible outcomes and their probabilities are irrelevant.All that matters is the probability of the outcome that actually happened (here, that the ball drawn was black) given the different hypotheses.We need only to know the likelihood, i.e., how the probability of the data that happened varies with the hypothesis.This simple rule about inference is known as the likelihood principle.The likelihood principle: given a generative model for data d given parameters θ, P (d | θ), and having observed a particular outcome d 1 , all inferences and predictions should depend only on the functionIn spite of the simplicity of this principle, many classical statistical methods violate it.The Shannon information content of an outcome x is defined to be.(2.34)It is measured in bits.[The word 'bit' is also used to denote a variable whose value is 0 or 1; I hope context will always make clear which of the two meanings is intended.]In the next few chapters, we will establish that the Shannon information content h(a i ) is indeed a natural measure of the information content of the event x = a i .At that point, we will shorten the name of this quantity to 'the information content'.The fourth column in table 2.9 shows the Shannon information content of the 27 possible outcomes when a random character is picked from an English document.The outcome x = z has a Shannon information content of 10.4 bits, and x = e has an information content of 3.5 bits.The entropy of an ensemble X is defined to be the average Shannon information content of an outcome:with the convention for P (x) = 0 that 0 × log 1/0 ≡ 0, since lim θ→0 + θ log 1/θ = 0.Like the information content, entropy is measured in bits.When it is convenient, we may also write H(X) as H(p), where p is the vector (p 1 , p 2 , . . ., p I ).Another name for the entropy of X is the uncertainty of X.Example 2.12.The entropy of a randomly selected letter in an English document is about 4.11 bits, assuming its probability is as given in table 2.9.We obtain this number by averaging log 1/p i (shown in the fourth column) under the probability distribution p i (shown in the third column).We now note some properties of the entropy function.• H(X) ≥ 0 with equality iff p i = 1 for one i.['iff' means 'if and only if'.] • Entropy is maximized if p is uniform:(2.36)denotes the number of elements in A X ; if x is a number, then |x| is the absolute value of x.The redundancy measures the fractional difference between H(X) and its maximum possible value, log(|A X |).The redundancy of X is:(2.37)We won't make use of 'redundancy' in this book, so I have not assigned a symbol to it.The joint entropy of X, Y is:Entropy is additive for independent random variables:H(X, Y ) = H(X) + H(Y ) iff P (x, y) = P (x)P (y).(2.39)Our definitions for information content so far apply only to discrete probability distributions over finite sets A X .The definitions can be extended to infinite sets, though the entropy may then be infinite.The case of a probability density over a continuous set is addressed in section 11.3.Further important definitions and exercises to do with entropy will come along in section 8.1.The entropy function satisfies a recursive property that can be very useful when computing entropies.For convenience, we'll stretch our notation so that we can write H(X) as H(p), where p is the probability vector associated with the ensemble X.Let's illustrate the property by an example first.Imagine that a random variable x ∈ {0, 1, 2} is created by first flipping a fair coin to determine whether x = 0; then, if x is not 0, flipping a fair coin a second time to determine whether x is 1 or 2. The probability distribution of x isWhat is the entropy of X? We can either compute it by brute force:H(X) = 1/ 2 log 2 + 1/ 4 log 4 + 1/ 4 log 4 = 1.5;(2.41)or we can use the following decomposition, in which the value of x is revealed gradually.Imagine first learning whether x = 0, and then, if x is not 0, learning which non-zero value is the case.The revelation of whether x = 0 or not entails revealing a binary variable whose probability distribution is { 1/ 2, 1/ 2}.This revelation has an entropy H( 1/ 2, 1/ 2) = 1 2 log 2 + 1 2 log 2 = 1 bit.If x is not 0, we learn the value of the second coin flip.This too is a binary variable whose probability distribution is { 1/ 2, 1/ 2}, and whose entropy is 1 bit.We only get to experience the second revelation half the time, however, so the entropy can be written:(2.42)Generalizing, the observation we are making about the entropy of any probability distribution p = {p 1 , p 2 , . . ., p I } is that(2.43) When it's written as a formula, this property looks regrettably ugly; nevertheless it is a simple property and one that you should make use of.Generalizing further, the entropy has the property for any m that.(2.44)Example 2.13.A source produces a character x from the alphabet A = {0, 1, . . ., 9, a, b, . . ., z}; with probability 1/ 3, x is a numeral (0, . . ., 9); with probability 1/ 3, x is a vowel (a, e, i, o, u); and with probability 1/ 3 it's one of the 21 consonants.All numerals are equiprobable, and the same goes for vowels and consonants.Estimate the entropy of X.Solution.log 3 + 1 3 (log 10 + log 5 + log 21) = log 3 + 1 3 log 1050 log 30 bits. 2The 'ei' in Leibler is pronounced the same as in heist.The relative entropy or Kullback-Leibler divergence between two probability distributions P (x) and Q(x) that are defined over the same alphabet A X is(2.45)The relative entropy satisfies Gibbs' inequalitywith equality only if P = Q.Note that in general the relative entropy is not symmetric under interchange of the distributions P and Q: in general D KL (P ||Q) = D KL (Q||P ), so D KL , although it is sometimes called the 'KL distance', is not strictly a distance.The relative entropy is important in pattern recognition and neural networks, as well as in information theory.The words 'convex ' and 'concave ' may be pronounced 'convex-smile' and 'concave-frown'.This terminology has useful redundancy: while one may forget which way up 'convex' and 'concave' are, it is harder to confuse a smile with a frown. of the function lies above the function, as shown in figure 2.10; that is, for all x 1 , x 2 ∈ (a, b) and 0 ≤ λ ≤ 1,(2.47)A function f is strictly convex if, for all x 1 , x 2 ∈ (a, b), the equality holds only for λ = 0 and λ = 1.Similar definitions apply to concave and strictly concave functions.Some strictly convex functions are• x 2 , e x and e −x for all x;• log(1/x) and x log x for x > 0.x 2Jensen's inequality.If f is a convex function and x is a random variable then:where E denotes expectation.If f is strictly convex and E [f (x)] = f (E[x]), then the random variable x is a constant.Jensen's inequality can also be rewritten for a concave function, with the direction of the inequality reversed.A physical version of Jensen's inequality runs as follows.If a collection of masses p i are placed on a convex curve f (x) at locations (x i , f (x i )), then the centre of gravity of those masses, which is at (E[x], E [f (x)]), lies above the curve.If this fails to convince you, then feel free to do the following exercise.p.41] Prove Jensen's inequality.Example 2.15.Three squares have average area Ā = 100 m 2 .The average of the lengths of their sides is l = 10 m.What can be said about the size of the largest of the three squares?[Use Jensen's inequality.]Solution.Let x be the length of the side of a square, and let the probability of x be 1/ 3, 1/ 3, 1/ 3 over the three lengths l 1 , l 2 , l 3 .Then the information that we have is that E [x] = 10 and E [f (x)] = 100, where f (x) = x 2 is the function mapping lengths to areas.This is a strictly convex function.We notice that the equality E [f (x)] = f (E[x]) holds, therefore x is a constant, and the three lengths must all be equal.The area of the largest square is 100 m 2 . 2If f (x) is concave and there exists a point at which ∂f ∂x k = 0 for all k, (2.49) then f (x) has its maximum value at that point.The converse does not hold: if a concave f (x) is maximized at some x it is not necessarily true that the gradient ∇f (x) is equal to zero there.For example, f (x) = −|x| is maximized at x = 0 where its derivative is undefined; and f (p) = log(p), for a probability p ∈ (0, 1), is maximized on the boundary of the range, at p = 1, where the gradient df (p)/dp = 1.p.41 ability distribution of the sum of the values?Sketch the probability distribution and estimate its mean and standard deviation.(c) How can two cubical dice be labelled using the numbers {0, 1, 2, 3, 4, 5, 6} so that when the two dice are thrown the sum has a uniform probability distribution over the integers 1-12?(d) Is there any way that one hundred dice could be labelled with integers such that the probability distribution of the sum is uniform?p.41]If q = 1 − p and a = ln p/q, show that.(2.50) Sketch this function and find its relationship to the hyperbolic tangent function tanh(u) = e u −e −u e u +e −u .It will be useful to be fluent in base-2 logarithms also.If b = log 2 p/q, what is p as a function of b?p.42] Let x and y be dependent random variables with x a binary variable taking values in A X = {0, 1}.Use Bayes' theorem to show that the log posterior probability ratio for x given y is logp.42] Let x, d 1 and d 2 be random variables such that d 1 and d 2 are conditionally independent given a binary variable x.Use Bayes' theorem to show that the posterior probability ratio for x given {d i } is(2.52)Life in high-dimensional spacesProbability distributions and volumes have some unexpected properties in high-dimensional spaces.p.42] Consider a sphere of radius r in an N -dimensional real space.Show that the fraction of the volume of the sphere that is in the surface shell lying at values of the radius between r − and r, where 0 < < r, is:(2.53)Evaluate f for the cases N = 2, N = 10 and N = 1000, with (a) /r = 0.01; (b) /r = 0.5.Implication: points that are uniformly distributed in a sphere in N dimensions, where N is large, are very likely to be in a thin shell near the surface.You are probably familiar with the idea of computing the expectation of a function of x,(2.54) Maybe you are not so comfortable with computing this expectation in cases where the function f (x) depends on the probability P (x).The next few examples address this concern.p.43] Let p a = 0.1, p b = 0.2, and p c = 0.7.Let f (a) = 10, f (b) = 5, andp.43]For an arbitrary ensemble, what is E [1/P (x)]?p.43] Let p a = 0.1, p b = 0.2, and p c = 0.7.Let g(a) = 0, g(b) = 1, and g(c) = 0. What is E [g(x)]?p.43] Let p a = 0.1, p b = 0.2, and p c = 0.7.What is the probability that P (x) ∈ [0.15, 0.5]?What isp.44] Prove that the relative entropy (equation (2.45)) satisfies D KL (P ||Q) ≥ 0 (Gibbs' inequality) with equality only if P = Q.Exercise 2.27. [2 ]Prove that the entropy is indeed decomposable as described in equations (2.43-2.44). 2 -Probability, Entropy, and Inferencep.45]A random variable x ∈ {0, 1, 2, 3} is selected by flipping a bent coin with bias f to determine whether the outcome is in {0, 1} or {2, 3}; then either flipping a second bent coin with bias g or a third bentcoin with bias h respectively.Write down the probability distribution of x.Use the decomposability of the entropy (2.44) to find the entropy of X. [Notice how compact an expression is obtained if you make use of the binary entropy function H 2 (x), compared with writing out the four-term entropy explicitly.]Find the derivative of H(X) with respect to f .[Hint: dH 2 (x)/dx = log((1 − x)/x).]p.45]An unbiased coin is flipped until one head is thrown.What is the entropy of the random variable x ∈ {1, 2, 3, . ..}, the number of flips?Repeat the calculation for the case of a biased coin with probability f of coming up heads.[Hint: solve the problem both directly and by using the decomposability of the entropy (2.43).]2.9 Further exercisesExercise 2.30. [1 ]An urn contains w white balls and b black balls.Two balls are drawn, one after the other, without replacement.Prove that the probability that the first ball is white is equal to the probability that the second is white.Exercise 2.31. [2 ]A circular coin of diameter a is thrown onto a square grid whose squares are b × b. (a < b) What is the probability that the coin will lie entirely within one square?[Ans:Exercise 2.32. [3 ]Buffon's needle.A needle of length a is thrown onto a plane covered with equally spaced parallel lines with separation b.What is the probability that the needle will cross a line?[Ans, if a < b: 2a/ πb] [Generalization -Buffon's noodle: on average, a random curve of length A is expected to intersect the lines 2A/ πb times.]Exercise 2.33. [2 ]Two points are selected at random on a straight line segment of length 1.What is the probability that a triangle can be constructed out of the three resulting segments?p.45]An unbiased coin is flipped until one head is thrown.What is the expected number of tails and the expected number of heads?Fred, who doesn't know that the coin is unbiased, estimates the bias using f ≡ h/(h + t), where h and t are the numbers of heads and tails tossed.Compute and sketch the probability distribution of f.N.B., this is a forward probability problem, a sampling theory problem, not an inference problem.Don't use Bayes' theorem.p.45] Fred rolls an unbiased six-sided die once per second, noting the occasions when the outcome is a six.(Exercise 2.36. [2 ]You meet Fred.Fred tells you he has two brothers, Alf and Bob.What is the probability that Fred is older than Bob?Fred tells you that he is older than Alf.Now, what is the probability that Fred is older than Bob? (That is, what is the conditional probability that F > B given that F > A?)Exercise 2.37. [2 ]The inhabitants of an island tell the truth one third of the time.They lie with probability 2/3.On an occasion, after one of them made a statement, you ask another 'was that statement true?' and he says 'yes'.What is the probability that the statement was indeed true?p.46] Compare two ways of computing the probability of error of the repetition code R 3 , assuming a binary symmetric channel (you did this once for exercise 1.2 (p.7)) and confirm that they give the same answer.Binomial distribution method.Add the probability that all three bits are flipped to the probability that exactly two bits are flipped.Sum rule method.Using the sum rule, compute the marginal probability that r takes on each of the eight possible values, P (r).[P (r) = s P (s)P (r | s).]Then compute the posterior probability of s for each of the eight values of r. [In fact, by symmetry, only two example cases r = (000) and r = (001) need be considered.]Notice that some of the inferred bits are better determined Equation (1.18) gives the posterior probability of the input s, given the received vector r.than others.From the posterior probability P (s | r) you can read out the case-by-case error probability, the probability that the more probable hypothesis is not correct, P (error | r).Find the average error probability using the sum rule, P (error) = r P (r)P (error | r).(2.55)3C, p.46]The frequency p n of the nth most frequent word in English is roughly approximated by Solution to exercise 2.4 (p.27).We define the fraction f B ≡ B/K.(a) The number of black balls has a binomial distribution.The mean and variance of this distribution are:(2.59)These results were derived in example 1.1 (p.1).The standard deviation of n B is varWhen B/K = 1/5 and N = 5, the expectation and variance of n B are 1 and 4/5.The standard deviation is 0.89.When B/K = 1/5 and N = 400, the expectation and variance of n B are 80 and 64.The standard deviation is 8.Solution to exercise 2.5 (p.27).The numerator of the quantitycan be recognized as (n B − E[n B ]) 2 ; the denominator is equal to the variance of n B (2.59), which is by definition the expectation of the numerator.So the expectation of z is 1. [A random variable like z, which measures the deviation of data from the expected value, is sometimes called χ 2 (chi-squared).]In the case N = 5 and f B = 1/5, N f B is 1, and var[n B ] is 4/5.The numerator has five possible values, only one of which is smaller than 1: (n B − f B N ) 2 = 0 has probability P (n B = 1) = 0.4096; so the probability that z < 1 is 0.4096.Solution to exercise 2.14 (p.35).We wish to prove, given the property(2.61)We proceed by recursion, working from the right-hand side.(This proof does not handle cases where some p i = 0; such details are left to the pedantic reader.)At the first line we use the definition of convexity (2.60) with λ = , and by the central-limit theorem the probability distribution is roughly Gaussian (but confined to the integers), with this mean and variance.(c) In order to obtain a sum that has a uniform distribution we have to start from random variables some of which have a spiky distribution with the probability mass concentrated at the extremes.The unique solution is to have one ordinary die and one with faces 6, 6, 6, 0, 0, 0.(d) Yes, a uniform distribution can be created in several ways, for exampleTo think about: does this uniform distribution contradict the central-limit theorem?by labelling the rth die with the numbers {0, 1, 2, 3, 4, 5} × 6 r .Solution to exercise 2.17 (p.36).a = ln p q ⇒ p q = e a (2.63) and q = 1 − p gives.(2.65)The hyperbolic tangent is tanh(a) = e a − e −a e a + e −a (2.66) 2 -Probability, Entropy, and Inference so2 − e −a/2 e a/2 + e −a/2 + 1 = 1 2 (tanh(a/2) + 1).(2.67)In the case b = log 2 p/q, we can repeat steps (2.63-2.65),replacing e by 2, to obtain(2.68)Solution to exercise 2.18 (p.36).This gives a separation of the posterior probability ratio into a series of factors, one for each data point, times the prior probability ratio.Life in high-dimensional spaces Solution to exercise 2.20 (p.37).The volume of a hypersphere of radius r in N dimensions is in factbut you don't need to know this.For this question all that we need is the r-dependence, V (r, N ) ∝ r N .So the fractional volume in (r − , r) is(2.76)The fractional volumes in the shells for the required cases are: Notice that no matter how small is, for large enough N essentially all the probability mass is in the surface shell of thickness .Solution to exercise 2.21 (p.37).p a = 0.1, p b = 0.2, p c = 0.7.f (a) = 10, f (b) = 5, and f (c) = 10/7.E [f (x)] = 0.1 × 10 + 0.2 × 5 + 0.7 × 10/7 = 3.(2.77)For each x, f (x) = 1/P (x), soSolution to exercise 2.22 (p.37).For general X, Solution to exercise 2. 25 (p.37).This type of question can be approached in two ways: either by differentiating the function to be maximized, finding the maximum, and proving it is a global maximum; this strategy is somewhat risky since it is possible for the maximum of a function to be at the boundary of the space, at a place where the derivative is not zero.Alternatively, a carefully chosen inequality can establish the answer.The second method is much neater.Proof by differentiation (not the recommended method).Since it is slightly easier to differentiate ln 1/p than log 2 1/p, we temporarily define H(X) to be measured using natural logarithms, thus scaling it down by a factor of log 2 e.we maximize subject to the constraint i p i = 1 which can be enforced with a Lagrange multiplier:(2.88) so all the p i are equal.That this extremum is indeed a maximum is established by finding the curvature:which is negative definite.Proof using Jensen's inequality (recommended method).First a reminder of the inequality.If f is a convex function and x is a random variable then:), then the random variable x is a constant (with probability 1).The secret of a proof using Jensen's inequality is to choose the right function and the right random variable.We could define(which is a convex function) and think of H(X) = p i log 1 p i as the mean of f (u) where u = P (x), but this would not get us there -it would give us an inequality in the wrong direction.If instead we define u = 1/P (x)(2.91) then we find:(2.92) now we know from exercise 2.22 (p.37 (2.94)We prove Gibbs' inequality using Jensen's inequality.Let f (u) = log 1/u and u = Q(x) P (x) .ThenSecond solution.In the above proof the expectations were with respect to the probability distribution P (x).A second solution method uses Jensen's inequality with Q(x) instead.We define f (u) = u log u and let u = P (x) Q(x) .ThenSolution to exercise 2. 28 (p.38).(2.99)Solution to exercise 2.29 (p.38).The probability that there are x − 1 tails and then one head (so we get the first head on the xth toss) isIf the first toss is a tail, the probability distribution for the future looks just like it did before we made the first toss.Thus we have a recursive expression for the entropy:Solution to exercise 2. 34 (p.38).The probability of the number of tails t isThe expected number of heads is 1, by definition of the problem.The expected number of tails iswhich may be shown to be 1 in a variety of ways.For example, since the situation after one tail is thrown is equivalent to the opening situation, we can write down the recurrence relation(2.105)The probability distribution of the 'estimator' f = 1/(1 + t), given that f = 1/2, is plotted in figure 2.12.The probability of f is simply the probability of the corresponding value of t.Solution to exercise 2. 35 (p.38).(a) The mean number of rolls from one six to the next six is six (assuming we start counting rolls after the first of the two sixes).The probability that the next six occurs on the rth roll is the probability of not getting a six for r − 1 rolls multiplied by the probability of then getting a six:This probability distribution of the number of rolls, r, may be called an exponential distribution, sinceand the probability distribution (dashed line) of the number of rolls from the 6 before 1pm to the next 6, r tot ,The probability P (r 1 > 6) is about 1/3; the probability P (r tot > 6) is about 2/3.The mean of r 1 is 6, and the mean of r tot is 11.Solution to exercise 2.38 (p.39).Binomial distribution method.From the solution to exercise 1.2,Sum rule method.The marginal probabilities of the eight values of r are illustrated by:(2.109)The posterior probabilities are represented by(2.110) and(2.111)The probabilities of error in these representative cases are thusNotice that while the average probability of error of R 3 is about 3f 2 , the probability (given r) that any particular bit is wrong is either about f 3 or f .The average error probability, using the sum rule, isThe first two terms are for the cases r = 000 and 111; the remaining 6 are for the other outcomes, which share the same probability of occurring and identical error probability, f .Solution to exercise 2.39 (p.40).The entropy is 9.7 bits per word.If you are eager to get on to information theory, data compression, and noisy channels, you can skip to Chapter 4. Data compression and data modelling are intimately connected, however, so you'll probably want to come back to this chapter by the time you get to Chapter 6.Before reading Chapter 3, it might be good to look at the following exercises.p.59]A die is selected at random from two twenty-faced dice on which the symbols 1-10 are written with nonuniform frequency as follows.Symbol 1 2 3 4 5 6 7 8 9 10Number of faces of die A 6 4 3 2 1 1 1 1 1 0 Number of faces of die B 3 3 2 2 2 2 2 2 1 1The randomly chosen die is rolled 7 times, with the following outcomes: 5, 3, 9, 3, 8, 4, 7.What is the probability that the die is die A?p.59] Assume that there is a third twenty-faced die, die C, on which the symbols 1-20 are written once each.As above, one of the three dice is selected at random and rolled 7 times, giving the outcomes: 3, 5, 4, 8, 3, 9, 7. What is the probability that the die is (a) die A, (b) die B, (c) die C?p.48] Inferring a decay constant Unstable particles are emitted from a source and decay at a distance x, a real number that has an exponential probability distribution with characteristic length λ.Decay events can be observed only if they occur in a window extending from x = 1 cm to x = 20 cm.N decays are observed at locations {x 1 , . . ., x N }.What is λ?p.55] Forensic evidence Two people have left traces of their own blood at the scene of a crime.A suspect, Oliver, is tested and found to have type 'O' blood.The blood groups of the two traces are found to be of type 'O' (a common type in the local population, having frequency 60%) and of type 'AB' (a rare type, with frequency 1%).Do these data (type 'O' and 'AB' blood were found at scene) give evidence in favour of the proposition that Oliver was one of the two people present at the crime?3It is not a controversial statement that Bayes' theorem provides the correct language for describing the inference of a message communicated over a noisy channel, as we used it in Chapter 1 (p.6).But strangely, when it comes to other inference problems, the use of Bayes' theorem is not so widespread.When I was an undergraduate in Cambridge, I was privileged to receive supervisions from Steve Gull.Sitting at his desk in a dishevelled office in St. John's College, I asked him how one ought to answer an old Tripos question (exercise 3.3):Unstable particles are emitted from a source and decay at a distance x, a real number that has an exponential probability distribution with characteristic length λ.Decay events can be observed only if they occur in a window extending from x = 1 cm to x = 20 cm.N decays are observed at locations {x 1 , . . ., x N }.What is λ? * * * * * * * * * x I had scratched my head over this for some time.My education had provided me with a couple of approaches to solving such inference problems: constructing 'estimators' of the unknown parameters; or 'fitting' the model to the data, or to a processed version of the data.Since the mean of an unconstrained exponential distribution is λ, it seemed reasonable to examine the sample mean x = n x n /N and see if an estimator λ could be obtained from it.It was evident that the estimator λ = x−1 would be appropriate for λ 20 cm, but not for cases where the truncation of the distribution at the right-hand side is significant; with a little ingenuity and the introduction of ad hoc bins, promising estimators for λ 20 cm could be constructed.But there was no obvious estimator that would work under all conditions.Nor could I find a satisfactory approach based on fitting the density P (x | λ) to a histogram derived from the data.I was stuck.What is the general solution to this problem and others like it?Is it always necessary, when confronted by a new inference problem, to grope in the dark for appropriate 'estimators' and worry about finding the 'best' estimator (whatever that means)?Steve wrote down the probability of one data point, given λ:whereThis seemed obvious enough.Then he wrote Bayes' theorem:Suddenly, the straightforward distribution P ({x 1 , . . ., x N } | λ), defining the probability of the data given the hypothesis λ, was being turned on its head so as to define the probability of a hypothesis given the data.A simple figure showed the probability of a single data point P (x | λ) as a familiar function of x, for different values of λ (figure 3.1).Each curve was an innocent exponential, normalized to have area 1. Plotting the same function as a function of λ for a fixed value of x, something remarkable happens: a peak emerges (figure 3.2).To help understand these two points of view of the one function, figure 3.3 shows a surface plot of P (x | λ) as a function of x and λ.x For a dataset consisting of several points, e.g., the six points {x} N n=1 = {1.5, 2, 3, 4, 5, 12}, the likelihood function P ({x} | λ) is the product of the N functions of λ, P (x n | λ) (figure 3.4).Probabilities are used here to quantify degrees of belief.To nip possible confusion in the bud, it must be emphasized that the hypothesis λ that correctly describes the situation is not a stochastic variable, and the fact that the Bayesian uses a probability distribution P does not mean that he thinks of the world as stochastically changing its nature between the states described by the different hypotheses.He uses the notation of probabilities to represent his beliefs about the mutually exclusive micro-hypotheses (here, values of λ), of which only one is actually true.That probabilities can denote degrees of belief, given assumptions, seemed reasonable to me.The posterior probability distribution (3.4) represents the unique and complete solution to the problem.There is no need to invent 'estimators'; nor do we need to invent criteria for comparing alternative estimators with each other.Whereas orthodox statisticians offer twenty ways of solving a problem, and another twenty different criteria for deciding which of these solutions is the best, Bayesian statistics only offers one answer to a well-posed problem.If you have any difficulty understanding this chapter I recommend ensuring you are happy with exercises 3.1 and 3.2 (p.47) then noting their similarity to exercise 3.3.Our inference is conditional on our assumptions [for example, the prior P (λ)].Critics view such priors as a difficulty because they are 'subjective', but I don't see how it could be otherwise.How can one perform inference without making assumptions?I believe that it is of great value that Bayesian methods force one to make these tacit assumptions explicit.First, once assumptions are made, the inferences are objective and unique, reproducible with complete agreement by anyone who has the same information and makes the same assumptions.For example, given the assumptions listed above, H, and the data D, everyone will agree about the posterior probability of the decay length λ:Second, when the assumptions are explicit, they are easier to criticize, and easier to modify -indeed, we can quantify the sensitivity of our inferences to the details of the assumptions.For example, we can note from the likelihood curves in figure 3.2 that in the case of a single data point at x = 5, the likelihood function is less strongly peaked than in the case x = 3; the details of the prior P (λ) become increasingly important as the sample mean x gets closer to the middle of the window, 10.5.In the case x = 12, the likelihood function doesn't have a peak at all -such data merely rule out small values of λ, and don't give any information about the relative probabilities of large values of λ.So in this case, the details of the prior at the small-λ end of things are not important, but at the large-λ end, the prior is important.Third, when we are not sure which of various alternative assumptions is the most appropriate for a problem, we can treat this question as another inference task.Thus, given data D, we can compare alternative assumptions H using Bayes' theorem:where I denotes the highest assumptions, which we are not questioning.Fourth, we can take into account our uncertainty regarding such assumptions when we make subsequent predictions.Rather than choosing one particular assumption H * , and working out our predictions about some quantity t, P (t | D, H * , I), we obtain predictions that take into account our uncertainty about H by using the sum rule:This is another contrast with orthodox statistics, in which it is conventional to 'test' a default model, and then, if the test 'accepts the model' at some 'significance level', to use exclusively that model to make predictions.Steve thus persuaded me that probability theory reaches parts that ad hoc methods cannot reach.Let's look at a few more examples of simple inference problems.A bent coin is tossed F times; we observe a sequence s of heads and tails (which we'll denote by the symbols a and b).We wish to know the bias of the coin, and predict the probability that the next toss will result in a head.We first encountered this task in example 2.7 (p.30), and we will encounter it again in Chapter 6, when we discuss adaptive data compression.It is also the original inference problem studied by Thomas Bayes in his essay published in 1763.As in exercise 2.8 (p.30), we will assume a uniform prior distribution and obtain a posterior distribution by multiplying by the likelihood.A critic might object, 'where did this prior come from?'I will not claim that the uniform prior is in any way fundamental; indeed we'll give examples of nonuniform priors later.The prior is a subjective assumption.One of the themes of this book is: you can't do inference -or data compression -without making assumptions.We give the name H 1 to our assumptions.[We'll be introducing an alternative set of assumptions in a moment.]The probability, given p a , that F tosses result in a sequence s that contains {F a , F b } counts of the two outcomes isOur first model assumes a uniform prior distribution for p a ,and p b ≡ 1 − p a .Given a string of length F of which [Predictions are always expressed as probabilities.So 'predicting whether the next character is an a' is the same as computing the probability that the next character is an a.] Assuming H 1 to be true, the posterior probability of p a , given a string s of length F that has counts {F a , F b }, is, by Bayes' theorem,The factor P (s | p a , F, H 1 ), which, as a function of p a , is known as the likelihood function, was given in equation (3.8); the prior P (p a | H 1 ) was given in equation (3.9).Our inference of p a is thus:The normalizing constant is given by the beta integral(3.12)p.59] Sketch the posterior probability P (p a | s = aba, F = 3).What is the most probable value of p a (i.e., the value that maximizes the posterior probability density)?What is the mean value of p a under this distribution?Answer the same questions for the posterior probabilityOur prediction about the next toss, the probability that the next toss is an a, is obtained by integrating over p a .This has the effect of taking into account our uncertainty about p a when making predictions.By the sum rule,The probability of an a given p a is simply p a , sowhich is known as Laplace's rule.Imagine that a scientist introduces another theory for our data.He asserts that the source is not really a bent coin but is really a perfectly formed die with one face painted heads ('a') and the other five painted tails ('b').Thus the parameter p a , which in the original model, H 1 , could take any value between 0 and 1, is according to the new hypothesis, H 0 , not a free parameter at all; rather, it is equal to 1/6.[This hypothesis is termed H 0 so that the suffix of each model indicates its number of free parameters.]How can we compare these two models in the light of data?We wish to infer how probable H 1 is relative to H 0 .In order to perform model comparison, we write down Bayes' theorem again, but this time with a different argument on the left-hand side.We wish to know how probable H 1 is given the data.By Bayes' theorem,Similarly, the posterior probability of H 0 isThe normalizing constant in both cases is P (s | F ), which is the total probability of getting the observed data.If H 1 and H 0 are the only models under consideration, this probability is given by the sum rule:To evaluate the posterior probabilities of the hypotheses we need to assign values to the prior probabilities P (H 1 ) and P (H 0 ); in this case, we might set these to 1/2 each.And we need to evaluate the data-dependent terms P (s | F, H 1 ) and P (s | F, H 0 ).We can give names to these quantities.The quantity P (s | F, H 1 ) is a measure of how much the data favour H 1 , and we call it the evidence for model H 1 .We already encountered this quantity in equation ( 3.10) where it appeared as the normalizing constant of the first inference we made -the inference of p a given the data.The evidence for a model is usually the normalizing constant of an earlier Bayesian inference.We evaluated the normalizing constant for model H 1 in (3.12).The evidence for model H 0 is very simple because this model has no parameters to infer.Defining p 0 to be 1/6, we haveThus the posterior probability ratio of model H 1 to model H 0 isSome values of this posterior probability ratio are illustrated in table 3.5.The first five lines illustrate that some outcomes favour one model, and some favour the other.No outcome is completely incompatible with either model.With small amounts of data (six tosses, say) it is typically not the case that one of the two models is overwhelmingly more probable than the other.But with more data, the evidence against H 0 given by any data set with the ratio F a : F b differing from 1: 5 mounts up.You can't predict in advance how much data are needed to be pretty sure which theory is true.It depends what p a is.The simpler model, H 0 , since it has no adjustable parameters, is able to lose out by the biggest margin.The odds may be hundreds to one against it.The more complex model can never lose out by a large margin; there's no data set that is actually unlikely given model   Exercise 3.6. [2 ]Show that after F tosses have taken place, the biggest value that the log evidence ratio logcan have scales linearly with F if H 1 is more probable, but the log evidence in favour of H 0 can grow at most as log F .p.60] Putting your sampling theory hat on, assuming F a has not yet been measured, compute a plausible range that the log evidence ratio might lie in, as a function of F and the true value of p a , and sketch it as a function of F for p a = p 0 = 1/6, p a = 0.25, and p a = 1/2.[Hint: sketch the log evidence as a function of the random variable F a and work out the mean and standard deviation of F a .]Typical behaviour of the evidence Figure 3.6 shows the log evidence ratio as a function of the number of tosses, F , in a number of simulated experiments.In the left-hand experiments, H 0 was true.In the right-hand ones, H 1 was true, and the value of p a was either 0.25 or 0.5.We will discuss model comparison more in a later chapter.3.4: An example of legal evidence 55The following example illustrates that there is more to Bayesian inference than the priors.Two people have left traces of their own blood at the scene of a crime.A suspect, Oliver, is tested and found to have type 'O' blood.The blood groups of the two traces are found to be of type 'O' (a common type in the local population, having frequency 60%) and of type 'AB' (a rare type, with frequency 1%).Do these data (type 'O' and 'AB' blood were found at scene) give evidence in favour of the proposition that Oliver was one of the two people present at the crime?A careless lawyer might claim that the fact that the suspect's blood type was found at the scene is positive evidence for the theory that he was present.But this is not so.Denote the proposition 'the suspect and one unknown person were present' by S. The alternative, S, states 'two unknown people from the population were present'.The prior in this problem is the prior probability ratio between the propositions S and S.This quantity is important to the final verdict and would be based on all other available information in the case.Our task here is just to evaluate the contribution made by the data D, that is, the likelihood ratio, P (D | S, H)/P (D | S, H).In my view, a jury's task should generally be to multiply together carefully evaluated likelihood ratios from each independent piece of admissible evidence with an equally carefully reasoned prior probability.[This view is shared by many statisticians but learned British appeal judges recently disagreed and actually overturned the verdict of a trial because the jurors had been taught to use Bayes' theorem to handle complicated DNA evidence.]The probability of the data given S is the probability that one unknown person drawn from the population has blood type AB:(since given S, we already know that one trace will be of type O).The probability of the data given S is the probability that two unknown people drawn from the population have types O and AB:In these equations H denotes the assumptions that two people were present and left blood there, and that the probability distribution of the blood groups of unknown people in an explanation is the same as the population frequencies.Dividing, we obtain the likelihood ratio:Thus the data in fact provide weak evidence against the supposition that Oliver was present.This result may be found surprising, so let us examine it from various points of view.First consider the case of another suspect, Alberto, who has type AB.Intuitively, the data do provide evidence in favour of the theory S 3 -More about Inference that this suspect was present, relative to the null hypothesis S.And indeed the likelihood ratio in this case is:Now let us change the situation slightly; imagine that 99% of people are of blood type O, and the rest are of type AB.Only these two blood types exist in the population.The data at the scene are the same as before.Consider again how these data influence our beliefs about Oliver, a suspect of type O, and Alberto, a suspect of type AB.Intuitively, we still believe that the presence of the rare AB blood provides positive evidence that Alberto was there.But does the fact that type O blood was detected at the scene favour the hypothesis that Oliver was present?If this were the case, that would mean that regardless of who the suspect is, the data make it more probable they were present; everyone in the population would be under greater suspicion, which would be absurd.The data may be compatible with any suspect of either blood type being present, but if they provide evidence for some theories, they must also provide evidence against other theories.Here is another way of thinking about this: imagine that instead of two people's blood stains there are ten, and that in the entire local population of one hundred, there are ninety type O suspects and ten type AB suspects.Consider a particular type O suspect, Oliver: without any other information, and before the blood test results come in, there is a one in 10 chance that he was at the scene, since we know that 10 out of the 100 suspects were present.We now get the results of blood tests, and find that nine of the ten stains are of type AB, and one of the stains is of type O. Does this make it more likely that Oliver was there?No, there is now only a one in ninety chance that he was there, since we know that only one person present was of type O.Maybe the intuition is aided finally by writing down the formulae for the general case where n O blood stains of individuals of type O are found, and n AB of type AB, a total of N individuals in all, and unknown people come from a large population with fractions p O , p AB .(There may be other blood types too.)The task is to evaluate the likelihood ratio for the two hypotheses: S, 'the type O suspect (Oliver) and N −1 unknown others left N stains'; and S, 'N unknowns left N stains'.The probability of the data under hypothesis S is just the probability of getting n O , n AB individuals of the two types when N individuals are drawn at random from the population:(3.28)In the case of hypothesis S, we need the distribution of the N −1 other individuals:The likelihood ratio is:This is an instructive result.The likelihood ratio, i.e. the contribution of these data to the question of whether Oliver was present, depends simply on a comparison of the frequency of his blood type in the observed data with the background frequency in the population.There is no dependence on the counts of the other types found at the scene, or their frequencies in the population.If there are more type O stains than the average number expected under hypothesis S, then the data give evidence in favour of the presence of Oliver.Conversely, if there are fewer type O stains than the expected number under S, then the data reduce the probability of the hypothesis that he was there.In the special case n O /N = p O , the data contribute no evidence either way, regardless of the fact that the data are compatible with the hypothesis S.p.60]The three doors, normal rules.On a game show, a contestant is told the rules as follows:There are three doors, labelled 1, 2, 3.A single prize has been hidden behind one of them.You get to select one door.Initially your chosen door will not be opened.Instead, the gameshow host will open one of the other two doors, and he will do so in such a way as not to reveal the prize.For example, if you first choose door 1, he will then open one of doors 2 and 3, and it is guaranteed that he will choose which one to open so that the prize will not be revealed.At this point, you will be given a fresh choice of door: you can either stick with your first choice, or you can switch to the other closed door.All the doors will then be opened and you will receive whatever is behind your final choice of door.Imagine that the contestant chooses door 1 first; then the gameshow host opens door 3, revealing nothing behind the door, as promised.Should the contestant (a) stick with door 1, or (b) switch to door 2, or (c) does it make no difference?p.61]The three doors, earthquake scenario.Imagine that the game happens again and just as the gameshow host is about to open one of the doors a violent earthquake rattles the building and one of the three doors flies open.It happens to be door 3, and it happens not to have the prize behind it.The contestant had initially chosen door 1.Repositioning his toupée, the host suggests, 'OK, since you chose door 1 initially, door 3 is a valid door for me to open, according to the rules of the game; I'll let door 3 stay open.Let's carry on as if nothing happened.'Should the contestant stick with door 1, or switch to door 2, or does it make no difference?Assume that the prize was placed randomly, that the gameshow host does not know where it is, and that the door flew open because its latch was broken by the earthquake.[A similar alternative scenario is a gameshow whose confused host forgets the rules, and where the prize is, and opens one of the unchosen doors at random.He opens door 3, and the prize is not revealed.Should the contestant choose what's behind door 1 or door 2? Does the optimal decision for the contestant depend on the contestant's beliefs about whether the gameshow host is confused or not?]Exercise 3.10. [2 ]Another example in which the emphasis is not on priors.You visit a family whose three children are all at the local school.You don't know anything about the sexes of the children.While walking clumsily round the home, you stumble through one of the three unlabelled bedroom doors that you know belong, one each, to the three children, and find that the bedroom contains girlie stuff in sufficient quantities to convince you that the child who lives in that bedroom is a girl.Later, you sneak a look at a letter addressed to the parents, which reads 'From the Headmaster: we are sending this letter to all parents who have male children at the school to inform them about the following boyish matters. . .'.These two sources of evidence establish that at least one of the three children is a girl, and that at least one of the children is a boy.What are the probabilities that there are (a) two girls and one boy; (b) two boys and one girl?p.61] Mrs S is found stabbed in her family garden.Mr S behaves strangely after her death and is considered as a suspect.On investigation of police and social records it is found that Mr S had beaten up his wife on at least nine previous occasions.The prosecution advances this data as evidence in favour of the hypothesis that Mr S is guilty of the murder.'Ah no,' says Mr S's highly paid lawyer, 'statistically, only one in a thousand wife-beaters actually goes on to murder his wife. 1 So the wife-beating is not strong evidence at all.In fact, given the wife-beating evidence alone, it's extremely unlikely that he would be the murderer of his wife -only a 1/1000 chance.You should therefore find him innocent.'Is the lawyer right to imply that the history of wife-beating does not point to Mr S's being the murderer?Or is the lawyer a slimy trickster?If the latter, what is wrong with his argument?[Having received an indignant letter from a lawyer about the preceding paragraph, I'd like to add an extra inference exercise at this point: Does my suggestion that Mr. S.'s lawyer may have been a slimy trickster imply that I believe all lawyers are slimy tricksters?(Answer: No.)]Exercise 3.12. [2 ]A bag contains one counter, known to be either white or black.A white counter is put in, the bag is shaken, and a counter is drawn out, which proves to be white.What is now the chance of drawing a white counter?[Notice that the state of the bag, after the operations, is exactly identical to its state before.]p.62]You move into a new house; the phone is connected, and you're pretty sure that the phone number is 740511, but not as sure as you would like to be.As an experiment, you pick up the phone and dial 740511; you obtain a 'busy' signal.Are you now more sure of your phone number?If so, how much?Exercise 3.14. [1 ]In a game, two coins are tossed.If either of the coins comes up heads, you have won a prize.To claim the prize, you must point to one of your coins that is a head and say 'look, that coin's a head, I've won'.You watch Fred play the game.He tosses the two coins, and hepoints to a coin and says 'look, that coin's a head, I've won'.What is the probability that the other coin is a head?p.63]A statistical statement appeared in The Guardian on Friday January 4, 2002:When spun on edge 250 times, a Belgian one-euro coin came up heads 140 times and tails 110.'It looks very suspicious to me', said Barry Blight, a statistics lecturer at the London School of Economics.'If the coin were unbiased the chance of getting a result as extreme as that would be less than 7%'.But do these data give evidence that the coin is biased rather than fair?[Hint: see equation (3.22).]Solution to exercise 3.1 (p.47).Let the data be D. Assuming equal prior probabilities,Solution to exercise 3.2 (p.47).The probability of the data given each hypothesis is:Posterior probability for the bias p a of a bent coin given two different data sets.Solution to exercise 3.5 (p.52).(a)The most probable value of p a (i.e., the value that maximizes the posterior probability density) is 2/3.The mean value of p a is 3/5.The most probable value of p a (i.e., the value that maximizes the posterior probability density) is 0. The mean value of p a is 1/5.See figure 3.7b.The solid line shows the log evidence if the random variable F a takes on its mean value, F a = p a F .The dotted lines show (approximately) the log evidence if F a is at its 2.5th or 97.5th percentile.(See also figure 3.6, p.54.)Solution to exercise 3.7 (p.54).The curves in figure 3.8 were found by finding the mean and standard deviation of F a , then setting F a to the mean ± two standard deviations to get a 95% plausible range for F a , and computing the three corresponding values of the log evidence ratio.Solution to exercise 3.8 (p.57).Let H i denote the hypothesis that the prize is behind door i.We make the following assumptions: the three hypotheses H 1 , H 2 and H 3 are equiprobable a priori, i.e.,The datum we receive, after choosing door 1, is one of D = 3 and D = 2 (meaning door 3 or 2 is opened, respectively).We assume that these two possible outcomes have the following probabilities.If the prize is behind door 1 then the host has a free choice; in this case we assume that the host selects at random between D = 2 and D = 3. Otherwise the choice of the host is forced and the probabilities are 0 and 1.Now, using Bayes' theorem, we evaluate the posterior probabilities of the hypotheses:(3.39)The denominator P (D = 3) is (1/2) because it is the normalizing constant for this posterior distribution.So(3.40)So the contestant should switch to door 2 in order to have the biggest chance of getting the prize.Many people find this outcome surprising.There are two ways to make it more intuitive.One is to play the game thirty times with a friend and keep track of the frequency with which switching gets the prize.Alternatively, you can perform a thought experiment in which the game is played with a million doors.The rules are now that the contestant chooses one door, then the game show host opens 999,998 doors in such a way as not to reveal the prize, leaving the contestant's selected door and one other door closed.The contestant may now stick or switch.Imagine the contestant confronted by a million doors, of which doors 1 and 234,598 have not been opened, door 1 having been the contestant's initial guess.Where do you think the prize is? Solution to exercise 3.9 (p.57).If door 3 is opened by an earthquake, the inference comes out differently -even though visually the scene looks the same.The nature of the data, and the probability of the data, are both now different.The possible data outcomes are, firstly, that any number of the doors might have opened.We could label the eight possible outcomes d = (0, 0, 0), (0, 0, 1), (0, 1, 0), (1, 0, 0), (0, 1, 1), . . ., (1, 1, 1).Secondly, it might be that the prize is visible after the earthquake has opened one or more doors.So the data D consists of the value of d, and a statement of whether the prize was revealed.It is hard to say what the probabilities of these outcomes are, since they depend on our beliefs about the reliability of the door latches and the properties of earthquakes, but it is possible to extract the desired posterior probability without naming the values of P (d | H i ) for each d.All that matters are the relative values of the quantities P (D | H 1 ), P (D | H 2 ), P (D | H 3 ), for the value of D that actually occurred.[This is the likelihood principle, which we met in section 2.3.]The value of D that actually occurred is 'd = (0, 0, 1), and no prize visible'.First, it is clear that P (D | H 3 ) = 0, since the datum that no prize is visible is incompatible with H 3 .Now, assuming that the contestant selected door 1, how does the probability P (D | H 1 ) compare with P (D | H 2 )?Assuming that earthquakes are not sensitive to decisions of game show contestants, these two quantities have to be equal, by symmetry.We don't know how likely it is that door 3 falls off its hinges, but however likely it is, it's just as likely to do so whether the prize is behind door 1 or door 2. So, if P (D | H 1 ) and P (D | H 2 ) are equal, we obtain:(3.41)The two possible hypotheses are now equally likely.If we assume that the host knows where the prize is and might be acting deceptively, then the answer might be further modified, because we have to view the host's words as part of the data.Confused?It's well worth making sure you understand these two gameshow problems.Don't worry, I slipped up on the second problem, the first time I met it.There is a general rule which helps immensely when you have a confusing probability problem: Always write down the probability of everything.From this joint probability, any desired inference can be mechanically obtained (figure 3.9).Where the prize isWhich doors opened by earthquake Figure 3.9.The probability of everything, for the second three-door problem, assuming an earthquake has just occurred.Here, p 3 is the probability that door 3 alone is opened by an earthquake.Solution to exercise 3.11 (p.58).The statistic quoted by the lawyer indicates the probability that a randomly selected wife-beater will also murder his wife.The probability that the husband was the murderer, given that the wife has been murdered, is a completely different quantity.To deduce the latter, we need to make further assumptions about the probability that the wife is murdered by someone else.If she lives in a neighbourhood with frequent random murders, then this probability is large and the posterior probability that the husband did it (in the absence of other evidence) may not be very large.But in more peaceful regions, it may well be that the most likely person to have murdered you, if you are found murdered, is one of your closest relatives.Let's work out some illustrative numbers with the help of the statistics on page 58.Let m = 1 denote the proposition that a woman has been murdered; h = 1, the proposition that the husband did it; and b = 1, the proposition that he beat her in the year preceding the murder.The statement 'someone else did it' is denoted by h = 0. We need to defineFrom the statistics, we can read out P (h = 1 | m = 1) = 0.28.And if two million women out of 100 million are beaten, then P (b = 1 | h = 0, m = 1) = 0.02.Finally, we need a value for P (b | h = 1, m = 1): if a man murders his wife, how likely is it that this is the first time he laid a finger on her?I expect it's pretty unlikely; so maybeBy Bayes' theorem, then,.9 × .28.9× .28+ .02× .72 95%.(3.42)One way to make obvious the sliminess of the lawyer on p.58 is to construct arguments, with the same logical structure as his, that are clearly wrong.For example, the lawyer could say 'Not only was Mrs. S murdered, she was murdered between 4.02pm and 4.03pm.Statistically, only one in a million wife-beaters actually goes on to murder his wife between 4.02pm and 4.03pm.So the wife-beating is not strong evidence at all.In fact, given the wife-beating evidence alone, it's extremely unlikely that he would murder his wife in this way -only a 1/1,000,000 chance.'Solution to exercise 3.13 (p.58).There are two hypotheses.H 0 : your number is 740511; H 1 : it is another number.The data, D, are 'when I dialed 740511, I got a busy signal'.What is the probability of D, given each hypothesis?If your number is 740511, then we expect a busy signal with certainty:On the other hand, if H 1 is true, then the probability that the number dialled returns a busy signal is smaller than 1, since various other outcomes were also possible (a ringing tone, or a number-unobtainable signal, for example).The value of this probability P (D | H 1 ) will depend on the probability α that a random phone number similar to your own phone number would be a valid phone number, and on the probability β that you get a busy signal when you dial a valid phone number.I estimate from the size of my phone book that Cambridge has about 75 000 valid phone numbers, all of length six digits.The probability that a random six-digit number is valid is therefore about 75 000/10 6 = 0.075.If we exclude numbers beginning with 0, 1, and 9 from the random choice, the probability α is about 75 000/700 000 0.1.If we assume that telephone numbers are clustered then a misremembered number might be more likely to be valid than a randomly chosen number; so the probability, α, that our guessed number would be valid, assuming H 1 is true, might be bigger than 3.6: Solutions 63 0.1.Anyway, α must be somewhere between 0.1 and 1.We can carry forward this uncertainty in the probability and see how much it matters at the end.The probability β that you get a busy signal when you dial a valid phone number is equal to the fraction of phones you think are in use or off-the-hook when you make your tentative call.This fraction varies from town to town and with the time of day.In Cambridge, during the day, I would guess that about 1% of phones are in use.At 4am, maybe 0.1%, or fewer.The probability P (D | H 1 ) is the product of α and β, that is, about 0.1 × 0.01 = 10 −3 .According to our estimates, there's about a one-in-a-thousand chance of getting a busy signal when you dial a random number; or one-in-ahundred, if valid numbers are strongly clustered; or one-in-10 4 , if you dial in the wee hours.How do the data affect your beliefs about your phone number?The posterior probability ratio is the likelihood ratio times the prior probability ratio:The likelihood ratio is about 100-to-1 or 1000-to-1, so the posterior probability ratio is swung by a factor of 100 or 1000 in favour of H 0 .If the prior probability of H 0 was 0.5 then the posterior probability is0.99 or 0.999.(3.44)Solution to exercise 3.15 (p.59).We compare the models H 0 -the coin is fair -and H 1 -the coin is biased, with the prior on its bias set to the uniform distribution P (p|H 1 ) = 1.[The use of a uniform prior seems reasonable to me, since I know that some coins, such as American pennies, have severe biases when spun on edge; so the situations p = 0.01 or p = 0.1 or p = 0.95 would not surprise me.]When I mention H 0 -the coin is fair -a pedant would say, 'how absurd to even consider that the coin is fair -any coin is surely biased to some extent'.And of course I would agree.So will pedants kindly understand H 0 as meaning 'the coin is fair to within one part in a thousand, i.e., p ∈ 0.5 ± 0.001'.The likelihood ratio is:Thus the data give scarcely any evidence either way; in fact they give weak evidence (two to one) in favour of H 0 !'No, no', objects the believer in bias, 'your silly uniform prior doesn't represent my prior beliefs about the bias of biased coins -I was expecting only a small bias'.To be as generous as possible to the H 1 , let's see how well it could fare if the prior were presciently set.Let us allow a prior of the form(a Beta distribution, with the original uniform prior reproduced by setting α = 1).By tweaking α, the likelihood ratio for H 1 over H 0 , Even the most favourable choice of α (α 50) can yield a likelihood ratio of only two to one in favour of H 1 .In conclusion, the data are not 'very suspicious'.They can be construed as giving at most two-to-one evidence in favour of one or other of the two hypotheses.Are these wimpy likelihood ratios the fault of over-restrictive priors?Is there any way of producing a 'very suspicious' conclusion?The prior that is bestmatched to the data, in terms of likelihood, is the prior that sets p to f ≡ 140/250 with probability one.Let's call this model H * .The likelihood ratio is P (D|H * )/P (D|H 0 ) = 2 250 f 140 (1 − f ) 110 = 6.1.So the strongest evidence that these data can possibly muster against the hypothesis that there is no bias is six-to-one.While we are noticing the absurdly misleading answers that 'sampling theory' statistics produces, such as the p-value of 7% in the exercise we just solved, let's stick the boot in.If we make a tiny change to the data set, increasing the number of heads in 250 tosses from 140 to 141, we find that the p-value goes below the mystical value of 0.05 (the p-value is 0.0497).The sampling theory statistician would happily squeak 'the probability of getting a result as extreme as 141 heads is smaller than 0.05 -we thus reject the null hypothesis at a significance level of 5%'.The correct answer is shown for several values of α in figure 3.12.The values worth highlighting from this table are, first, the likelihood ratio when H 1 uses the standard uniform prior, which is 1:0.61 in favour of the null hypothesis H 0 .Second, the most favourable choice of α, from the point of view of H 1 , can only yield a likelihood ratio of about 2.3:1 in favour of H 1 .Be warned!A p-value of 0.05 is often interpreted as implying that the odds are stacked about twenty-to-one against the null hypothesis.But the truth in this case is that the evidence either slightly favours the null hypothesis, or disfavours it by at most 2.3 to one, depending on the choice of prior.The p-values and 'significance levels' of classical statistics should be treated with extreme caution.Shun them!Here ends the sermon.In this chapter we discuss how to measure the information content of the outcome of a random experiment.This chapter has some tough bits.If you find the mathematical details hard, skim through them and keep going -you'll be able to enjoy Chapters 5 and 6 without this chapter's tools.The following exercise is intended to help you think about how to measure information content.p.69] -Please work on this problem before reading Chapter 4.You are given 12 balls, all equal in weight except for one that is either heavier or lighter.You are also given a two-pan balance to use.In each use of the balance you may put any number of the 12 balls on the left pan, and the same number on the right pan, and push a button to initiate the weighing; there are three possible outcomes: either the weights are equal, or the balls on the left are heavier, or the balls on the left are lighter.Your task is to design a strategy to determine which is the odd ball and whether it is heavier or lighter than the others in as few uses of the balance as possible.While thinking about this problem, you may find it helpful to consider the following questions: 4The Source Coding Theorem 4.1 How to measure the information content of a random variable?In the next few chapters, we'll be talking about probability distributions and random variables.Most of the time we can get by with sloppy notation, but occasionally, we will need precise notation.Here is the notation that we established in Chapter 2.An ensemble X is a triple (x, A X , P X ), where the outcome x is the value of a random variable, which takes on one of a set of possible values, A X = {a 1 , a 2 , . . ., a i , . . ., a I }, having probabilitiesHow can we measure the information content of an outcome x = a i from such an ensemble?In this chapter we examine the assertions 1. that the Shannon information content,is a sensible measure of the information content of the outcome x = a i , and 2. that the entropy of the ensemble,is a sensible measure of the ensemble's average information content.which is the entropy of the ensemble X whose alphabet and probability distribution are A X = {a, b}, P X = {p, (1 − p)}. 4 -The Source Coding TheoremWhy should log 1/p i have anything to do with the information content?Why not some other function of p i ?We'll explore this question in detail shortly, but first, notice a nice property of this particular function h(x) = log 1/p(x).Imagine learning the value of two independent random variables, x and y.The definition of independence is that the probability distribution is separable into a product: P (x, y) = P (x)P (y).(4.4)Intuitively, we might want any measure of the 'amount of information gained' to have the property of additivity -that is, for independent random variables x and y, the information gained when we learn x and y should equal the sum of the information gained if x alone were learned and the information gained if y alone were learned.The Shannon information content of the outcome x, y is+ log 1 P (y) (4.5) so it does indeed satisfy h(x, y) = h(x) + h(y), if x and y are independent.(4.6)p.86] Show that, if x and y are independent, the entropy of the outcome x, y satisfiesIn words, entropy is additive for independent variables.We now explore these ideas with some examples; then, in section 4.4 and in Chapters 5 and 6, we prove that the Shannon information content and the entropy are related to the number of bits needed to describe the outcome of an experiment.Have you solved the weighing problem (exercise 4.1, p.66) yet?Are you sure?Notice that in three uses of the balance -which reads either 'left heavier', 'right heavier', or 'balanced' -the number of conceivable outcomes is 3 3 = 27, whereas the number of possible states of the world is 24: the odd ball could be any of twelve balls, and it could be heavy or light.So in principle, the problem might be solvable in three weighings -but not in two, since 3 2 < 24.If you know how you can determine the odd weight and whether it is heavy or light in three weighings, then you may read on.If you haven't found a strategy that always gets there in three weighings, I encourage you to think about exercise 4.1 some more.Why is your strategy optimal?What is it about your series of weighings that allows useful information to be gained as quickly as possible?The answer is that at each step of an optimal procedure, the three outcomes ('left heavier', 'right heavier', and 'balance') are as close as possible to equiprobable.An optimal solution is shown in figure 4.2.Suboptimal strategies, such as weighing balls 1-6 against 7-12 on the first step, do not achieve all outcomes with equal probability: these two sets of balls can never balance, so the only possible outcomes are 'left heavy' and 'right heavy'.Such a binary outcome rules out only half of the possible hypotheses, The 24 hypotheses are written 1 + , . . ., 12 − , with, e.g., 1 + denoting that 1 is the odd ball and it is heavy.Weighings are written by listing the names of the balls on the two pans, separated by a line; for example, in the first weighing, balls 1, 2, 3, and 4 are put on the left-hand side and 5, 6, 7, and 8 on the right.In each triplet of arrows the upper arrow leads to the situation when the left side is heavier, the middle arrow to the situation when the right side is heavier, and the lower arrow to the situation when the outcome is balanced.The three points labelled correspond to impossible outcomes.so a strategy that uses such outcomes must sometimes take longer to find the right answer.The insight that the outcomes should be as near as possible to equiprobable makes it easier to search for an optimal strategy.The first weighing must divide the 24 possible hypotheses into three groups of eight.Then the second weighing must be chosen so that there is a 3:3:2 split of the hypotheses.Thus we might conclude:the outcome of a random experiment is guaranteed to be most informative if the probability distribution over outcomes is uniform.This conclusion agrees with the property of the entropy that you proved when you solved exercise 2.25 (p.37): the entropy of an ensemble X is biggest if all the outcomes have equal probabilityIn the game of twenty questions, one player thinks of an object, and the other player attempts to guess what the object is by asking questions that have yes/no answers, for example, 'is it alive?',or 'is it human?'The aim is to identify the object with as few questions as possible.What is the best strategy for playing this game?For simplicity, imagine that we are playing the rather dull version of twenty questions called 'sixty-three'.Example 4.3.The game 'sixty-three'.What's the smallest number of yes/no questions needed to identify an integer x between 0 and 63?Intuitively, the best questions successively divide the 64 possibilities into equal sized sets.Six questions suffice.One reasonable strategy asks the following questions:[The notation x mod 32, pronounced 'x modulo 32', denotes the remainder when x is divided by 32; for example, 35 mod 32 = 3 and 32 mod 32 = 0.] The answers to these questions, if translated from {yes, no} to {1, 0}, give the binary expansion of x, for example 35 ⇒ 100011.2 What are the Shannon information contents of the outcomes in this example?If we assume that all values of x are equally likely, then the answers to the questions are independent and each has Shannon information content log 2 (1/0.5)= 1 bit; the total Shannon information gained is always six bits.Furthermore, the number x that we learn from these questions is a six-bit binary number.Our questioning strategy defines a way of encoding the random variable x as a binary file.So far, the Shannon information content makes sense: it measures the length of a binary file that encodes x.However, we have not yet studied ensembles where the outcomes have unequal probabilities.Does the Shannon information content make sense there too?4.1: How to measure the information content of a random variable?71 The game of submarine: how many bits can one bit convey?In the game of battleships, each player hides a fleet of ships in a sea represented by a square grid.On each turn, one player attempts to hit the other's ships by firing at one square in the opponent's sea.The response to a selected square such as 'G3' is either 'miss', 'hit', or 'hit and destroyed'.In a boring version of battleships called submarine, each player hides just one submarine in one square of an eight-by-eight grid.Figure 4.3 shows a few pictures of this game in progress: the circle represents the square that is being fired at, and the ×s show squares in which the outcome was a miss, x = n; the submarine is hit (outcome x = y shown by the symbol s) on the 49th attempt.Each shot made by a player defines an ensemble.The two possible outcomes are {y, n}, corresponding to a hit and a miss, and their probabilities depend on the state of the board.At the beginning, P (y) = 1/64 and P (n) = 63/64.At the second shot, if the first shot missed, P (y) = 1/63 and P (n) = 62/63.At the third shot, if the first two shots missed, P (y) = 1/62 and P (n) = 61/62.The Shannon information gained from an outcome x is h(x) = log(1/P (x)).If we are lucky, and hit the submarine on the first shot, then h(x) = h (1) (y) = log 2 64 = 6 bits.(4.8)Now, it might seem a little strange that one binary outcome can convey six bits.But we have learnt the hiding place, which could have been any of 64 squares; so we have, by one lucky binary question, indeed learnt six bits.What if the first shot misses?The Shannon information that we gain from this outcome is We now know that the submarine is not in any of the 32 squares we fired at; learning that fact is just like playing a game of sixty-three (p.70), asking as our first question 'is x one of the thirty-two numbers corresponding to these squares I fired at?', and receiving the answer 'no'.This answer rules out half of the hypotheses, so it gives us one bit.After 48 unsuccessful shots, the information gained is 2 bits: the unknown location has been narrowed down to one quarter of the original hypothesis space.What if we hit the submarine on the 49th shot, when there were 16 squares left?The Shannon information content of this outcome is So once we know where the submarine is, the total Shannon information content gained is 6 bits.This result holds regardless of when we hit the submarine.If we hit it when there are n squares left to choose from -n was 16 in equation (4.13)then the total information gained is:What have we learned from the examples so far?I think the submarine example makes quite a convincing case for the claim that the Shannon information content is a sensible measure of information content.And the game of sixty-three shows that the Shannon information content can be intimately connected to the size of a file that encodes the outcomes of a random experiment, thus suggesting a possible connection to data compression.In case you're not convinced, let's look at one more example.Wenglish is a language similar to English.Wenglish sentences consist of words drawn at random from the Wenglish dictionary, which contains 2 15 = 32,768 words, all of length 5 characters.Each word in the Wenglish dictionary was constructed at random by picking five letters from the probability distribution over a. ..z depicted in figure 2.1.Some entries from the dictionary are shown in alphabetical order in figure 4.4.Notice that the number of words in the dictionary (32,768) is much smaller than the total number of possible words of length 5 letters, 26 5 12,000,000.Because the probability of the letter z is about 1/1000, only 32 of the words in the dictionary begin with the letter z.In contrast, the probability of the letter a is about 0.0625, and 2048 of the words begin with the letter a.Of those 2048 words, two start az, and 128 start aa.Let's imagine that we are reading a Wenglish document, and let's discuss the Shannon information content of the characters as we acquire them.If we are given the text one word at a time, the Shannon information content of each five-character word is log 32,768 = 15 bits, since Wenglish uses all its words with equal probability.The average information content per character is therefore 3 bits.Now let's look at the information content if we read the document one character at a time.If, say, the first letter of a word is a, the Shannon information content is log 1/0.0625 4 bits.If the first letter is z, the Shannon information content is log 1/0.001 10 bits.The information content is thus highly variable at the first character.The total information content of the 5 characters in a word, however, is exactly 15 bits; so the letters that follow an initial z have lower average information content per character than the letters that follow an initial a.A rare initial letter such as z indeed conveys more information about what the word is than a common initial letter.Similarly, in English, if rare characters occur at the start of the word (e.g.xyl...), then often we can identify the whole word immediately; whereas words that start with common characters (e.g.pro...) require more characters before we can identify them.The preceding examples justify the idea that the Shannon information content of an outcome is a natural measure of its information content.Improbable outcomes do convey more information than probable outcomes.We now discuss the information content of a source by considering how many bits are needed to describe the outcome of an experiment.If we can show that we can compress data from a particular source into a file of L bits per source symbol and recover the data reliably, then we will say that the average information content of that source is at most L bits per symbol.A file is composed of a sequence of bytes.A byte is composed of 8 bits and Here we use the word 'bit' with its meaning, 'a symbol with two values', not to be confused with the unit of information content.can have a decimal value between 0 and 255.A typical text file is composed of the ASCII character set (decimal values 0 to 127).This character set uses only seven of the eight bits in a byte.p.86]By how much could the size of a file be reduced given that it is an ASCII file?How would you achieve this reduction?Intuitively, it seems reasonable to assert that an ASCII file contains 7/8 as much information as an arbitrary file of the same size, since we already know one out of every eight bits before we even look at the file.This is a simple example of redundancy.Most sources of data have further redundancy: English text files use the ASCII characters with non-equal frequency; certain pairs of letters are more probable than others; and entire words can be predicted given the context and a semantic understanding of the text.One way of measuring the information content of a random variable is simply to count the number of possible outcomes, |A X |.(The number of elements in a set A is denoted by |A|.)If we gave a binary name to each outcome, the length of each name would be log 2 |A X | bits, if |A X | happened to be a power of 2. We thus make the following definition.The raw bit content of X isH 0 (X) is a lower bound for the number of binary questions that are always guaranteed to identify an outcome from the ensemble X.It is an additive quantity: the raw bit content of an ordered pair x, y, havingThis measure of information content does not include any probabilistic element, and the encoding rule it corresponds to does not 'compress' the source data, it simply maps each outcome to a constant-length binary string.p.86] Could there be a compressor that maps an outcome x to a binary code c(x), and a decompressor that maps c back to x, such that every possible outcome is compressed into a binary code of length shorter than H 0 (X) bits?Even though a simple counting argument shows that it is impossible to make a reversible compression program that reduces the size of all files, amateur compression enthusiasts frequently announce that they have invented a program that can do this -indeed that they can further compress compressed files by putting them through their compressor several times.Stranger yet, patents have been granted to these modern-day alchemists.See the comp.compressionfrequently asked questions for further reading. 1 There are only two ways in which a 'compressor' can actually compress files:1.A lossy compressor compresses some files, but maps some files to the same encoding.We'll assume that the user requires perfect recovery of the source file, so the occurrence of one of these confusable files leads to a failure (though in applications such as image compression, lossy compression is viewed as satisfactory).We'll denote by δ the probability that the source string is one of the confusable files, so a lossy compressor has a probability δ of failure.If δ can be made very small then a lossy compressor may be practically useful.2. A lossless compressor maps all files to different encodings; if it shortens some files, it necessarily makes others longer.We try to design the compressor so that the probability that a file is lengthened is very small, and the probability that it is shortened is large.In this chapter we discuss a simple lossy compressor.In subsequent chapters we discuss lossless compression methods.Whichever type of compressor we construct, we need somehow to take into account the probabilities of the different outcomes.Imagine comparing the information contents of two text files -one in which all 128 ASCII characters are used with equal probability, and one in which the characters are used with their frequencies in English text.Can we define a measure of information content that distinguishes between these two files?Intuitively, the latter file contains less information per character because it is more predictable.One simple way to use our knowledge that some symbols have a smaller probability is to imagine recoding the observations into a smaller alphabet -thus losing the ability to encode some of the more improbable symbolsand then measuring the raw bit content of the new alphabet.For example, we might take a risk when compressing English text, guessing that the most infrequent characters won't occur, and make a reduced ASCII code that omits the characters { !, @, #, %, ^, *, ~, <, >, /, \, _, {, }, [, ], | }, thereby reducing the size of the alphabet by seventeen.The larger the risk we are willing to take, the smaller our final alphabet becomes.We introduce a parameter δ that describes the risk we are taking when using this compression method: δ is the probability that there will be no name for an outcome x.Example 4.6.Let A X = { a, b, c, d, e, f, g, h }, and(4.17)The raw bit content of this ensemble is 3 bits, corresponding to 8 binary names.But notice that P (x ∈ {a, b, c, d}) = 15/16.So if we are willing to run a risk of δ = 1/16 of not having a name for x, then we can get by with four names -half as many names as are needed if every x ∈ A X has a name.Table 4.5 shows binary names that could be given to the different outcomes in the cases δ = 0 and δ = 1/16.When δ = 0 we need 3 bits to encode the outcome; when δ = 1/16 we need only 2 bits.Let us now formalize this idea.To make a compression strategy with risk δ, we make the smallest possible subset S δ such that the probability that x is not in S δ is less than or equal to δ, i.e., P (x ∈ S δ ) ≤ δ.For each value of δ we can then define a new measure of information content -the log of the size of this smallest subset S δ .[In ensembles in which several elements have the same probability, there may be several smallest subsets that contain different elements, but all that matters is their sizes (which are equal), so we will not dwell on this ambiguity.]The smallest δ-sufficient subset S δ is the smallest subset of A X satisfying(4.18)The subset S δ can be constructed by ranking the elements of A X in order of decreasing probability and adding successive elements starting from the most probable elements until the total probability is ≥ (1−δ).We can make a data compression code by assigning a binary name to each element of the smallest sufficient subset.This compression scheme motivates the following measure of information content:The essential bit content of X is:Note that H 0 (X) is the special case of H δ (X) with δ = 0 (if P (x) > 0 for all x ∈ A X ).[Caution: do not confuse H 0 (X) and H δ (X) with the function H 2 (p) displayed in figure 4.1.]Figure 4.6 shows H δ (X) for the ensemble of example 4.6 as a function of δ.Is this compression method any more useful if we compress blocks of symbols from a source?We now turn to examples where the outcome x = (x 1 , x 2 , . . ., x N ) is a string of N independent identically distributed random variables from a single ensemble X.We will denote by X N the ensemble (X 1 , X 2 , . . ., X N ).Remember that entropy is additive for independent variables (exercise 4.2 (p.68)), so H(X N ) = N H(X).Example 4.7.Consider a string of N flips of a bent coin, x = (x 1 , x 2 , . . ., x N ), where x n ∈ {0, 1}, with probabilities p 0 = 0.9, p 1 = 0.1.The most probable strings x are those with most 0s.If r(x) is the number of 1s in x thenTo evaluate H δ (X N ) we must find the smallest sufficient subset S δ .This subset will contain all x with r(x) = 0, 1, 2, . . ., up to some r max (δ) − 1, and some of the x with r(x) = r max (δ).p.86] What are the mathematical shapes of the curves between the cusps?For the examples shown in figures 4.6-4.8,H δ (X N ) depends strongly on the value of δ, so it might not seem a fundamental or useful definition of information content.But we will consider what happens as N , the number of independent variables in X N , increases.We will find the remarkable result that H δ (X N ) becomes almost independent of δ -and for all δ it is very close to N H(X), where H(X) is the entropy of one of the random variables.Figure 4.9 illustrates this asymptotic tendency for the binary ensemble of example 4.7.As N increases, 1N H δ (X N ) becomes an increasingly flat function,   , where p 1 = 0.1 and p 0 = 0.9.The bottom two are the most and least probable strings in this ensemble.The final column shows the log-probabilities of the random strings, which may be compared with the entropy H(X 100 ) = 46.9bits.except for tails close to δ = 0 and 1.As long as we are allowed a tiny probability of error δ, compression down to N H bits is possible.Even if we are allowed a large probability of error, we still can compress only down to N H bits.This is the source coding theorem.Theorem 4.1 Shannon's source coding theorem.Let X be an ensemble with entropy H(X) = H bits. Given > 0 and 0 < δ < 1, there exists a positive integer N 0 such that for N > N 0 ,Why does increasing N help?Let's examine long strings from X N .Table 4.10 shows fifteen samples from X N for N = 100 and p 1 = 0.1.The probability of a string x that contains r 1s and N −r 0s isThe number of strings that contain r 1s isSo the number of 1s, r, has a binomial distribution:These functions are shown in figure 4.11.The mean of r is N p 1 , and its standard deviation is N p 1 (1 − p 1 ) (p.1).If N is 100 then Notice that as N gets bigger, the probability distribution of r becomes more concentrated, in the sense that while the range of possible values of r grows as N , the standard deviation of r grows only as √ N .That r is most likely to fall in a small range of values implies that the outcome x is also most likely to fall in a corresponding small subset of outcomes that we will call the typical set.Let us define typicality for an arbitrary ensemble X with alphabet A X .Our definition of a typical string will involve the string's probability.A long string of N symbols will usually contain about p 1 N occurrences of the first symbol, p 2 N occurrences of the second, etc. Hence the probability of this string is roughlySo the random variable log 2 1/ P (x), which is the information content of x, is very likely to be close in value to N H.We build our definition of typicality on this observation.We define the typical elements of A N X to be those elements that have probability close to 2 −N H . (Note that the typical set, unlike the smallest sufficient subset, does not include the most probable elements of A N X , but we will show that these most probable elements contribute negligible probability.)We introduce a parameter β that defines how close the probability has to be to 2 −N H for an element to be 'typical'.We call the set of typical elements the typical set, T N β :We will show that whatever value of β we choose, the typical set contains almost all the probability as N increases.This important result is sometimes called the 'asymptotic equipartition' principle.'Asymptotic equipartition' principle.For an ensemble of N independent identically distributed (i.i.d.) random variables X N ≡ (X 1 , X 2 , . . ., X N ), with N sufficiently large, the outcomeThe term equipartition is chosen to describe the idea that the members of the typical set have roughly equal probability.[This should not be taken too literally, hence my use of quotes around 'asymptotic equipartition'; see page 83.]A second meaning for equipartition, in thermal physics, is the idea that each degree of freedom of a classical system has equal average energy, 1 2 kT .This second meaning is not intended here.The 'asymptotic equipartition' principle is equivalent to: Shannon's source coding theorem (verbal statement).N i.i.d.random variables each with entropy H(X) can be compressed into more than N H(X) bits with negligible risk of information loss, as N → ∞; conversely if they are compressed into fewer than N H(X) bits it is virtually certain that information will be lost.These two theorems are equivalent because we can define a compression algorithm that gives a distinct name of length N H(X) bits to each x in the typical set.This section may be skipped if found tough going.Our proof of the source coding theorem uses the law of large numbers.Mean and variance of a real random variable areTechnical note: strictly I am assuming here that u is a function u(x) of a sample x from a finite discrete ensemble X.Then the summations u P (u)f (u) should be written x P (x)f (u(x)).This means that P (u) is a finite sum of delta functions.This restriction guarantees that the mean and variance of u do exist, which is not necessarily the case for general P (u).Chebyshev's inequality 1.Let t be a non-negative real random variable, and let α be a positive real number.Then(4.30)Proof: P (t ≥ α) = t≥α P (t).We multiply each term by t/α ≥ 1 and obtain: P (t ≥ α) ≤ t≥α P (t)t/α.We add the (non-negative) missing terms and obtain:Chebyshev's inequality 2. Let x be a random variable, and let α be a positive real number.ThenProof: Take t = (x − x) 2 and apply the previous proposition.2Weak law of large numbers.Take x to be the average of N independent random variables h 1 , . . ., h N , having common mean h and common variance σ 2Proof: obtained by showing that x = h and that σ 2 x = σ 2 h /N . 2 We are interested in x being very close to the mean (α very small).No matter how large σ 2 h is, and no matter how small the required α is, and no matter how small the desired probability that (x − h) 2 ≥ α, we can always achieve it by taking N large enough.We apply the law of large numbers to the random variable 1 N log 2 1 P (x) defined for x drawn from the ensemble X N .This random variable can be written as the average of N information contents h n = log 2 (1/P (x n )), each of which is a random variable with mean H = H(X) and variance(Each term h n is the Shannon information content of the nth outcome.)We again define the typical set with parameters N and β thus:For all x ∈ T N β , the probability of x satisfies 2 −N (H+β) < P (x) < 2 −N (H−β) .(4.34)And by the law of large numbers,We have thus proved the 'asymptotic equipartition' principle.As N increases, the probability that x falls in T N β approaches 1, for any β.How does this result relate to source coding?We must relate T N β to H δ (X N ).We will show that for any given δ there is a sufficiently big N such that H δ (X N ) N H.The set T N β is not the best subset for compression.So the size of T N β gives an upper bound on H δ .We show how small H δ (X N ) must be by calculating how big T N β could possibly be.We are free to set β to any convenient value.The smallest possible probability that a member of T N β can have is 2 −N (H+β) , and the total probability contained by T N β can't be any bigger than 1.Sothat is, the size of the typical set is bounded by H+β) .(4.37)If we set β = and N 0 such that σ 2 2 N0 ≤ δ, then P (T N β ) ≥ 1 − δ, and the set T N β becomes a witness to the fact that Part 2: 1 N H δ (X N ) > H − .Imagine that someone claims this second part is not so -that, for any N , the smallest δ-sufficient subset S δ is smaller than the above inequality would allow.We can make use of our typical set to show that they must be mistaken.Remember that we are free to set β to any value we choose.We will set β = /2, so that our task is to prove that a subset S having |S | ≤ 2 N (H−2β) and achieving P (x ∈ S ) ≥ 1 − δ cannot exist (for N greater than an N 0 that we will specify).So, let us consider the probability of falling in this rival smaller subset S .The probability of the subset S iswhere T N β denotes the complement {x ∈ T N β }.The maximum value of the first term is found if S ∩ T N β contains 2 N (H−2β) outcomes all with the maximum probability, 2 −N (H−β) .The maximum value the second term can have is P (x ∈ T N β ).So:We can now set β = /2 and N 0 such that P (x ∈ S ) < 1 − δ, which shows that S cannot satisfy the definition of a sufficient subset S δ .Thus any subset S with size |S | ≤ 2 N (H− ) has probability less than 1 − δ, so by the definition ofThus for large enough N , the function 1 N H δ (X N ) is essentially a constant function of δ, for 0 < δ < 1, as illustrated in figures 4.9 and 4.13. 2The source coding theorem (p.78) has two parts, 1 N H δ (X N ) < H + , andThe first part tells us that even if the probability of error δ is extremely small, the number of bits per symbol 1 N H δ (X N ) needed to specify a long N -symbol string x with vanishingly small error probability does not have to exceed H + bits.We need to have only a tiny tolerance for error, and the number of bits required drops significantly from H 0 (X) to (H + ).What happens if we are yet more tolerant to compression errors?Part 2 tells us that even if δ is very close to 1, so that errors are made most of the time, the average number of bits per symbol needed to specify x must still be at least H − bits.These two extremes tell us that regardless of our specific allowance for error, the number of bits per symbol needed to specify x is H bits; no more and no less.I put the words 'asymptotic equipartition' in quotes because it is important not to think that the elements of the typical set T N β really do have roughly the same probability as each other.They are similar in probability only in the sense that their values of log 2 1 P (x) are within 2N β of each other.Now, as β is decreased, how does N have to increase, if we are to keep our bound on the mass of the typical set, P (x ∈ T N β ) ≥ 1 − σ 2 β 2 N , constant?N must grow as 1/β 2 , so, if we write β in terms of N as α/ √ N , for some constant α, then the most probable string in the typical set will be of order 2 α √ N times greater than the least probable string in the typical set.As β decreases, N increases, and this ratio 2 α √ N grows exponentially.Thus we have 'equipartition' only in a weak sense!Why did we introduce the typical set?The best choice of subset for block compression is (by definition) S δ , not a typical set.So why did we bother introducing the typical set?The answer is, we can count the typical set.We know that all its elements have 'almost identical' probability (2 −N H ), and we know the whole set has probability almost 1, so the typical set must have roughly 2 N H elements.Without the help of the typical set (which is very similar to S δ ) it would have been hard to count how many elements there are in S δ .Exercise 4.9. [1 ]While some people, when they first encounter the weighing problem with 12 balls and the three-outcome balance (exercise 4.1 (p.66)), think that weighing six balls against six balls is a good first weighing, others say 'no, weighing six against six conveys no information at all'.Explain to the second group why they are both right and wrong.Compute the information gained about which is the odd ball, and the information gained about which is the odd ball and whether it is heavy or light.Exercise 4.10. [2 ]Solve the weighing problem for the case where there are 39 balls of which one is known to be odd.Exercise 4.11. [2 ]You are given 16 balls, all of which are equal in weight except for one that is either heavier or lighter.You are also given a bizarre twopan balance that can report only two outcomes: 'the two sides balance' or 'the two sides do not balance'.Design a strategy to determine which is the odd ball in as few uses of the balance as possible.Exercise 4.12. [2 ]You have a two-pan balance; your job is to weigh out bags of flour with integer weights 1 to 40 pounds inclusive.How many weights do you need?[You are allowed to put weights on either pan.You're only allowed to put one flour bag on the balance at a time.]p.86] (a) Is it possible to solve exercise 4.1 (p.66) (the weighing problem with 12 balls and the three-outcome balance) using a sequence of three fixed weighings, such that the balls chosen for the second weighing do not depend on the outcome of the first, and the third weighing does not depend on the first or second?(b) Find a solution to the general N -ball weighing problem in which exactly one of N balls is odd.Show that in W weighings, an odd ball can be identified from among N = (3 W − 3)/2 balls.Exercise 4.14. [3 ]You are given 12 balls and the three-outcome balance of exercise 4.1; this time, two of the balls are odd; each odd ball may be heavy or light, and we don't know which.We want to identify the odd balls and in which direction they are odd.(a) Estimate how many weighings are required by the optimal strategy.And what if there are three odd balls?(b) How do your answers change if it is known that all the regular balls weigh 100 g, that light balls weigh 99 g, and heavy ones weigh 110 g?p.87] Let P X = {0.2,0.8}.Sketch 1 N H δ (X N ) as a function of δ for N = 1, 2 and 1000.Exercise 4.16. [2 ]Let P Y = {0.5, 0.5}.Sketch 1 N H δ (Y N ) as a function of δ for N = 1, 2, 3 and 100.p.87] (For physics students.)Discuss the relationship between the proof of the 'asymptotic equipartition' principle and the equivalence (for large systems) of the Boltzmann entropy and the Gibbs entropy.The law of large numbers, which we used in this chapter, shows that the mean of a set of N i.i.d.random variables has a probability distribution that becomes narrower, with width ∝ 1/ √ N , as N increases.However, we have proved this property only for discrete random variables, that is, for real numbers taking on a finite set of possible values.While many random variables with continuous probability distributions also satisfy the law of large numbers, there are important distributions that do not.Some continuous distributions do not have a mean or variance.p.88] Sketch the Cauchy distributionWhat is its normalizing constant Z? Can you evaluate its mean or variance?Consider the sum z = x 1 + x 2 , where x 1 and x 2 are independent random variables from a Cauchy distribution.What is P (z)?What is the probability distribution of the mean of x 1 and x 2 , x = (x 1 + x 2 )/2?What is the probability distribution of the mean of N samples from this Cauchy distribution?Exercise 4.19. [3 ]Chernoff bound.We derived the weak law of large numbers from Chebyshev's inequality (4.30) by letting the random variable t in the inequality P (t ≥ α) ≤ t/α be a function, t = (x− x) 2 , of the random variable x we were interested in.Other useful inequalities can be obtained by using other functions.The Chernoff bound, which is useful for bounding the tails of a distribution, is obtained by letting t = exp(sx).p.89]This exercise has no purpose at all; it's included for the enjoyment of those who like mathematical curiosities.Sketch the functionfor x ≥ 0. Hint: Work out the inverse function to f -that is, the function g(y) such that if x = g(y) then y = f (x) -it's closely related to p log 1/p.).An ASCII file can be reduced in size by a factor of 7/8.This reduction could be achieved by a block code that maps 8-byte blocks into 7-byte blocks by copying the 56 information-carrying bits into 7 bytes, and ignoring the last bit of every character.Solution to exercise 4.5 (p.74).The pigeon-hole principle states: you can't put 16 pigeons into 15 holes without using one of the holes twice.Similarly, you can't give A X outcomes unique binary names of some length l shorter than log 2 |A X | bits, because there are only 2 l such binary names, and l < log 2 |A X | implies 2 l < |A X |, so at least two different inputs to the compressor would compress to the same output file.Solution to exercise 4.8 (p.76).Between the cusps, all the changes in probability are equal, and the number of elements in T changes by one at each step.So H δ varies logarithmically with (−δ).Solution to exercise 4.17 (p.85).The Gibbs entropy is k B i p i ln 1 p i , where i runs over all states of the system.This entropy is equivalent (apart from the factor of k B ) to the Shannon entropy of the ensemble.Whereas the Gibbs entropy can be defined for any ensemble, the Boltzmann entropy is only defined for microcanonical ensembles, which have a probability distribution that is uniform over a set of accessible states.The Boltzmann entropy is defined to be S B = k B ln Ω where Ω is the number of accessible states of the microcanonical ensemble.This is equivalent (apart from the factor of k B ) to the perfect information content H 0 of that constrained ensemble.The Gibbs entropy of a microcanonical ensemble is trivially equal to the Boltzmann entropy.4 -The Source Coding TheoremWe now consider a thermal distribution (the canonical ensemble), where the probability of a state x isWith this canonical ensemble we can associate a corresponding microcanonical ensemble, an ensemble with total energy fixed to the mean energy of the canonical ensemble (fixed to within some precision ).Now, fixing the total energy to a precision is equivalent to fixing the value of ln 1/ P (x) to within k B T .Our definition of the typical set T N β was precisely that it consisted of all elements that have a value of log P (x) very close to the mean value of log P (x) under the canonical ensemble, −N H(X).Thus the microcanonical ensemble is equivalent to a uniform distribution over the typical set of the canonical ensemble.Our proof of the 'asymptotic equipartition' principle thus proves -for the case of a system whose energy is separable into a sum of independent terms -that the Boltzmann entropy of the microcanonical ensemble is very close (for large N ) to the Gibbs entropy of the canonical ensemble, if the energy of the microcanonical ensemble is constrained to equal the mean energy of the canonical ensemble.Solution to exercise 4.18 (p.85).The normalizing constant of the Cauchy distributionThe mean and variance of this distribution are both undefined.(The distribution is symmetrical about zero, but this does not imply that its mean is zero.The mean is the value of a divergent integral.)The sum z = x 1 + x 2 , where x 1 and x 2 both have Cauchy distributions, has probability density given by the convolutionwhich after a considerable labour using standard methods giveswhich we recognize as a Cauchy distribution with width parameter 2 (where the original distribution has width parameter 1).This implies that the mean of the two points, x = (x 1 + x 2 )/2 = z/2, has a Cauchy distribution with width parameter 1. Generalizing, the mean of N samples from a Cauchy distribution is Cauchy-distributed with the same parameters as the individual samples.The probability distribution of the mean does not become narrower as 1/ √ N .The central-limit theorem does not apply to the Cauchy distribution, because it does not have a finite variance.An alternative neat method for getting to equation (4.53) makes use of the Fourier transform of the Cauchy distribution, which is a biexponential e −|ω| .Convolution in real space corresponds to multiplication in Fourier space, so the Fourier transform of z is simply e −|2ω| .Reversing the transform, we obtain equation (4.53).I obtained a tentative graph of f (x) by plotting g(y) with y along the vertical axis and g(y) along the horizontal axis.The resulting graph suggests that f (x) is single valued for x ∈ (0, 1), and looks surprisingly well-behaved and ordinary; for x ∈ (1, e 1/e ), f (x) is two-valued.f ( √ 2) is equal both to 2 and 4. For x > e 1/e (which is about 1.44), f (x) is infinite.However, it might be argued that this approach to sketching f (x) is only partly valid, if we define f as the limit of the sequence of functions x, x x , x x x , . ..; this sequence does not have a limit for 0 ≤ x ≤ (1/e) e 0.07 on account of a pitchfork bifurcation at x = (1/e) e ; and for x ∈ (1, e 1/e ), the sequence's limit is single-valued -the lower of the two values sketched in the figure.In the last chapter, we saw a proof of the fundamental status of the entropy as a measure of average information content.We defined a data compression scheme using fixed length block codes, and proved that as N increases, it is possible to encode N i.i.d.variables x = (x 1 , . . ., x N ) into a block of N (H(X)+) bits with vanishing probability of error, whereas if we attempt to encode X N into N (H(X) − ) bits, the probability of error is virtually 1.We thus verified the possibility of data compression, but the block coding defined in the proof did not give a practical algorithm.In this chapter and the next, we study practical data compression algorithms.Whereas the last chapter's compression scheme used large blocks of fixed size and was lossy, in the next chapter we discuss variable-length compression schemes that are practical for small block sizes and that are not lossy.Imagine a rubber glove filled with water.If we compress two fingers of the glove, some other part of the glove has to expand, because the total volume of water is constant.(Water is essentially incompressible.)Similarly, when we shorten the codewords for some outcomes, there must be other codewords that get longer, if the scheme is not lossy.In this chapter we will discover the information-theoretic equivalent of water volume.Before reading Chapter 5, you should have worked on exercise 2.26 (p.37).We will use the following notation for intervals:x ∈ [1, 2) means that x ≥ 1 and x < 2; x ∈ (1, 2] means that x > 1 and x ≤ 2.In this chapter, we discuss variable-length symbol codes, which encode one source symbol at a time, instead of encoding huge strings of N source symbols.These codes are lossless: unlike the last chapter's block codes, they are guaranteed to compress and decompress without any errors; but there is a chance that the codes may sometimes produce encoded strings longer than the original source string.The idea is that we can achieve compression, on average, by assigning shorter encodings to the more probable outcomes and longer encodings to the less probable.The key issues are:What are the implications if a symbol code is lossless?If some codewords are shortened, by how much do other codewords have to be lengthened?Making compression practical.How can we ensure that a symbol code is easy to decode?Optimal symbol codes.How should we assign codelengths to achieve the best compression, and what is the best achievable compression?We again verify the fundamental status of the Shannon information content and the entropy, proving:Source coding theorem (symbol codes).There exists a variable-length encoding C of an ensemble X such that the average length of an encoded symbol, L(C, X), satisfies L(C, X) ∈ [H(X), H(X) + 1).The average length is equal to the entropy H(X) only if the codelength for each outcome is equal to its Shannon information content.We will also define a constructive procedure, the Huffman coding algorithm, that produces optimal symbol codes.Notation for alphabets.A N denotes the set of ordered N -tuples of elements from the set A, i.e., all strings of length N .The symbol A + will denote the set of all strings of finite length composed of elements from the set A.Example 5.1.{0, 1} 3 = {000, 001, 010, 011, 100, 101, 110, 111}.Example 5.2.{0, 1} + = {0, 1, 00, 01, 10, 11, 000, 001, . ..}. 5 -Symbol CodesA (binary) symbol code C for an ensemble X is a mapping from the range of x, A X = {a 1 , . . ., a I }, to {0, 1} + .c(x) will denote the codeword corresponding to x, and l(x) will denote its length, with l i = l(a i ).The extended code C + is a mapping from A + X to {0, 1} + obtained by concatenation, without punctuation, of the corresponding codewords:(5.1)[The term 'mapping' here is a synonym for 'function'.]Example 5.3.A symbol code for the ensemble X defined byUsing the extended code, we may encode acdbac as c + (acdbac) = 100000100001010010000010.(5.3)There are basic requirements for a useful symbol code.First, any encoded string must have a unique decoding.Second, the symbol code must be easy to decode.And third, the code should achieve as much compression as possible.A code C(X) is uniquely decodeable if, under the extended code C + , no two distinct strings have the same encoding, i.e., ∀ x, y ∈ A + X , x = y ⇒ c + (x) = c + (y).(5.4)The code C 0 defined above is an example of a uniquely decodeable code.The symbol code must be easy to decodeA symbol code is easiest to decode if it is possible to identify the end of a codeword as soon as it arrives, which means that no codeword can be a prefix of another codeword.[A word c is a prefix of another word d if there exists a tail string t such that the concatenation ct is identical to d.For example, 1 is a prefix of 101, and so is 10.]We will show later that we don't lose any performance if we constrain our symbol code to be a prefix code.A symbol code is called a prefix code if no codeword is a prefix of any other codeword.A prefix code is also known as an instantaneous or self-punctuating code, because an encoded string can be decoded from left to right without looking ahead to subsequent codewords.The end of a codeword is immediately recognizable.A prefix code is uniquely decodeable.Prefix codes are also known as 'prefix-free codes' or 'prefix condition codes'.Prefix codes correspond to trees, as illustrated in the margin of the next page.Example 5.4.The code C 1 = {0, 101} is a prefix code because 0 is not a prefix of 101, nor is 101 a prefix of 0.Example 5.5.Let C 2 = {1, 101}.This code is not a prefix code because 1 is a prefix of 101.Example 5.6.The code C 3 = {0, 10, 110, 111} is a prefix code.Example 5.7.The code C 4 = {00, 01, 10, 11} is a prefix code.p.104] Is C 2 uniquely decodeable?Example 5.9.Consider exercise 4.1 (p.66) and figure 4.2 (p.69).Any weighing strategy that identifies the odd ball and whether it is heavy or light can be viewed as assigning a ternary code to each of the 24 possible states.This code is a prefix code.The expected length L(C, X) of a symbol code C for ensemble X is(5.5)We may also write this quantity aswhereExample 5.10.Let A X = { a, b, c, d }, and P X = { 1/ 2, 1/ 4, 1/ 8, 1/ 8 },(5.7)and consider the code C 3 .The entropy of X is 1.75 bits, and the expected length L(C 3 , X) of this code is also 1.75 bits.The sequence of symbols x = (acdbac) is encoded as c + (x) = 0110111100110.C 3 is a prefix code and is therefore uniquely decodeable.Notice that the codeword lengths satisfy l i = log 2 (1/p i ), or equivalently, p i = 2 −l i .Example 5.11.Consider the fixed length code for the same ensemble X, C 4 .The expected length L(C 4 , X) is 2 bits.C 4 C 5 a 00 0 b 01 1 c 10 00 d 11 11 Example 5.12.Consider C 5 .The expected length L(C 5 , X) is 1.25 bits, which is less than H(X).But the code is not uniquely decodeable.The sequence x = (acdbac) encodes as 000111000, which can also be decoded as (cabdca).Example 5.13.Consider the code C 6 .The expected length L(C 6 , X) of this C 6 : Is C 6 uniquely decodeable?This is not so obvious.If you think that it might not be uniquely decodeable, try to prove it so by finding a pair of strings x and y that have the same encoding.[The definition of unique decodeability is given in equation (5.4).]C 6 certainly isn't easy to decode.When we receive '00', it is possible that x could start 'aa', 'ab' or 'ac'.Once we have received '001111', the second symbol is still ambiguous, as x could be 'abd. . .' or 'acd. . .'.But eventually a unique decoding crystallizes, once the next 0 appears in the encoded stream.C 6 is in fact uniquely decodeable.Comparing with the prefix code C 3 , we see that the codewords of C 6 are the reverse of C 3 's.That C 3 is uniquely decodeable proves that C 6 is too, since any string from C 6 is identical to a string from C 3 read backwards.We now ask, given a list of positive integers {l i }, does there exist a uniquely decodeable code with those integers as its codeword lengths?At this stage, we ignore the probabilities of the different symbols; once we understand unique decodeability better, we'll reintroduce the probabilities and discuss how to make an optimal uniquely decodeable symbol code.In the examples above, we have observed that if we take a code such as {00, 01, 10, 11}, and shorten one of its codewords, for example 00 → 0, then we can retain unique decodeability only if we lengthen other codewords.Thus there seems to be a constrained budget that we can spend on codewords, with shorter codewords being more expensive.Let us explore the nature of this budget.If we build a code purely from codewords of length l equal to three, how many codewords can we have and retain unique decodeability?The answer is 2 l = 8.Once we have chosen all eight of these codewords, is there any way we could add to the code another codeword of some other length and retain unique decodeability?It would seem not.What if we make a code that includes a length-one codeword, '0', with the other codewords being of length three?How many length-three codewords can we have?If we restrict attention to prefix codes, then we can have only four codewords of length three, namely {100, 101, 110, 111}.What about other codes?Is there any other way of choosing codewords of length 3 that can give more codewords?Intuitively, we think this unlikely.A codeword of length 3 appears to have a cost that is 2 2 times smaller than a codeword of length 1.Let's define a total budget of size 1, which we can spend on codewords.If we set the cost of a codeword whose length is l to 2 −l , then we have a pricing system that fits the examples discussed above.Codewords of length 3 cost 1/ 8 each; codewords of length 1 cost 1/2 each.We can spend our budget on any codewords.If we go over our budget then the code will certainly not be uniquely decodeable.If, on the other hand,(5.8)then the code may be uniquely decodeable.This inequality is the Kraft inequality.Kraft inequality.For any uniquely decodeable code C(X) over the binary alphabet {0, 1}, the codeword lengths must satisfy: (5.9)whereCompleteness.If a uniquely decodeable code satisfies the Kraft inequality with equality then it is called a complete code.We want codes that are uniquely decodeable; prefix codes are uniquely decodeable, and are easy to decode.So life would be simpler for us if we could restrict attention to prefix codes.Fortunately, for any source there is an optimal symbol code that is also a prefix code.Kraft inequality and prefix codes.Given a set of codeword lengths that satisfy the Kraft inequality, there exists a uniquely decodeable prefix code with these codeword lengths.The Kraft inequality might be more accurately referred to as the Kraft-McMillan inequality: Kraft proved that if the inequality is satisfied, then a prefix code exists with the given lengths.McMillan (1956) proved the converse, that unique decodeability implies that the inequality holds.Proof of the Kraft inequality.Define S = i 2 −l i .Consider the quantity(5.10)The quantity in the exponent, (, is the length of the encoding of the string x = a i1 a i2 . . .a i N .For every string x of length N , there is one term in the above sum.Introduce an array A l that counts how many strings x have encoded length l.Then, defining l min = min i l i and l max = max i l i :(5.11)Now assume C is uniquely decodeable, so that for all x = y, c + (x) = c + (y).Concentrate on the x that have encoded length l.There are a total of 2 l distinct bit strings of length l, so it must be the case that(5.12)Thus S N ≤ l max N for all N .Now if S were greater than 1, then as N increases, S N would be an exponentially growing function, and for large enough N , an exponential always exceeds a polynomial such as l max N .But our result (S N ≤ l max N ) is true for any N .Therefore S ≤ 1. 2p.104] Prove the result stated above, that for any set of codeword lengths {l i } satisfying the Kraft inequality, there is a prefix code having those lengths.The total symbol code budget" ¡" ¡" ¡" ¡" ¡" ¡" ¡" ¡" ¡" " ¡" ¡" ¡" ¡" ¡" ¡" ¡" ¡" ¡" " ¡" ¡" ¡" ¡" ¡" ¡" ¡" ¡" ¡" " ¡" ¡" ¡" ¡" ¡" ¡" ¡" ¡" ¡" " ¡" ¡" ¡" ¡" ¡" ¡" ¡" ¡" ¡" " ¡" ¡" ¡" ¡" ¡" ¡" ¡" ¡" ¡" " ¡" ¡" ¡" ¡" ¡" ¡" ¡" ¡" ¡" " ¡" ¡" ¡" ¡" ¡" ¡" ¡" ¡" ¡" " ¡" ¡" ¡" ¡" ¡" ¡" ¡" ¡" ¡" " ¡" ¡" ¡" ¡" ¡" ¡" ¡" ¡" ¡" " ¡" ¡" ¡" ¡" ¡" ¡" ¡" ¡" ¡" " ¡" ¡" ¡" ¡" ¡" ¡" ¡" ¡" ¡" " ¡" ¡" ¡" ¡" ¡" ¡" ¡" ¡" ¡" " ¡" ¡" ¡" ¡" ¡" ¡" ¡" ¡" ¡" " ¡" ¡" ¡" ¡" ¡" ¡" ¡" ¡" ¡" " ¡" ¡" ¡" ¡" ¡" ¡" ¡" ¡" ¡" " ¡" ¡" ¡" ¡" ¡" ¡" ¡" ¡" ¡" " ¡" ¡" ¡" ¡" ¡" ¡" ¡" ¡" ¡" " ¡" ¡" ¡" ¡" ¡" ¡" ¡" ¡" ¡" " ¡" ¡" ¡" ¡" ¡" ¡" ¡" ¡" ¡"   ( ¡( ¡( ¡( ¡( ¡( ¡( ¡( ¡( ¡(  ( ¡( ¡( ¡( ¡( ¡( ¡( ¡( ¡( ¡(  ( ¡( ¡( ¡( ¡( ¡( ¡( ¡( ¡( ¡(  ( ¡( ¡( ¡( ¡( ¡( ¡( ¡( ¡( ¡(  ( ¡( ¡( ¡( ¡( ¡( ¡( ¡( ¡( ¡(  ( ¡( ¡( ¡( ¡( ¡( ¡( ¡( ¡( ¡(  ( ¡( ¡( ¡( ¡( ¡( ¡( ¡( ¡( ¡(  ( ¡( ¡( ¡( ¡( ¡( ¡( ¡( ¡( ¡(  ( ¡( ¡( ¡( ¡( ¡( ¡( ¡( ¡( ¡(  ( ¡( ¡( ¡( ¡( ¡( ¡( ¡( ¡( ¡A pictorial view of the Kraft inequality may help you solve this exercise.Imagine that we are choosing the codewords to make a symbol code.We can draw the set of all candidate codewords in a supermarket that displays the 'cost' of the codeword by the area of a box (figure 5.1).The total budget available -the '1' on the right-hand side of the Kraft inequality -is shown at one side.Some of the codes discussed in section 5.1 are illustrated in figure 5.2.Notice that the codes that are prefix codes, C 0 , C 3 , and C 4 , have the property that to the right of any selected codeword, there are no other selected codewords -because prefix codes correspond to trees.Notice that a complete prefix code corresponds to a complete tree having no unused branches.We are now ready to put back the symbols' probabilities {p i }.Given a set of symbol probabilities (the English language probabilities of figure 2.1, for example), how do we make the best symbol code -one with the smallest possible expected length L(C, X)?And what is that smallest possible expected length?It's not obvious how to assign the codeword lengths.If we give short codewords to the more probable symbols then the expected length might be reduced; on the other hand, shortening some codewords necessarily causes others to lengthen, by the Kraft inequality.We wish to minimize the expected length of a code,(5.13)As you might have guessed, the entropy appears as the lower bound on the expected length of a code.Lower bound on expected length.The expected length L(C, X) of a uniquely decodeable code is bounded below by H(X).Proof.We define the implicit probabilities q i ≡ 2 −l i /z, where z = i 2 −l i , so that l i = log 1/q i − log z.We then use Gibbs' inequality, i p i log 1/q i ≥ i p i log 1/p i , with equality if q i = p i , and the Kraft inequality z ≤ 1:≥ H(X).(5.16)The equality L(C, X) = H(X) is achieved only if the Kraft equality z = 1 is satisfied, and if the codelengths satisfy l i = log(1/p i ). 2 This is an important result so let's say it again:Optimal source codelengths.The expected length is minimized and is equal to H(X) only if the codelengths are equal to the Shannon information contents:(5.17)Implicit probabilities defined by codelengths.Conversely, any choice of codelengths {l i } implicitly defines a probability distribution {q i }, (5.18) for which those codelengths would be the optimal codelengths.If the code is complete then z = 1 and the implicit probabilities are given by q i = 2 −l i .5 -Symbol CodesSo, we can't compress below the entropy.How close can we expect to get to the entropy?Theorem 5.1 Source coding theorem for symbol codes.For an ensemble X there exists a prefix code C with expected length satisfying H(X) ≤ L(C, X) < H(X) + 1.(5.19)Proof.We set the codelengths to integers slightly larger than the optimum lengths:where l * denotes the smallest integer greater than or equal to l * .[We are not asserting that the optimal code necessarily uses these lengths, we are simply choosing these lengths because we can use them to prove the theorem.]We check that there is a prefix code with these lengths by confirming that the Kraft inequality is satisfied.(5.21)Then we confirmThe cost of using the wrong codelengthsIf we use a code whose lengths are not equal to the optimal codelengths, the average message length will be larger than the entropy.If the true probabilities are {p i } and we use a complete code with lengths l i , we can view those lengths as defining implicit probabilities q i = 2 −l i .Continuing from equation (5.14), the average length isi.e., it exceeds the entropy by the relative entropy D KL (p||q) (as defined on p.34).Given a set of probabilities P, how can we design an optimal prefix code?For example, what is the best symbol code for the English language ensemble shown in figure 5.3?When we say 'optimal', let's assume our aim is to minimize the expected length L(C, X).One might try to roughly split the set A X in two, and continue bisecting the subsets so as to define a binary tree from the root.This construction has the right spirit, as in the weighing problem, but it is not necessarily optimal; it achieves L(C, X) ≤ H(X) + 2.We now present a beautifully simple algorithm for finding an optimal prefix code.The trick is to construct the code backwards starting from the tails of the codewords; we build the binary tree from its leaves.Algorithm 5.4.Huffman coding algorithm.1. Take the two least probable symbols in the alphabet.These two symbols will be given the longest codewords, which will have equal length, and differ only in the last digit.2. Combine these two symbols into a single symbol, and repeat.Since each step reduces the size of the alphabet by one, this algorithm will have assigned strings to all the symbols after |A X | − 1 steps.The codewords are then obtained by concatenating the binary digits in reverse order: C = {00, 10, 11, 010, 011}.The codelengths selected by the Huffman algorithm (column 4 of table 5.5) are in some cases longer and in some cases shorter than the ideal codelengths, the Shannon information contents log 2 1/ pi (column 3).The expected length of the code is L = 2.30 bits, whereas the entropy is H = 2.2855 bits.2If at any point there is more than one way of selecting the two least probable symbols then the choice may be made in any manner -the expected length of the code will not depend on the choice.p.105] Prove that there is no better symbol code for a source than the Huffman code.Example 5.17.We can make a Huffman code for the probability distribution over the alphabet introduced in figure 2.1.The result is shown in figure 5.6.This code has an expected length of 4.15 bits; the entropy of the ensemble is 4.11 bits.Observe the disparities between the assigned codelengths and the ideal codelengths log 2 1/ pi.In previous chapters we studied weighing problems in which we built ternary or binary trees.We noticed that balanced trees -ones in which, at every step, the two possible outcomes were as close as possible to equiprobable -appeared to describe the most efficient experiments.This gave an intuitive motivation for entropy as a measure of information content.It is not the case, however, that optimal codes can always be constructed by a greedy top-down method in which the alphabet is successively divided into subsets that are as near as possible to equiprobable.Example 5.18.Find the optimal binary symbol code for the ensemble:g } P X = { 0.01, 0.24, 0.05, 0.20, 0.47, 0.01, 0.02 } .(5.24)Notice that a greedy top-down method can split this set into two subsets {a, b, c, d} and {e, f, g} which both have probability 1/2, and that {a, b, c, d} can be divided into subsets {a, b} and {c, d}, which have probability 1/4; so a greedy top-down method gives the code shown in the third column of coding algorithm yields the code shown in the fourth column, which has expected length 1.97. 2The Huffman algorithm produces an optimal symbol code for an ensemble, but this is not the end of the story.Both the word 'ensemble' and the phrase 'symbol code' need careful attention.If we wish to communicate a sequence of outcomes from one unchanging ensemble, then a Huffman code may be convenient.But often the appropriate 5.6: Disadvantages of the Huffman code 101 ensemble changes.If for example we are compressing text, then the symbol frequencies will vary with context: in English the letter u is much more probable after a q than after an e (figure 2.3).And furthermore, our knowledge of these context-dependent symbol frequencies will also change as we learn the statistical properties of the text source.Huffman codes do not handle changing ensemble probabilities with any elegance.One brute-force approach would be to recompute the Huffman code every time the probability over symbols changes.Another attitude is to deny the option of adaptation, and instead run through the entire file in advance and compute a good probability distribution, which will then remain fixed throughout transmission.The code itself must also be communicated in this scenario.Such a technique is not only cumbersome and restrictive, it is also suboptimal, since the initial message specifying the code and the document itself are partially redundant.This technique therefore wastes bits.An equally serious problem with Huffman codes is the innocuous-looking 'extra bit' relative to the ideal average length of H(X) -a Huffman code achieves a length that satisfies H(X) ≤ L(C, X) < H(X)+1, as proved in theorem 5.1.A Huffman code thus incurs an overhead of between 0 and 1 bits per symbol.If H(X) were large, then this overhead would be an unimportant fractional increase.But for many applications, the entropy may be as low as one bit per symbol, or even smaller, so the overhead L(C, X) − H(X) may dominate the encoded file length.Consider English text: in some contexts, long strings of characters may be highly predictable.For example, in the context 'strings_of_ch', one might predict the next nine symbols to be 'aracters_' with a probability of 0.99 each.A traditional Huffman code would be obliged to use at least one bit per character, making a total cost of nine bits where virtually no information is being conveyed (0.13 bits in total, to be precise).The entropy of English, given a good model, is about one bit per character (Shannon, 1948), so a Huffman code is likely to be highly inefficient.A traditional patch-up of Huffman codes uses them to compress blocks of symbols, for example the 'extended sources' X N we discussed in Chapter 4. The overhead per block is at most 1 bit so the overhead per symbol is at most 1/N bits.For sufficiently large blocks, the problem of the extra bit may be removed -but only at the expenses of (a) losing the elegant instantaneous decodeability of simple Huffman coding; and (b) having to compute the probabilities of all relevant strings and build the associated Huffman tree.One will end up explicitly computing the probabilities and codes for a huge number of strings, most of which will never actually occur.(See exercise 5.29 (p.103).)Huffman codes, therefore, although widely trumpeted as 'optimal', have many defects for practical purposes.They are optimal symbol codes, but for practical purposes we don't want a symbol code.The defects of Huffman codes are rectified by arithmetic coding, which dispenses with the restriction that each symbol must translate into an integer number of bits.Arithmetic coding is the main topic of the next chapter.Kraft inequality.If a code is uniquely decodeable its lengths must satisfy i 2 −l i ≤ 1.(5.25)For any lengths satisfying the Kraft inequality, there exists a prefix code with those lengths.Optimal source codelengths for an ensemble are equal to the Shannon information contentsand conversely, any choice of codelengths defines implicit probabilities(5.27)The relative entropy D KL (p||q) measures how many bits per symbol are wasted by using a code whose implicit probabilities are q, when the ensemble's true probability distribution is p.Source coding theorem for symbol codes.For an ensemble X, there exists a prefix code whose expected length satisfies(5.28)The Huffman coding algorithm generates an optimal symbol code iteratively.At each iteration, the two least probable symbols are combined.Exercise 5.19. [2 ]Is the code {00, 11, 0101, 111, 1010, 100100, 0110} uniquely decodeable?Exercise 5.20. [2 ]Is the ternary code {00, 012, 0110, 0112, 100, 201, 212, 22} uniquely decodeable?p.106] Make Huffman codes for X 2 , X 3 and X 4 where A X = {0, 1} and P X = {0.9,0.1}.Compute their expected lengths and compare them with the entropies H(X 2 ), H(X 3 ) and H(X 4 ).Repeat this exercise for X 2 and X 4 where P X = {0.6,0.4}.p.106] Find a probability distribution {p 1 , p 2 , p 3 , p 4 } such that there are two optimal codes that assign different lengths {l i } to the four symbols.Exercise 5.23. [3 ](Continuation of exercise 5.22.)Assume that the four probabilities {p 1 , p 2 , p 3 , p 4 } are ordered such that p 1 ≥ p 2 ≥ p 3 ≥ p 4 ≥ 0. Let Q be the set of all probability vectors p such that there are two optimal codes with different lengths.Give a complete description of Q. Find three probability vectors q (1) , q (2) , q (3) , which are the convex hull of Q, i.e., such that any p ∈ Q can be written as p = µ 1 q (1) + µ 2 q (2) + µ 3 q (3) , (5.29)where {µ i } are positive.Exercise 5.24. [1 ]Write a short essay discussing how to play the game of twenty questions optimally.[In twenty questions, one player thinks of an object, and the other player has to guess the object using as few binary questions as possible, preferably fewer than twenty.]Exercise 5.25. [2 ]Show that, if each probability p i is equal to an integer power of 2 then there exists a source code whose expected length equals the entropy.p.106] Make ensembles for which the difference between the entropy and the expected length of the Huffman code is as big as possible.p.106]A source X has an alphabet of eleven characters {a, b, c, d, e, f, g, h, i, j, k}, all of which have equal probability, 1/11.Find an optimal uniquely decodeable symbol code for this source.How much greater is the expected length of this optimal code than the entropy of X?Exercise 5.28. [2 ]Consider the optimal symbol code for an ensemble X with alphabet size I from which all symbols have identical probability p = 1/I.I is not a power of 2.Show that the fraction f + of the I symbols that are assigned codelengths equal to l + ≡ log 2 I (5.30) satisfiesand that the expected length of the optimal symbol code is(5.32)By differentiating the excess length ∆L ≡ L − H(X) with respect to I, show that the excess length is bounded by(5.33)Exercise 5.29. [2 ]Consider a sparse binary source with P X = {0.99,0.01}.Discuss how Huffman codes could be used to compress this source efficiently.Estimate how many codewords your proposed solutions require.Exercise 5.30. [2 ]Scientific American carried the following puzzle in 1975.The poisoned glass.'Mathematicians are curious birds', the police commissioner said to his wife.'You see, we had all those partly filled glasses lined up in rows on a table in the hotel kitchen.Only one contained poison, and we wanted to know which one before searching that glass for fingerprints.Our lab could test the liquid in each glass, but the tests take time and money, so we wanted to make as few of them as possible by simultaneously testing mixtures of small samples from groups of glasses.The university sent over a mathematics professor to help us.He counted the glasses, smiled and said: ' "Pick any glass you want, Commissioner.We'll test it first."' "But won't that waste a test?"I asked.' "No," he said, "it's part of the best procedure.We can test one glass first.It doesn't matter which one." ' 'How many glasses were there to start with?' the commissioner's wife asked.'I don't remember.Somewhere between 100 and 200.' What was the exact number of glasses?Solve this puzzle and then explain why the professor was in fact wrong and the commissioner was right.What is in fact the optimal procedure for identifying the one poisoned glass?What is the expected waste relative to this optimum if one followed the professor's strategy?Explain the relationship to symbol coding.p.106] Assume that a sequence of symbols from the ensemble X introduced at the beginning of this chapter is compressed using the code C 3 .Imagine picking one bit at random from the binary encoded C 3 :. What is the probability that this bit is a 1?p.107] How should the binary Huffman encoding scheme be modified to make optimal symbol codes in an encoding alphabet with q symbols?(Also known as 'radix q'.)It is a tempting idea to construct a 'metacode' from several symbol codes that assign different-length codewords to the alternative symbols, then switch from one code to another, choosing whichever assigns the shortest codeword to the current symbol.Clearly we cannot do this for free.If one wishes to choose between two codes, then it is necessary to lengthen the message in a way that indicates which of the two codes is being used.If we indicate this choice by a single leading bit, it will be found that the resulting code is suboptimal because it is incomplete (that is, it fails the Kraft equality).p.108] Prove that this metacode is incomplete, and explain why this combined code is suboptimal.Solution to exercise 5.8 (p.93).Yes, C 2 = {1, 101} is uniquely decodeable, even though it is not a prefix code, because no two different strings can map onto the same string; only the codeword c(a 2 ) = 101 contains the symbol 0.Solution to exercise 5.14 (p.95).We wish to prove that for any set of codeword lengths {l i } satisfying the Kraft inequality, there is a prefix code having those lengths.This is readily proved by thinking of the codewords illustrated in figure 5.8 as being in a 'codeword supermarket', with size indicating cost.We imagine purchasing codewords one at a time, starting from the shortest codewords (i.e., the biggest purchases), using the budget shown at the right of figure 5.8.We start at one side of the codeword supermarket, say the The total symbol code budget symbol probability Huffman Rival code's Modified rival codewords codewords codeFigure 5.9.Proof that Huffman coding makes an optimal symbol code.We assume that the rival code, which is said to be optimal, assigns unequal length codewords to the two symbols with smallest probability, a and b.By interchanging codewords a and c of the rival code, where c is a symbol with rival codelength as long as b's, we can make a code better than the rival code.This shows that the rival code was not optimal.top, and purchase the first codeword of the required length.We advance down the supermarket a distance 2 −l , and purchase the next codeword of the next required length, and so forth.Because the codeword lengths are getting longer, and the corresponding intervals are getting shorter, we can always buy an adjacent codeword to the latest purchase, so there is no wasting of the budget.Thus at the Ith codeword we have advanced a distance I i=1 2 −l i down the supermarket; if 2 −l i ≤ 1, we will have purchased all the codewords without running out of budget.Solution to exercise 5.16 (p.99).The proof that Huffman coding is optimal depends on proving that the key step in the algorithm -the decision to give the two symbols with smallest probability equal encoded lengths -cannot lead to a larger expected length than any other code.We can prove this by contradiction.Assume that the two symbols with smallest probability, called a and b, to which the Huffman algorithm would assign equal length codewords, do not have equal lengths in any optimal symbol code.The optimal symbol code is some other rival code in which these two codewords have unequal lengths l a and l b with l a < l b .Without loss of generality we can assume that this other code is a complete prefix code, because any codelengths of a uniquely decodeable code can be realized by a prefix code.In this rival code, there must be some other symbol c whose probability p c is greater than p a and whose length in the rival code is greater than or equal to l b , because the code for b must have an adjacent codeword of equal or greater length -a complete prefix code never has a solo codeword of the maximum length.Consider exchanging the codewords of a and c (figure 5.9), so that a is encoded with the longer codeword that was c's, and c, which is more probable than a, gets the shorter codeword.Clearly this reduces the expected length of the code.The change in expected length is (p a − p c )(l c − l a ).Thus we have contradicted the assumption that the rival code is optimal.Therefore it is valid to give the two symbols with smallest probability equal encoded lengths.Huffman coding produces optimal symbol codes. 2Solution to exercise 5.21 (p.102).A Huffman code for X 2 where A X = {0, 1} and P X = {0.9,0.1} is {00, 01, 10, 11} → {1, 01, 000, 001}.This code has L(C, X 2 ) = 1.29, whereas the entropy H(X 2 ) is 0.938.A Huffman code for X 3 is {000, 100, 010, 001, 101, 011, 110, 111} → {1, 011, 010, 001, 00000, 00001, 00010, 00011}.This has expected length L(C, X 3 ) = 1.598 whereas the entropy H(X 3 ) is 1.4069.A Huffman code for X 4 maps the sixteen source strings to the following codelengths: {0000, 1000, 0100, 0010, 0001, 1100, 0110, 0011, 0101, 1010, 1001, 1110, 1101, 1011, 0111, 1111} → {1, 3, 3, 3, 4, 6, 7, 7, 7, 7, 7, 9, 9, 9, 10, 10}.This has expected length L(C, X 4 ) = 1.9702 whereas the entropy H(X 4 ) is 1.876.When P X = {0.6,0.4}, the Huffman code for X 2 has lengths {2, 2, 2, 2}; the expected length is 2 bits, and the entropy is 1.94 bits.A Huffman code for X 4 is shown in table 5.10.The expected length is 3.92 bits, and the entropy is 3.88 bits.when p 0 = 0.6.Column 3 shows the assigned codelengths and column the codewords.Some strings whose probabilities are identical, e.g., the fourth and fifth, receive different codelengths.Solution to exercise 5.22 (p.102).The set of probabilities {p 1 , p 2 , p 3 , p 4 } = { 1/ 6, 1/ 6, 1/ 3, 1/ 3} gives rise to two different optimal sets of codelengths, because at the second step of the Huffman coding algorithm we can choose any of the three possible pairings.We may either put them in a constant length code {00, 01, 10, 11} or the code {000, 001, 01, 1}.Both codes have expected length 2.Another solution is {p 1 , p 2 , p 3 , p 4 } = { 1/ 5, 1/ 5, 1/ 5, 2/ 5}.And a third is {p 1 , p 2 , p 3 , p 4 } = { 1/ 3, 1/ 3, 1/ 3, 0}.Solution to exercise 5.26 (p.103).Let p max be the largest probability in p 1 , p 2 , . . ., p I .The difference between the expected length L and the entropy H can be no bigger than max(p max , 0.086) (Gallager, 1978).See exercises 5.27-5.28 to understand where the curious 0.086 comes from.Solution to exercise 5.27 (p.103).Length − entropy = 0.086.There are two ways to answer this problem correctly, and one popular way to answer it incorrectly.Let's give the incorrect answer first:Erroneous answer."We can pick a random bit by first picking a random source symbol x i with probability p i , then picking a random bit from c(x i ).If we define f i to be the fraction of the bits of c(x i ) that are 1s, we findThis answer is wrong because it falls for the bus-stop fallacy, which was introduced in exercise 2.35 (p.38): if buses arrive at random, and we are interested in 'the average time from one bus until the next', we must distinguish two possible averages: (a) the average time from a randomly chosen bus until the next; (b) the average time between the bus you just missed and the next bus.The second 'average' is twice as big as the first because, by waiting for a bus at a random time, you bias your selection of a bus in favour of buses that follow a large gap.You're unlikely to catch a bus that comes 10 seconds after a preceding bus!Similarly, the symbols c and d get encoded into longer-length binary strings than a, so when we pick a bit from the compressed string at random, we are more likely to land in a bit belonging to a c or a d than would be given by the probabilities p i in the expectation (5.34).All the probabilities need to be scaled up by l i , and renormalized.Correct answer in the same style.Every time symbol x i is encoded, l i bits are added to the binary string, of which f i l i are 1s.The expected number of 1s added per symbol is(5.36) and the expected total number of bits added per symbol is i p i l i .(5.37)So the fraction of 1s in the transmitted string isFor a general symbol code and a general ensemble, the expectation (5.38) is the correct answer.But in this case, we can use a more powerful argument.Information-theoretic answer.The encoded string c is the output of an optimal compressor that compresses samples from X down to an expected length of H(X) bits.We can't expect to compress this data any further.But if the probability P (bit is 1) were not equal to 1/ 2 then it would be possible to compress the binary string further (using a block compression code, say).Therefore P (bit is 1) must be equal to 1/ 2; indeed the probability of any sequence of l bits in the compressed stream taking on any particular value must be 2 −l .The output of a perfect compressor is always perfectly random bits.To put it another way, if the probability P (bit is 1) were not equal to 1/ 2, then the information content per bit of the compressed string would be at most H 2 (P (1)), which would be less than 1; but this contradicts the fact that we can recover the original data from c, so the information content per bit of the compressed string must be H(X)/L(C, X) = 1.Solution to exercise 5.32 (p.104).The general Huffman coding algorithm for an encoding alphabet with q symbols has one difference from the binary case.The process of combining q symbols into 1 symbol reduces the number of symbols by q − 1.So if we start with A symbols, we'll only end up with a complete q-ary tree if A mod (q −1) is equal to 1. Otherwise, we know that whatever prefix code we make, it must be an incomplete tree with a number of missing leaves equal, modulo (q −1), to A mod (q −1) − 1.For example, if a ternary tree is built for eight symbols, then there will unavoidably be one missing leaf in the tree.The optimal q-ary code is made by putting these extra leaves in the longest branch of the tree.This can be achieved by adding the appropriate number of symbols to the original source symbol set, all of these extra symbols having probability zero.The total number of leaves is then equal to r(q−1) + 1, for some integer r.The symbols are then repeatedly combined by taking the q symbols with smallest probability and replacing them by a single symbol, as in the binary Huffman coding algorithm.Solution to exercise 5.33 (p.104).We wish to show that a greedy metacode, which picks the code which gives the shortest encoding, is actually suboptimal, because it violates the Kraft inequality.We'll assume that each symbol x is assigned lengths l k (x) by each of the candidate codes C k .Let us assume there are K alternative codes and that we can encode which code is being used with a header of length log K bits.Then the metacode assigns lengths l (x) that are given by(5.39)We compute the Kraft sum:(5.40)Let's divide the set A X into non-overlapping subsets {A k } K k=1 such that subset A k contains all the symbols x that the metacode sends via code k.Then(5.41)Now if one sub-code k satisfies the Kraft equality x∈A X 2 −l k (x) = 1, then it must be the case thatwith equality only if all the symbols x are in A k , which would mean that we are only using one of the K codes.Sowith equality only if equation (5.42) is an equality for all codes k.But it's impossible for all the symbols to be in all the non-overlapping subsets {A k } K k=1 , so we can't have equality (5.42) holding for all k.So S < 1.Another way of seeing that a mixture code is suboptimal is to consider the binary tree that it defines.Think of the special case of two codes.The first bit we send identifies which code we are using.Now, in a complete code, any subsequent binary string is a valid string.But once we know that we are using, say, code A, we know that what follows can only be a codeword corresponding to a symbol x whose encoding is shorter under code A than code B. So some strings are invalid continuations, and the mixture code is incomplete and suboptimal.For further discussion of this issue and its relationship to probabilistic modelling read about 'bits back coding' in section 28.3 and in Frey (1998).Before reading Chapter 6, you should have read the previous chapter and worked on most of the exercises in it.We'll also make use of some Bayesian modelling ideas that arrived in the vicinity of exercise 2.8 (p.30).In this chapter we discuss two data compression schemes.Arithmetic coding is a beautiful method that goes hand in hand with the philosophy that compression of data from a source entails probabilistic modelling of that source.As of 1999, the best compression methods for text files use arithmetic coding, and several state-of-the-art image compression systems use it too.Lempel-Ziv coding is a 'universal' method, designed under the philosophy that we would like a single compression algorithm that will do a reasonable job for any source.In fact, for many real life sources, this algorithm's universal properties hold only in the limit of unfeasibly large amounts of data, but, all the same, Lempel-Ziv compression is widely used and often effective.As a motivation for these two compression methods, consider the redundancy in a typical English text file.Such files have redundancy at several levels: for example, they contain the ASCII characters with non-equal frequency; certain consecutive pairs of letters are more probable than others; and entire words can be predicted given the context and a semantic understanding of the text.To illustrate the redundancy of English, and a curious way in which it could be compressed, we can imagine a guessing game in which an English speaker repeatedly attempts to predict the next character in a text file.For simplicity, let us assume that the allowed alphabet consists of the 26 upper case letters A,B,C,..., Z and a space '-'.The game involves asking the subject to guess the next character repeatedly, the only feedback being whether the guess is correct or not, until the character is correctly guessed.After a correct guess, we note the number of guesses that were made when the character was identified, and ask the subject to guess the next character in the same way.One sentence gave the following result when a human was asked to guess a sentence.The numbers of guesses are listed below each character.1 1 1 5 1 1 2 1 1 2 1 1 15 1 17 1 1 1 2 1 3 2 1 2 2 7 1 1 1 1 4 1 1 1 1 1 Notice that in many cases, the next letter is guessed immediately, in one guess.In other cases, particularly at the start of syllables, more guesses are needed.What do this game and these results offer us?First, they demonstrate the redundancy of English from the point of view of an English speaker.Second, this game might be used in a data compression scheme, as follows.The string of numbers '1, 1, 1, 5, 1, . . .', listed above, was obtained by presenting the text to the subject.The maximum number of guesses that the subject will make for a given letter is twenty-seven, so what the subject is doing for us is performing a time-varying mapping of the twenty-seven letters {A, B, C, . . ., Z, −} onto the twenty-seven numbers {1, 2, 3, . . ., 27}, which we can view as symbols in a new alphabet.The total number of symbols has not been reduced, but since he uses some of these symbols much more frequently than others -for example, 1 and 2 -it should be easy to compress this new string of symbols.How would the uncompression of the sequence of numbers '1, 1, 1, 5, 1, . . .' work?At uncompression time, we do not have the original string 'THERE. . .', we have only the encoded sequence.Imagine that our subject has an absolutely identical twin who also plays the guessing game with us, as if we knew the source text.If we stop him whenever he has made a number of guesses equal to the given number, then he will have just guessed the correct letter, and we can then say 'yes, that's right', and move to the next character.Alternatively, if the identical twin is not available, we could design a compression system with the help of just one human as follows.We choose a window length L, that is, a number of characters of context to show the human.For every one of the 27 L possible strings of length L, we ask them, 'What would you predict is the next character?',and 'If that prediction were wrong, what would your next guesses be?'.After tabulating their answers to these 26 × 27 L questions, we could use two copies of these enormous tables at the encoder and the decoder in place of the two human twins.Such a language model is called an Lth order Markov model.These systems are clearly unrealistic for practical compression, but they illustrate several principles that we will make use of now.When we discussed variable-length symbol codes, and the optimal Huffman algorithm for constructing them, we concluded by pointing out two practical and theoretical problems with Huffman codes (section 5.6).These defects are rectified by arithmetic codes, which were invented by Elias, by Rissanen and by Pasco, and subsequently made practical by Witten et al. (1987).In an arithmetic code, the probabilistic modelling is clearly separated from the encoding operation.The system is rather similar to the guessing game.The human predictor is replaced by a probabilistic model of the source.As each symbol is produced by the source, the probabilistic model supplies a predictive distribution over all possible values of the next symbol, that is, a list of positive numbers {p i } that sum to one.If we choose to model the source as producing i.i.d.symbols with some known distribution, then the predictive distribution is the same every time; but arithmetic coding can with equal ease handle complex adaptive models that produce context-dependent predictive distributions.The predictive model is usually implemented in a computer program.The encoder makes use of the model's predictions to create a binary string.The decoder makes use of an identical twin of the model (just as in the guessing game) to interpret the binary string.Let the source alphabet be A X = {a 1 , . . ., a I }, and let the Ith symbol a I have the special meaning 'end of transmission'.The source spits out a sequence x 1 , x 2 , . . ., x n , . . . .The source does not necessarily produce i.i.d.symbols.We will assume that a computer program is provided to the encoder that assigns a predictive probability distribution over a i given the sequence that has occurred thus far, P (x n = a i | x 1 , . . ., x n−1 ).The receiver has an identical program that produces the same predictive probability distribution P (x n = a i | x 1 , . . ., x n−1 ).Notation for intervals.The interval [0.01, 0.10) is all numbers between 0.01 and 0.10, including 0.01 0 ≡ 0.01000 . . .but not 0.10 0 ≡ 0.10000 . . . .A binary transmission defines an interval within the real line from 0 to 1.For example, the string 01 is interpreted as a binary real number 0.01. . ., which corresponds to the interval [0.01, 0.10) in binary, i.e., the interval [0.25, 0.50) in base ten.The longer string 01101 corresponds to a smaller interval [0.01101, 0.01110).Because 01101 has the first string, 01, as a prefix, the new interval is a sub-interval of the interval [0.01, 0.10).A one-megabyte binary file (2 23 bits) is thus viewed as specifying a number between 0 and 1 to a precision of about two million decimal places -two million decimal digits, because each byte translates into a little more than two decimal digits.Now, we can also divide the real line [0,1) into I intervals of lengths equal to the probabilities P (x 1 = a i ), as shown in figure 6.2.0.00 P (x 1 = a 1 ) We may then take each interval a i and subdivide it into intervals denoted a i a 1 , a i a 2 , . . ., a i a I , such that the length of a i a j is proportional to P (x 2 = a j | x 1 = a i ).Indeed the length of the interval a i a j will be precisely the joint probabilityIterating this procedure, the interval [0, 1) can be divided into a sequence of intervals corresponding to all possible finite length strings x 1 x 2 . . .x N , such that the length of an interval is equal to the probability of the string given our model.Algorithm 6.3.Arithmetic coding.Iterative procedure to find the interval [u, v) for the string x 1 x 2 . . .x N .u := 0.0The process depicted in figure 6.2 can be written explicitly as follows.The intervals are defined in terms of the lower and upper cumulative probabilitiesAs the nth symbol arrives, we subdivide the n−1th interval at the points defined by Q n and R n .For example, starting with the first symbol, the intervals 'a 1 ', 'a 2 ', and 'a I ' are)) = [P (x = a 1 ), P (x = a 1 ) + P (x = a 2 )) , (6.5) andAlgorithm 6.3 describes the general procedure.To encode a string x 1 x 2 . . .x N , we locate the interval corresponding to x 1 x 2 . . .x N , and send a binary string whose interval lies within that interval.This encoding can be performed on the fly, as we now illustrate.Imagine that we watch as a bent coin is tossed some number of times (cf.example 2.7 (p.30) and section 3.2 (p.51)).The two outcomes when the coin is tossed are denoted a and b.A third possibility is that the experiment is halted, an event denoted by the 'end of file' symbol, '2'.Because the coin is bent, we expect that the probabilities of the outcomes a and b are not equal, though beforehand we don't know which is the more probable outcome.Let the source string be 'bbba2'.We pass along the string one symbol at a time and use our model to compute the probability distribution of the next 6 -Stream Codes symbol given the string thus far.Let these probabilities be: When the first symbol 'b' is observed, the encoder knows that the encoded string will start '01', '10', or '11', but does not know which.The encoder writes nothing for the time being, and examines the next symbol, which is 'b'.The interval 'bb' lies wholly within interval '1', so the encoder can write the first bit: '1'.The third symbol 'b' narrows down the interval a little, but not quite enough for it to lie wholly within interval '10'.Only when the next 'a' is read from the source can we transmit some more bits.Interval 'bbba' lies wholly within the interval '1001', so the encoder adds '001' to the '1' it has written.Finally when the '2' arrives, we need a procedure for terminating the encoding.Magnifying the interval 'bbba2' (figure 6.4, right) we note that the marked interval '100111101' is wholly contained by bbba2, so the encoding can be completed by appending '11101'.p.127] Show that the overhead required to terminate a message is never more than 2 bits, relative to the ideal message length given the probabilistic modelThis is an important result.Arithmetic coding is very nearly optimal.The message length is always within two bits of the Shannon information content of the entire source string, so the expected message length is within two bits of the entropy of the entire message.The decoder receives the string '100111101' and passes along it one symbol at a time.First, the probabilities P (a), P (b), P (2) are computed using the identical program that the encoder used and the intervals 'a', 'b' and '2' are deduced.Once the first two bits '10' have been examined, it is certain that the original string must have been started with a 'b', since the interval '10' lies wholly within interval 'b'.The decoder can then use the model to computeand deduce the boundaries of the intervals 'ba', 'bb' and 'b2'.Continuing, we decode the second b once we reach '1001', the third b once we reach '100111', and so forth, with the unambiguous identification of 'bbba2' once the whole binary string has been read.With the convention that '2' denotes the end of the message, the decoder knows to stop decoding.How might one use arithmetic coding to communicate several distinct files over the binary channel?Once the 2 character has been transmitted, we imagine that the decoder is reset into its initial state.There is no transfer of the learnt statistics of the first file to the second file.If, however, we did believe that there is a relationship among the files that we are going to compress, we could define our alphabet differently, introducing a second end-of-file character that marks the end of the file but instructs the encoder and decoder to continue using the same probabilistic model.Notice that to communicate a string of N letters both the encoder and the decoder needed to compute only N |A| conditional probabilities -the probabilities of each possible letter in each context actually encountered -just as in the guessing game.This cost can be contrasted with the alternative of using a Huffman code with a large block size (in order to reduce the possible onebit-per-symbol overhead discussed in section 5.6), where all block sequences that could occur must be considered and their probabilities evaluated.Notice how flexible arithmetic coding is: it can be used with any source alphabet and any encoded alphabet.The size of the source alphabet and the encoded alphabet can change with time.Arithmetic coding can be used with any probability distribution, which can change utterly from context to context.Furthermore, if we would like the symbols of the encoding alphabet (say, 0 and 1) to be used with unequal frequency, that can easily be arranged by subdividing the right-hand interval in proportion to the required frequencies.The technique of arithmetic coding does not force one to produce the predictive probability in any particular way, but the predictive distributions might This model anticipates that the source is likely to be biased towards one of a and b, so sequences having lots of as or lots of bs have larger intervals than sequences of the same length that are 50:50 as and bs.naturally be produced by a Bayesian model.Figure 6.4 was generated using a simple model that always assigns a probability of 0.15 to 2, and assigns the remaining 0.85 to a and b, divided in proportion to probabilities given by Laplace's rule,where F a (x 1 , . . ., x n−1 ) is the number of times that a has occurred so far, and F b is the count of bs.These predictions correspond to a simple Bayesian model that expects and adapts to a non-equal frequency of use of the source symbols a and b within a file.Figure 6.5 displays the intervals corresponding to a number of strings of length up to five.Note that if the string so far has contained a large number of bs then the probability of b relative to a is increased, and conversely if many as occur then as are made more probable.Larger intervals, remember, require fewer bits to encode.Having emphasized that any model could be used -arithmetic coding is not wedded to any particular set of probabilities -let me explain the simple adaptiveprobabilistic model used in the preceding example; we first encountered this model in exercise 2.8 (p.30).The model will be described using parameters p 2 , p a and p b , defined below, which should not be confused with the predictive probabilities in a particular context, for example, P (a | s = baa).A bent coin labelled a and b is tossed some number of times l, which we don't know beforehand.The coin's probability of coming up a when tossed is p a , and p b = 1 − p a ; the parameters p a , p b are not known beforehand.The source string s = baaba2 indicates that l was 5 and the sequence of outcomes was baaba.1.It is assumed that the length of the string l has an exponential probability distribution (6.8)This distribution corresponds to assuming a constant probability p 2 for the termination symbol '2' at each character.2. It is assumed that the non-terminal characters in the string are selected independently at random from an ensemble with probabilities P = {p a , p b }; the probability p a is fixed throughout the string to some unknown value that could be anywhere between 0 and 1.The probability of an a occurring as the next symbol, given p a (if only we knew it), is (1 − p 2 )p a .The probability, given p a , that an unterminated string of length F is a given string s that contains {F a , F b } counts of the two outcomes is the Bernoulli distribution(6.9)3. We assume a uniform prior distribution for p a , P (p a ) = 1, p a ∈ [0, 1], (6.10) and define p b ≡ 1 − p a .It would be easy to assume other priors on p a , with beta distributions being the most convenient to handle.This model was studied in section 3.2.The key result we require is the predictive distribution for the next symbol, given the string so far, s.This probability that the next character is a or b (assuming that it is not '2') was derived in equation (3.16) and is precisely Laplace's rule (6.7).Exercise 6.2. [3 ]Compare the expected message length when an ASCII file is compressed by the following three methods.Huffman-with-header. Read the whole file, find the empirical frequency of each symbol, construct a Huffman code for those frequencies, transmit the code by transmitting the lengths of the Huffman codewords, then transmit the file using the Huffman code.(The actual codewords don't need to be transmitted, since we can use a deterministic method for building the tree given the codelengths.)Arithmetic code using the Laplace model.. (6.11)Arithmetic code using a Dirichlet model.This model's predictions are:where α is fixed to a number such as 0.01.A small value of α corresponds to a more responsive version of the Laplace model; the probability over characters is expected to be more nonuniform; α = 1 reproduces the Laplace model.Take care that the header of your Huffman message is self-delimiting.Special cases worth considering are (a) short files with just a few hundred characters; (b) large files in which some characters are never used.Arithmetic coding not only offers a way to compress strings believed to come from a given model; it also offers a way to generate random strings from a model.Imagine sticking a pin into the unit interval at random, that line having been divided into subintervals in proportion to probabilities p i ; the probability that your pin will lie in interval i is p i .So to generate a sample from a model, all we need to do is feed ordinary random bits into an arithmetic decoder for that model.An infinite random bit sequence corresponds to the selection of a point at random from the line [0, 1), so the decoder will then select a string at random from the assumed distribution.This arithmetic method is guaranteed to use very nearly the smallest number of random bits possible to make the selection -an important point in communities where random numbers are expensive![This is not a joke.Large amounts of money are spent on generating random bits in software and hardware.Random numbers are valuable.]A simple example of the use of this technique is in the generation of random bits with a nonuniform distribution {p 0 , p 1 }.p.128] Compare the following two techniques for generating random symbols from a nonuniform distribution {p 0 , p 1 } = {0.99,0.01}:(a) The standard method: use a standard random number generator to generate an integer between 1 and 2 32 .Rescale the integer to (0, 1).Test whether this uniformly distributed random variable is less than 0.99, and emit a 0 or 1 accordingly.(b) Arithmetic coding using the correct model, fed with standard random bits.Roughly how many random bits will each method use to generate a thousand samples from this sparse distribution?When we enter text into a computer, we make gestures of some sort -maybe we tap a keyboard, or scribble with a pointer, or click with a mouse; an efficient text entry system is one where the number of gestures required to enter a given text string is small.Writing can be viewed as an inverse process to data compression.In dataCompression: text → bitsWriting: text ← gestures compression, the aim is to map a given text string into a small number of bits.In text entry, we want a small sequence of gestures to produce our intended text.By inverting an arithmetic coder, we can obtain an information-efficient text entry device that is driven by continuous pointing gestures (Ward et al., 6.4: Lempel-Ziv coding 119 2000).In this system, called Dasher, the user zooms in on the unit interval to locate the interval corresponding to their intended string, in the same style as figure 6.4.A language model (exactly as used in text compression) controls the sizes of the intervals such that probable strings are quick and easy to identify.After an hour's practice, a novice user can write with one finger driving Dasher at about 25 words per minute -that's about half their normal ten-finger typing speed on a regular keyboard.It's even possible to write at 25 words per minute, hands-free, using gaze direction to drive Dasher (Ward and MacKay, 2002).Dasher is available as free software for various platforms. 1The Lempel-Ziv algorithms, which are widely used for data compression (e.g., the compress and gzip commands), are different in philosophy to arithmetic coding.There is no separation between modelling and coding, and no opportunity for explicit modelling.The method of compression is to replace a substring with a pointer to an earlier occurrence of the same substring.For example if the string is 1011010100010. . ., we parse it into an ordered dictionary of substrings that have not appeared before as follows: λ, 1, 0, 11, 01, 010, 00, 10, . . . .We include the empty substring λ as the first substring in the dictionary and order the substrings in the dictionary by the order in which they emerged from the source.After every comma, we look along the next part of the input sequence until we have read a substring that has not been marked off before.A moment's reflection will confirm that this substring is longer by one bit than a substring that has occurred earlier in the dictionary.This means that we can encode each substring by giving a pointer to the earlier occurrence of that prefix and then sending the extra bit by which the new substring in the dictionary differs from the earlier substring.If, at the nth bit, we have enumerated s(n) substrings, then we can give the value of the pointer in log 2 s(n) bits.The code for the above sequence is then as shown in the fourth line of the following (, 1) (0, 0) (01, 1) (10, 1) (100, 0) (010, 0) (001, 0)Notice that the first pointer we send is empty, because, given that there is only one substring in the dictionary -the string λ -no bits are needed to convey the 'choice' of that substring as the prefix.The encoded string is 100011101100001000010.The encoding, in this simple case, is actually a longer string than the source string, because there was no obvious redundancy in the source string.Exercise 6.4. [2 ]Prove that any uniquely decodeable code from {0, 1} + to {0, 1} + necessarily makes some strings longer if it makes some strings shorter.120One reason why the algorithm described above lengthens a lot of strings is because it is inefficient -it transmits unnecessary bits; to put it another way, its code is not complete.Once a substring in the dictionary has been joined there by both of its children, then we can be sure that it will not be needed (except possibly as part of our protocol for terminating a message); so at that point we could drop it from our dictionary of substrings and shuffle them all along one, thereby reducing the length of subsequent pointer messages.Equivalently, we could write the second prefix into the dictionary at the point previously occupied by the parent.A second unnecessary overhead is the transmission of the new bit in these cases -the second time a prefix is used, we can be sure of the identity of the next bit.The decoder again involves an identical twin at the decoding end who constructs the dictionary of substrings as the data are decoded.p.128] Encode the string 000000000000100000000000 using the basic Lempel-Ziv algorithm described above.p.128] Decode the string 00101011101100100100011010101000011 that was encoded using the basic Lempel-Ziv algorithm.In this description I have not discussed the method for terminating a string.There are many variations on the Lempel-Ziv algorithm, all exploiting the same idea but using different procedures for dictionary management, etc.The resulting programs are fast, but their performance on compression of English text, although useful, does not match the standards set in the arithmetic coding literature.In contrast to the block code, Huffman code, and arithmetic coding methods we discussed in the last three chapters, the Lempel-Ziv algorithm is defined without making any mention of a probabilistic model for the source.Yet, given any ergodic source (i.e., one that is memoryless on sufficiently long timescales), the Lempel-Ziv algorithm can be proven asymptotically to compress down to the entropy of the source.This is why it is called a 'universal' compression algorithm.For a proof of this property, see Cover and Thomas (1991).It achieves its compression, however, only by memorizing substrings that have happened so that it has a short name for them the next time they occur.The asymptotic timescale on which this universal performance is achieved may, for many sources, be unfeasibly long, because the number of typical substrings that need memorizing may be enormous.The useful performance of the algorithm in practice is a reflection of the fact that many files contain multiple repetitions of particular short sequences of characters, a form of redundancy to which the algorithm is well suited.Common groundI have emphasized the difference in philosophy behind arithmetic coding and Lempel-Ziv coding.There is common ground between them, though: in principle, one can design adaptive probabilistic models, and thence arithmetic codes, that are 'universal', that is, models that will asymptotically compress any source in some class to within some factor (preferably 1) of its entropy.However, for practical purposes, I think such universal models can only be constructed if the class of sources is severely restricted.A general purpose compressor that can discover the probability distribution of any source would be a general purpose artificial intelligence!A general purpose artificial intelligence does not yet exist.An interactive aid for exploring arithmetic coding, dasher.tcl, is available. 2A demonstration arithmetic-coding software package written by Radford Neal 3 consists of encoding and decoding modules to which the user adds a module defining the probabilistic model.It should be emphasized that there is no single general-purpose arithmetic-coding compressor; a new model has to be written for each type of source.Radford Neal's package includes a simple adaptive model similar to the Bayesian model demonstrated in section 6.2.The results using this Laplace model should be viewed as a basic benchmark since it is the simplest possible probabilistic model -it simply assumes the characters in the file come independently from a fixed ensemble.The counts {F i } of the symbols {a i } are rescaled and rounded as the file is read such that all the counts lie between 1 and 256.A state-of-the-art compressor for documents containing text and images, DjVu, uses arithmetic coding. 4It uses a carefully designed approximate arithmetic coder for binary alphabets called the Z-coder (Bottou et al., 1998), which is much faster than the arithmetic coding software described above.One of the neat tricks the Z-coder uses is this: the adaptive model adapts only occasionally (to save on computer time), with the decision about when to adapt being pseudo-randomly controlled by whether the arithmetic encoder emitted a bit.The JBIG image compression standard for binary images uses arithmetic coding with a context-dependent model, which adapts using a rule similar to Laplace's rule.PPM (Teahan, 1995) is a leading method for text compression, and it uses arithmetic coding.There are many Lempel-Ziv-based programs.gzip is based on a version of Lempel-Ziv called 'LZ77' (Ziv and Lempel, 1977).compress is based on 'LZW' (Welch, 1984).In my experience the best is gzip, with compress being inferior on most files.bzip is a block-sorting file compressor, which makes use of a neat hack called the Burrows-Wheeler transform (Burrows and Wheeler, 1994).This method is not based on an explicit probabilistic model, and it only works well for files larger than several thousand characters; but in practice it is a very effective compressor for files in which the context of a character is a good predictor for that character.Interestingly, gzip does not always do so well.Table 6.7 gives the compression achieved when these programs are applied to a text file containing 10 6 characters, each of which is either 0 and 1 with probabilities 0.99 and 0.01.The Laplace model is quite well matched to this source, and the benchmark arithmetic coder gives good performance, followed closely by compress; gzip is worst.An ideal model for this source would compress the file into about 10 6 H 2 (0.01)/8 10 100 bytes.The Laplace-model compressor falls short of this performance because it is implemented using only eight-bit precision.The ppmz compressor compresses the best of all, but takes much more computer time.CompressionIn the last three chapters we have studied three classes of data compression codes.Fixed-length block codes (Chapter 4).These are mappings from a fixed number of source symbols to a fixed-length binary message.Only a tiny fraction of the source strings are given an encoding.These codes were fun for identifying the entropy as the measure of compressibility but they are of little practical use.Symbol codes (Chapter 5).Symbol codes employ a variable-length code for each symbol in the source alphabet, the codelengths being integer lengths determined by the probabilities of the symbols.Huffman's algorithm constructs an optimal symbol code for a given set of symbol probabilities.Every source string has a uniquely decodeable encoding, and if the source symbols come from the assumed distribution then the symbol code will compress to an expected length per character L lying in the interval [H, H + 1).Statistical fluctuations in the source may make the actual length longer or shorter than this mean length.If the source is not well matched to the assumed distribution then the mean length is increased by the relative entropy D KL between the source distribution and the code's implicit distribution.For sources with small entropy, the symbol has to emit at least one bit per source symbol; compression below one bit per source symbol can be achieved only by the cumbersome procedure of putting the source data into blocks.Stream codes.The distinctive property of stream codes, compared with symbol codes, is that they are not constrained to emit at least one bit for every symbol read from the source stream.So large numbers of source symbols may be coded into a smaller number of bits.This property could be obtained using a symbol code only if the source stream were somehow chopped into blocks.• Arithmetic codes combine a probabilistic model with an encoding algorithm that identifies each string with a sub-interval of [0, 1) of size equal to the probability of that string under the model.This code is almost optimal in the sense that the compressed length of a string x closely matches the Shannon information content of x given the probabilistic model.Arithmetic codes fit with the philosophy that good compression requires data modelling, in the form of an adaptive Bayesian model.• Lempel-Ziv codes are adaptive in the sense that they memorize strings that have already occurred.They are built on the philosophy that we don't know anything at all about what the probability distribution of the source will be, and we want a compression algorithm that will perform reasonably well whatever that distribution is.Both arithmetic codes and Lempel-Ziv codes will fail to decode correctly if any of the bits of the compressed file are altered.So if compressed files are to be stored or transmitted over noisy media, error-correcting codes will be essential.Reliable communication over unreliable channels is the topic of Part II.Exercise 6.7. [2 ]Describe an arithmetic coding algorithm to encode random bit strings of length N and weight K (i.e., K ones and N − K zeroes) where N and K are given.For the case N = 5, K = 2, show in detail the intervals corresponding to all source substrings of lengths 1-5.p.128] How many bits are needed to specify a selection of K objects from N objects?(N and K are assumed to be known and the selection of K objects is unordered.)How might such a selection be made at random without being wasteful of random bits?Exercise 6.9. [2 ]A binary source X emits independent identically distributed symbols with probability distribution {f 0 , f 1 }, where f 1 = 0.01.Find an optimal uniquely-decodeable symbol code for a string x = x 1 x 2 x 3 of three successive samples from this source.Estimate (to one decimal place) the factor by which the expected length of this optimal code is greater than the entropy of the three-bit string x.[H 2 (0.01) 0.08, whereAn arithmetic code is used to compress a string of 1000 samples from the source X. Estimate the mean and standard deviation of the length of the compressed file.Exercise 6.10. [2 ]Describe an arithmetic coding algorithm to generate random bit strings of length N with density f (i.e., each bit has probability f of being a one) where N is given.Exercise 6.11. [2 ]Use a modified Lempel-Ziv algorithm in which, as discussed on p.120, the dictionary of prefixes is pruned by writing new prefixes into the space occupied by prefixes that will not be needed again.Such prefixes can be identified when both their children have been added to the dictionary of prefixes.(You may neglect the issue of termination of encoding.)Use this algorithm to encode the string 0100001000100010101000001. Highlight the bits that follow a prefix on the second occasion that that prefix is used.(As discussed earlier, these bits could be omitted.)p.128] Show that this modified Lempel-Ziv code is still not 'complete', that is, there are binary strings that are not encodings of any string.p.128] Give examples of simple sources that have low entropy but would not be compressed well by the Lempel-Ziv algorithm.The following exercises may be skipped by the reader who is eager to learn about noisy channels.p.130] Consider a Gaussian distribution in N dimensions,Define the radius of a point x to be r = n x 2 n 1/2 .Estimate the mean and variance of the square of the radius, r 2 = n x 2 n .You may find helpful the integral Assuming that N is large, show that nearly all the probability of a Gaussian is contained in a thin shell of radius √ N σ.Find the thickness of the shell.Evaluate the probability density (6.13) at a point in that thin shell and at the origin x = 0 and compare.Use the case N = 1000 as an example.Notice that nearly all the probability mass is located in a different part of the space from the region of highest probability density.Exercise 6.15. [2 ]Explain what is meant by an optimal binary symbol code.Find an optimal binary symbol code for the ensemble: and compute the expected length of the code.Exercise 6.16. [2 ]A string y = x 1 x 2 consists of two independent samples from an ensembleWhat is the entropy of y? Construct an optimal binary symbol code for the string y, and find its expected length.Exercise 6.17. [2 ]Strings of N independent samples from an ensemble with P = {0.1,0.9} are compressed using an arithmetic code that is matched to that ensemble.Estimate the mean and standard deviation of the compressed strings' lengths for the case N = 1000.[H 2 (0.1) 0.47] Exercise 6.18. [3 ]Source coding with variable-length symbols.In the chapters on source coding, we assumed that we were encoding into a binary alphabet {0, 1} in which both symbols should be used with equal frequency.In this question we explore how the encoding alphabet should be used if the symbols take different times to transmit.A poverty-stricken student communicates for free with a friend using a telephone by selecting an integer n ∈ {1, 2, 3 . ..}, making the friend's phone ring n times, then hanging up in the middle of the nth ring.This process is repeated so that a string of symbols n 1 n 2 n 3 . . . is received.What is the optimal way to communicate?If large integers n are selected then the message takes longer to communicate.If only small integers n are used then the information content per symbol is small.We aim to maximize the rate of information transfer, per unit time.Assume that the time taken to transmit a number of rings n and to redial is l n seconds.Consider a probability distribution over n, {p n }.Defining the average duration per symbol to beand the entropy per symbol to be (6.16)show that for the average information rate per second to be maximized, the symbols must be used with probabilities of the formwhere Z = n 2 −βln and β satisfies the implicit equationthat is, β is the rate of communication.Show that these two equations (6.17, 6.18) imply that β must be set such that log Z = 0. (6.19)Assuming that the channel has the property (6.20) find the optimal distribution p and show that the maximal information rate is 1 bit per second.How does this compare with the information rate per second achieved if p is set to (1/2, 1/2, 0, 0, 0, 0, . ..) -that is, only the symbols n = 1 and n = 2 are selected, and they have equal probability?Discuss the relationship between the results (6.17, 6.19) derived above, and the Kraft inequality from source coding theory.How might a random binary source be efficiently encoded into a sequence of symbols n 1 n 2 n 3 . . .for transmission over the channel defined in equation (6.20)?Exercise 6.19. [1 ]How many bits does it take to shuffle a pack of cards?Exercise 6.20. [2 ]In the card game Bridge, the four players receive 13 cards each from the deck of 52 and start each game by looking at their own hand and bidding.The legal bids are, in ascending order 1♣, 1♦, 1♥, 1♠, 1N T, 2♣, 2♦, . . .7♥, 7♠, 7N T , and successive bids must follow this order; a bid of, say, 2♥ may only be followed by higher bids such as 2♠ or 3♣ or 7N T .(Let us neglect the 'double' bid.)The players have several aims when bidding.One of the aims is for two partners to communicate to each other as much as possible about what cards are in their hands.Let us concentrate on this task.(a) After the cards have been dealt, how many bits are needed for North to convey to South what her hand is?(b) Assuming that E and W do not bid at all, what is the maximum total information that N and S can convey to each other while bidding?Assume that N starts the bidding, and that once either N or S stops bidding, the bidding stops.6.9: Solutions 127 Exercise 6.21. [2 ]My old 'arabic' microwave oven had 11 buttons for entering cooking times, and my new 'roman' microwave has just five.The buttons of the roman microwave are labelled '10 minutes', '1 minute', '10 seconds', '1 second', and 'Start'; I'll abbreviate these five strings to the symbols M, C, X, I, 2. To enter one minute and twenty-three seconds (1:23), the arabic sequence is 1232, (6.21)and the roman sequence isEach of these keypads defines a code mapping the 3599 cooking times from 0:01 to 59:59 into a string of symbols.(a) Which times can be produced with two or three symbols?(For example, 0:20 can be produced by three symbols in either code: XX2 and 202.)(b) Are the two codes complete?Give a detailed answer.(c) For each code, name a cooking time that it can produce in four symbols that the other code cannot.(d) Discuss the implicit probability distributions over times to which each of these codes is best matched.(e) Concoct a plausible probability distribution over times that a real user might use, and evaluate roughly the expected number of symbols, and maximum number of symbols, that each code requires.Discuss the ways in which each code is inefficient or efficient.(f) Invent a more efficient cooking-time-encoding system for a microwave oven.p.132] Is the standard binary representation for positive integers (e.g.c b (5) = 101) a uniquely decodeable code?Design a binary code for the positive integers, i.e., a mapping from n ∈ {1, 2, 3, . ..} to c(n) ∈ {0, 1} + , that is uniquely decodeable.Try to design codes that are prefix codes and that satisfy the Kraft equality n 2 −ln = 1.Motivations: any data file terminated by a special end of file character can be mapped onto an integer, so a prefix code for integers can be used as a self-delimiting encoding of files too.Large files correspond to large integers.Also, one of the building blocks of a 'universal' coding schemethat is, a coding scheme that will work OK for a large variety of sources -is the ability to encode integers.Finally, in microwave ovens, cooking times are positive integers!Discuss criteria by which one might compare alternative codes for integers (or, equivalently, alternative self-delimiting codes for files).Solution to exercise 6.1 (p.115).The worst-case situation is when the interval to be represented lies just inside a binary interval.In this case, we may choose either of two binary intervals as shown in figure 6.10.These binary intervals are no smaller than P (x|H)/4, so the binary encoding has a length no greater than log 2 1/P (x|H) + log 2 4, which is two bits more than the ideal message length.Solution to exercise 6.3 (p.118).The standard method uses 32 random bits per generated symbol and so requires 32 000 bits to generate one thousand samples.Arithmetic coding uses on average about H 2 (0.01) = 0.081 bits per generated symbol, and so requires about 83 bits to generate one thousand samples (assuming an overhead of roughly two bits associated with termination).Fluctuations in the number of 1s would produce variations around this mean with standard deviation 21.Solution to exercise 6.5 (p.120).The encoding is 010100110010110001100, which comes from the parsing 0, 00, 000, 0000, 001, 00000, 000000 (6.23) which is encoded thus:(, 0), (1, 0), (10, 0), (11, 0), (010, 1), (100, 0), (110, 0).(6.24)Solution to exercise 6.6 (p.120).The decoding is 0100001000100010101000001.Solution to exercise 6.8 (p.123).This problem is equivalent to exercise 6.7 (p.123).The selection of K objects from N objects requires log 2 N K bits N H 2 (K/N ) bits.This selection could be made using arithmetic coding.The selection corresponds to a binary string of length N in which the 1 bits represent which objects are selected.Initially the probability of a 1 is K/N and the probability of a 0 is (N −K)/N .Thereafter, given that the emitted string thus far, of length n, contains k 1s, the probability of a 1 is (K −k)/(N −n) and the probability of a 0 is 1Solution to exercise 6.12 (p.124).This modified Lempel-Ziv code is still not 'complete', because, for example, after five prefixes have been collected, the pointer could be any of the strings 000, 001, 010, 011, 100, but it cannot be 101, 110 or 111.Thus there are some binary strings that cannot be produced as encodings.Solution to exercise 6.13 (p.124).Sources with low entropy that are not well compressed by Lempel-Ziv include: 6.9: Solutions 129(a) Sources with some symbols that have long range correlations and intervening random junk.An ideal model should capture what's correlated and compress it.Lempel-Ziv can compress the correlated features only by memorizing all cases of the intervening junk.As a simple example, consider a telephone book in which every line contains an (old number, new number) pair:285-3820:572-58922 258-8302:593-20102 The number of characters per line is 18, drawn from the 13-character alphabet {0, 1, . . ., 9, −, :, 2}.The characters '-', ':' and '2' occur in a predictable sequence, so the true information content per line, assuming all the phone numbers are seven digits long, and assuming that they are random sequences, is about 14 bans.(A ban is the information content of a random integer between 0 and 9.) A finite state language model could easily capture the regularities in these data.A Lempel-Ziv algorithm will take a long time before it compresses such a file down to 14 bans per line, however, because in order for it to 'learn' that the string :ddd is always followed by -, for any three digits ddd, it will have to see all those strings.So near-optimal compression will only be achieved after thousands of lines of the file have been read.(b) Sources with long range correlations, for example two-dimensional images that are represented by a sequence of pixels, row by row, so that vertically adjacent pixels are a distance w apart in the source stream, where w is the image width.Consider, for example, a fax transmission in which each line is very similar to the previous line (figure 6.11).The true entropy is only H 2 (f ) per pixel, where f is the probability that a pixel differs from its parent.Lempel-Ziv algorithms will only compress down to the entropy once all strings of length 2 w = 2 400 have occurred and their successors have been memorized.There are only about 2 300 particles in the universe, so we can confidently say that Lempel-Ziv codes will never capture the redundancy of such an image.Another highly redundant texture is shown in figure 6.12.The image was made by dropping horizontal and vertical pins randomly on the plane.It contains both long-range vertical correlations and long-range horizontal correlations.There is no practical way that Lempel-Ziv, fed with a pixel-by-pixel scan of this image, could capture both these correlations.Biological computational systems can readily identify the redundancy in these images and in images that are much more complex; thus we might anticipate that the best data compression algorithms will result from the development of artificial intelligence methods.(c) Sources with intricate redundancy, such as files generated by computers.For example, a L A T E X file followed by its encoding into a PostScript file.The information content of this pair of files is roughly equal to the information content of the L A T E X file alone.(d) A picture of the Mandelbrot set.The picture has an information content equal to the number of bits required to specify the range of the complex plane studied, the pixel sizes, and the colouring rule used.(e) A picture of a ground state of a frustrated antiferromagnetic Ising model (figure 6.13), which we will discuss in Chapter 31.Like figure 6.12, this binary image has interesting correlations in two directions.(f) Cellular automata -figure 6.14 shows the state history of 100 steps of a cellular automaton with 400 cells.The update rule, in which each cell's new state depends on the state of five preceding cells, was selected at random.The information content is equal to the information in the boundary (400 bits), and the propagation rule, which here can be described in 32 bits.An optimal compressor will thus give a compressed file length which is essentially constant, independent of the vertical height of the image.Lempel-Ziv would only give this zero-cost compression once the cellular automaton has entered a periodic limit cycle, which could easily take about 2 100 iterations.In contrast, the JBIG compression method, which models the probability of a pixel given its local context and uses arithmetic coding, would do a good job on these images.Solution to exercise 6.14 (p.124).For a one-dimensional Gaussian, the variance of x, E[x 2 ], is σ 2 .So the mean value of r 2 in N dimensions, since the components of x are independent random variables, is(6.25)The variance of r 2 , similarly, is N times the variance of x 2 , where x is a one-dimensional Gaussian variable.varThe integral is found to be 3σ 4 (equation (6.14)), so var(x 2 ) = 2σ 4 .Thus the variance of r 2 is 2N σ 4 .For large N , the central-limit theorem indicates that r 2 has a Gaussian distribution with mean N σ 2 and standard deviation √ 2N σ 2 , so the probability density of r must similarly be concentrated about r √ N σ.The thickness of this shell is given by turning the standard deviation of r 2 into a standard deviation on r: for small δr/r, δ log rThe probability density of the Gaussian at a point x shell where r = √ N σ isWhereas the probability density at the origin is(6.28)Thus P (x shell )/P (x = 0) = exp (−N/2) .The probability density at the typical radius is e −N/2 times smaller than the density at the origin.If N = 1000, then the probability density at the origin is e 500 times greater.Unary code.An integer n is encoded by sending a string of n−1 0s followed by a 1.The unary code has length l U (n) = n.The unary code is the optimal code for integers if the probability distribution over n is p U (n) = 2 −n .We can use the unary code to encode the length of the binary encoding of n and make a self-delimiting code:Code C α .We send the unary code for l b (n), followed by the headless binary representation of n.Table 7.1 shows the codes for some integers.The overlining indicates the division of each string into the parts c U [l b (n)] and c B (n).We mightequivalently view c α (n) as consisting of a string of (l b (n) − 1) zeroes followed by the standard binary representation of n, c b (n).The implicit probability distribution over n for the code C α is separable into the product of a probability distribution over the length l,and a uniform distribution over integers having that length,Now, for the above code, the header that communicates the length always occupies the same number of bits as the standard binary representation of the integer (give or take one).If we are expecting to encounter large integers (large files) then this representation seems suboptimal, since it leads to all files occupying a size that is double their original uncoded size.Instead of using the unary code to encode the length l b (n), we could use C α .Code C β .We send the length l b (n) using C α , followed by the headless binary representation of n.Iterating this procedure, we can define a sequence of codes.Codes with end-of-file symbolsWe can also make byte-based representations.(Let's use the term byte flexibly here, to denote any fixed-length string of bits, not just a string of length 8 bits.)If we encode the number in some base, for example decimal, then we can represent each digit in a byte.In order to represent a digit from 0 to 9 in a byte we need four bits.Because 2 4 = 16, this leaves 6 extra four-bit symbols, {1010, 1011, 1100, 1101, 1110, 1111}, that correspond to no decimal digit.We can use these as end-of-file symbols to indicate the end of our positive integer.Clearly it is redundant to have more than one end-of-file symbol, so a more efficient code would encode the integer into base 15, and use just the sixteenth symbol, 1111, as the punctuation character.Generalizing this idea, we can make similar byte-based codes for integers in bases 3 and 7, and in any base of the form 2 n − 1.These codes are almost complete.(Recall that a code is 'complete' if it satisfies the Kraft inequality with equality.)The codes' remaining inefficiency is that they provide the ability to encode the integer zero and the empty string, neither of which was required.p.136] Consider the implicit probability distribution over integers corresponding to the code with an end-of-file character.(a) If the code has eight-bit blocks (i.e., the integer is coded in base 255), what is the mean length in bits of the integer, under the implicit distribution?(b) If one wishes to encode binary files of expected size about one hundred kilobytes using a code with an end-of-file character, what is the optimal block size?To illustrate the codes we have discussed, we now use each code to encode a small file consisting of just 14 characters, Claude Shannon .• If we map the ASCII characters onto seven-bit symbols (e.g., in decimal, C = 67, l = 108, etc.), this 14 character file corresponds to the integer n = 167 987 786 364 950 891 085 602 469 870 (decimal).• The unary code for n consists of this many (less one) zeroes, followed by a one.If all the oceans were turned into ink, and if we wrote a hundred bits with every cubic millimeter, there might be enough ink to write c U (n).• The standard binary representation of n is this length-98 sequence of bits:Exercise 7.2. [2 ]Write down or describe the following self-delimiting representations of the above number n:and c 15 (n).Which of these encodings is the shortest?[Answer: c 15 .]One could answer the question 'which of two codes is superior?' by a sentence of the form 'For n > k, code 1 is superior, for n < k, code 2 is superior' but I contend that such an answer misses the point: any complete code corresponds to a prior for which it is optimal; you should not say that any other code is superior to it.Other codes are optimal for other priors.These implicit priors should be thought about so as to achieve the best code for one's application.Notice that one cannot, for free, switch from one code to another, choosing whichever is shorter.If one were to do this, then it would be necessary to lengthen the message in some way that indicates which of the two codes is being used.If this is done by a single leading bit, it will be found that the resulting code is suboptimal because it fails the Kraft equality, as was discussed in exercise 5.33 (p.104).Another way to compare codes for integers is to consider a sequence of probability distributions, such as monotonic probability distributions over n ≥ 1, and rank the codes as to how well they encode any of these distributions.A code is called a 'universal' code if for any distribution in a given class, it encodes into an average length that is within some factor of the ideal average length.Let me say this again.We are meeting an alternative world view -rather than figuring out a good prior over integers, as advocated above, many theorists have studied the problem of creating codes that are reasonably good codes for any priors in a broad class.Here the class of priors conventionally considered is the set of priors that (a) assign a monotonically decreasing probability over integers and (b) have finite entropy.Several of the codes we have discussed above are universal.Another code which elegantly transcends the sequence of self-delimiting codes is Elias's 'universal code for integers ' (Elias, 1975), which effectively chooses from all the codes C α , C β , . . . .It works by sending a sequence of messages each of which encodes the length of the next message, and indicates by a single bit whether or not that message is the final integer (in its standard binary representation).Because a length is a positive integer and all positive integers begin with '1', all the leading 1s can be omitted.The encoder of C ω is shown in algorithm 7.4.The encoding is generated from right to left.Table 7.5 shows the resulting codewords.Exercise 7.3. [2 ]Show that the Elias code is not actually the best code for a prior distribution that expects very large integers.(Do this by constructing another code and specifying how large n must be for your code to give a shorter length than Elias's.)Solution to exercise 7.1 (p.134).The use of the end-of-file symbol in a code that represents the integer in some base q corresponds to a belief that there is a probability of (1/(q + 1)) that the current character is the last character of the number.Thus the prior to which this code is matched puts an exponential prior distribution over the length of the integer.(a) The expected number of characters is q +1 = 256, so the expected length of the integer is 256 × 8 2000 bits.(b) We wish to find q such that q log q 800 000 bits.A value of q between 2 15 and 2 16 satisfies this constraint, so 16-bit blocks are roughly the optimal size, assuming there is one end-of-file character.Noisy-Channel Coding 8Dependent Random VariablesIn the last three chapters on data compression we concentrated on random vectors x coming from an extremely simple probability distribution, namely the separable distribution in which each component x n is independent of the others.In this chapter, we consider joint ensembles in which the random variables are dependent.This material has two motivations.First, data from the real world have interesting correlations, so to do data compression well, we need to know how to work with models that include dependences.Second, a noisy channel with input x and output y defines a joint ensemble in which x and y are dependent -if they were independent, it would be impossible to communicate over the channel -so communication over noisy channels (the topic of chapters 9-11) is described in terms of the entropy of joint ensembles.This section gives definitions and exercises to do with entropy, carrying on from section 2.4.The joint entropy of X, Y is:Entropy is additive for independent random variables:The conditional entropy of X given y = b k is the entropy of the probability distributionThe conditional entropy of X given Y is the average, over y, of the conditional entropy of X given y.This measures the average uncertainty that remains about x when y is known.The marginal entropy of X is another name for the entropy of X, H(X), used to contrast it with the conditional entropies listed above.Chain rule for information content.From the product rule for probabilities, equation (2.6), we obtain:In words, this says that the information content of x and y is the information content of x plus the information content of y given x.Chain rule for entropy.The joint entropy, conditional entropy and marginal entropy are related by:In words, this says that the uncertainty of X and Y is the uncertainty of X plus the uncertainty of Y given X.The mutual information between X and Y isand satisfies I(X; Y ) = I(Y ; X), and I(X; Y ) ≥ 0. It measures the average reduction in uncertainty about x that results from learning the value of y; or vice versa, the average amount of information that x conveys about y.The conditional mutual information between X and Y given z = c k is the mutual information between the random variables X and Y in the joint ensemble P (x, y | z = c k ),The conditional mutual information between X and Y given Z is the average over z of the above conditional mutual information.The relationship between joint information, marginal entropy, conditional entropy and mutual entropy.Exercise 8.1. [1 ]Consider three independent random variables u, v, w with entropiesp.142] Referring to the definitions of conditional entropy (8.3-8.4),confirm (with an example) that it is possible for H(X | y = b k ) to exceed H(X), but that the average, H(X | Y ), is less than H(X).So data are helpful -they do not increase uncertainty, on average.p.143] Prove the chain rule for entropy, equation (8.7).[p.143] Prove that the mutual information[Hint: see exercise 2.26 (p.37) and note that I(X; Y ) = D KL (P (x, y)||P (x)P (y)).](8.11)Exercise 8.5. [4 ]The 'entropy distance' between two random variables can be defined to be the difference between their joint entropy and their mutual information:Prove that the entropy distance satisfies the axioms for a distance -[Incidentally, we are unlikely to see D H (X, Y ) again but it is a good function on which to practise inequality-proving.]p.147]A joint ensemble XY has the following joint distribution.p.143] Consider the ensemble XY Z in which A X = A Y = A Z = {0, 1}, x and y are independent with P X = {p, 1 − p} and P Y = {q, 1−q} and z = (x + y) mod 2. (8.13)(b) For general p and q, what is P Z ?What is I(Z; X)? Notice that this ensemble is related to the binary symmetric channel, with x = input, y = noise, and z = output.p.143]Many texts draw figure 8.1 in the form of a Venn diagram (figure 8.2).Discuss why this diagram is a misleading representation of entropies.Hint: consider the three-variable ensemble XY Z in which x ∈ {0, 1} and y ∈ {0, 1} are independent binary variables and z ∈ {0, 1} is defined to be z = x + y mod 2.The data-processing theoremThe data processing theorem states that data processing can only destroy information.p.144] Prove this theorem by considering an ensemble W DR in which w is the state of the world, d is data gathered, and r is the processed data, so that these three variables form a Markov chainthat is, the probability P (w, d, r) can be written asShow that the average information that R conveys about W, I(W ; R), is less than or equal to the average information that D conveys about W , I(W ; D).This theorem is as much a caution about our definition of 'information' as it is a caution about data processing!Exercise 8.10. [2 ]The three cards.(a) One card is white on both faces; one is black on both faces; and one is white on one side and black on the other.The three cards are shuffled and their orientations randomized.One card is drawn and placed on the table.The upper face is black.What is the colour of its lower face?(Solve the inference problem.)(b) Does seeing the top face convey information about the colour of the bottom face?Discuss the information contents and entropies in this situation.Let the value of the upper face's colour be u and the value of the lower face's colour be l.Imagine that we draw a random card and learn both u and l.What is the entropy of u, H(U )?What is the entropy of l, H(L)?What is the mutual information between U and L, I(U ; L)?Exercise 8.11. [3 ]In the guessing game, we imagined predicting the next letter in a document starting from the beginning and working towards the end.Consider the task of predicting the reversed text, that is, predicting the letter that precedes those already known.Most people find this a harder task.Assuming that we model the language using an N -gram model (which says the probability of the next character depends only on the N − 1 preceding characters), is there any difference between the average information contents of the reversed language and the forward language?Solution to exercise 8.2 (p.140).See exercise 8.6 (p.140) for an example where H(X | y) exceeds H(X) (set y = 3).We can prove the inequality H(X | Y ) ≤ H(X) by turning the expression into a relative entropy (using Bayes' theorem) and invoking Gibbs' inequality (exercise 2.26 (p.37)):The last expression is a sum of relative entropies between the distributions P (y | x) and P (y).Sowith equality only if P (y | x) = P (y) for all x and y (that is, only if X and Y are independent).Solution to exercise 8.3 (p.140).The chain rule for entropy follows from the decomposition of a joint probability:Solution to exercise 8.4 (p.140).Symmetry of mutual information:This expression is symmetric in x and y soWe can prove that mutual information is positive in two ways.One is to continue from I(X; Y ) =x,y P (x, y) log P (x, y) P (x)P (y) (8.29)which is a relative entropy and use Gibbs' inequality (proved on p.44), which asserts that this relative entropy is ≥ 0, with equality only if P (x, y) = P (x)P (y), that is, if X and Y are independent.The other is to use Jensen's inequality on −x,y P (x, y) log P (x)P (y) P (x, y) ≥ − logx,y P (x, y) P (x, y) P (x)P (y) = log 1 = 0. (8.30)Solution to exercise 8.7 (p.141).z = x + y mod 2.(a(b) For general q and p,Solution to exercise 8.8 (p.141).The depiction of entropies in terms of Venn diagrams is misleading for at least two reasons.First, one is used to thinking of Venn diagrams as depicting sets; but what are the 'sets' H(X) and H(Y ) depicted in figure 8.2, and what are the objects that are members of those sets?I think this diagram encourages the novice student to make inappropriate analogies.For example, some students imagineH(X,Y|Z) that the random outcome (x, y) might correspond to a point in the diagram, and thus confuse entropies with probabilities.Secondly, the depiction in terms of Venn diagrams encourages one to believe that all the areas correspond to positive quantities.In the special case of two random variables it is indeed true that H(X | Y ), I(X; Y ) and H(Y | X) are positive quantities.But as soon as we progress to three-variable ensembles, we obtain a diagram with positive-looking areas that may actually correspond to negative quantities. Figure 8.3 correctly shows relationships such as(8.31)But it gives the misleading impression that the conditional mutual information I(X; Y | Z) is less than the mutual information I(X; Y ).In fact the area labelled A can correspond to a negative quantity.Consider the joint ensemble (X, Y, Z) in which x ∈ {0, 1} and y ∈ {0, 1} are independent binary variables and z ∈ {0, 1} is defined to be z = x + y mod 2. Then clearly The above example is not at all a capricious or exceptional illustration.The binary symmetric channel with input X, noise Y , and output Z is a situation in which I(X; Y ) = 0 (input and noise are independent) but I(X; Y | Z) > 0 (once you see the output, the unknown input and the unknown noise are intimately related!).The Venn diagram representation is therefore valid only if one is aware that positive areas may represent negative quantities.With this proviso kept in mind, the interpretation of entropies in terms of sets can be helpful (Yeung, 1991).Solution to exercise 8.9 (p.141).For any joint ensemble XY Z, the following chain rule for mutual information holds.In Chapters 4-6, we discussed source coding with block codes, symbol codes and stream codes.We implicitly assumed that the channel from the compressor to the decompressor was noise-free.Real channels are noisy.We will now spend two chapters on the subject of noisy-channel coding -the fundamental possibilities and limitations of error-free communication through a noisy channel.The aim of channel coding is to make the noisy channel behave like a noiseless channel.We will assume that the data to be transmitted has been through a good compressor, so the bit stream has no obvious redundancy.The channel code, which makes the transmission, will put back redundancy of a special sort, designed to make the noisy received signal decodeable.Suppose we transmit 1000 bits per second with p 0 = p 1 = 1/ 2 over a noisy channel that flips bits with probability f = 0.1.What is the rate of transmission of information?We might guess that the rate is 900 bits per second by subtracting the expected number of errors per second.But this is not correct, because the recipient does not know where the errors occurred.Consider the case where the noise is so great that the received symbols are independent of the transmitted symbols.This corresponds to a noise level of f = 0.5, since half of the received symbols are correct due to chance alone.But when f = 0.5, no information is transmitted at all.Given what we have learnt about entropy, it seems reasonable that a measure of the information transmitted is given by the mutual information between the source and the received signal, that is, the entropy of the source minus the conditional entropy of the source given the received signal.We will now review the definition of conditional entropy and mutual information.Then we will examine whether it is possible to use such a noisy channel to communicate reliably.We will show that for any channel Q there is a non-zero rate, the capacity C(Q), up to which information can be sent 9.2: Review of probability and information 147with arbitrarily small probability of error.As an example, we take the joint distribution XY from exercise 8.6 (p.140).The marginal distributions P (x) and P (y) are shown in the margins.P (x, y) x P (y)The joint entropy is H(X, Y ) = 27/8 bits.The marginal entropies are H(X) = 7/4 bits and H(Y ) = 2 bits.We can compute the conditional distribution of x for each value of y, and the entropy of each of those conditional distributions:Note that whereas H(X | y = 4) = 0 is less than H(X), H(X | y = 3) is greater than H(X).So in some cases, learning y can increase our uncertainty about x.Note also that although P (x | y = 2) is a different distribution from P (x), the conditional entropy H(X | y = 2) is equal to H(X).So learning that y is 2 changes our knowledge about x but does not reduce the uncertainty of x, as measured by the entropy.On average though, learning y does convey information about x, since H(X | Y ) < H(X).One may also evaluate H(Y |X) = 13/8 bits.The mutual information isA discrete memoryless channel Q is characterized by an input alphabet A X , an output alphabet A Y , and a set of conditional probability distributions P (y | x), one for each x ∈ A X .These transition probabilities may be written in a matrixI usually orient this matrix with the output variable j indexing the rows and the input variable i indexing the columns, so that each column of Q is a probability vector.With this convention, we can obtain the probability of the output, p Y , from a probability distribution over the input, p X , by right-multiplication:xIf we assume that the input x to a channel comes from an ensemble X, then we obtain a joint ensemble XY in which the random variables x and y have the joint distribution: P (x, y) = P (y | x)P (x).( 9.3)Now if we receive a particular symbol y, what was the input symbol x?We typically won't know for certain.We can write down the posterior distribution of the input using Bayes' theorem:x P (y | x )P (x ).(9.4)Example 9.1.Consider a binary symmetric channel with probability of error f = 0.15.Let the input ensemble be P X : {p 0 = 0.9, p 1 = 0.1}.Assume we observe y = 1.P (x = 1 | y = 1) = P (y = 1 | x = 1)P (x = 1)x P (y | x )P (x ) = 0.85 × 0.1 0.85 × 0.1 + 0.15 × 0.9 = 0.085 0.22 = 0.39.(9.5)Thus 'x = 1' is still less probable than 'x = 0', although it is not as improbable as it was before.p.157] Now assume we observe y = 0. Compute the probability of x = 1 given y = 0.Example 9.3.Consider a Z channel with probability of error f = 0.15.Let the input ensemble be P X : {p 0 = 0.9, p 1 = 0.1}.Assume we observe y = 1.P (x = 1 | y = 1) = 0.85 × 0.1 0.85 × 0.1 + 0 × 0.9 = 0.085 0.085 = 1.0.(9.6)So given the output y = 1 we become certain of the input.p.157]Alternatively, assume we observe y = 0. Compute P (x = 1 | y = 0).We now consider how much information can be communicated through a channel.In operational terms, we are interested in finding ways of using the channel such that all the bits that are communicated are recovered with negligible probability of error.In mathematical terms, assuming a particular input ensemble X, we can measure how much information the output conveys about the input by the mutual information:(9.7)Our aim is to establish the connection between these two ideas.Let us evaluate I(X; Y ) for some of the channels above.We will tend to think of I(X; Y ) as H(X) − H(X | Y ), i.e., how much the uncertainty of the input X is reduced when we look at the output Y .But for computational purposes it is often handy to evaluate Example 9.5.Consider the binary symmetric channel again, with f = 0.15 and P X : {p 0 = 0.9, p 1 = 0.1}.We already evaluated the marginal probabilities P (y) implicitly above: P (y = 0) = 0.78; P (y = 1) = 0.22.The mutual information is:  (9.8)This may be contrasted with the entropy of the source H(X) = H 2 (0.1) = 0.47 bits.Note: here we have used the binary entropy functionThroughout this book, log means log 2 .Example 9.6.And now the Z channel, with P X as above.P (y = 1) = 0.085.= 0.42 − (0.1 × 0.61) = 0.36 bits.(9.9)The entropy of the source, as above, is H(X) = 0.47 bits.Notice that the mutual information I(X; Y ) for the Z channel is bigger than the mutual information for the binary symmetric channel with the same f .The Z channel is a more reliable channel.p.157] Compute the mutual information between X and Y for the binary symmetric channel with f = 0.15 when the input distribution is P X = {p 0 = 0.5, p 1 = 0.5}.p.157] Compute the mutual information between X and Y for the Z channel with f = 0.15 when the input distribution is P X : {p 0 = 0.5, p 1 = 0.5}.We have observed in the above examples that the mutual information between the input and the output depends on the chosen input ensemble.Let us assume that we wish to maximize the mutual information conveyed by the channel by choosing the best possible input ensemble.We define the capacity of the channel to be its maximum mutual information.The capacity of a channel Q is:(9.10)The distribution P X that achieves the maximum is called the optimal input distribution, denoted by P * X .[There may be multiple optimal input distributions achieving the same value of I(X; Y ).]In Chapter 10 we will show that the capacity does indeed measure the maximum amount of error-free information that can be transmitted over the channel per unit time.Example 9.9.Consider the binary symmetric channel with f = 0.15.Above, we considered P X = {p 0 = 0.9, p 1 = 0.1}, and found I(X; Y ) = 0.15 bits.How much better can we do?By symmetry, the optimal input distribution is {0.5, 0.5} and the capacity is C(Q BSC ) = H 2 (0.5) − H 2 (0.15) = 1.0 − 0.61 = 0.39 bits.(9.11)We'll justify the symmetry argument later.If there's any doubt about the symmetry argument, we can always resort to explicit maximization of the mutual information I(X; Y ),(9.12)Example 9.10.The noisy typewriter.The optimal input distribution is a uniform distribution over x, and gives C = log 2 9 bits.Example 9.11.Consider the Z channel with f = 0.15.Identifying the optimal input distribution is not so straightforward.We evaluate I(X; Y ) explicitly for P X = {p 0 , p 1 }.First, we need to compute P (y).The probability of y = 1 is easiest to write down:Then the mutual information is:This is a non-trivial function of p 1 , shown in figure 9.3.It is maximized for f = 0.15 by p * 1 = 0.445.We find C(Q Z ) = 0.685.Notice the optimal input distribution is not {0.5, 0.5}.We can communicate slightly more information by using input symbol 0 more frequently than 1.p.158] What is the capacity of the binary symmetric channel for general f ?p.158] Show that the capacity of the binary erasure channel with f = 0.15 is C BEC = 0.85.What is its capacity for general f ?Comment.It seems plausible that the 'capacity' we have defined may be a measure of information conveyed by a channel; what is not obvious, and what we will prove in the next chapter, is that the capacity indeed measures the rate at which blocks of data can be communicated over the channel with arbitrarily small probability of error.We make the following definitions.each of length N .Using this code we can encode a signal s ∈ {1, 2, 3, . . ., 2 K } as x (s) .[The number of codewords S is an integer, but the number of bits specified by choosing a codeword, K ≡ log 2 S, is not necessarily an integer.] 9 -Communication over a Noisy ChannelThe rate of the code is R = K/N bits per channel use.[We will use this definition of the rate for any channel, not only channels with binary inputs; note however that it is sometimes conventional to define the rate of a code for a channel with q input symbols to be K/(N log q).]A decoder for an (N, K) block code is a mapping from the set of length-N strings of channel outputs, A N Y , to a codeword label ŝ ∈ {0, 1, 2, . . ., 2 K }.The extra symbol ŝ = 0 can be used to indicate a 'failure'.The probability of block error of a code and decoder, for a given channel, and for a given probability distribution over the encoded signal P (s in ), is:The maximal probability of block error isThe optimal decoder for a channel code is the one that minimizes the probability of block error.It decodes an output y as the input s that has maximum posterior probability P (s | y).A uniform prior distribution on s is usually assumed, in which case the optimal decoder is also the maximum likelihood decoder, i.e., the decoder that maps an output y to the input s that has maximum likelihood P (y | s).The probability of bit error p b is defined assuming that the codeword number s is represented by a binary vector s of length K bits; it is the average probability that a bit of s out is not equal to the corresponding bit of s in (averaging over all K bits).Shannon's noisy-channel coding theorem (part one).Associated with each discrete memoryless channel, there is a non-negative number C (called the channel capacity) with the following property.For any > 0 and R < C, for large enough N , there exists a block code of length N and rate ≥ R and a decoding algorithm, such that the maximal probability of block error is < .In the case of the noisy typewriter, we can easily confirm the theorem, because we can create a completely error-free communication strategy using a block code of length N = 1: we use only the letters B, E, H, . . ., Z, i.e., every third letter.These letters form a non-confusable subset of the input alphabet (see figure 9.5).Any output can be uniquely decoded.The number of inputs in the non-confusable subset is 9, so the error-free information rate of this system is log 2 9 bits, which is equal to the capacity C, which we evaluated in example 9.10 (p.151).9.7: Intuitive preview of proof 153 How does this translate into the terms of the theorem?The following table explains.How it applies to the noisy typewriter Associated with each discrete memoryless channel, there is a non-negative number C.The capacity C is log 2 9.For any > 0 and R < C, for large enough N ,No matter what and R are, we set the blocklength N to 1.there exists a block code of length N and rate ≥ R The block code is {B, E, . . ., Z}.The value of K is given by 2 K = 9, so K = log 2 9, and this code has rate log 2 9, which is greater than the requested value of R.and a decoding algorithm, The decoding algorithm maps the received letter to the nearest letter in the code;such that the maximal probability of block error is < .the maximal probability of block error is zero, which is less than the given .To prove the theorem for any given channel, we consider the extended channel corresponding to N uses of the channel.p.159] Find the transition probability matrices Q for the extended channel, with N = 2, derived from the binary erasure channel having erasure probability 0.15.By selecting two columns of this transition probability matrix, we can define a rate-1/ 2 code for this channel with blocklength N = 2. What is the best choice of two columns?What is the decoding algorithm?To prove the noisy-channel coding theorem, we make use of large blocklengths N .The intuitive idea is that, if N is large, an extended channel looks a lot like the noisy typewriter.Any particular input x is very likely to produce an output in a small subspace of the output alphabet -the typical output set, given that input.So we can find a non-confusable subset of the inputs that produce essentially disjoint output sequences.For a given N , let us consider a way of generating such a non-confusable subset of the inputs, and count up how many distinct inputs it contains.Imagine making an input sequence x for the extended channel by drawing it from an ensemble X N , where X is an arbitrary ensemble over the input alphabet.Recall the source coding theorem of Chapter 4, and consider the number of probable output sequences y.The total number of typical output sequences y is 2 N H(Y ) , all having similar probability.For any particular typical input sequence x, there are about 2 N H(Y |X) probable sequences.Some of these subsets of A N Y are depicted by circles in figure 9.8a.We now imagine restricting ourselves to a subset of the typical inputs x such that the corresponding typical output sets do not overlap, as shown in figure 9.8b.We can then bound the number of non-confusable inputs by dividing the size of the typical y set, 2 N H(Y ) , by the size of each typical-y-9.8:Further exercises 155given-typical-x set, 2 N H(Y |X) .So the number of non-confusable inputs, if they are selected from the set of typical inputsThe maximum value of this bound is achieved if X is the ensemble that maximizes I(X; Y ), in which case the number of non-confusable inputs is ≤ 2 N C .Thus asymptotically up to C bits per cycle, and no more, can be communicated with vanishing error probability.2 This sketch has not rigorously proved that reliable communication really is possible -that's our task for the next chapter.p.159] Refer back to the computation of the capacity of the Z channel with f = 0.15.(a) Why is p * 1 less than 0.5?One could argue that it is good to favour the 0 input, since it is transmitted without error -and also argue that it is good to favour the 1 input, since it often gives rise to the highly prized 1 output, which allows certain identification of the input!Try to make a convincing argument.(b) In the case of general f , show that the optimal input distribution isp.159] Sketch graphs of the capacity of the Z channel, the binary symmetric channel and the binary erasure channel as a function of f .Exercise 9.17. [2 ]p.159] Consider a Gaussian channel with binary input x ∈ {−1, +1} and real output alphabet A Y , with transition probability density (9.21)where α is the signal amplitude.(a) Compute the posterior probability of x given y, assuming that the two inputs are equiprobable.Put your answer in the form y) .(9.22) Pattern recognition as a noisy channelWe may think of many pattern recognition problems in terms of communication channels.Consider the case of recognizing handwritten digits (such as postcodes on envelopes).The author of the digit wishes to communicate a message from the set A X = {0, 1, 2, 3, . . ., 9}; this selected message is the input to the channel.What comes out of the channel is a pattern of ink on paper.If the ink pattern is represented using 256 binary pixels, the channel Q has as its output a random variable y ∈ A Y = {0, 1} 256 .An example of an element from this alphabet is shown in the margin.Exercise 9.19. [2 ]Estimate how many patterns in A Y are recognizable as the character '2'.[The aim of this problem is to try to demonstrate the existence of as many patterns as possible that are recognizable as 2s.]This strategy is known as full probabilistic modelling or generative modelling.This is essentially how current speech recognition systems work.In addition to the channel model, P (y | x), one uses a prior probability distribution P (x), which in the case of both character recognition and speech recognition is a language model that specifies the probability of the next character/word given the context and the known grammar and statistics of the language.p.160] Given twenty-four people in a room, what is the probability that there are at least two people present who have the same birthday (i.e., day and month of birth)?What is the expected number of pairs of people with the same birthday?Which of these two questions is easiest to solve?Which answer gives most insight?You may find it helpful to solve these problems and those that follow using notation such as A = number of days in year = 365 and S = number of people = 24.Exercise 9.21. [2 ]The birthday problem may be related to a coding scheme.Assume we wish to convey a message to an outsider identifying one of 9.9: Solutions 157 the twenty-four people.We could simply communicate a number s from A S = {1, 2, . . ., 24}, having agreed a mapping of people onto numbers; alternatively, we could convey a number from A X = {1, 2, . . ., 365}, identifying the day of the year that is the selected person's birthday (with apologies to leapyearians).[The receiver is assumed to know all the people's birthdays.]What, roughly, is the probability of error of this communication scheme, assuming it is used for a single transmission?What is the capacity of the communication channel, and what is the rate of communication attempted by this scheme?Exercise 9.22. [2 ]Now imagine that there are K rooms in a building, each containing q people.(You might think of K = 2 and q = 24 as an example.)The aim is to communicate a selection of one person from each room by transmitting an ordered list of K days (from A X ).Compare the probability of error of the following two schemes.(a) As before, where each room transmits the birthday of the selected person.(b) To each K-tuple of people, one drawn from each room, an ordered K-tuple of randomly selected days from A X is assigned (this Ktuple has nothing to do with their birthdays).This enormous list of S = q K strings is known to the receiver.When the building has selected a particular person from each room, the ordered string of days corresponding to that K-tuple of people is transmitted.What is the probability of error when q = 364 and K = 1?What is the probability of error when q = 364 and K is large, e.g.K = 6000?9.9 Solutions Solution to exercise 9.7 (p.150).The probability that y = 1 is 0.5, so the mutual information is: Solution to exercise 9.12 (p.151).By symmetry, the optimal input distribution is {0.5, 0.5}.Then the capacity isWould you like to find the optimal input distribution without invoking symmetry?We can do this by computing the mutual information in the general case where the input ensemble is {p 0 , p 1 }:The only p-dependence is in the first term H 2 (p 0 f + p 1 (1 − f )), which is maximized by setting the argument to 0.5.This value is given by setting p 0 = 1/2.Solution to exercise 9.13 (p.151).Answer 1.By symmetry, the optimal input distribution is {0.5, 0.5}.The capacity is most easily evaluated by writing the mutual information as I(X; Y ) = H(X) − H(X | Y ).The conditional entropy H(X | Y ) is y P (y)H(X | y); when y is known, x is uncertain only if y = ?, which occurs with probability f /2 + f /2, so the conditional entropyThe binary erasure channel fails a fraction f of the time.Its capacity is precisely 1 − f , which is the fraction of the time that the channel is reliable.This result seems very reasonable, but it is far from obvious how to encode information so as to communicate reliably over this channel.Answer 2. Alternatively, without invoking the symmetry assumed above, we can start from the input ensemble {p 0 , p 1 }.The probability that y = ?is p 0 f + p 1 f = f , and when we receive y = ?, the posterior probability of x is the same as the prior probability, so: (9.46)This mutual information achieves its maximum value of (1−f ) when p 1 = 1/2.x (1) x ( 2) Solution to exercise 9.14 (p.153).The extended channel is shown in figure 9.10.The best code for this channel with N = 2 is obtained by choosing two columns that have minimal overlap, for example, columns 00 and 11.The decoding algorithm returns '00' if the extended channel output is among the top four and '11' if it's among the bottom four, and gives up if the output is '??'.Solution to exercise 9.15 (p.155).In example 9.11 (p.151) we showed that the mutual information between input and output of the Z channel is(9.47)We differentiate this expression with respect to p 1 , taking care not to confuse log 2 with log e : (9.48) Setting this derivative to zero and rearranging using skills developed in exercise 2.17 (p.36), we obtain: (9.49) so the optimal input distribution isAs the noise level f tends to 1, this expression tends to 1/e (as you can prove using L'Hôpital's rule).For all values of f, p * 1 is smaller than 1/2.A rough intuition for why input 1 is used less than input 0 is that when input 1 is used, the noisy channel injects entropy into the received string; whereas when input 0 is used, the noise has zero entropy.Solution to exercise 9. 16 (p.155).The capacities of the three channels are shown in figure 9.11.For any f < 0.5, the BEC is the channel with highest capacity and the BSC the lowest.Solution to exercise 9. 18 (p.155).The logarithm of the posterior probability ratio, given y, is a codeword, the sth in the code s the number of a chosen codeword (mnemonic: the source selects s) S = 2 K the total number of codewords in the code K = log 2 S the number of bits conveyed by the choice of one codeword from S, assuming it is chosen with uniform probability s a binary representation of the number s R = K/N the rate of the code, in bits per channel use (sometimes called R instead) ŝ the decoder's guess of s 10The Noisy-Channel Coding TheoremThe theorem has three parts, two positive and one negative.The main positive result is the first.1.For every discrete memoryless channel, the channel capacityhas the following property.For any > 0 and R < C, for large enough N , there exists a code of length N and rate ≥ R and a decoding algorithm, such that the maximal probability of block error is < .2. If a probability of bit error p b is acceptable, rates up to R(p b ) are achievable, where.(10.2) 3.For any p b , rates greater than R(p b ) are not achievable.We formalize the intuitive preview of the last chapter.We will define codewords x (s) as coming from an ensemble X N , and consider the random selection of one codeword and a corresponding channel output y, thus defining a joint ensemble (XY ) N .We will use a typical-set decoder, which decodes a received signal y as s if x (s) and y are jointly typical, a term to be defined shortly.The proof will then centre on determining the probabilities (a) that the true input codeword is not jointly typical with the output sequence; and (b) that a false input codeword is jointly typical with the output.We will show that, for large N , both probabilities go to zero as long as there are fewer than 2 N C codewords, and the ensemble X is the optimal input distribution.Joint typicality.A pair of sequences x, y of length N are defined to be jointly typical (to tolerance β) with respect to the distribution P (x, y) ifx is typical of P (x), i.e., 1y is typical of P (y), i.e., 1and x, y is typical of P (x, y), i.e., 1The jointly-typical set J N β is the set of all jointly-typical sequence pairs of length N .Example.Here is a jointly-typical pair of length N = 100 for the ensemble P (x, y) in which P (x) has (p 0 , p 1 ) = (0.9, 0.1) and P (y | x) corresponds to a binary symmetric channel with noise level 0.2.x 1111111111000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000 y 0011111111000000000000000000000000000000000000000000000000000000000000000000000000111111111111111111Notice that x has 10 1s, and so is typical of the probability P (x) (at any tolerance β); and y has 26 1s, so it is typical of P (y) (because P (y = 1) = 0.26); and x and y differ in 20 bits, which is the typical number of flips for this channel.Joint typicality theorem.Let x, y be drawn from the ensemble (XY ) N defined byThen 1. the probability that x, y are jointly typical (to tolerance β) tends to 1 as N → ∞; 2. the number of jointly-typical sequencesTo be precise,x and y are independent samples with the same marginal distribution as P (x, y), then the probability that (x , y ) lands in the jointly-typical set is about 2 −N I(X;Y ) .To be precise, P ((x , y ) ∈ J N β ) ≤ 2 −N (I(X;Y )−3β) .(10.4)Proof.The proof of parts 1 and 2 by the law of large numbers follows that of the source coding theorem in Chapter 4. For part 2, let the pair x, y play the role of x in the source coding theorem, replacing P (x) there by the probability distribution P (x, y).For the third part,A cartoon of the jointly-typical set is shown in figure 10.2.Two independent typical vectors are jointly typical with probabilitybecause the total number of independent typical pairs is the area of the dashed rectangle, 2 N H(X) 2 N H(Y ) , and the number of jointly-typical pairs is roughly 2 N H(X,Y ) , so the probability of hitting a jointly-typical pair is roughlyq q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q 2 N H(X,Y ) dots.2.The jointly-typical set.The horizontal direction represents A N X , the set of all input strings of length N .The vertical direction represents A N Y , the set of all output strings of length N .The outer box contains all conceivable input-output pairs.Each dot represents a jointly-typical pair of sequences (x, y).The total number of jointly-typical sequences is about 2 N H(X,Y ) .Imagine that we wish to prove that there is a baby in a class of one hundred babies who weighs less than 10 kg.Individual babies are difficult to catch and weigh.Shannon's method of solving the task is to scoop up all the babies and weigh them all at once on a big weighing machine.If we find that their average weight is smaller than 10 kg, there must exist at least one baby who weighs less than 10 kg -indeed there must be many!Shannon's method isn't guaranteed to reveal the existence of an underweight child, since it relies on there being a tiny number of elephants in the class.But if we use his method and get a total weight smaller than 1000 kg then our task is solved.We wish to show that there exists a code and a decoder having small probability of error.Evaluating the probability of error of any particular coding and decoding system is not easy.Shannon's innovation was this: instead of constructing a good coding and decoding system and evaluating its error probability, Shannon calculated the average probability of block error of all codes, and proved that this average is small.There must then exist individual codes that have small probability of block error.Consider the following encoding-decoding system, whose rate is R . 1.We fix P (x) and generate the S = 2 N R codewords of a (N, N R ) = 10.3:Proof of the noisy-channel coding theorem 165x (2) x (4) q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q x (3) x (1) x (2) x (4)ŝ(y a ) = 0 q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q (a) (b) A sequence that is not jointly typical with any of the codewords, such as y a , is decoded as ŝ = 0.A sequence that is jointly typical with codeword x (3) alone, y b , is decoded as ŝ = 3.Similarly, y c is decoded as ŝ = 4.A sequence that is jointly typical with more than one codeword, such as y d , is decoded as ŝ = 0.(N, K) code C at random according to(10.11)A random code is shown schematically in figure 10.4a.2. The code is known to both sender and receiver.3. A message s is chosen from {1, 2, . . ., 2 N R }, and x (s) is transmitted.The received signal is y, with(10.12)4. The signal is decoded by typical-set decoding.Typical-set decoding.Decode y as ŝ if (x (ŝ) , y) are jointly typical and there is no other s such that (x (s ) , y) are jointly typical; otherwise declare a failure (ŝ = 0).This is not the optimal decoding algorithm, but it will be good enough, and easier to analyze.The typical-set decoder is illustrated in figure 10.4b.There are three probabilities of error that we can distinguish.First, there is the probability of block error for a particular code C, that is,(10.13)This is a difficult quantity to evaluate for any given code.Second, there is the average over all codes of this block error probability, Fortunately, this quantity is much easier to evaluate than the first quantity P (ŝ = s | C).p B is just the probability that there is a decoding error at step 5 of the five-step process on the previous page.Third, the maximal block error probability of a code C, (10.15) is the quantity we are most interested in: we wish to show that there exists a code C with the required rate whose maximal block error probability is small.We will get to this result by first finding the average block error probability, p B .Once we have shown that this can be made smaller than a desired small number, we immediately deduce that there must exist at least one code C whose block error probability is also less than this small number.Finally, we show that this code, whose block error probability is satisfactorily small but whose maximal block error probability is unknown (and could conceivably be enormous), can be modified to make a code of slightly smaller rate whose maximal block error probability is also guaranteed to be small.We modify the code by throwing away the worst 50% of its codewords.We therefore now embark on finding the average probability of block error.There are two sources of error when we use typical-set decoding.Either (a) the output y is not jointly typical with the transmitted codeword x (s) , or (b) there is some other codeword in C that is jointly typical with y.By the symmetry of the code construction, the average probability of error averaged over all codes does not depend on the selected value of s; we can assume without loss of generality that s = 1.(a) The probability that the input x (1) and the output y are not jointly typical vanishes, by the joint typicality theorem's first part (p.163).We give a name, δ, to the upper bound on this probability, satisfying δ → 0 as N → ∞; for any desired δ, we can find a blocklength N (δ) such that the P ((x (1) , y) ∈ J N β ) ≤ δ.(b) The probability that x (s ) and y are jointly typical, for a given s = 1 is ≤ 2 −N (I(X;Y )−3β) , by part 3.And there are (2 N R − 1) rival values of s to worry about.Thus the average probability of error p B satisfies:(10.17)The inequality (10.16) that bounds a total probability of error P TOT by the sum of the probabilities P s of all sorts of events s each of which is sufficient to cause error,It is only an equality if the different events that cause error never occur at the same time as each other.The average probability of error (10.17) can be made < 2δ by increasing N if .18)We are almost there.We make three modifications:1. We choose P (x) in the proof to be the optimal input distribution of the channel.Then the condition R < I(X; Y ) − 3β becomes R < C − 3β.The resulting code has slightly fewer codewords, so has a slightly lower rate, and its maximal probability of error is greatly reduced.2. Since the average probability of error over all codes is < 2δ, there must exist a code with mean probability of block error p B (C) < 2δ.3. To show that not only the average but also the maximal probability of error, p BM , can be made small, we modify this code by throwing away the worst half of the codewords -the ones most likely to produce errors.Those that remain must all have conditional probability of error less than 4δ.We use these remaining codewords to define a new code.This new code has 2 N R −1 codewords, i.e., we have reduced the rate from R to R − 1/ N (a negligible reduction, if N is large), and achieved p BM < 4δ.This trick is called expurgation (figure 10.5).The resulting code may not be the best code of its rate and length, but it is still good enough to prove the noisy-channel coding theorem, which is what we are trying to do here.In conclusion, we can 'construct' a code of rate R − 1/ N , where R < C − 3β, with maximal probability of error < 4δ.We obtain the theorem as stated by setting R = (R + C)/2, δ = /4, β < (C − R )/3, and N sufficiently large for the remaining conditions to hold.We have proved, for any discrete memoryless channel, the achievability of a portion of the R, p b plane shown in figure 10.6.We have shown that we can turn any noisy channel into an essentially noiseless binary channel with rate up to C bits per cycle.We now extend the right-hand boundary of the region of achievability at non-zero error probabilities.[This is called rate-distortion theory.]We do this with a new trick.Since we know we can make the noisy channel into a perfect channel with a smaller rate, it is sufficient to consider communication with errors over a noiseless channel.How fast can we communicate over a noiseless channel, if we are allowed to make errors?Consider a noiseless binary channel, and assume that we force communication at a rate greater than its capacity of 1 bit.For example, if we require the sender to attempt to communicate at R = 2 bits per cycle then he must effectively throw away half of the information.What is the best way to do this if the aim is to achieve the smallest possible probability of bit error?One simple strategy is to communicate a fraction 1/R of the source bits, and ignore the rest.The receiver guesses the missing fraction 1 − 1/R at random, and 10 -The Noisy-Channel Coding Theorem the average probability of bit error isThe curve corresponding to this strategy is shown by the dashed line in figure 10.7.We can do better than this (in terms of minimizing p b ) by spreading out the risk of corruption evenly among all the bits.In fact, we can achieve can this optimum be achieved?We reuse a tool that we just developed, namely the (N, K) code for a noisy channel, and we turn it on its head, using the decoder to define a lossy compressor.Specifically, we take an excellent (N, K) code for the binary symmetric channel.Assume that such a code has a rate R = K/N , and that it is capable of correcting errors introduced by a binary symmetric channel whose transition probability is q.Asymptotically, rate-R codes exist that have R 1 − H 2 (q).Recall that, if we attach one of these capacity-achieving codes of length N to a binary symmetric channel then (a) the probability distribution over the outputs is close to uniform, since the entropy of the output is equal to the entropy of the source (N R ) plus the entropy of the noise (N H 2 (q)), and (b) the optimal decoder of the code, in this situation, typically maps a received vector of length N to a transmitted vector differing in qN bits from the received vector.We take the signal that we wish to send, and chop it into blocks of length N (yes, N , not K).We pass each block through the decoder, and obtain a shorter signal of length K bits, which we communicate over the noiseless channel.To decode the transmission, we pass the K bit message to the encoder of the original code.The reconstituted message will now differ from the original message in some of its bits -typically qN of them.So the probability of bit error will be p b = q.The rate of this lossy compressor is R =Now, attaching this lossy compressor to our capacity-C error-free communicator, we have proved the achievability of communication up to the curve (p b , R) defined by:For further reading about rate-distortion theory, see Gallager (1968), p. 451, or McEliece (2002, p. 75.The source, encoder, noisy channel and decoder define a Markov chain: s → x → y → ŝ P (s, x, y, ŝ) = P (s)P (x | s)P (y | x)P (ŝ | y).(10.21)The data processing inequality (exercise 8.9, p.141) must apply to this chain: I(s; ŝ) ≤ I(x; y).Furthermore, by the definition of channel capacity, I(x; y) ≤ N C, so I(s; ŝ) ≤ N C. Assume that a system achieves a rate R and a bit error probability p b ; then the mutual informationExercise 10.1. [3 ]Fill in the details in the preceding argument.If the bit errors between ŝ and s are independent then we have I(s; ŝ) = N R(1−H 2 (p b )).What if we have complex correlations among those bit errors?Why does the inequality I(s; ŝ) ≥ N R(1 − H 2 (p b )) hold?We have proved that the capacity of a channel is the maximum rate at whichSections 10.6-10.8contain advanced material.The first-time reader is encouraged to skip to section 10.9 (p.172).reliable communication can be achieved.How can we compute the capacity of a given discrete memoryless channel?We need to find its optimal input distribution.In general we can find the optimal input distribution by a computer search, making use of the derivative of the mutual information with respect to the input probabilities.Exercise 10.2. [2 ]Find the derivative of I(X; Y ) with respect to the input probability p i , ∂I(X; Y )/∂p i , for a channel with conditional probabilities Q j|i .Exercise 10.3. [2 ]Show that I(X; Y ) is a concave function of the input probability vector p.Since I(X; Y ) is concave in the input distribution p, any probability distribution p at which I(X; Y ) is stationary must be a global maximum of I(X; Y ).So it is tempting to put the derivative of I(X; Y ) into a routine that finds a local maximum of I(X; Y ), that is, an input distribution P (x) such thatwhere λ is a Lagrange multiplier associated with the constraint i p i = 1.However, this approach may fail to find the right answer, because I(X; Y ) might be maximized by a distribution that has p i = 0 for some inputs.A simple example is given by the ternary confusion channel.Whenever the input ? is used, the output is random; the other inputs are reliable inputs.The maximum information rate of 1 bit is achieved by making no use of the input ?.p.173] Sketch the mutual information for this channel as a function of the input distribution p. Pick a convenient two-dimensional representation of p.The optimization routine must therefore take account of the possibility that, as we go up hill on I(X; Y ), we may run into the inequality constraints p i ≥ 0.p.174]  3.There may be several optimal input distributions, but they all look the same at the output.Exercise 10.6. [2 ]Prove that no output y is unused by an optimal input distribution, unless it is unreachable, that is, has Q(y | x) = 0 for all x.Exercise 10.7. [2 ]Prove that I(X; Y ) is a convex function of Q(y | x).Exercise 10.8. [2 ]Prove that all optimal input distributions of a channel have the same output probability distribution P (y) = x P (x)Q(y | x).These results, along with the fact that I(X; Y ) is a concave function of the input probability vector p, prove the validity of the symmetry argument that we have used when finding the capacity of symmetric channels.If a channel is invariant under a group of symmetry operations -for example, interchanging the input symbols and interchanging the output symbols -then, given any optimal input distribution that is not symmetric, i.e., is not invariant under these operations, we can create another input distribution by averaging together this optimal input distribution and all its permuted forms that we can make by applying the symmetry operations to the original optimal input distribution.The permuted distributions must have the same I(X; Y ) as the original, by symmetry, so the new input distribution created by averaging must have I(X; Y ) bigger than or equal to that of the original distribution, because of the concavity of I.In order to use symmetry arguments, it will help to have a definition of a symmetric channel.I like Gallager's (1968) definition.A discrete memoryless channel is a symmetric channel if the set of outputs can be partitioned into subsets in such a way that for each subset the matrix of transition probabilities has the property that each row (if more than 1) is a permutation of each other row and each column is a permutation of each other column.Example 10.9.This channel .23) is a symmetric channel because its outputs can be partitioned into (0, 1) and ?, so that the matrix can be rewritten: Symmetry is a useful property because, as we will see in a later chapter, communication at capacity can be achieved over symmetric channels by linear codes.Exercise 10.10. [2 ]Prove that for a symmetric channel with any number of inputs, the uniform distribution over the inputs is an optimal input distribution.p.174] Are there channels that are not symmetric whose optimal input distributions are uniform?Find one, or prove there are none.The noisy-channel coding theorem that we proved in this chapter is quite general, applying to any discrete memoryless channel; but it is not very specific.The theorem only says that reliable communication with error probability and rate R can be achieved by using codes with sufficiently large blocklength N .The theorem does not say how large N needs to be to achieve given values of R and .Presumably, the smaller is and the closer R is to C, the larger N has to be.For a discrete memoryless channel, a blocklength N and a rate R, there exist block codes of length N whose average probability of error satisfies:where E r (R) is the random-coding exponent of the channel, a convex , decreasing, positive function of R for 0 ≤ R < C. The random-coding exponent is also known as the reliability function.[By an expurgation argument it can also be shown that there exist block codes for which the maximal probability of error p BM is also exponentially small in N .]The definition of E r (R) is given in Gallager (1968), p. 139.E r (R) approaches zero as R → C; the typical behaviour of this function is illustrated in figure 10.8.The computation of the random-coding exponent for interesting channels is a challenging task on which much effort has been expended.Even for simple channels like the binary symmetric channel, there is no simple expression for E r (R).The theorem stated above asserts that there are codes with p B smaller than exp [−N E r (R)].But how small can the error probability be?Could it be much smaller?For any code with blocklength N on a discrete memoryless channel, the probability of error assuming all source messages are used with equal probability satisfies 10.26) where the function E sp (R), the sphere-packing exponent of the channel, is a convex , decreasing, positive function of R for 0 ≤ R < C.For a precise statement of this result and further references, see Gallager (1968), p. 157.Imagine a customer who wants to buy an error-correcting code and decoder for a noisy channel.The results described above allow us to offer the following service: if he tells us the properties of his channel, the desired rate R and the desired error probability p B , we can, after working out the relevant functions C, E r (R), and E sp (R), advise him that there exists a solution to his problem using a particular blocklength N ; indeed that almost any randomly chosen code with that blocklength should do the job.Unfortunately we have not found out how to implement these encoders and decoders in practice; the cost of implementing the encoder and decoder for a random code with large N would be exponentially large in N .Furthermore, for practical purposes, the customer is unlikely to know exactly what channel he is dealing with.So Berlekamp (1980) suggests that the sensible way to approach error-correction is to design encoding-decoding systems and plot their performance on a variety of idealized channels as a function of the channel's noise level.These charts (one of which is illustrated on page 568) can then be shown to the customer, who can choose among the systems on offer without having to specify what he really thinks his channel is like.With this attitude to the practical problem, the importance of the functions E r (R) and E sp (R) is diminished.Exercise 10.12. [2 ]A binary erasure channel with input x and output y has transition probability matrix:Find the mutual information I(X; Y ) between the input and output for general input distribution {p 0 , p 1 }, and show that the capacity of this channel is C = 1 − q bits.A Z channel has transition probability matrix:Show that, using a (2, 1) code, two uses of a Z channel can be made to emulate one use of an erasure channel, and state the erasure probability of that erasure channel.Hence show that the capacity of the Z channel, C Z , satisfies C Z ≥ 1 2 (1 − q) bits.Explain why the result C Z ≥ 1 2 (1 − q) is an inequality rather than an equality.p.174]A transatlantic cable contains N = 20 indistinguishable electrical wires.You have the job of figuring out which wire is which, that is, to create a consistent labelling of the wires at each end.Your only tools are the ability to connect wires to each other in groups of two or more, and to test for connectedness with a continuity tester.What is the smallest number of transatlantic trips you need to make, and how do you do it?How would you solve the problem for larger N such as N = 1000?As an illustration, if N were 3 then the task can be solved in two steps by labelling one wire at one end a, connecting the other two together, crossing the Atlantic, measuring which two wires are connected, labelling them b and c and the unconnected one a, then connecting b to a and returning across the Atlantic, whereupon on disconnecting b from c, the identities of b and c can be deduced.This problem can be solved by persistent search, but the reason it is posed in this chapter is that it can also be solved by a greedy approach based on maximizing the acquired information.Let the unknown permutation of wires be x.Having chosen a set of connections of wires C at one end, you can then make measurements at the other end, and these measurements y convey information about x.How much?And for what set of connections is the information that y conveys about x maximized?We can build a good sketch of this function in two ways: by careful inspection of the function, or by looking at special cases.For the plots, the two-dimensional representation of p I will use has p 0 and p 1 as the independent variables, so that p = (p 0 , p ? , p 1 ) = (p 0 , (1−p 0 −p 1 ), p 1 ).By inspection.If we use the quantities p * ≡ p 0 + p ? /2 and p ? as our two degrees of freedom, the mutual information becomes very simple: I(X; Y ) = H 2 (p * ) − p ? .Converting back to p 0 = p * − p ? /2 and p 1 = 1 − p * − p ? /2, we obtain the sketch shown at the left below.This function is like a tunnel rising up the direction of increasing p 0 and p 1 .To obtain the required plot of I(X; Y ) we have to strip away the parts of this tunnel that live outside the feasible simplex of probabilities; we do this by redrawing the surface, showing only the parts where p 0 > 0 and p 1 > 0. A full plot of the function is shown at the right.Special cases.In the special case p ? = 0, the channel is a noiseless binary channel, and I(X; Y ) = H 2 (p 0 ).In the special case p 0 = p 1 , the term H 2 (p 0 + p ? /2) is equal to 1, so I(X; Y ) = 1 − p ? .In the special case p 0 = 0, the channel is a Z channel with error probability 0.5.We know how to sketch that, from the previous chapter (figure 9.3).These special cases allow us to construct the skeleton shown in figure 10.9.where λ is a constant related to the capacity by C = λ + log 2 e.This result can be used in a computer program that evaluates the derivatives, and increments and decrements the probabilities p i in proportion to the differences between those derivatives.This result is also useful for lazy human capacity-finders who are good guessers.Having guessed the optimal input distribution, one can simply confirm that equation (10.28) holds.Solution to exercise 10.11 (p.171).We certainly expect nonsymmetric channels with uniform optimal input distributions to exist, since when inventing a channel we have I(J − 1) degrees of freedom whereas the optimal input distribution is just (I − 1)-dimensional; so in the I(J −1)-dimensional space of perturbations around a symmetric channel, we expect there to be a subspace of perturbations of dimension I(J − 1) − (I − 1) = I(J − 2) + 1 that leave the optimal input distribution unchanged.Here is an explicit example, a bit like a Z channel.The key step in the information-theoretic approach to this problem is to write down the information content of one partition, the combinatorial object that is the connecting together of subsets of wires.If N wires are grouped together into g 1 subsets of size 1, g 2 subsets of size 2, . . ., then the number of such partitions is .30)and the information content of one such partition is the log of this quantity.In a greedy strategy we choose the first partition to maximize this information content.One game we can play is to maximize this information content with respect to the quantities g r , treated as real numbers, subject to the constraint r g r r = N .Introducing a Lagrange multiplier λ for the constraint, the derivative is (10.31)which, when set to zero, leads to the rather nice expression g r = e λr r! ; (10.32) the optimal g r is proportional to a Poisson distribution!We can solve for the Lagrange multiplier by plugging g r into the constraint r g r r = N , which gives the implicit equation N = µ e µ , (10.33)where µ ≡ e λ is a convenient reparameterization of the Lagrange multiplier.Figure 10.10a shows a graph of µ(N ); figure 10.10b shows the deduced noninteger assignments g r when µ = 2.2, and nearby integers g r = {1, 2, 2, 1, 1} that motivate setting the first partition to (a)(bc)(de)(fgh)(ijk)(lmno)(pqrst).This partition produces a random partition at the other end, which has an information content of log Ω = 40.4bits, which is a lot more than half the total information content we need to acquire to infer the transatlantic permutation, log 20! 61 bits.[In contrast, if all the wires are joined together in pairs, the information content generated is only about 29 bits.]How to choose the second partition is left to the reader.A Shannonesque approach is appropriate, picking a random partition at the other end, using the same {g r }; you need to ensure the two partitions are as unlike each other as possible.If N = 2, 5 or 9, then the labelling problem has solutions that are particularly simple to implement, called Knowlton-Graham partitions: partition {1, . . ., N } into disjoint sets in two ways A and B, subject to the condition that at most one element appears both in an A set of cardinality j and in a B set of cardinality k, for each j and k (Graham, 1966;Graham and Knowlton, 1968).Before reading Chapter 11, you should have read Chapters 9 and 10.You will also need to be familiar with the Gaussian distribution.One-dimensional Gaussian distribution.If a random variable y is Gaussian and has mean µ and variance σ 2 , which we write:y ∼ Normal(µ, σ 2 ), or P (y) = Normal(y; µ, σ 2 ), (11.1) then the distribution of y is: (11.2) [I use the symbol P for both probability densities and probabilities.]The inverse-variance τ ≡ 1/ σ 2 is sometimes called the precision of the Gaussian distribution.Multi-dimensional Gaussian distribution.If y = (y 1 , y 2 , . . ., y N ) has a multivariate Gaussian distribution, thenwhere x is the mean of the distribution, A is the inverse of the variance-covariance matrix, and the normalizing constant isThis distribution has the property that the variance Σ ii of y i , and the covariance Σ ij of y i and y j are given by (11.4) where A −1 is the inverse of the matrix A.The marginal distribution P (y i ) of one component y i is Gaussian; the joint marginal distribution of any subset of the components is multivariate-Gaussian; and the conditional density of any subset, given the values of another subset, for example, P (y i | y j ), is also Gaussian.Error-Correcting Codes & Real ChannelsThe noisy-channel coding theorem that we have proved shows that there exist reliable error-correcting codes for any noisy channel.In this chapter we address two questions.First, many practical channels have real, rather than discrete, inputs and outputs.What can Shannon tell us about these continuous channels?And how should digital signals be mapped into analogue waveforms, and vice versa?Second, how are practical error-correcting codes made, and what is achieved in practice, relative to the possibilities proved by Shannon?The most popular model of a real-input, real-output channel is the Gaussian channel.The Gaussian channel has a real input x and a real output y.The conditional distribution of y given x is a Gaussian distribution: (11.5)This channel has a continuous input and output but is discrete in time.We will show below that certain continuous-time channels are equivalent to the discrete-time Gaussian channel.This channel is sometimes called the additive white Gaussian noise (AWGN) channel.As with discrete channels, we will discuss what rate of error-free information communication can be achieved over this channel.Consider a physical (electrical, say) channel with inputs and outputs that are continuous in time.We put in x(t), and out comes y(t) = x(t) + n(t).Our transmission has a power cost.The average power of a transmission of length T may be constrained thus:(11.6)The received signal is assumed to differ from x(t) by additive noise n(t) (for example Johnson noise), which we will model as white Gaussian noise.The magnitude of this noise is quantified by the noise spectral density, N 0 .How could such a channel be used to communicate information?Consider φ 1 (t)x(t) transmitting a set of N real numbers {x n } N n=1 in a signal of duration T made up of a weighted combination of orthonormal basis functions φ n (t),x n φ n (t), (11.7)where T 0 dt φ n (t)φ m (t) = δ nm .The receiver can then compute the scalars: Before returning to the Gaussian channel, we define the bandwidth (measured in Hertz) of the continuous channel to be:where N max is the maximum number of orthonormal functions that can be produced in an interval of length T .This definition can be motivated by imagining creating a band-limited signal of duration T from orthonormal cosine and sine curves of maximum frequency W .The number of orthonormal functions is N max = 2W T .This definition relates to the Nyquist sampling theorem: if the highest frequency present in a signal is W , then the signal can be fully determined from its values at a series of discrete sample points separated by the Nyquist interval ∆t = 1/ 2W seconds.So the use of a real continuous channel with bandwidth W , noise spectral density N 0 , and power P is equivalent to N/T = 2W uses per second of a Gaussian channel with noise level σ 2 = N 0 /2 and subject to the signal power constraint x 2 n ≤ P/ 2W .Definition of E b /N 0 Imagine that the Gaussian channel y n = x n + n n is used with an encoding system to transmit binary source bits at a rate of R bits per channel use.How can we compare two encoding systems that have different rates of communication R and that use different powers x 2 n ?Transmitting at a large rate R is good; using small power is good too.It is conventional to measure the rate-compensated signal-to-noise ratio by the ratio of the power per source bit E b = x 2 n /R to the noise spectral density N 0 : E b /N 0 is dimensionless, but it is usually reported in the units of decibels; the value given is 10 log noise is Gaussian with probability density (11.14)where A is the inverse of the variance-covariance matrix of the noise, a symmetric and positive-definite matrix.(If A is a multiple of the identity matrix, I/σ 2 , then the noise is 'white'.For more general A, the noise is 'coloured'.)The probability of the received vector y given that the source signal was s (either zero or one) is thenThe optimal detector is based on the posterior probability ratio: (11.17)where θ is a constant independent of the received vector y,If the detector is forced to make a decision (i.e., guess either s = 1 or s = 0) then the decision that minimizes the probability of error is to guess the most probable hypothesis.We can write the optimal decision in terms of a discriminant function:with the decisions w  (11.21)where w ≡ A(x 1 − x 0 ).Until now we have measured the joint, marginal, and conditional entropy of discrete variables only.In order to define the information conveyed by continuous variables, there are two issues we must address -the infinite length of the real line, and the infinite precision of real numbers.How much information can we convey in one use of a Gaussian channel?If we are allowed to put any real number x into the Gaussian channel, we could communicate an enormous string of N digits d 1 d 2 d 3 . . .d N by setting x = d 1 d 2 d 3 . . .d N 000 . . .000.The amount of error-free information conveyed in just a single transmission could be made arbitrarily large by increasing N , and the communication could be made arbitrarily reliable by increasing the number of zeroes at the end of x.There is usually some power cost associated with large inputs, however, not to mention practical limits in the dynamic range acceptable to a receiver.It is therefore conventional to introduce a cost function v(x) for every input x, and constrain codes to have an average cost v less than or equal to some maximum value.A generalized channel coding theorem, including a cost function for the inputs, can be proved -see McEliece (1977).The result is a channel capacity C(v) that is a function of the permitted cost.For the Gaussian channel we will assume a costsuch that the 'average power' x 2 of the input is constrained.We motivated this cost function above in the case of real electrical channels in which the physical power consumption is indeed quadratic in x.The constraint x 2 = v makes it impossible to communicate infinite information in one use of the Gaussian channel.It is tempting to define joint, marginal, and conditional entropies for real variables simply by replacing summations by integrals, but this is not a well defined operation.As we discretize an interval into smaller and smaller divisions, the entropy of the discrete distribution diverges (as the logarithm of the granularity) (figure 11.4).Also, it is not permissible to take the logarithm of a dimensional quantity such as a probability density P (x) (whose dimensions are [x] −1 ).There is one information measure, however, that has a well-behaved limit, namely the mutual information -and this is the one that really matters, since it measures how much information one variable conveys about another.In the discrete case, I(X; Y ) = We can now ask these questions for the Gaussian channel: (a) what probability distribution P (x) maximizes the mutual information (subject to the constraint x 2 = v)?and (b) does the maximal mutual information still measure the maximum error-free communication rate of this real channel, as it did for the discrete channel?p.189] Prove that the probability distribution P (x) that maximizes the mutual information (subject to the constraint x 2 = v) is a Gaussian distribution of mean zero and variance v.p.189] Show that the mutual information I(X; Y ), in the case of this optimized distribution, is (11.26)This is an important result.We see that the capacity of the Gaussian channel is a function of the signal-to-noise ratio v/σ 2 .If P (x) = Normal(x; 0, v) and P (y | x) = Normal(y; x, σ 2 ) then the marginal distribution of y is P (y) = Normal(y; 0, v+σ 2 ) and the posterior distribution of the input, given that the output is y, is:[The step from (11.28) to (11.29) is made by completing the square in the exponent.]This formula deserves careful study.The mean of the posterior distribution, v v+σ 2 y, can be viewed as a weighted combination of the value that best fits the output, x = y, and the value that best fits the prior, x = 0:(11.30)The weights 1/σ 2 and 1/v are the precisions of the two Gaussians that we multiplied together in equation (11.28): the prior and the likelihood.The precision of the posterior distribution is the sum of these two precisions.This is a general property: whenever two independent sources contribute information, via Gaussian distributions, about an unknown variable, the precisions add.[This is the dual to the better-known relationship 'when independent variables are added, their variances add'.]We have evaluated a maximal mutual information.Does it correspond to a maximum possible rate of error-free information transmission?One way of proving that this is so is to define a sequence of discrete channels, all derived from the Gaussian channel, with increasing numbers of inputs and outputs, and prove that the maximum mutual information of these channels tends to the asserted C. The noisy-channel coding theorem for discrete channels applies to each of these derived channels, thus we obtain a coding theorem for the continuous channel.Alternatively, we can make an intuitive argument for the coding theorem specific for the Gaussian channel.11 -Error-Correcting Codes and Real Channels Geometrical view of the noisy-channel coding theorem: sphere packing Consider a sequence x = (x 1 , . . ., x N ) of inputs, and the corresponding output y, as defining two points in an N dimensional space.For large N , the noise power is very likely to be close (fractionally) to N σ 2 .The output y is therefore very likely to be close to the surface of a sphere of radius √ N σ 2 centred on x.Similarly, if the original signal x is generated at random subject to an average power constraint x 2 = v, then x is likely to lie close to a sphere, centred on the origin, of radius √ N v; and because the total average power of y is v + σ 2 , the received signal y is likely to lie on the surface of a sphere of radius N (v + σ 2 ), centred on the origin.The volume of an N -dimensional sphere of radius r is (11.31)Now consider making a communication system based on non-confusable inputs x, that is, inputs whose spheres do not overlap significantly.The maximum number S of non-confusable inputs is given by dividing the volume of the sphere of probable ys by the volume of the sphere for y given x:Thus the capacity is bounded by:(11.33)A more detailed argument like the one used in the previous chapter can establish equality.Recall that the use of a real continuous channel with bandwidth W , noise spectral density N 0 and power P is equivalent to N/T = 2W uses per second of a Gaussian channel with σ 2 = N 0 /2 and subject to the constraint x 2 n ≤ P/2W .Substituting the result for the capacity of the Gaussian channel, we find the capacity of the continuous channel to be: (11.34)This formula gives insight into the tradeoffs of practical communication.Imagine that we have a fixed power constraint.What is the best bandwidth to make use of that power?Introducing W 0 = P/N 0 , i.e., the bandwidth for which the signal-to-noise ratio is 1, figure 11.5 shows C/W 0 = W/W 0 log(1 + W 0 /W ) as a function of W/W 0 .The capacity increases to an asymptote of W 0 log e.It is dramatically better (in terms of capacity for fixed power) to transmit at a low signal-to-noise ratio over a large bandwidth, than with high signal-to-noise in a narrow bandwidth; this is one motivation for wideband communication methods such as the 'direct sequence spread-spectrum' approach used in 3G mobile phones.Of course, you are not alone, and your electromagnetic neighbours may not be pleased if you use a large bandwidth, so for social reasons, engineers often have to make do with higher-power, narrow-bandwidth transmitters.Nearly all codes are good, but nearly all codes require exponential look-up tables for practical implementation of the encoder and decoder -exponential in the blocklength N .And the coding theorem required N to be large.By a practical error-correcting code, we mean one that can be encoded and decoded in a reasonable amount of time, for example, a time that scales as a polynomial function of the blocklength N -preferably linearly.The non-constructive proof of the noisy-channel coding theorem showed that good block codes exist for any noisy channel, and indeed that nearly all block codes are good.But writing down an explicit and practical encoder and decoder that are as good as promised by Shannon is still an unsolved problem.Very good codes.Given a channel, a family of block codes that achieve arbitrarily small probability of error at any communication rate up to the capacity of the channel are called 'very good' codes for that channel.Good codes are code families that achieve arbitrarily small probability of error at non-zero communication rates up to some maximum rate that may be less than the capacity of the given channel.Bad codes are code families that cannot achieve arbitrarily small probability of error, or that can achieve arbitrarily small probability of error only by decreasing the information rate to zero.Repetition codes are an example of a bad code family.(Bad codes are not necessarily useless for practical purposes.)Practical codes are code families that can be encoded and decoded in time and space polynomial in the blocklength.Let us review the definition of a block code, and then add the definition of a linear block code.An (N, K) block code for a channel Q is a list of S = 2 K codewords {x (1) , x (2) , . . ., x (2 K ) }, each of length N : x (s) ∈ A N X .The signal to be encoded, s, which comes from an alphabet of size 2 K , is encoded as x (s) .A linear (N, K) block code is a block code in which the codewords {x (s) } make up a K-dimensional subspace of A N X .The encoding operation can be represented by an N × K binary matrix G T such that if the signal to be encoded, in binary notation, is s (a vector of length K bits), then the encoded signal is t = G T s modulo 2.The codewords {t} can be defined as the set of vectors satisfying Ht = 0 mod 2, where H is the parity-check matrix of the code.For example the (7, 4) Hamming code of section 1.2 takes K = 4 signal bits, s, and transmits them followed by three parity-check bits.The N = 7 transmitted symbols are given by G T s mod 2.Coding theory was born with the work of Hamming, who invented a family of practical error-correcting codes, each able to correct one error in a block of length N , of which the repetition code R 3 and the (7, 4) code are the simplest.Since then most established codes have been generalizations of Hamming's codes: Bose-Chaudhury-Hocquenhem codes, Reed-Müller codes, Reed-Solomon codes, and Goppa codes, to name a few.Another family of linear codes are convolutional codes, which do not divide the source stream into blocks, but instead read and transmit bits continuously.The transmitted bits are a linear function of the past source bits.Usually the rule for generating the transmitted bits involves feeding the present source bit into a linear-feedback shift-register of length k, and transmitting one or more linear functions of the state of the shift register at each iteration.The resulting transmitted bit stream is the convolution of the source stream with a linear filter.The impulse-response function of this filter may have finite or infinite duration, depending on the choice of feedback shift-register.We will discuss convolutional codes in Chapter 48.Are linear codes 'good' ?One might ask, is the reason that the Shannon limit is not achieved in practice because linear codes are inherently not as good as random codes?The answer is no, the noisy-channel coding theorem can still be proved for linear codes, at least for some channels (see Chapter 14), though the proofs, like Shannon's proof for random codes, are non-constructive.Linear codes are easy to implement at the encoding end.Is decoding a linear code also easy?Not necessarily.The general decoding problem (find the maximum likelihood s in the equation G T s + n = r) is in fact NP-complete (Berlekamp et al., 1978).[NP-complete problems are computational problems that are all equally difficult and which are widely believed to require exponential computer time to solve in general.]So attention focuses on families of codes for which there is a fast decoding algorithm.One trick for building codes with practical decoders is the idea of concatenation.An encoder-channel-decoder system C → Q → D can be viewed as defining C → C → Q → D → D Q a super-channel Q with a smaller probability of error, and with complex correlations among its errors.We can create an encoder C and decoder D for this super-channel Q .The code consisting of the outer code C followed by the inner code C is known as a concatenated code.Some concatenated codes make use of the idea of interleaving.We read the data in blocks, the size of each block being larger than the blocklengths of the constituent codes C and C .After encoding the data of one block using code C , the bits are reordered within the block in such a way that nearby bits are separated from each other once the block is fed to the second code C. A simple example of an interleaver is a rectangular code or product code in which the data are arranged in a K 2 × K 1 block, and encoded horizontally using an (N 1 , K 1 ) linear code, then vertically using a (N 2 , K 2 ) linear code.Exercise 11.3. [3 ]Show that either of the two codes can be viewed as the inner code or the outer code.As an example, figure 11.6 shows a product code in which we encode first with the repetition code R 3 (also known as the Hamming code H(3, 1)) horizontally then with H(7, 4) vertically.The blocklength of the concatenated code is 27.The number of source bits per codeword is four, shown by the small rectangle.We can decode conveniently (though not optimally) by using the individual decoders for each of the subcodes in some sequence.It makes most sense to first decode the code which has the lowest rate and hence the greatest errorcorrecting ability.Figure 11.6(c-e) shows what happens if we receive the codeword of figure 11.6a with some errors (five bits flipped, as shown) and apply the decoder for H(3, 1) first, and then the decoder for H(7, 4).The first decoder corrects three of the errors, but erroneously modifies the third bit in the second row where there are two bit errors.The (7, 4) decoder can then correct all three of these errors.shows what happens if we decode the two codes in the other order.In columns one and two there are two errors, so the (7, 4) decoder introduces two extra errors.It corrects the one error in column 3. The (3, 1) decoder then cleans up four of the errors, but erroneously infers the second bit.The motivation for interleaving is that by spreading out bits that are nearby in one code, we make it possible to ignore the complex correlations among the errors that are produced by the inner code.Maybe the inner code will mess up an entire codeword; but that codeword is spread out one bit at a time over several codewords of the outer code.So we can treat the errors introduced by the inner code as if they are independent.In addition to the binary symmetric channel and the Gaussian channel, coding theorists keep more complex channels in mind also.Burst-error channels are important models in practice.Reed-Solomon codes use Galois fields (see Appendix C.1) with large numbers of elements (e.g. 2 16 ) as their input alphabets, and thereby automatically achieve a degree of burst-error tolerance in that even if 17 successive bits are corrupted, only 2 successive symbols in the Galois field representation are corrupted.Concatenation and interleaving can give further protection against burst errors.The concatenated Reed-Solomon codes used on digital compact discs are able to correct bursts of errors of length 4000 bits.p.189]The technique of interleaving, which allows bursts of errors to be treated as independent, is widely used, but is theoretically a poor way to protect data against burst errors, in terms of the amount of redundancy required.Explain why interleaving is a poor method, using the following burst-error channel as an example.Time is divided into chunks of length N = 100 clock cycles; during each chunk, there is a burst with probability b = 0.2; during a burst, the channel is a binary symmetric channel with f = 0.5.If there is no burst, the channel is an error-free binary channel.Compute the capacity of this channel and compare it with the maximum communication rate that could conceivably be achieved if one used interleaving and treated the errors as independent.Fading channels are real channels like Gaussian channels except that the received power is assumed to vary with time.A moving mobile phone is an important example.The incoming radio signal is reflected off nearby objects so that there are interference patterns and the intensity of the signal received by the phone varies with its location.The received power can easily vary by 10 decibels (a factor of ten) as the phone's antenna moves through a distance similar to the wavelength of the radio signal (a few centimetres).What are the best known codes for communicating over Gaussian channels?All the practical codes are linear codes, and are either based on convolutional codes or block codes.Textbook convolutional codes.The 'de facto standard' error-correcting code for satellite communications is a convolutional code with constraint length 7. Convolutional codes are discussed in Chapter 48.Concatenated convolutional codes.The above convolutional code can be used as the inner code of a concatenated code whose outer code is a Reed-Solomon code with eight-bit symbols.This code was used in deep space communication systems such as the Voyager spacecraft.For further reading about Reed-Solomon codes, see Lin and Costello (1983).The code for Galileo.A code using the same format but using a longer constraint length -15 -for its convolutional code and a larger Reed-Solomon code was developed by the Jet Propulsion Laboratory (Swanson, 1988).The details of this code are unpublished outside JPL, and the decoding is only possible using a room full of special-purpose hardware.In 1992, this was the best code known of rate 1/ 4.Turbo codes.In 1993, Berrou, Glavieux and Thitimajshima reported work on turbo codes.The encoder of a turbo code is based on the encoders of two convolutional codes.The source bits are fed into each encoder, the order of the source bits being permuted in a random way, and the resulting parity bits from each constituent code are transmitted.The decoding algorithm involves iteratively decoding each constituent code using its standard decoding algorithm, then using the output of The source bits are reordered using a permutation π before they are fed to C 2 .The transmitted codeword is obtained by concatenating or interleaving the outputs of the two convolutional codes.The random permutation is chosen when the code is designed, and fixed thereafter.the decoder as the input to the other decoder.This decoding algorithm is an instance of a message-passing algorithm called the sum-product algorithm.Turbo codes are discussed in Chapter 48, and message passing in Chapters 16,17,25,and 26.Each bit participates in j = 3 constraints, represented by squares.Each constraint forces the sum of the k = 4 bits to which it is connected to be even.This code is a (16, 4) code.Outstanding performance is obtained when the blocklength is increased to N 10 000.for Gaussian channels were invented by Gallager in 1962 but were promptly forgotten by most of the coding theory community.They were rediscovered in 1995 and shown to have outstanding theoretical and practical properties.Like turbo codes, they are decoded by message-passing algorithms.We will discuss these beautifully simple codes in Chapter 47.The performances of the above codes are compared for Gaussian channels in figure 47.17, p.568.Random codes are good, but they require exponential resources to encode and decode them.Non-random codes tend for the most part not to be as good as random codes.For a non-random code, encoding may be easy, but even for simply-defined linear codes, the decoding problem remains very difficult.The best practical codes (a) employ very large block sizes; (b) are based on semi-random code constructions; and (c) make use of probabilitybased decoding algorithms.Most practically used codes are linear, but not all.Digital soundtracks are encoded onto cinema film as a binary pattern.The likely errors affecting the film involve dirt and scratches, which produce large numbers of 1s and 0s respectively.We want none of the codewords to look like all-1s or all-0s, so that it will be easy to detect errors caused by dirt and scratches.One of the codes used in digital cinema sound systems is a nonlinear (8, 6) code consisting of 64 of the 8 4 binary patterns of weight 4.Another source of uncertainty for the receiver is uncertainty about the timing of the transmitted signal x(t).In ordinary coding theory and information theory, the transmitter's time t and the receiver's time u are assumed to be perfectly synchronized.But if the receiver receives a signal y(u), where the receiver's time, u, is an imperfectly known function u(t) of the transmitter's time t, then the capacity of this channel for communication is reduced.The theory of such channels is incomplete, compared with the synchronized channels we have discussed thus far.Not even the capacity of channels with synchronization errors is known (Levenshtein, 1966;Ferreira et al., 1997); codes for reliable communication over channels with synchronization errors remain an active research area (Davey and MacKay, 2001).Writing a Taylor expansion of ln[P (y)σ] = a+by+cy 2 +• • •, only a quadratic function ln[P (y)σ] = a + cy 2 would satisfy the constraint (11.40).(Any higher order terms y p , p > 2, would produce terms in x p that are not present on the right-hand side.)Therefore P (y) is Gaussian.We can obtain this optimal output distribution by using a Gaussian input distribution P (x).Solution to exercise 11.2 (p.181).Given a Gaussian input distribution of variance v, the output distribution is Normal(0, v + σ 2 ), since x and the noise are independent random variables, and variances add for independent random variables.The mutual information is:  (11.44)In contrast, interleaving, which treats bursts of errors as independent, causes the channel to be treated as a binary symmetric channel with f = 0.2 × 0.5 = 0.1, whose capacity is about 0.53.Interleaving throws away the useful information about the correlatedness of the errors.Theoretically, we should be able to communicate about (0.79/0.53) 1.6 times faster using a code and decoder that explicitly treat bursts as bursts.Solution to exercise 11.5 (p.188).Solution to exercise 11.9 (p.188).There are several RAID systems.One of the easiest to understand consists of 7 disk drives which store data at rate 4/7 using a (7, 4) Hamming code: each successive four bits are encoded with the code and the seven codeword bits are written one to each disk.Two or perhaps three disk drives can go down and the others can recover the data.The effective channel model here is a binary erasure channel, because it is assumed that we can tell when a disk is dead.It is not possible to recover the data for some choices of the three dead disk drives; can you see why?p.190] Give an example of three disk drives that, if lost, lead to failure of the above RAID system, and three that can be lost without failure.Solution to exercise 11.10 (p.190).The (7, 4) Hamming code has codewords of weight 3.If any set of three disk drives corresponding to one of those codewords is lost, then the other four disks can recover only 3 bits of information about the four source bits; a fourth bit is lost.[cf.exercise 13.13 (p.220) with q = 2: there are no binary MDS codes.This deficit is discussed further in section 13.11.]Any other set of three disk drives can be lost without problems because the corresponding four by four submatrix of the generator matrix is invertible.A better code would be a digital fountain -see Chapter 50.12 Hash Codes: Codes for Efficient Information Retrieval12.1 The information-retrieval problemA simple example of an information-retrieval problem is the task of implementing a phone directory service, which, in response to a person's name, returns (a) a confirmation that that person is listed in the directory; and (b) the person's phone number and other details.We could formalize this problem as follows, with S being the number of names that must be stored in the directory.You are given a list of S binary strings of length N bits, {x (1) , . . ., x (S) }, where S is considerably smaller than the total number of possible strings, 2 N .We will call the superscript 's' in x (s) the record number of the string.The idea is that s runs over customers in the order in which they are added to the directory and x (s) is the name of customer s.We assume for simplicity that all people have names of the same length.The name length might be, say, N = 200 bits, and we might want to store the details of ten million customers, so S 10 7 2 23 .We will ignore the possibility that two customers have identical names.The task is to construct the inverse of the mapping from s to x (s) , i.e., to make a system that, given a string x, returns the value of s such that x = x (s) if one exists, and otherwise reports that no such s exists.(Once we have the record number, we can go and look in memory location s in a separate memory full of phone numbers to find the required number.)The aim, when solving this task, is to use minimal computational resources in terms of the amount of memory used to store the inverse mapping from x to s and the amount of time to compute the inverse mapping.And, preferably, the inverse mapping should be implemented in such a way that further new strings can be added to the directory in a small amount of computer time too.The simplest and dumbest solutions to the information-retrieval problem are a look-up table and a raw list.The look-up table is a piece of memory of size 2 N log 2 S, log 2 S being the amount of memory required to store an integer between 1 and S. In each of the 2 N locations, we put a zero, except for the locations x that correspond to strings x (s) , into which we write the value of s.The look-up table is a simple and quick solution, but only if there is sufficient memory for the table, and if the cost of looking up entries in 12 -Hash Codes: Codes for Efficient Information Retrieval memory is independent of the memory size.But in our definition of the task, we assumed that N is about 200 bits or more, so the amount of memory required would be of size 2 200 ; this solution is completely out of the question.Bear in mind that the number of particles in the solar system is only about 2 190 .The raw list is a simple list of ordered pairs (s, x (s) ) ordered by the value of s.The mapping from x to s is achieved by searching through the list of strings, starting from the top, and comparing the incoming string x with each record x (s) until a match is found.This system is very easy to maintain, and uses a small amount of memory, about SN bits, but is rather slow to use, since on average five million pairwise comparisons will be made.p.202] Show that the average time taken to find the required string in a raw list, assuming that the original names were chosen at random, is about S + N binary comparisons.(Note that you don't have to compare the whole string of length N , since a comparison can be terminated as soon as a mismatch occurs; show that you need on average two binary comparisons per incorrect string match.)Compare this with the worst-case search time -assuming that the devil chooses the set of strings and the search key.The standard way in which phone directories are made improves on the look-up table and the raw list by using an alphabetically-ordered list.Alphabetical list.The strings {x (s) } are sorted into alphabetical order.Searching for an entry now usually takes less time than was needed for the raw list because we can take advantage of the sortedness; for example, we can open the phonebook at its middle page, and compare the name we find there with the target string; if the target is 'greater' than the middle string then we know that the required string, if it exists, will be found in the second half of the alphabetical directory.Otherwise, we look in the first half.By iterating this splitting-in-the-middle procedure, we can identify the target string, or establish that the string is not listed, in log 2 S string comparisons.The expected number of binary comparisons per string comparison will tend to increase as the search progresses, but the total number of binary comparisons required will be no greater than log 2 S N .The amount of memory required is the same as that required for the raw list.Adding new strings to the database requires that we insert them in the correct location in the list.To find that location takes about log 2 S binary comparisons.Can we improve on the well-established alphabetized list?Let us consider our task from some new viewpoints.The task is to construct a mapping x → s from N bits to log 2 S bits.This is a pseudo-invertible mapping, since for any x that maps to a non-zero s, the customer database contains the pair (s, x (s) ) that takes us back.Where have we come across the idea of mapping from N bits to M bits before?We encountered this idea twice: first, in source coding, we studied block codes which were mappings from strings of N symbols to a selection of one label in a list.The task of information retrieval is similar to the task (which we never actually solved) of making an encoder for a typical-set compression code.The second time that we mapped bit strings to bit strings of another dimensionality was when we studied channel codes.There, we considered codes that mapped from K bits to N bits, with N greater than K, and we made theoretical progress using random codes.In hash codes, we put together these two notions.We will study random codes that map from N bits to M bits where M is smaller than N .The idea is that we will map the original high-dimensional space down into a lower-dimensional space, one in which it is feasible to implement the dumb look-up table method which we rejected a moment ago.First we will describe how a hash code works, then we will study the properties of idealized hash codes.A hash code implements a solution to the informationretrieval problem, that is, a mapping from x to s, with the help of a pseudorandom function called a hash function, which maps the N -bit string x to an M -bit string h(x), where M is smaller than N .M is typically chosen such that the 'table size' T 2 M is a little bigger than S -say, ten times bigger.For example, if we were expecting S to be about a million, we might map x into a 30-bit hash h (regardless of the size N of each item x).The hash function is some fixed deterministic function which should ideally be indistinguishable from a fixed random code.For practical purposes, the hash function must be quick to compute.Two simple examples of hash functions are:Division method.The table size T is a prime number, preferably one that is not close to a power of 2. The hash value is the remainder when the integer x is divided by T .Variable string addition method.This method assumes that x is a string of bytes and that the table size T is 256.The characters of x are added, modulo 256.This hash function has the defect that it maps strings that are anagrams of each other onto the same hash.It may be improved by putting the running total through a fixed pseudorandom permutation after each character is added.In the variable string exclusive-or method with table size ≤ 65 536, the string is hashed twice in this way, with the initial running total being set to 0 and 1 respectively (algorithm 12.3).The result is a 16-bit hash.Having picked a hash function h(x), we implement an information retriever as follows.(See figure 12.4.)Encoding.A piece of memory called the hash table is created of size 2 M b memory units, where b is the amount of memory needed to represent an integer between 0 and S.This table is initially set to zero throughout.Each memory x (s) is put through the hash function, and at the location in the hash table corresponding to the resulting vector h (s) = h(x (s) ), the integer s is written -unless that entry in the hash table is already occupied, in which case we have a collision between x (s) and some earlier x (s ) which both happen to have the same hash code.Collisions can be handled in various ways -we will discuss some in a moment -but first let us complete the basic picture.this.As an example, we could store in location h in the hash table a pointer (which must be distinguishable from a valid record number s) to a 'bucket' where all the strings that have hash code h are stored in a sorted list.The encoder sorts the strings in each bucket alphabetically as the hash table and buckets are created.The decoder simply has to go and look in the relevant bucket and then check the short list of strings that are there by a brief alphabetical search.This method of storing the strings in buckets allows the option of making the hash table quite small, which may have practical benefits.We may make it so small that almost all strings are involved in collisions, so all buckets contain a small number of strings.It only takes a small number of binary comparisons to identify which of the strings in the bucket matches the cue x.p.202]If we wish to store S entries using a hash function whose output has M bits, how many collisions should we expect to happen, assuming that our hash function is an ideal random function?What size M of hash table is needed if we would like the expected number of collisions to be smaller than 1?What size M of hash table is needed if we would like the expected number of collisions to be a small fraction, say 1%, of S?[Notice the similarity of this problem to exercise 9.20 (p.156).]12.5 Other roles for hash codesIf you wish to check an addition that was done by hand, you may find useful the method of casting out nines.In casting out nines, one finds the sum, modulo nine, of all the digits of the numbers to be summed and compares it with the sum, modulo nine, of the digits of the putative answer.[With a little practice, these sums can be computed much more rapidly than the full original addition.]Example 12.4.In the calculation shown in the margin the sum, modulo nine, of 189 +1254 + 238 1681 the digits in 189+1254+238 is 7, and the sum, modulo nine, of 1+6+8+1 is 7.The calculation thus passes the casting-out-nines test.Casting out nines gives a simple example of a hash function.p.203] What evidence does a correct casting-out-nines match give in favour of the hypothesis that the addition has been done correctly?12.5: Other roles for hash codes 199Are two files the same?If the files are on the same computer, we could just compare them bit by bit.But if the two files are on separate machines, it would be nice to have a way of confirming that two files are identical without having to transfer one of the files from A to B. [And even if we did transfer one of the files, we would still like a way to confirm whether it has been received without modifications!]This problem can be solved using hash codes.Let Alice and Bob be the holders of the two files; Alice sent the file to Bob, and they wish to confirm it has been received without error.If Alice computes the hash of her file and sends it to Bob, and Bob computes the hash of his file, using the same M -bit hash function, and the two hashes match, then Bob can deduce that the two files are almost surely the same.Example 12.6.What is the probability of a false negative, i.e., the probability, given that the two files do differ, that the two hashes are nevertheless identical?If we assume that the hash function is random and that the process that causes the files to differ knows nothing about the hash function, then the probability of a false negative is 2 −M . 2 A 32-bit hash gives a probability of false negative of about 10 −10 .It is common practice to use a linear hash function called a 32-bit cyclic redundancy check to detect errors in files.(A cyclic redundancy check is a set of 32 paritycheck bits similar to the 3 parity-check bits of the (7, 4) Hamming code.)To have a false-negative rate smaller than one in a billion, M = 32 bits is plenty, if the errors are produced by noise.p.203]Such a simple parity-check code only detects errors; it doesn't help correct them.Since error-correcting codes exist, why not use one of them to get some error-correcting capability too?What if the differences between the two files are not simply 'noise', but are introduced by an adversary, a clever forger called Fiona, who modifies the original file to make a forgery that purports to be Alice's file?How can Alice make a digital signature for the file so that Bob can confirm that no-one has tampered with the file?And how can we prevent Fiona from listening in on Alice's signature and attaching it to other files?Let's assume that Alice computes a hash function for the file and sends it securely to Bob.If Alice computes a simple hash function for the file like the linear cyclic redundancy check, and Fiona knows that this is the method of verifying the file's integrity, Fiona can make her chosen modifications to the file and then easily identify (by linear algebra) a further 32-or-so single bits that, when flipped, restore the hash function of the file to its original value.Linear hash functions give no security against forgers.We must therefore require that the hash function be hard to invert so that no-one can construct a tampering that leaves the hash function unaffected.We would still like the hash function to be easy to compute, however, so that Bob doesn't have to do hours of work to verify every file he received.Such a hash function -easy to compute, but hard to invert -is called a one-way 12 -Hash Codes: Codes for Efficient Information Retrieval hash function.Finding such functions is one of the active research areas of cryptography.A hash function that is widely used in the free software community to confirm that two files do not differ is MD5, which produces a 128-bit hash.The details of how it works are quite complicated, involving convoluted exclusiveor-ing and if-ing and and-ing. 1 Even with a good one-way hash function, the digital signatures described above are still vulnerable to attack, if Fiona has access to the hash function.Fiona could take the tampered file and hunt for a further tiny modification to it such that its hash matches the original hash of Alice's file.This would take some time -on average, about 2 32 attempts, if the hash function has 32 bitsbut eventually Fiona would find a tampered file that matches the given hash.To be secure against forgery, digital signatures must either have enough bits for such a random search to take too long, or the hash function itself must be kept secret.Fiona has to hash 2 M files to cheat. 2 32 file modifications is not very many, so a 32-bit hash function is not large enough for forgery prevention.Another person who might have a motivation for forgery is Alice herself.For example, she might be making a bet on the outcome of a race, without wishing to broadcast her prediction publicly; a method for placing bets would be for her to send to Bob the bookie the hash of her bet.Later on, she could send Bob the details of her bet.Everyone can confirm that her bet is consistent with the previously publicized hash.[This method of secret publication was used by Isaac Newton and Robert Hooke when they wished to establish priority for scientific ideas without revealing them.Hooke's hash function was alphabetization as illustrated by the conversion of UT TENSIO, SIC VIS into the anagram CEIIINOSSSTTUV.]Such a protocol relies on the assumption that Alice cannot change her bet after the event without the hash coming out wrong.How big a hash function do we need to use to ensure that Alice cannot cheat?The answer is different from the size of the hash we needed in order to defeat Fiona above, because Alice is the author of both files.Alice could cheat by searching for two files that have identical hashes to each other.For example, if she'd like to cheat by placing two bets for the price of one, she could make a large number N 1 of versions of bet one (differing from each other in minor details only), and a large number N 2 of versions of bet two, and hash them all.If there's a collision between the hashes of two bets of different types, then she can submit the common hash and thus buy herself the option of placing either bet.Example 12.8.If the hash has M bits, how big do N 1 and N 2 need to be for Alice to have a good chance of finding two different bets with the same hash?This is a birthday problem like exercise 9.20 (p.156).If there are N 1 Montagues and N 2 Capulets at a party, and each is assigned a 'birthday' of M bits, the expected number of collisions between a Montague and a Capulet is (12.3)This chapter is an aside, which may safely be skipped.Solution to exercise 6. 22 (p.127)To discuss the coding of integers we need some definitions.The standard binary representation of a positive integer n will be denoted by c b (n), e.g., c b (5) = 101, c b (45) = 101101.The standard binary length of a positive integer n, l b (n), is the length of the string c b (n).For example, l b (5) = 3, l b (45) = 6.The standard binary representation c b (n) is not a uniquely decodeable code for integers since there is no way of knowing when an integer has ended.For example, c b (5)c b (5) is identical to c b (45).It would be uniquely decodeable if we knew the standard binary length of each integer before it was received.Noticing that all positive integers have a standard binary representation that starts with a 1, we might define another representation:The headless binary representation of a positive integer n will be denoted by c B (n), e.g., c B (5) = 01, c B (45) = 01101 and c B (1) = λ (where λ denotes the null string).This representation would be uniquely decodeable if we knew the length l b (n) of the integer.So, how can we make a uniquely decodeable code for integers?Two strategies can be distinguished.1. Self-delimiting codes.We first communicate somehow the length of the integer, l b (n), which is also a positive integer; then communicate the original integer n itself using c B (n).Before reading Chapter 9, you should have read Chapter 1 and worked on exercise 2.26 (p.37), and exercises 8. 2-8.7 (pp.140-141).Using our skills picked up from exercise 2.17 (p.36), we rewrite this in the form y) .(9.52)The optimal decoder selects the most probable hypothesis; this can be done simply by looking at the sign of a(y).If a(y) > 0 then decode as x = 1.The probability of error isSolution to exercise 9.20 (p.156).The probability that S = 24 people whose birthdays are drawn at random from A = 365 days all have distinct birthdays isThe probability that two (or more) people share a birthday is one minus this quantity, which, for S = 24 and A = 365, is about 0.5.This exact way of answering the question is not very informative since it is not clear for what value of S the probability changes from being close to 0 to being close to 1.The number of pairs is S(S − 1)/2, and the probability that a particular pair shares a birthday is 1/A, so the expected number of collisions isThis answer is more instructive.The expected number of collisions is tiny if S √ A and big if S √ A. We can also approximate the probability that all birthdays are distinct, for small S, thus:For a review of the history of spread-spectrum methods, see Scholtz (1982).The Gaussian channelp.190] Consider a Gaussian channel with a real input x, and signal to noise ratio v/σ 2 .(a) What is its capacity C?(c) If in addition the output of the channel is thresholded using the mapping (11.35) what is the capacity C of the resulting channel?(d) Plot the three capacities above as a function of v/σ 2 from 0.1 to 2.[You'll need to do a numerical integral to evaluate C .]Exercise 11.6. [3 ]For large integers K and N , what fraction of all binary errorcorrecting codes of length N and rate R = K/N are linear codes?[The answer will depend on whether you choose to define the code to be an ordered list of 2 K codewords, that is, a mapping from s ∈ {1, 2, . . ., 2 K } to x (s) , or to define the code to be an unordered list, so that two codes consisting of the same codewords are identical.Use the latter definition: a code is a set of codewords; how the encoder operates is not part of the definition of the code.]Exercise 11.7. [4 ]Design a code for the binary erasure channel, and a decoding algorithm, and evaluate their probability of error.[The design of good codes for erasure channels is an active research area (Spielman, 1996;Byers et al., 1998); see also Chapter 50.]Exercise 11.8. [5 ]Design a code for the q-ary erasure channel, whose input x is drawn from 0, 1, 2, 3, . . ., (q − 1), and whose output y is equal to x with probability (1 − f ) and equal to ?otherwise.[This erasure channel is a good model for packets transmitted over the internet, which are either received reliably or are lost.]p.190] How do redundant arrays of independent disks (RAID) work?These are information storage systems consisting of about ten[Some people say RAID stands for 'redundant array of inexpensive disks', but I think that's silly -RAID would still be a good idea even if the disks were expensive!]disk drives, of which any two or three can be disabled and the others are able to still able to reconstruct any requested file.What codes are used, and how far are these systems from the Shannon limit for the problem they are solving?How would you design a better RAID system?Some information is provided in the solution section.See http://www.acnc.com/raid2.html;see also Chapter 50.Further Topics in Information TheoryIn Chapters 1-11, we concentrated on two aspects of information theory and coding theory: source coding -the compression of information so as to make efficient use of data transmission and storage channels; and channel codingthe redundant encoding of information so as to be able to detect and correct communication errors.In both these areas we started by ignoring practical considerations, concentrating on the question of the theoretical limitations and possibilities of coding.We then discussed practical source-coding and channel-coding schemes, shifting the emphasis towards computational feasibility.But the prime criterion for comparing encoding schemes remained the efficiency of the code in terms of the channel resources it required: the best source codes were those that achieved the greatest compression; the best channel codes were those that communicated at the highest rate with a given probability of error.In this chapter we now shift our viewpoint a little, thinking of ease of information retrieval as a primary goal.It turns out that the random codes which were theoretically useful in our study of channel coding are also useful for rapid information retrieval.Efficient information retrieval is one of the problems that brains seem to solve effortlessly, and content-addressable memory is one of the topics we will study when we look at neural networks.Decoding.To retrieve a piece of information corresponding to a target vector x, we compute the hash h of x and look at the corresponding location in the hash table.If there is a zero, then we know immediately that the string x is not in the database.The cost of this answer is the cost of one hash-function evaluation and one look-up in the table of size 2 M .If, on the other hand, there is a non-zero entry s in the table, there are two possibilities: either the vector x is indeed equal to x (s) ; or the vector x (s) is another vector that happens to have the same hash code as the target x.(A third possibility is that this non-zero entry might have something to do with our yet-to-be-discussed collision-resolution system.)To check whether x is indeed equal to x (s) , we take the tentative answer s, look up x (s) in the original forward database, and compare it bit by bit with x; if it matches then we report s as the desired answer.This successful retrieval has an overall cost of one hash-function evaluation, one look-up in the table of size 2 M , another look-up in a table of size S, and N binary comparisons -which may be much cheaper than the simple solutions presented in section 12.1.p.202]If we have checked the first few bits of x (s) with x and found them to be equal, what is the probability that the correct entry has been retrieved, if the alternative hypothesis is that x is actually not in the database?Assume that the original source strings are random, and the hash function is a random hash function.How many binary evaluations are needed to be sure with odds of a billion to one that the correct entry has been retrieved?The hashing method of information retrieval can be used for strings x of arbitrary length, if the hash function h(x) can be applied to strings of any length.We will study two ways of resolving collisions: appending in the table, and storing elsewhere.When encoding, if a collision occurs, we continue down the hash table and write the value of s into the next available location in memory that currently contains a zero.If we reach the bottom of the table before encountering a zero, we continue from the top.When decoding, if we compute the hash code for x and find that the s contained in the table doesn't point to an x (s) that matches the cue x, we continue down the hash table until we either find an s whose x (s) does match the cue x, in which case we are done, or else encounter a zero, in which case we know that the cue x is not in the database.For this method, it is essential that the table be substantially bigger in size than S. If 2 M < S then the encoding rule will become stuck with nowhere to put the last strings.A more robust and flexible method is to use pointers to additional pieces of memory in which collided strings are stored.There are many ways of doing 12.6: Further exercises 201 so to minimize the number of files hashed, N 1 + N 2 , Alice should make N 1 and N 2 equal, and will need to hash about 2 M/2 files until she finds two that match. 2Alice has to hash 2 M/2 files to cheat.[This is the square root of the number of hashes Fiona had to make.]If Alice has the use of C = 10 6 computers for T = 10 years, each computer taking t = 1 ns to evaluate a hash, the bet-communication system is secure against Alice's dishonesty only if M 2 log 2 CT /t 160 bits.The Bible for hash codes is volume 3 of Knuth (1968).I highly recommend the story of Doug McIlroy's spell program, as told in section 13.8 of Programming Pearls (Bentley, 2000).This astonishing piece of software makes use of a 64kilobyte data structure to store the spellings of all the words of 75 000-word dictionary.12.6 Further exercises Exercise 12.9. [1 ]What is the shortest the address on a typical international letter could be, if it is to get to a unique human recipient?(Assume the permitted characters are [A-Z,0-9].)How long are typical email addresses?p.203] How long does a piece of text need to be for you to be pretty sure that no human has written that string of characters before?How many notes are there in a new melody that has not been composed before?p.204] Pattern recognition by molecules.Some proteins produced in a cell have a regulatory role.A regulatory protein controls the transcription of specific genes in the genome.This control often involves the protein's binding to a particular DNA sequence in the vicinity of the regulated gene.The presence of the bound protein either promotes or inhibits transcription of the gene.(a) Use information-theoretic arguments to obtain a lower bound on the size of a typical protein that acts as a regulator specific to one gene in the whole human genome.Assume that the genome is a sequence of 3 × 10 9 nucleotides drawn from a four letter alphabet {A, C, G, T}; a protein is a sequence of amino acids drawn from a twenty letter alphabet.[Hint: establish how long the recognized DNA sequence has to be in order for that sequence to be unique to the vicinity of one gene, treating the rest of the genome as a random sequence.Then discuss how big the protein must be to recognize a sequence of that length uniquely.](b) Some of the sequences recognized by DNA-binding regulatory proteins consist of a subsequence that is repeated twice or more, for example the sequence (12.4)This chapter is an aside, which may safely be skipped.Solution to exercise 6. 22 (p.127)To discuss the coding of integers we need some definitions.The standard binary representation of a positive integer n will be denoted by c b (n), e.g., c b (5) = 101, c b (45) = 101101.The standard binary length of a positive integer n, l b (n), is the length of the string c b (n).For example, l b (5) = 3, l b (45) = 6.The standard binary representation c b (n) is not a uniquely decodeable code for integers since there is no way of knowing when an integer has ended.For example, c b (5)c b (5) is identical to c b (45).It would be uniquely decodeable if we knew the standard binary length of each integer before it was received.Noticing that all positive integers have a standard binary representation that starts with a 1, we might define another representation:The headless binary representation of a positive integer n will be denoted by c B (n), e.g., c B (5) = 01, c B (45) = 01101 and c B (1) = λ (where λ denotes the null string).This representation would be uniquely decodeable if we knew the length l b (n) of the integer.So, how can we make a uniquely decodeable code for integers?Two strategies can be distinguished.1. Self-delimiting codes.We first communicate somehow the length of the integer, l b (n), which is also a positive integer; then communicate the original integer n itself using c B (n).Before reading Chapter 9, you should have read Chapter 1 and worked on exercise 2.26 (p.37), and exercises 8. 2-8.7 (pp.140-141).Using our skills picked up from exercise 2.17 (p.36), we rewrite this in the form y) .(9.52)The optimal decoder selects the most probable hypothesis; this can be done simply by looking at the sign of a(y).If a(y) > 0 then decode as x = 1.The probability of error isSolution to exercise 9.20 (p.156).The probability that S = 24 people whose birthdays are drawn at random from A = 365 days all have distinct birthdays isThe probability that two (or more) people share a birthday is one minus this quantity, which, for S = 24 and A = 365, is about 0.5.This exact way of answering the question is not very informative since it is not clear for what value of S the probability changes from being close to 0 to being close to 1.The number of pairs is S(S − 1)/2, and the probability that a particular pair shares a birthday is 1/A, so the expected number of collisions isThis answer is more instructive.The expected number of collisions is tiny if S √ A and big if S √ A. We can also approximate the probability that all birthdays are distinct, for small S, thus:For a review of the history of spread-spectrum methods, see Scholtz (1982).The Gaussian channelp.190] Consider a Gaussian channel with a real input x, and signal to noise ratio v/σ 2 .(a) What is its capacity C?(c) If in addition the output of the channel is thresholded using the mapping (11.35) what is the capacity C of the resulting channel?(d) Plot the three capacities above as a function of v/σ 2 from 0.1 to 2.[You'll need to do a numerical integral to evaluate C .]Exercise 11.6. [3 ]For large integers K and N , what fraction of all binary errorcorrecting codes of length N and rate R = K/N are linear codes?[The answer will depend on whether you choose to define the code to be an ordered list of 2 K codewords, that is, a mapping from s ∈ {1, 2, . . ., 2 K } to x (s) , or to define the code to be an unordered list, so that two codes consisting of the same codewords are identical.Use the latter definition: a code is a set of codewords; how the encoder operates is not part of the definition of the code.]Exercise 11.7. [4 ]Design a code for the binary erasure channel, and a decoding algorithm, and evaluate their probability of error.[The design of good codes for erasure channels is an active research area (Spielman, 1996;Byers et al., 1998); see also Chapter 50.]Exercise 11.8. [5 ]Design a code for the q-ary erasure channel, whose input x is drawn from 0, 1, 2, 3, . . ., (q − 1), and whose output y is equal to x with probability (1 − f ) and equal to ?otherwise.[This erasure channel is a good model for packets transmitted over the internet, which are either received reliably or are lost.]p.190] How do redundant arrays of independent disks (RAID) work?These are information storage systems consisting of about ten[Some people say RAID stands for 'redundant array of inexpensive disks', but I think that's silly -RAID would still be a good idea even if the disks were expensive!]disk drives, of which any two or three can be disabled and the others are able to still able to reconstruct any requested file.What codes are used, and how far are these systems from the Shannon limit for the problem they are solving?How would you design a better RAID system?Some information is provided in the solution section.See http://www.acnc.com/raid2.html;see also Chapter 50.Further Topics in Information TheoryIn Chapters 1-11, we concentrated on two aspects of information theory and coding theory: source coding -the compression of information so as to make efficient use of data transmission and storage channels; and channel codingthe redundant encoding of information so as to be able to detect and correct communication errors.In both these areas we started by ignoring practical considerations, concentrating on the question of the theoretical limitations and possibilities of coding.We then discussed practical source-coding and channel-coding schemes, shifting the emphasis towards computational feasibility.But the prime criterion for comparing encoding schemes remained the efficiency of the code in terms of the channel resources it required: the best source codes were those that achieved the greatest compression; the best channel codes were those that communicated at the highest rate with a given probability of error.In this chapter we now shift our viewpoint a little, thinking of ease of information retrieval as a primary goal.It turns out that the random codes which were theoretically useful in our study of channel coding are also useful for rapid information retrieval.Efficient information retrieval is one of the problems that brains seem to solve effortlessly, and content-addressable memory is one of the topics we will study when we look at neural networks.Decoding.To retrieve a piece of information corresponding to a target vector x, we compute the hash h of x and look at the corresponding location in the hash table.If there is a zero, then we know immediately that the string x is not in the database.The cost of this answer is the cost of one hash-function evaluation and one look-up in the table of size 2 M .If, on the other hand, there is a non-zero entry s in the table, there are two possibilities: either the vector x is indeed equal to x (s) ; or the vector x (s) is another vector that happens to have the same hash code as the target x.(A third possibility is that this non-zero entry might have something to do with our yet-to-be-discussed collision-resolution system.)To check whether x is indeed equal to x (s) , we take the tentative answer s, look up x (s) in the original forward database, and compare it bit by bit with x; if it matches then we report s as the desired answer.This successful retrieval has an overall cost of one hash-function evaluation, one look-up in the table of size 2 M , another look-up in a table of size S, and N binary comparisons -which may be much cheaper than the simple solutions presented in section 12.1.p.202]If we have checked the first few bits of x (s) with x and found them to be equal, what is the probability that the correct entry has been retrieved, if the alternative hypothesis is that x is actually not in the database?Assume that the original source strings are random, and the hash function is a random hash function.How many binary evaluations are needed to be sure with odds of a billion to one that the correct entry has been retrieved?The hashing method of information retrieval can be used for strings x of arbitrary length, if the hash function h(x) can be applied to strings of any length.We will study two ways of resolving collisions: appending in the table, and storing elsewhere.When encoding, if a collision occurs, we continue down the hash table and write the value of s into the next available location in memory that currently contains a zero.If we reach the bottom of the table before encountering a zero, we continue from the top.When decoding, if we compute the hash code for x and find that the s contained in the table doesn't point to an x (s) that matches the cue x, we continue down the hash table until we either find an s whose x (s) does match the cue x, in which case we are done, or else encounter a zero, in which case we know that the cue x is not in the database.For this method, it is essential that the table be substantially bigger in size than S. If 2 M < S then the encoding rule will become stuck with nowhere to put the last strings.A more robust and flexible method is to use pointers to additional pieces of memory in which collided strings are stored.There are many ways of doing 12.6: Further exercises 201 so to minimize the number of files hashed, N 1 + N 2 , Alice should make N 1 and N 2 equal, and will need to hash about 2 M/2 files until she finds two that match. 2Alice has to hash 2 M/2 files to cheat.[This is the square root of the number of hashes Fiona had to make.]If Alice has the use of C = 10 6 computers for T = 10 years, each computer taking t = 1 ns to evaluate a hash, the bet-communication system is secure against Alice's dishonesty only if M 2 log 2 CT /t 160 bits.The Bible for hash codes is volume 3 of Knuth (1968).I highly recommend the story of Doug McIlroy's spell program, as told in section 13.8 of Programming Pearls (Bentley, 2000).This astonishing piece of software makes use of a 64kilobyte data structure to store the spellings of all the words of 75 000-word dictionary.12.6 Further exercises Exercise 12.9. [1 ]What is the shortest the address on a typical international letter could be, if it is to get to a unique human recipient?(Assume the permitted characters are [A-Z,0-9].)How long are typical email addresses?p.203] How long does a piece of text need to be for you to be pretty sure that no human has written that string of characters before?How many notes are there in a new melody that has not been composed before?p.204] Pattern recognition by molecules.Some proteins produced in a cell have a regulatory role.A regulatory protein controls the transcription of specific genes in the genome.This control often involves the protein's binding to a particular DNA sequence in the vicinity of the regulated gene.The presence of the bound protein either promotes or inhibits transcription of the gene.(a) Use information-theoretic arguments to obtain a lower bound on the size of a typical protein that acts as a regulator specific to one gene in the whole human genome.Assume that the genome is a sequence of 3 × 10 9 nucleotides drawn from a four letter alphabet {A, C, G, T}; a protein is a sequence of amino acids drawn from a twenty letter alphabet.[Hint: establish how long the recognized DNA sequence has to be in order for that sequence to be unique to the vicinity of one gene, treating the rest of the genome as a random sequence.Then discuss how big the protein must be to recognize a sequence of that length uniquely.](b) Some of the sequences recognized by DNA-binding regulatory proteins consist of a subsequence that is repeated twice or more, for example the sequence (12.4)