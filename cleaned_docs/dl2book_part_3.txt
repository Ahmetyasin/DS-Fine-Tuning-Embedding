A diffusion model consists of an encoder and a decoder.The encoder takes a data sample x and maps it through a series of intermediate latent variables z 1 . . .z T .The decoder reverses this process; it starts with z T and maps back through z T −1 , . . ., z 1 until it finally (re-)creates a data point x.In both encoder and decoder, the mappings are stochastic rather than deterministic.The encoder is prespecified; it gradually blends the input with samples of white noise (figure 18.1).With enough steps, the conditional distribution q(z T |x) and marginal distribution q(z T ) of the final latent variable both become the standard normal distribution.Since this process is prespecified, all the learned parameters are in the decoder.In the decoder, a series of networks are trained to map backward between each adjacent pair of latent variables z t and z t−1 .The loss function encourages each network to invert the corresponding encoder step.The result is that noise is gradually removed from the representation until a realistic-looking data example remains.To generate a new data example x, we draw a sample from q(z T ) and pass it through the decoder.In section 18.2, we consider the encoder in detail.Its properties are non-obvious but are critical for the learning algorithm.In section 18.3, we discuss the decoder.Section 18.4 derives the training algorithm, and section 18.5 reformulates it to be more practical.Section 18.6 discusses implementation details, including how to make the generation conditional on text prompts.The diffusion or forward process 1 (figure 18.2) maps a data example x through a series of intermediate variables z 1 , z 2 , . . ., z T with the same size as x according to:where ϵ t is noise drawn from a standard normal distribution.The first term attenuates the data plus any noise added so far, and the second adds more noise.The hyperparameters β t ∈ [0, 1] determine how quickly the noise is blended and are collectively known as the noise schedule.The forward process can equivalently be written as:Figure 18.2 Forward process.a) We consider one-dimensional data x with T = 100 latent variables z1, . . ., z100 and β = 0.03 at all steps.Three values of x (gray, cyan, and orange) are initialized (top row).These are propagated through z1, . . ., z100.At each step, the variable is updated by attenuating its value by √ 1 − β and adding noise with mean zero and variance β (equation 18.1).Accordingly, the three examples noisily propagate through the variables with a tendency to move toward zero.b) The conditional probabilities P r(z1|x) and P r(zt|zt−1) are normal distributions with a mean that is slightly closer to zero than the current point and a fixed variance βt (equation 18.2).q(z 1 |x) = Norm z1 1 − β 1 x, β 1 I (18.2)This is a Markov chain because the probability of z t is determined entirely by the value of the immediately preceding variable z t−1 .With sufficient steps T , all traces of the original data are removed, and q(z T |x) = q(z T ) becomes a standard normal distribution. 2Problem 18.1The joint distribution of all of the latent variables z 1 , z 2 , . . ., z T given input x is:q(z t |z t−1 ).(18.3)Figure 18.3 Diffusion kernel.a) The point x * = 2.0 is propagated through the latent variables using equation 18.1 (five paths shown in gray).The diffusion kernel q(zt|x * ) is the probability distribution over variable zt given that we started from x * .It can be computed in closed-form and is a normal distribution whose mean moves toward zero and whose variance increases as t increases.Heatmap shows q(zt|x * ) for each variable.Cyan lines show ±2 standard deviations from the mean.b) The diffusion kernel q(zt|x * ) is shown explicitly for t = 20, 40, 80.In practice, the diffusion kernel allows us to sample a latent variable zt corresponding to a given x * without computing the intermediate variables z1, . . ., zt−1.When t becomes very large, the diffusion kernel becomes a standard normal.Figure 18.4 Marginal distributions.a) Given an initial density P r(x) (top row), the diffusion process gradually blurs the distribution as it passes through the latent variables zt and moves it toward a standard normal distribution.Each subsequent horizontal line of heatmap represents a marginal distribution q(zt).b) The top graph shows the initial distribution P r(x).The other two graphs show the marginal distributions q(z20) and q(z60), respectively.To train the decoder to invert this process, we use multiple samples z t at time t for the same example x.However, generating these sequentially using equation 18.1 is timeconsuming when t is large.Fortunately, there is a closed-form expression for q(z t |x), which allows us to directly draw samples z t given initial data point x without computing the intermediate variables z 1 . . .z t−1 .This is known as the diffusion kernel (figure 18.3).To derive an expression for q(z t |x), consider the first two steps of the forward process: (18.4) Substituting the first equation into the second, we get:The last two terms are independent samples from mean-zero normal distributions with variances 1 − β 2 − (1 − β 2 )(1 − β 1 ) and β 2 , respectively.The mean of this sum is zero,and its variance is the sum of the component variances (see problem 18.2), so: (18.6)where ϵ is also a sample from a standard normal distribution.If we continue this process by substituting this equation into the expression for z 3 and so on, we can show that: (18.7)where α t = t s=1 1 − β s .We can equivalently write this in probabilistic form:For any starting data point x, variable z t is normally distributed with a known mean and variance.Consequently, if we don't care about the history of the evolution through the intermediate variables z 1 . . .z t−1 , it is easy to generate samples from q(z t |x).The marginal distribution q(z t ) is the probability of observing a value of z t given the distribution of possible starting points x and the possible diffusion paths for each starting point (figure 18.4).It can be computed by considering the joint distribution q(x, z 1...t )Appendix C.1.2Marginalization and marginalizing over all the variables except z t :q(z t ) = q(z 1...t , x)dz 1...t−1 dx = q(z 1...t |x)P r(x)dz 1...t−1 dx, (18.9)where q(z 1...t |x) was defined in equation 18.3.However, since we now have an expression for the diffusion kernel q(z t |x) that "skips" the intervening variables, we can equivalently write: q(z t ) = q(z t |x)P r(x)dx.(18.10)Hence, if we repeatedly sample from the data distribution P r(x) and superimpose the diffusion kernel q(z t |x) on each sample, the result is the marginal distribution q(z t ) (fig-Notebook 18.1 Diffusion encoder ure 18.4).However, the marginal distribution cannot be written in closed form because we don't know the original data distribution P r(x).We defined the conditional probability q(z t |z t−1 ) as the mixing process (equation 18.2).To reverse this process, we apply Bayes' rule:q(z t−1 |z t ) = q(z t |z t−1 )q(z t−1 ) q(z t ) .(18.11)This is intractable since we cannot compute the marginal distribution q(z t−1 ).For this simple 1D example, it's possible to evaluate q(z t−1 |z t ) numerically (figure 18.5).In general, their form is complex, but in many cases, they are well-approximated by a normal distribution.This is important because when we build the decoder, we will approximate the reverse process using a normal distribution.There is one final distribution related to the encoder to consider.We noted above that we could not find the conditional distribution q(z t−1 |z t ) because we do not know the marginal distribution q(z t−1 ).However, if we know the starting variable x, then we do know the distribution q(z t−1 |x) at the time before.This is just the diffusion kernel (figure 18.3), and it is normally distributed.Hence, it is possible to compute the conditional diffusion distribution q(z t−1 |z t , x) in closed form (figure 18.6).This distribution is used to train the decoder.It is the distribution over z t−1 when we know the current latent variable z t and the training Figure 18.5 Conditional distribution q(zt−1|zt).a) The marginal densities q(zt) with three points z * t highlighted.b) The probability q(zt−1|z * t ) (cyan curves) is computed via Bayes' rule and is proportional to q(z * t |zt−1)q(zt−1).In general, it is not normally distributed (top graph), although often the normal is a good approximation (bottom two graphs).The first likelihood term q(z * t |zt−1) is normal in zt−1 (equation 18.2) with a mean that is slightly further from zero than z * t (brown curves).The second term is the marginal density q(zt−1) (gray curves).The probability q(zt−1|z * t , x * ) is computed via Bayes' rule and is proportional to q(z * t |zt−1)q(zt−1|x * ).This is normally distributed and can be computed in closed form.The first likelihood term q(z * t |zt−1) is normal in zt−1 (equation 18.2) with a mean that is slightly further from zero than z * t (brown curves).The second term is the diffusion kernel q(zt−1|x * ) (gray curves).data example x (which, of course, we do when training).To compute an expression for q(z t−1 |z t , x) we start with Bayes' rule: q(z t−1 |z t , x) = q(z t |z t−1 , x)q(z t−1 |x) q(z t |x) (18.12)∝ q(z t |z t−1 )q(z t−1 |x)where between the first two lines, we have used the fact that q(z t |z t−1 , x) = q(z t |z t−1 ) because the diffusion process is Markov, and all information about z t is captured by z t−1 .Between lines three and four, we use the Gaussian change of variables identity: (18.13) to rewrite the first distribution in terms of z t−1 .We then use a second Gaussian identity:Problems 18.4-18.5Norm w [a, A] • Norm w [b, B] ∝ (18.14)to combine the two normal distributions in z t−1 , which gives:1 − α t I .(18.15)Note that the constants of proportionality in equations 18.12, 18.13, and 18.14 must cancel out since the final result is already a correctly normalized probability distribution.When we learn a diffusion model, we learn the reverse process.In other words, we learn a series of probabilistic mappings back from latent variable z T to z T −1 , from z T −1 to z T −2 , and so on, until we reach the data x.The true reverse distributions q(z t−1 |z t ) of the diffusion process are complex multi-modal distributions (figure 18.5) that depend on the data distribution P r(x).We approximate these as normal distributions: (18.16)where f t [z t , ϕ t ] is a neural network that computes the mean of the normal distribution in the estimated mapping from z t to the preceding latent variable z t−1 .The terms {σ 2 t } are predetermined.If the hyperparameters β t in the diffusion process are close to zero (and the number of time steps T is large), then this normal approximation will be reasonable.We generate new examples from P r(x) using ancestral sampling.We start by drawing z T from P r(z T ).Then we sample z T −1 from P r(z T −1 |z T , ϕ T ), sample z T −2 from P r(z T −2 |z T −1 , ϕ T −1 ) and so on until we finally generate x from P r(x|z 1 , ϕ 1 ).The joint distribution of the observed variable x and the latent variables {z t } is: P r(x, z 1...T |ϕ 1...T ) = P r(x|z 1 , ϕ 1 ) T t=2 P r(z t−1 |z t , ϕ t ) • P r(z T ).(18.17)The likelihood of the observed data P r(x|ϕ 1...T ) is found by marginalizing over the latent We can't maximize this directly because the marginalization in equation 18.18 is intractable.Hence, we use Jensen's inequality to define a lower bound on the likelihood and optimize the parameters ϕ 1...T with respect to this bound exactly as we did for the VAE (see section 17.3.1).To derive the lower bound, we multiply and divide the log-likelihood by the encoder distribution q(z 1...T |x) and apply Jensen's inequality (see section 17.3.2):log [P r(x|ϕ 1...T )] = log P r(x, z 1...T |ϕ 1...T )dz 1...T = log q(z 1...T |x) P r(x, z 1...T |ϕ 1...T ) q(z 1...T |x) dz 1...T ≥ q(z 1...T |x) log P r(x, z 1...T |ϕ 1...T ) q(z 1...T |x) dz 1...T .(18.20)This gives us the evidence lower bound (ELBO):ELBO ϕ 1...T = q(z 1...T |x) log P r(x, z 1...T |ϕ 1...T ) q(z 1...T |x) dz 1...T .(18.21)In the VAE, the encoder q(z|x) approximates the posterior distribution over the latent variables to make the bound tight, and the decoder maximizes this bound (figure 17.10).In diffusion models, the decoder must do all the work since the encoder has no parameters.It makes the bound tighter by both (i) changing its parameters so that the static encoder does approximate the posterior P r(z 1...T |x, ϕ 1...T ) and (ii) optimizing its own parameters with respect to that bound (see figure 17.6).We now manipulate the log term from the ELBO into the final form that we will optimize.We first substitute in the definitions for the numerator and denominator from equations 18.17 and 18.3, respectively: log P r(x, z 1...T |ϕ 1...T ) q(z 1...T |x) = log P r(x|z 1 , ϕ 1 )P r(x|z 1 , ϕ 1 ) q(z 1 |x) +log T t=2 P r(z t−1 |z t , ϕ t ) T t=2 q(z t |z t−1 ) +log P r(z T ) .Then we expand the denominator of the second term:q(z t |z t−1 ) = q(z t |z t−1 , x) = q(z t−1 |z t , x)q(z t |x) q(z t−1 |x) , (18.23) where the first equality follows because all of the information about variable z t is encompassed in z t−1 , so the extra conditioning on the data x is irrelevant.The second where all but two of the terms in the product of the ratios q(z t−1 |x)/q(z t |x) cancel out between lines two and three leaving only q(z 1 |x) and q(z T |x).The last term in the third line is approximately log[1] = 0 since the result of the forward process q(z T |x) is a standard normal distribution, and so is equal to the prior P r(z T ).The simplified ELBO is hence:ELBO ϕ 1...T (18.25)= q(z 1...T |x) log P r(x, z 1...T |ϕ 1...T ) q(z 1...T |x) dz 1...T ≈ q(z 1...T |x) log [P r(x|z 1 , ϕ 1 )] +where we have marginalized over the irrelevant variables in q(z 1...T |x) between lines two Problem 18.7Appendix C.5.1 KL divergenceand three and used the definition of KL divergence (see problem 18.7).The first probability term in the ELBO was defined in equation 18.16: (18.26) and is equivalent to the reconstruction term in the VAE.The ELBO will be larger if the model prediction matches the observed data.As for the VAE, we will approximate the expectation over the log of this quantity using a Monte Carlo estimate (see equations 17. 22-17.23), in which we estimate the expectation with a sample from q(z 1 |x).The KL divergence terms in the ELBO measure the distance between P r(z t−1 |z t , ϕ t ) and q(z t−1 |z t , x), which were defined in equations 18.16 and 18.15, respectively:The KL divergence between two normal distributions has a closed-form expression.More-Appendix C.5.4 KL divergence between normal distributions over, many of the terms in this expression do not depend on ϕ (see problem 18.8), andProblem 18.8the expression simplifies to the squared difference between the means plus a constant C: D KL q(z t−1 |z t , x) P r(z t−1 |z t , ϕ t ) = (18.28)T I] and so on until we reach x (five paths shown).The estimated marginal densities (heatmap) are the aggregation of these samples and are similar to the true marginal densities (figure 18.4).b) The estimated distribution P r(zt−1|zt) (brown curve) is a reasonable approximation to the true posterior of the diffusion model q(zt−1|zt) (cyan curve) from figure 18.5.The marginal distributions P r(zt) and q(zt) of the estimated and true models (dark blue and gray curves, respectively) are also similar.To fit the model, we maximize the ELBO with respect to the parameters ϕ 1...T .We recast this as a minimization by multiplying with minus one and approximating the expectations with samples to give the loss function:target, mean of q(z t−1 |z t , x), where x i is the i th data point, and z it is the associated latent variable at diffusion step t.This loss function can be used to train a network for each diffusion time step.It minimizes the difference between the estimate f t [z t , ϕ t ] of the hidden variable at the previous time step and the most likely value that it took given the ground truth de-noised data x.Figures 18.7 and 18.8 show the fitted reverse process for the simple 1D example.This model was trained by (i) taking a large dataset of examples x from the original density, (ii) using the diffusion kernel to predict many corresponding values for the latent Notebook 18.2 1D diffusion model variable z t at each time t, and then (iii) training the models f t [z t , ϕ t ] to minimize the loss function in equation 18.29.These models were nonparametric (i.e., lookup tables relating 1D input to 1D output), but more typically, they would be deep neural networks.Although the loss function in equation 18.29 can be used, diffusion models have been found to work better with a different parameterization; the loss function is modified so that the model aims to predict the noise that was mixed with the original data example to create the current variable.Section 18.5.1 discusses reparameterizing the target (first two terms in second line of equation 18.29), and section 18.5.2discusses reparameterizing the network (last term in second line of equation 18.29).The original diffusion update was given by:It follows that the data term x in equation 18.28 can be expressed as the diffused image minus the noise that was added to it: (18.31)Substituting this into the target terms from equation 18.29 gives:where we have used the fact thatProblem 18.9lines.Simplifying further, we get:where we have multiplied the numerator and denominator of the first term by √ 1 − β t between lines two and three, multiplied out the terms, and simplified the numerator in the first term between lines three and four.Problem 18.10 Substituting this back into the loss function (equation 18.29), we have:Now we replace the model ẑt−which predicts the noise ϵ that was mixed with x to create z t :Substituting the new model into equation 18.34 produces the criterion:The log normal can be written as a least squares loss plus a constant C i (section 5.3.1):Substituting in the definitions of x and f 1 [z 1 , ϕ 1 ] from equations 18.31 and 18.35, spectively, the first term simplifies to:(18.37) Adding this back to the final loss function yields: (18.38)where we have disregarded the additive constants C i .In practice, the scaling factors (which might be different at each time step) are ignored, giving an even simpler formulation:where we have rewritten z t using the diffusion kernel (equation 18.30) in the second line.// Generate sample from z 1 without noiseDiffusion models have been very successful in modeling image data.Here, we need to construct models that can take a noisy image and predict the noise that was added at each step.The obvious architectural choice for this image-to-image mapping is the U-Net (figure 11.10).However, there may be a very large number of diffusion steps, and training and storing multiple U-Nets is inefficient.The solution is to train a single U-Net that also takes a predetermined vector representing the time step as input (figure 18.9).In practice, this is resized to match the number of channels at each stage of the U-Net and used to offset and/or scale the representation at each spatial position.A large number of time steps are needed as the conditional probabilities q(z t−1 |z t ) become closer to normal when the hyperparameters β t are close to zero, matching the form of the decoder distributions P r(z t−1 |z t , ϕ t ).However, this makes sampling slow.We might have to run the U-Net model through T = 1000 steps to generate good images.The loss function (equation 18.39) requires the diffusion kernel to have the formThe same loss function will be valid for any forward process with this relation, and there is a family of such compatible processes.These are all optimized by the same loss function but have different rules for the forward process and different corresponding rules for how to use the estimated noise g[z t , ϕ t ] to predict z t−1 from z t in the reverse process (figure 18.10).Among this family are denoising diffusion implicit models, which are no longer stochastic after the first step from x to z 1 , and accelerated sampling models, where the forward process is defined only on a sub-sequence of time steps.This allows a reverse process that skips time steps and hence makes sampling much more efficient; good sam-Notebook 18.4Families of diffusion models ples can be created with 50 time steps when the forward process is no longer stochastic.This is much faster than before but still slower than most other generative models.If the data has associated labels c, these can be exploited to control the generation.Sometimes this can improve generation results in GANs, and we might expect this to be the case in diffusion models as well; it's easier to denoise an image if you have some information about what that image contains.One approach to conditional synthesis in diffusion models is classifier guidance.This modifies the denoising update from z t to z t−1 to take into account class information c.In practice, this means adding an extra term into the final update step in algorithm 18.2 to yield:The new term depends on the gradient of a classifier P r(c|z t ) that is based on the latent variable z t .This maps features from the downsampling half of the U-Net to the class c.Like the U-Net, it is usually shared across all time steps and takes time as an input.The update from z t to z t−1 now makes the class c more likely.Classifier-free guidance avoids learning a separate classifier P r(c|z t ) but instead incorporates class information into the main model g t [z t , ϕ t , c].In practice, this usually takes the form of adding an embedding based on c to the layers of the U-Net in a similar way to how the time step is added (see figure 18.9).This model is jointly trained on conditional and unconditional objectives by randomly dropping the class information if the conditioning information is over-weighted, the model tends to produce very high quality but slightly stereotypical examples.This is somewhat analogous to the use of truncation in GANs (figure 15.10).As for other generative models, the highest quality results result from applying a combination of tricks and extensions to the basic model.First, it's been noted that it also helps to estimate the variances σ 2 t of the reverse process as well as the mean (i.e., the widths of the brown normal distributions in figure 18.7).This particularly improves the results when sampling with fewer steps.Second, it's possible to modify the noise schedule in the forward process so that β t varies at each step, and this can also improve results.Third, to generate high-resolution images, a cascade of diffusion models is used.The first creates a low-resolution image (possibly guided by class information).The subsequent diffusion models generate progressively higher-resolution images.They condition on the lower-resolution image by resizing this and appending it to the layers of the constituent U-Net, as well as any other class information (figure 18.11).Combining all of these techniques allows the generation of very high-quality images.Figure 18.12 shows examples of images generated from a model conditioned on the Ima-geNet class.It is particularly impressive that the same model can learn to generate such diverse classes.Figure 18.13 shows images generated from a model that is trained to condition on text captions encoded by a language model like BERT, which are inserted into the model in the same way as the time step (figures 18.9 and 18.11).This results in very realistic images that agree with the caption.Since the diffusion model is stochastic by nature, it's possible to generate multiple images that are conditioned on the same caption.Diffusion models map the data examples through a series of latent variables by repeatedly blending the current representation with random noise.After sufficient steps, the representation becomes indistinguishable from white noise.Since these steps are small, the reverse denoising process at each step can be approximated with a normal distribution and predicted by a deep learning model.The loss function is based on the evidence lower bound (ELBO) and ultimately results in a simple least-squares formulation.For image generation, each denoising step is implemented using a U-Net, so sampling is slow compared to other generative models.To improve generation speed, it's possible to change the diffusion model to a deterministic formulation, and here sampling with fewer steps works well.Several methods have been proposed to condition generation on class information, images, and text information.Combining these methods produces impressive text-to-image synthesis results.Denoising diffusion models were introduced by Sohl-Dickstein et al. (2015), and early related work based on score-matching was carried out by .Ho et al. (2020) produced image samples that were competitive with GANs and kick-started a wave of interest in this area.Most of the exposition in this chapter, including the original formulation and the reparameterization, is derived from this paper.improved the quality of these results and showed for the first time that images from diffusion models were quantitatively superior to GAN models in terms of Fréchet Inception Distance.At the time  of writing, the state-of-the-art results for conditional image synthesis have been achieved by Karras et al. (2022).Surveys of denoising diffusion models can be found in Croitoru et al. (2022), Cao et al. (2022), Luo (2022), andYang et al. (2022).Applications for images: Applications of diffusion models include text-to-image generation (Nichol et al., 2022;Ramesh et al., 2022;Saharia et al., 2022b), image-to-image tasks such as colorization, inpainting, uncropping and restoration (Saharia et al., 2022a), super-resolution (Saharia et al., 2022c), image editing (Hertz et al., 2022;Meng et al., 2021), removing adversarial perturbations (Nie et al., 2022), semantic segmentation (Baranchuk et al., 2022), and medical imaging (Song et al., 2021b;Peng et al., 2022;Xie & Li, 2022;Luo et al., 2022) where the diffusion model is sometimes used as a prior.Different data types: Diffusion models have also been applied to video data (Ho et al., 2022b;Harvey et al., 2022;Yang et al., 2022;Höppe et al., 2022;Voleti et al., 2022) for generation, past and future frame prediction, and interpolation.They have been used for 3D shape generation (Zhou et al., 2021;Luo & Hu, 2021), and recently a technique has been introduced to generate 3D models using only a 2D text-to-image diffusion model (Poole et al., 2023).Austin et al. (2021) and Hoogeboom et al. (2021) investigated diffusion models for discrete data.Kong et al. (2021) and Chen et al. (2021d) applied diffusion models to audio data.The diffusion models in this chapter mix noise with the data and build a model to gradually denoise the result.However, degrading the image using noise is not necessary.Rissanen et al. (2022) devised a method that progressively blurred the image and Bansal et al. (2022) showed that the same ideas work with a large family of degradations that do not have to be stochastic.These include masking, morphing, blurring, and pixelating.Comparison to other generative models: Diffusion models synthesize higher quality images than other generative models and are simple to train.They can be thought of as a special case of a hierarchical VAE (Vahdat & Kautz, 2020;Sønderby et al., 2016b) where the encoder is fixed, and the latent space is the same size as the data.They are probabilistic, but in their basic form, they can only compute a lower bound on the likelihood of a data point.However, Kingma et al. (2021) show that this lower bound improves on the exact log-likelihoods for test data from normalizing flows and autoregressive models.The likelihood for diffusion models can be computed by converting to an ordinary differential equation (Song et al., 2021c) or by training a continuous normalizing flow model with a diffusion-based criterion (Lipman et al., 2022).The main disadvantages of diffusion models are that they are slow and that the latent space has no semantic interpretation.Improving quality: Many techniques have been proposed to improve image quality.These include the reparameterization of the network described in section 18.5 and the equal weighting of the subsequent terms (Ho et al., 2020).Choi et al. (2022) subsequently investigated different weightings of terms in the loss function.Kingma et al. (2021) improved the test log-likelihood of the model by learning the denoising weights βt.Conversely,  improved performance by learning separate variances σ 2 of the denoising estimate at each time step in addition to the mean.Bao et al. (2022) show how to learn the variances after training the model.Ho et al. (2022a) developed the cascaded method for producing very high-resolution images (figure 18.11).To prevent artifacts in lower-resolution images from being propagated to higher resolutions, they introduced noise conditioning augmentation; here, the lower-resolution conditioning image is degraded by adding noise at each training step.This reduces the reliance on the exact details of the lower-resolution image during training. I is also done during inference, where the best noise level is chosen by sweeping over different values.Improving speed: One of the major drawbacks of diffusion models is that they take a long time to train and sample from.Stable diffusion (Rombach et al., 2022) projects the original data to a smaller latent space using a conventional autoencoder and then runs the diffusion process in this smaller space.This has the advantages of reducing the dimensionality of the training data for the diffusion process and allowing other data types (text, graphs, etc.) to be described by diffusion models.Vahdat et al. (2021) applied a similar approach.Song et al. (2021a) showed that an entire family of diffusion processes is compatible with the training objective.Most of these processes are non-Markovian (i.e., the diffusion step does not only depend on the results of the previous step).One of these models is the denoising diffusion implicit model (DDIM), in which the updates are not stochastic (figure 18.10b).This model is amenable to taking larger steps (figure 18.10b) without inducing large errors.It effectively converts the model into an ordinary differential equation (ODE) in which the trajectories have low curvature and allows efficient numerical methods for solving ODEs to be applied.Song et al. (2021c) propose converting the underlying stochastic differential equations into a probability flow ODE which has the same marginal distributions as the original process.Vahdat et al. (2021), Xiao et al. (2022b), andKarras et al. (2022) all exploit techniques for solving ODEs to speed up synthesis.Karras et al. (2022) identified the best-performing time discretization for sampling and evaluated different sampler schedules.The result of these and other improvements has been a significant drop in steps required during synthesis.Sampling is slow because many small diffusion steps are required to ensure that the posterior distribution q(zt−1|zt) is close to Gaussian (figure 18.5), so the Gaussian distribution in the decoder is appropriate.If we use a model that describes a more complex distribution at each denoising step, then we can use fewer diffusion steps in the first place.To this end, Xiao et al. (2022b) have investigated using conditional GAN models, and Gao et al. (2021) investigated using conditional energy-based models.Although these models cannot describe the original data distribution, they suffice to predict the (much simpler) reverse diffusion step.distilled adjacent steps of the denoising process into a single step to speed up synthesis.Dockhorn et al. (2022) introduced momentum into the diffusion process.This makes the trajectories smoother and so more amenable to coarse sampling.introduced classifier guidance, in which a classifier learns to identify the category of object being synthesized at each step, and this is used to bias the denoising update toward that class.This works well, but training a separate classifier is expensive.Classifier-free guidance  concurrently trains conditional and unconditional denoising models by dropping the class information some proportion of the time in a process akin to dropout.This technique allows control of the relative contributions of the conditional and unconditional components.Over-weighting the conditional component causes the model to produce more typical and realistic samples.The standard technique for conditioning on images is to append the (resized) image to the different layers of the U-Net.For example, this was used in the cascaded generation process for super-resolution (Ho et al., 2022a).Choi et al. (2021) provide a method for conditioning on images in an unconditional diffusion model by matching the latent variables with those of a conditioning image.The standard technique for conditioning on text is to linearly transform the text embedding to the same size as the U-Net layer and then add it to the representation in the same way that the time embedding is introduced (figure 18.9).Existing diffusion models can also be fine-tuned to be conditioned on edge maps, joint positions, segmentation, depth maps, etc., using a neural network structure called a control network (Zhang & Agrawala, 2023).Text-to-image: Before diffusion models, state-of-the-art text-to-image systems were based on transformers (e.g., Ramesh et al., 2021).GLIDE (Nichol et al., 2022) and Dall•E 2 (Ramesh et al., 2022) are both conditioned on embeddings from the CLIP model (Radford et al., 2021), which generates joint embeddings for text and image data.Imagen (Saharia et al., 2022b) showed that text embeddings from a large language model could produce even better results (see figure 18.13).The same authors introduced a benchmark (DrawBench) which is designed to evaluate the ability of a model to render colors, numbers of objects, spatial relations, and other characteristics.Feng et al. (2022) developed a Chinese text-to-image model.This chapter described diffusion models as hierarchical variational autoencoders because this approach connects most closely with the other parts of this book.However, diffusion models also have close connections with stochastic differential equations (consider the paths in figure 18.5) and with score matching .Song et al. (2021c) presented a framework based on stochastic differential equations that encompasses both the denoising and score matching interpretations.Diffusion models also have close connections to normalizing flows .Yang et al. (2022) present an overview of the relationship between diffusion models and other generative approaches.Problem 18.1 Show that if Var[xt−1] = I and we use the update: (18.41) then Var[xt] = I, so the variance stays the same.Problem 18.2 Consider the variable: (18.42)where both ϵ1 and ϵ2 are drawn from independent standard normal distributions with mean zero and unit variance.Show that: (18.43) so we could equivalently compute z = √ a 2 + b 2 • ϵ where ϵ is also drawn from a standard normal distribution.Problem 18.3 Continue the process in equation 18.5 to show that: (18.44)where ϵ ′ is a draw from a standard normal distribution.Problem 18.4 * Prove the relation: (18.45)Problem 18.5 * Prove the relation:(18.47)Substitute the definitions from equation 18.27 into this expression and show that the only term that depends on the parameters ϕ is the first term from equation 18.28.Problem 18.9 * If αt = t s=1 1 − βs, then show that:Problem 18.10 * If αt = t s=1 1 − βs, then show that: Problem 18.12 Classifier-free guidance allows us to create more stereotyped "canonical" images of a given class.When we described transformer decoders, generative adversarial networks, and the GLOW algorithm, we also discussed methods to reduce the amount of variation and produce more stereotyped outputs.What were these?Do you think it's inevitable that we should limit the output of generative models in this way?Chapter 19Reinforcement learning (RL) is a sequential decision-making framework in which agents learn to perform actions in an environment with the goal of maximizing received rewards.For example, an RL algorithm might control the moves (actions) of a character (the agent) in a video game (the environment), aiming to maximize the score (the reward).In robotics, an RL algorithm might control the movements (actions) of a robot (the agent) in the world (the environment) to perform a task (earning a reward).In finance, an RL algorithm might control a virtual trader (the agent) who buys or sells assets (the actions) on a trading platform (the environment) to maximize profit (the reward).Consider learning to play chess.Here, there is a reward of +1, −1, or 0 at the end of the game if the agent wins, loses, or draws and 0 at every other time step.This illustrates the challenges of RL.First, the reward is sparse; here, we must play an entire game to receive feedback.Second, the reward is temporally offset from the action that caused it; a decisive advantage might be gained thirty moves before victory.We must associate the reward with this critical action.This is termed the temporal credit assignment problem.Third, the environment is stochastic; the opponent doesn't always make the same move in the same situation, so it's hard to know if an action was truly good or just lucky.Finally, the agent must balance exploring the environment (e.g., trying new opening moves) with exploiting what it already knows (e.g., sticking to a previously successful opening).This is termed the exploration-exploitation trade-off.Reinforcement learning is an overarching framework that does not necessarily require deep learning.However, in practice, state-of-the-art systems often use deep networks.They encode the environment (the video game display, robot sensors, financial time series, or chessboard) and map this directly or indirectly to the next action (figure 1.13).Reinforcement learning maps observations of an environment to actions, aiming to maximize a numerical quantity that is connected to the rewards received.In the most common case, we learn a policy that maximizes the expected return in a Markov decision process.This section explains these terms.The ice is slippery, so at each time, it has an equal probability of moving to any adjacent state.For example, in position 6, it has a 25% chance of moving to states 2, 5, 7, and 10.A trajectory τ = [s1, s2, s3, . ..] from this process consists of a sequence of states.A Markov process assumes that the world is always in one of a set of possible states.The word Markov implies that the probability of being in a state depends only on the previous state and not on the states before.The changes between states are captured by the transition probabilities P r(s t+1 |s t ) of moving to the next state s t+1 given the current state s t , where t indexes the time step.Hence, a Markov process is an evolving system that produces a sequence s 1 , s 2 , s 3 . . . of states (figure 19.1).A Markov reward process extends the Markov process to include a distribution P r(r t+1 |s t ) over the possible rewards r t+1 received at the next time step, given that we are in state s t .This produces a sequence s 1 , r 2 , s 2 , r 3 , s 3 , r 4 . . . of states and the associated rewards (figure 19.2).The Markov reward process also includes a discount factor γ ∈ (0, 1] that is used to compute the return G t at time t:The return is the sum of the cumulative discounted future rewards; it measures the future benefit of being on this trajectory.A discount factor of less than one makes rewards that are closer in time more valuable than rewards that are further away.Figure 19.2Markov reward process.This associates a distribution P r(rt+1|st) of rewards rt+1 with each state st.a) Here, the rewards are deterministic; the penguin will receive a reward of +1 if it lands on a fish and 0 otherwise.The trajectory τ now consists of a sequence s1, r2, s2, r3, s3, r4 . . . of alternating states and rewards, terminating after eight steps.The return Gt of the sequence is the sum of discounted future rewards, here with discount factor γ = 0.9.b-c) As the penguin proceeds along the trajectory and gets closer to reaching the rewards, the return increases.The penguin moves in the intended direction with 50% probability, but the ice is slippery, so it may slide to one of the other adjacent positions with equal probability.Accordingly, in panel (a), the action taken (gray arrows) doesn't always line up with the trajectory (orange line).Here, the action does not affect the reward, so P r(rt+1|st, at) = P r(rt+1|st).The trajectory τ from an MDP consists of a sequence s1, a1, r2, s2, a2, r3, s3, a3, r4 . . . of alternating states st, actions at, and rewards, rt+1.Note that here the penguin receives the reward when it leaves a state with a fish (i.e., the reward is received for passing through the fish square, regardless of whether the penguin arrived there intentionally or not).  .Some policies are better than others.This policy is not optimal but still generally steers the penguin from top-left to bottom-right where the reward lies.b) This policy is more random.c) A stochastic policy has a probability distribution over actions for each state (probability indicated by size of arrows).This has the advantage that the agent explores the states more thoroughly and can be necessary for optimal performance in partially observable Markov decision processes.A Markov decision process or MDP adds a set of possible actions at each time step.The action a t changes the transition probabilities, which are now written as P r(s t+1 |s t , a t ).The rewards can also depend on the action and are now written as P r(r t+1 |s t , a t ).An MDP produces a sequence s 1 , a 1 , r 2 , s 2 , a 2 , r 3 , s 3 , a 3 , r 4 . . . of states, actions, and rewards (figure 19.3).The entity that performs the actions is known as the agent.In a partially observable Markov decision process or POMDP, the state is not directly visible (figure 19.4).Instead, the agent receives an observation o t drawn from P r(o t |s t ).Hence, a POMDP generates a sequence s 1 , o 1 , a 1 , r 2 , s 2 , o 2 , a 2 , r 3 , o 3 , a 3 , s 3 , r 4 , . . . of states, observations, actions, and rewards.In general, each observation will be more compatible with some states than others but insufficient to identify the state uniquely.The rules that determine the agent's action for each state are known as the policy (figure 19.5).This may be stochastic (the policy defines a distribution over actions for each state) or deterministic (the agent always takes the same action in a given state).A stochastic policy π[a|s] returns a probability distribution over each possible action a for state s, from which a new action is sampled.A deterministic policy π[a|s] returns one for the action a that is chosen for state s and zero otherwise.A stationary policy depends only on the current state.A non-stationary policy also depends on the time step.The environment and the agent form a loop (figure 19.6).The agent receives theThe previous section introduced the Markov decision process and the idea of an agent carrying out actions according to a policy.We want to choose a policy that maximizes the expected return.In this section, we make this idea mathematically precise.To do that, we assign a value to each state s t and state-action pair {s t , a t }.The return G t depends on the state s t and the policy π [a|s].From this state, the agent will pass through a sequence of states, taking actions and receiving rewards.This sequence differs every time the agent starts in the same place since, in general, the policy  value q[st, at, π] of an action at in state st (four numbers at each position/state corresponding to four actions) is the expected return given that this particular action is taken in this state.In this case, it gets larger as we get closer to the fish and is larger for actions that head in the direction of the fish.c) If we know the action values at a state, then the policy can be modified so that it chooses the maximum of these values (red numbers in panel b).The action value tells us the long-term reward we can expect on average if we start in this state, take this action, and follow the specified policy thereafter.Through this quantity, reinforcement learning algorithms connect future rewards to current actions (i.e., resolve the temporal credit assignment problem).We want a policy that maximizes the expected return.For MDPs (but not POMDPs), there is always a deterministic, stationary policy that maximizes the value of every state.If we know this optimal policy, then we get the optimal state-value function v * [s t ]:Similarly, the optimal state-action value function is obtained under the optimal policy:We may not know the state values v[s t ] or action values q[s t , a t ] for any policy. 2However, we know that they must be consistent with one another, and it's easy to write relations between these quantities. 1 The notation π[at|st] ← a in equations 19.6, 19.12, and 19.13 means set π[at|s] to one for action a and π [at|s] to zero for other actions.2 For simplicity, we will just write v[st] and q[st, at] instead of v[st|π] and q[st, at|π] from now on. 3We also assume from now on that the rewards are deterministic and can be written as r [st, at].The latter two relations are the Bellman equations and are the backbone of many RL methods.In short, they say that the state (action) values have to be self-consistent.Consequently, when we update an estimate of one state (action) value, this will have a ripple effect that causes modifications to all the others.Tabular RL algorithms (i.e., those that don't rely on function approximation) are divided into model-based and model-free methods.Model-based methods use the MDP structure explicitly and find the best policy from the transition matrix P r(s t+1 |s t , a t ) and reward structure r [s, a].If these are known, this is a straightforward optimization problem that can be tackled using dynamic programming.If they are unknown, they must first be estimated from observed MDP trajectories. 4onversely, model-free methods eschew a model of the MDP and fall into two classes:1. Value estimation approaches estimate the optimal state-action value function and then assign the policy according to the action in each state with the greatest value.2. Policy estimation approaches directly estimate the optimal policy using a gradient descent technique without the intermediate steps of estimating the model or values.Within each family, Monte Carlo methods simulate many trajectories through the MDP for a given policy to gather information from which this policy can be improved.Sometimes it is not feasible or practical to simulate many trajectories before updating the policy.Temporal difference (TD) methods update the policy while the agent traverses the MDP.We now briefly describe dynamic programming methods, Monte Carlo value estimation methods, and TD value estimation methods.Section 19.4 describes how deep networks have been used in TD value estimation methods.We return to policy estimation in section 19.5.The policy is updated to move the agent to states with the highest value (equation 19.12).c) After several iterations, the algorithm converges to the optimal policy, in which the penguin tries to avoid the holes and reach the fish.Dynamic programming algorithms assume we have perfect knowledge of the transition and reward structure.In this respect, they are distinguished from most RL algorithms which observe the agent interacting with the environment to gather information about these quantities indirectly.The state values v[s] are initialized arbitrarily (usually to zero).The deterministic policy π[a|s] is also initialized (e.g., by choosing a random action for each state).The algorithm then alternates between iteratively computing the state values for the current policy (policy evaluation) and improving that policy (policy improvement).We sweep through the states s t , updating their values: (19.11)where s t+1 is the successor state and P r(s t+1 |s t , a t ) is the state transition probability.Each update makes v[s t ] consistent with the value at the successor state s t+1 using the Bellman equation for state values (equation 19.9).This is termed bootstrapping.To update the policy, we greedily choose the action that maximizes the value for each state:This is guaranteed to improve the policy according to the policy improvement theorem.These two steps are iterated until the policy converges (figure 19.10).There are many variations of this approach.In policy iteration, the policy evaluation step is iterated until convergence before policy improvement.The values can be updated either in place or synchronously in each sweep.In value iteration, the policy evaluation Notebook 19.2 Dynamic programming procedure sweeps through the values just once before policy improvement.Asynchronous dynamic programming algorithms don't have to systematically sweep through all the values at each step but can update a subset of the states in place in an arbitrary order.Unlike dynamic programming algorithms, Monte Carlo methods don't assume knowledge of the MDP's transition probabilities and reward structure.Instead, they gain experience by repeatedly sampling trajectories from the MDP and observing the rewards.They alternate between computing the action values (based on this experience) and updating the policy (based on the action values).To estimate the action values q[s, a], a series of episodes are run.Each starts with a given state and action and thereafter follows the current policy, producing a series of actions, states, and returns (figure 19.11a).The action value for a given state-action pair under the current policy is estimated as the average of the empirical returns that follow after each time this pair is observed (figure 19.11b).Then the policy is updated by choosing the action with the maximum value at every state (figure 19.11c):This is an on-policy method; the current best policy is used to guide the agent through the environment.This policy is based on the observed action values in every state, but of course, it's not possible to estimate the value of actions that haven't been used, and there is nothing to encourage the algorithm to explore these.One solution is to use exploring starts.Here, episodes with all possible state-action pairs are initiated, so every combination is observed at least once.However, this is impractical if the number of states is large or the starting point cannot be controlled.A different approach is Problem 19.4to use an epsilon greedy policy, in which a random action is taken with probability ϵ, and the optimal action is allotted the remaining probability.The choice of ϵ trades off exploitation and exploration.Here, an on-policy method will seek the best policy from this epsilon-greedy family, which will not generally be the best overall policy.Conversely, in off-policy methods, the optimal policy π (the target policy) is learned based on episodes generated by a different behavior policy π ′ .Typically, the target policy is deterministic, and the behavior policy is stochastic (e.g., an epsilon-greedy policy).Hence, the behavior policy can explore the environment, but the learned target Notebook 19.3 Monte Carlo methods policy remains efficient.Some off-policy methods explicitly use importance sampling (section 17.8.1) to estimate the action value under policy π using samples from π ′ .Others, such as Q-learning (described in the next section), estimate the values based on the greedy action, even though this is not necessarily what was chosen.Dynamic programming methods use a bootstrapping process to update the values to make them self-consistent under the current policy.Monte Carlo methods sampled the MDP to acquire information.Temporal difference (TD) methods combine both bootstrapping and sampling.However, unlike Monte Carlo methods, they update the values and policy while the agent traverses the states of the MDP instead of afterward.SARSA (State-Action-Reward-State-Action) is an on-policy algorithm with update:where α ∈ R + is the learning rate.The bracketed term is called the TD error and measures the consistency between the estimated action value q[s t , a t ] and the estimate r[s t , a t ]+γ • q[s t+1 , a t+1 ] after taking a single step.By contrast, Q-Learning is an off-policy algorithm with update (figure 19.12): .15)where now the choice of action at each step is derived from a different behavior policy π ′ .In both cases, the policy is updated by taking the maximum of the action values at each state (equation 19.13).It can be shown that these updates are contraction Problem 19.5 mappings (see equation 16.20); the action values will eventually converge, assuming that every state-action pair is visited an infinite number of times.The maximum action value at the new state is found (here 0.43).c) The action value for action 2 in the original state is updated to 1.12 based on the current estimate of the maximum action value at the subsequent state, the reward, discount factor γ = 0.9, and learning rate α = 0.1.This changes the highest action value at the original state, so the policy changes.The tabular Monte Carlo and TD algorithms described above repeatedly traverse the entire MDP and update the action values.However, this is only practical if the stateaction space is small.Unfortunately, this is rarely the case; even for the constrained environment of a chessboard, there are more than 10 40 possible legal states.In fitted Q-learning, the discrete representation q[s t , a t ] of the action values is replaced by a machine learning model q[s t , a t , ϕ], where now the state is represented by a vector s t rather than just an index.We then define a least squares loss based on the consistency of adjacent action values (similarly to in Q-learning, see equation 19.15): .16)which in turn leads to the update:(19.17)Fitted Q-learning differs from Q-Learning in that convergence is no longer guaranteed.A change to the parameters potentially modifies both the target r[s t , a t ] + γ • max at+1 [q[s t+1 , a t+1 , ϕ]] (the maximum value may change) and the prediction q[s t , a t , ϕ].This can be shown both theoretically and empirically to damage convergence.d) Even for games with a single screen, the state is not fully observable from a single frame because the velocity of the objects is unknown.Consequently, it is usual to use several adjacent frames (here, four) to represent the state.e) The action simulates the user input via a joystick.f) There are eighteen actions corresponding to eight directions of movement or no movement, and for each of these nine cases, the button being pressed or not.Deep networks are ideally suited to making predictions from a high-dimensional state space, so they are a natural choice for the model in fitted Q-learning.In principle, they could take both state and action as input and predict the values, but in practice, the network takes only the state and simultaneously predicts the values for each action.The Deep Q-Network was a breakthrough reinforcement learning architecture that exploited deep networks to learn to play ATARI 2600 games.The observed data comprises 220×160 images with 128 possible colors at each pixel (figure 19.13).This was reshaped to size 84×84, and only the brightness value was retained.Unfortunately, the full state is not observable from a single frame.For example, the velocity of game objects is unknown.To help resolve this problem, the network ingests the last four frames at each time step to form s t .It maps these frames through three convolutional layers followed by a fully connected layer to predict the value of every action (figure 19.14).Several modifications were made to the standard training procedure.First, the rewards (which were driven by the score in the game) were clipped to −1 for a negative change and +1 for a positive change.This compensates for the wide variation in scores between different games and allows the same learning rate to be used.Second, the system exploited experience replay.Rather than update the network based on the tuple < s t , a t , r t+1 , s t+1 > at the current step or with a batch of the last I tuples, all recent The input st consists of four adjacent frames of the ATARI game.Each is resized to 84×84 and converted to grayscale.These frames are represented as four channels and processed by an 8×8 convolution with stride four, followed by a 4×4 convolution with stride 2, followed by two fully connected layers.The final output predicts the action value q[st, at] for each of the 18 actions in this state.tuples were stored in a buffer.This buffer was sampled randomly to generate a batch at each step.This approach reuses data samples many times and reduces correlations between the samples in the batch that arise due to the similarity of adjacent frames.Finally, the issue of convergence in fitted Q-Networks was tackled by fixing the target parameters to values ϕ − and only updating them periodically.This gives the update:Now the network no longer chases a moving target and is less prone to oscillation.Using these and other heuristics and with an ϵ-greedy policy, Deep Q-Networks performed at a level comparable to a professional game tester across a set of 49 games using the same network (trained separately for each game).It should be noted that the training process was data-intensive.It took around 38 full days of experience to learn each game.In some games, the algorithm exceeded human performance.On other games like "Montezuma's Revenge," it barely made any progress.This game features sparse rewards and multiple screens with quite different appearances.One potential flaw of Q-Learning is that the maximization over the actions in the update:leads to a systematic bias in the estimated state values q[s t , a t ].Consider two actions that provide the same average reward, but one is stochastic and the other deterministic.The stochastic reward will exceed the average roughly half of the time and be chosen by the maximum operation, causing the corresponding action value q[s t , a t ] to be overestimated.A similar argument can be made about random inaccuracies in the output of the network q[s t , a t , ϕ] or random initializations of the q-function.The underlying problem is that the same network both selects the target (by the maximization operation) and updates the value.Double Q-Learning tackles this problem by training two models q 1 [s t , a t , π 1 ] and q 2 [s t , a t , π 2 ] simultaneously:Now the choice of the target and the target itself are decoupled, which helps prevent these biases.In practice, new tuples < s, a, r, s ′ > are randomly assigned to update one model or another.This is known as double Q-learning.Double deep Q-networks or double DQNs use deep networks q[s t , a t , ϕ 1 ] and q[s t , a t , ϕ 2 ] to estimate the action values, and the update becomes:(19.21)Q-learning estimates the action values first and then uses these to update the policy.Conversely, policy-based methods directly learn a stochastic policy π[a t |s t , θ].This is a function with trainable parameters θ that maps a state s t to a distribution P r(a t |s t ) over actions a t from which we can sample.In MDPs, there is always an optimal deterministic policy.However, there are three reasons to use a stochastic policy:1.A stochastic policy naturally helps with exploration of the space; we are not obliged to take the best action at each time step.(19.23)where the return is the sum of all the rewards received along the trajectory.To maximize this quantity, we use the gradient ascent update:where α is the learning rate.We want to approximate this integral with a sum over empirically observed trajectories.These are drawn from the distribution P r(τ |θ), so to make progress, we multiply and divide the integrand by this distribution:This equation has a simple interpretation (figure 19.15); the update changes the parameters θ to increase the likelihood P r(τ i |θ) of an observed trajectory τ i in proportion to the reward r[τ i ] from that trajectory.However, it also normalizes by the probability of observing that trajectory in the first place to compensate for the fact that some trajectories are observed more often than others.If a trajectory is already common and yields high rewards, then we don't need to change much.The biggest updates will come from trajectories that are uncommon but create large rewards.We can simplify this expression using the likelihood ratio identity: Five episodes for the same policy (brighter indicates higher reward).Trajectories 1, 2, and 3 generate consistently high rewards, but similar trajectories already frequently occur with this policy, so there is no need to change.Conversely, trajectory 4 receives low rewards, so the policy should be modified to avoid producing similar trajectories.Trajectory 5 receives high rewards and is unusual.This will cause the largest change to the policy under equation 19.25.(19.26) which yields the update:The log probability log[P r(τ |θ)] of a trajectory is given by:and noting that only the center term depends on θ, we can rewrite the update from equation 19.27 as:where s it is the state at time t in episode i, and a it is the action taken at time t in episode i.Note that since the terms relating to the state evolution P r(s t+1 |s t , a t ) disappear, this parameter update does not assume a Markov time evolution process.We can further simplify this by noting that: (19.30)where r it is the reward at time t in the i th episode.The first term (the rewards before time t) does not affect the update from time t, so we can write:REINFORCE is an early policy gradient algorithm that exploits this result and incorporates discounting.It is a Monte Carlo method that generates episodesFor discrete actions, this policy could be determined by a neural network π[s|θ], which takes the current state s and returns one output for each possible action.These outputs are passed through a softmax function to create a distribution over actions, which is sampled at each time step.For each episode i, we loop through each step t and calculate the empirical discounted return for the partial trajectory τ it that starts at time t: (19.32) and then we update the parameters for each time step t in each trajectory: (19.33)where π at [s t , θ] is the probability of a t produced by the neural network given the current state s t and parameters θ, and α is the learning rate.The extra term γ t ensures that the rewards are discounted relative to the start of the sequence because we maximize the log probability of returns in the whole sequence (equation 19.23).Policy gradient methods have the drawback that they exhibit high variance; many episodes may be needed to get stable updates of the derivatives.One way to reduce this variance is to subtract the trajectory returns r[τ ] from a baseline b:  (19.35) and the expected value will not change.However, if the baseline co-varies with irrelevant This raises the question of how we should choose b.We can find the value of b that minimizes the variance by writing an expression for the variance, taking the derivative with respect to b, setting the result to zero, and solving to yield:In practice, this is often approximated as: .37)Subtracting this baseline factors out variance that might occur when the returns r[τ i ] from all trajectories are greater than is typical but only because they happen to pass through states with higher than average returns whatever actions are taken.A better option is to use a baseline b[s it ] that depends on the current state s it .Here, we are compensating for variance introduced by some states having greater overall returns than others, whichever actions we take.A sensible choice is the expected future reward based on the current state, which is just the state value v [s].In this case, the difference between the empirically observed rewards and the baseline is known as the advantage estimate.Since we are in a Monte Carlo context, this can be parameterized by a neural network b[s] = v[s, ϕ] with parameters ϕ, which we can fit to the observed returns using least squares loss:(19.39)Actor-critic algorithms are temporal difference (TD) policy gradient algorithms.They can update the parameters of the policy network at each step.This contrasts with the Monte Carlo REINFORCE algorithm, which must wait for one or more episodes to complete before updating the parameters.In the TD approach, we do not have access to the future rewards r[τ t ] = T k=t r k along this trajectory.Actor-critic algorithms approximate the sum over all the future rewards with the observed current reward plus the discounted value of the next state:Concurrently, we update the parameters ϕ by bootstrapping using the loss function:The policy network π[s t , θ] that predicts P r(a|s t ) is termed the actor.The value network v[s t , ϕ] is termed the critic.Often the same network represents both actor and Figure 19.17Decision transformer.The decision transformer treats offline reinforcement learning as a sequence prediction task.The input is a sequence of states, actions, and returns-to-go (remaining rewards in the episode), each of which is mapped to a fixed-size embedding.At each time step, the network predicts the next action.During testing, the returns-to-go are unknown; in practice, an initial estimate is made from which subsequent observed rewards are subtracted.the critic, with two sets of outputs that predict the policy and the values, respectively.Note that although actor-critic methods can update the policy parameters at each step, this is rarely done in practice.The agent typically collects a batch of experience over many time steps before the policy is updated.Interaction with the environment is at the core of reinforcement learning.However, there are some scenarios where it is not practical to send a naïve agent into an environment to explore the effect of different actions.This may be because erratic behavior in the environment is dangerous (e.g., driving autonomous vehicles) or because data collection is time-consuming or expensive (e.g., making financial trades).However, it is possible to gather historical data from human agents in both cases.Offline RL or batch RL aims to learn how to take actions that maximize rewards on future episodes by observing past sequences s 1 , a 1 , r 2 , s 2 , a 2 , r 3 , . .., without ever interacting with the environment.It is distinct from imitation learning, a related technique that (i) does not have access to the rewards and (ii) attempts to replicate the performance of a historical agent rather than improve it.Although there are offline RL methods based on Q-Learning and policy gradients, this paradigm opens up new possibilities.In particular, we can treat this as a sequence learning problem, in which the goal is to predict the next action, given the history of states, rewards, and actions.The decision transformer exploits a transformer decoder framework (section 12.7) to make these predictions (figure 19.17).However, the goal is to predict actions based on future rewards, and these are not captured in a standard s, a, r sequence.Hence, the decision transformer replaces the reward r t with the returns-to-go R t:T = T t ′ =t r t ′ (i.e., the sum of the empirically observed future rewards).The remaining framework is very similar to a standard transformer decoder.The states, actions, and returns-to-go are converted to fixed-size embeddings via learned mappings.For Atari games, the state embedding might be converted via a convolutional network similar to that in figure 19.14.The embeddings for the actions and returns-to-go can be learned in the same way as word embeddings (figure 12.9).The transformer is trained with masked self-attention and position embeddings.This formulation is natural during training but poses a quandary during inference because we don't know the returns-to-go.This can be resolved by using the desired total return at the first step and decrementing this as rewards are received.For example, in an Atari game, the desired total return would be the total score required to win.Decision transformers can also be fine-tuned from online experience and hence learn over time.They have the advantage of dispensing with most of the reinforcement learning machinery and its associated instability and replacing this with standard supervised learning.Transformers can learn from enormous quantities of data and integrate information across large time contexts (making the temporal credit assignment problem more tractable).This represents an intriguing new direction for reinforcement learning.Reinforcement learning is a sequential decision-making framework for Markov decision processes and similar systems.This chapter reviewed tabular approaches to RL, including dynamic programming (in which the environment model is known), Monte Carlo methods (in which multiple episodes are run and the action values and policy subsequently changed based on the rewards received), and temporal difference methods (in which these values are updated while the episode is ongoing).Deep Q-Learning is a temporal difference method where deep neural networks are used to predict the action value for every state.It can train agents to perform Atari 2600 games at a level similar to humans.Policy gradient methods directly optimize the policy rather than assigning values to actions.They produce stochastic policies, which are important when the environment is partially observable.The updates are noisy, and many refinements have been introduced to reduce their variance.Offline reinforcement learning is used when we cannot interact with the environment but must learn from historical data.The decision transformer leverages recent progress in deep learning to build a model of the state-action-reward sequence and predict the actions that will maximize the rewards.Sutton & Barto (2018) cover tabular reinforcement learning methods in depth.Li (2017), Arulkumaran et al. (2017), François-Lavet et al. (2018, and Wang et al. (2022c) all provide overviews of deep reinforcement learning.Graesser & Keng (2019) is an excellent introductory resource that includes Python code.Most landmark achievements of reinforcement learning have been in either video games or real-world games since these provide constrained environments with limited actions and fixed rules.Deep Q-Learning (Mnih et al., 2015) achieved human-level performance across a benchmark of ATARI games.AlphaGo (Silver et al., 2016) beat the world champion at Go.This game was previously considered very difficult for computers to play.Berner et al. (2019) built a system that beat the world champion team in the five vs. five-player game Defense of the Ancients 2, which requires cooperation across players.Ye et al. (2021) built a system that could beat humans on Atari games with limited data (in contrast to previous systems, which need much more experience than humans).More recently, the Cicero system demonstrated human-level performance in the game Diplomacy which requires natural language negotiations and coordination between players (FAIR, 2022).RL has also been applied successfully to combinatorial optimization problems (see Mazyavkina et al., 2021).For example, Kool et al. (2019) learned a model that performed similarly to the best heuristics for the traveling salesman problem.Recently, AlphaTensor (Fawzi et al., 2022) treated matrix multiplication as a game and learned faster ways to multiply matrices using fewer multiplication operations.Since deep learning relies heavily on matrix multiplication, this is one of the first examples of self-improvement in AI.Very early contributions to the theory of MDPs were made by Thompson (1933) and Thompson (1935).The Bellman recursions were introduced by Bellman (1966).Howard (1960) introduced policy iteration.Sutton & Barto (2018) identify the work of Andreae (1969) as being the first to describe RL using the MDP formalism.The modern era of reinforcement learning arguably originated in the Ph.D. theses of Sutton (1984) and Watkins (1989).Sutton (1988) introduced the term temporal difference learning.Watkins (1989) and Watkins & Dayan (1992) introduced Q-Learning and showed that it converges to a fixed point by Banach's theorem because the Bellman operator is a contraction mapping.Watkins (1989) made the first explicit connection between dynamic programming and reinforcement learning.SARSA was developed by Rummery & Niranjan (1994).Gordon (1995) introduced fitted Q-learning in which a machine learning model is used to predict the action value for each state-action pair.Riedmiller (2005) introduced neural-fitted Q-learning, which used a neural network to predict all the action values at once from a state.Early work on Monte Carlo methods was carried out by Singh & Sutton (1996), and the exploring starts algorithm was introduced by Sutton & Barto (1999).Note that this is an extremely cursory summary of more than fifty years of work.A much more thorough treatment can be found in Sutton & Barto (2018).Deep Q-Networks: Deep Q-Learning was devised by Mnih et al. (2015) and is an intellectual descendent of neural-fitted Q-learning.It exploited the then-recent successes of convolutional networks to develop a fitted Q-Learning method that could achieve human-level performance on a benchmark of ATARI games.Deep Q-Learning suffers from the deadly triad issue (Sutton & Barto, 2018): training can be unstable in any scheme that incorporates (i) bootstrapping, (ii) off-policy learning, and (iii) function approximation.Much subsequent work has aimed to make training more stable.Mnih et al. (2015) introduced the experience replay buffer (Lin, 1992), which was subsequently improved by Schaul et al. (2016) to favor more important tuples and hence increase learning speed.This is termed prioritized experience replay.The original Q-Learning paper concatenated four frames so the network could observe the velocities of objects and make the underlying process closer to fully observable.Hausknecht & Stone (2015) introduced deep recurrent Q-learning, which used a recurrent network architecture that only ingested a single frame at a time because it could "remember" the previous states.Van Hasselt (2010) identified the systematic overestimation of the state values due to the max operation and proposed double Q-Learning in which two models are trained simultaneously to remedy this.This was subsequently applied in the context of deep Q-learning (Van Hasselt et al., 2016), although its efficacy has since been questioned (Hessel et al., 2018).Wang et al. (2016) introduced deep dueling networks in which two heads of the same network predict (i) the state value and (ii) the advantage (relative value) of each action.The intuition here is that sometimes it is the state value that is important, and it doesn't matter much which action is taken, and decoupling these estimates improves stability.Fortunato et al. (2018) introduced noisy deep Q-Networks, in which some weights in the Q-Network are multiplied by noise to add stochasticity to the predictions and encourage exploration.The network can learn to decrease the magnitudes of the noise over time as it converges to a sensible policy.Distributional DQN (Bellemare et al., 2017a;Dabney et al., 2018following Morimura et al., 2010 aims to estimate more complete information about the distribution of returns than just the expectation.This potentially allows the network to mitigate against worst-case outcomes and can also improve performance, as predicting higher moments provides a richer training signal.Rainbow (Hessel et al., 2018) combined six improvements to the original deep Q-learning algorithm, including dueling networks, distributional DQN, and noisy DQN, to improve both the training speed and the final performance on the ATARI benchmark.Policy gradients: Williams (1992) introduced the REINFORCE algorithm.The term "policy gradient method" dates to .Konda & Tsitsiklis (1999) introduced the actorcritic algorithm.Decreasing the variance by using different baselines is discussed in Greensmith et al. (2004) and Peters & Schaal (2008).It has since been argued that the value baseline primarily reduces the aggressiveness of the updates rather than their variance (Mei et al., 2022).Policy gradients have been adapted to produce deterministic policies (Silver et al., 2014;Lillicrap et al., 2016;Fujimoto et al., 2018).The most direct approach is to maximize over the possible actions, but if the action space is continuous, this requires an optimization procedure at each step.The deep deterministic policy gradient algorithm (Lillicrap et al., 2016) moves the policy in the direction of the gradient of the action value (implying the use of an actor-critic method).We introduced policy gradients in terms of the parameter update.However, they can also be viewed as optimizing a surrogate loss based on importance sampling of the expected rewards, using trajectories from the current policy parameters.This view allows us to take multiple optimization steps validly.However, this can cause very large policy updates.Overstepping is a minor problem in supervised learning, as the trajectory can be corrected later.However, in RL, it affects future data collection and can be extremely destructive.Several methods have been proposed to moderate these updates.Natural policy gradients (Kakade, 2001) are based on natural gradients (Amari, 1998), which modify the descent direction by the Fisher information matrix.This provides a better update which is less likely to get stuck in local plateaus.However, the Fisher matrix is impractical to compute in models with many parameters.In trust-region policy optimization or TRPO (Schulman et al., 2015), the surrogate objective is maximized subject to a constraint on the KL divergence between the old and new policies.Schulman et al. (2017) propose a simpler formulation in which this KL divergence appears as a regularization term.The regularization weight is adapted based on the distance between the KL divergence and a target indicating how much we want the policy to change.Proximal policy optimization or PPO (Schulman et al., 2017) is an even simpler approach in which the loss is clipped to ensure smaller updates.In the actor-critic algorithm (Konda & Tsitsiklis, 1999) described in section 19.6, the critic used a 1-step estimator.It's also possible to use k-step estimators (in which we observe k discounted rewards and approximate subsequent rewards with an estimate of the state value).As k increases, the variance of the estimate increases, but the bias decreases.Generalized advantage estimation (Schulman et al., 2016) weights together estimates from many steps and parameterizes the weighting by a single term that trades off the bias and the variance.Mnih et al. (2016) introduced asynchronous actor-critic or A3C in which multiple agents are run independently in parallel environments and update the same parameters.Both the policy and value function are updated every T time steps using a mix of k-step returns.Wang et al. (2017) introduced several methods designed to make asynchronous actor-critic more efficient.Soft actor-critic (Haarnoja et al., 2018b) adds an entropy term to the cost function, which encourages exploration and reduces overfitting as the policy is encouraged to be less confident.In offline reinforcement learning, the policy is learned by observing the behavior of other agents, including the rewards they receive, without the ability to change the policy.It is related to imitation learning, where the goal is to copy the behavior of another agent without access to rewards (see Hussein et al., 2017).One approach is to treat offline RL in the same way as off-policy reinforcement learning.However, in practice, the distributional shift between the observed and applied policy manifests in overly optimistic estimates of the action value and poor performance (see Fujimoto et al., 2019;Kumar et al., 2019a;Agarwal et al., 2020).Conservative Q-learning (Kumar et al., 2020b) learns conservative, lower-bound estimates of the value function by regularizing the Q-values.The decision transformer (Chen et al., 2021c) is a simple approach to offline learning that takes advantage of the well-studied self-attention architecture.It can subsequently be fine-tuned with online training (Zheng et al., 2022).Chatbots can be trained using a technique known as reinforcement learning with human feedback or RLHF (Christiano et al., 2018;Stiennon et al., 2020).For example, InstructGPT (the forerunner of ChatGPT, Ouyang et al., 2022) starts with a standard transformer decoder model.This is then fine-tuned based on prompt-response pairs where the response was written by human annotators.During this training step, the model is optimized to predict the next word in the ground truth response.Unfortunately, such training data are expensive to produce in sufficient quantities to support high-quality performance.To resolve this problem, human annotators then indicate which of several model responses they prefer.These (much cheaper) data are used to train a reward model.This is a second transformer network that ingests the prompt and model response and returns a scalar indicating how good the response is.Finally, the fine-tuned chatbot model is further trained to produce high rewards using the reward model as supervision.Here, standard gradient descent cannot be used as it's not possible to compute derivatives through the sampling procedure in the chatbot output.Hence, the model is trained with proximal policy optimization (a policy gradient method where the derivatives are tractable) to generate higher rewards.Reinforcement learning is an enormous area, which easily justifies its own book, and this literature review is extremely superficial.Other notable areas of RL that we have not discussed include model-based RL, in which the state transition probabilities and reward functions are modeled (see Moerland et al., 2023).This allows forward planning and has the advantage that the same model can be reused for different reward structures.Hybrid methods such as AlphaGo (Silver et al., 2016) and MuZero (Schrittwieser et al., 2020) have separate models for the dynamics of the states, the policy, and the value of future positions.This chapter has only discussed simple methods for exploration, like the epsilon-greedy approach, noisy Q-learning, and adding an entropy term to penalize overconfident policies.Intrinsic motivation refers to methods that add rewards for exploration and thus imbue the agent with "curiosity" (see Barto, 2013;Aubret et al., 2019).Hierarchical reinforcement learning (see Pateria et al., 2021) refers to methods that break down the final objective into sub-tasks.Multiagent reinforcement learning (see Zhang et al., 2021a) considers the case where multiple agents coexist in a shared environment.This may be in either a competitive or cooperative context.Problem 19.1 Figure 19.18 shows a single trajectory through the example MDP.Calculate the return for each step in the trajectory given that the discount factor γ is 0.9.Problem 19.2 * Prove the policy improvement theorem.Consider changing from policy π to policy π ′ , where for state st the new policy π ′ chooses the action that maximizes the expected return:and for all other states, the policies are the same.Show that the value v[st|π] for the original policy must be less than or equal to v[st|π ′ ] = q st, π ′ [a|st] π for the new policy:  19.10b after two iterations of (i) policy evaluation (in which all states are updated based on their current values and then replace the previous ones) and (ii) policy improvement.The state transition allots half the probability to the direction the policy indicates and divides the remaining probability equally between the other valid actions.The reward function returns -2 irrespective of the action when the penguin leaves a hole.The reward function returns +3 regardless of the action when the penguin leaves the fish tile and the episode ends, so the fish tile has a value of +3.The Boltzmann policy strikes a balance between exploration and exploitation by basing the action probabilities π[a|s] on the current state-action reward function q[s, a]:Explain how the temperature parameter τ can be varied to prioritize exploration or exploitation.Problem 19.5 * When the learning rate α is one, the Q-Learning update is given by:Show that this is a contraction mapping (equation 16.30) so that: and that the updates will eventually converge.Draft: please send errata to udlbookmail@gmail.com.Show that the value of b that minimizes the variance of the gradient estimate is given by: Chapter 20This chapter differs from those that precede it.Instead of presenting established results, it poses questions about how and why deep learning works so well.These questions are rarely discussed in textbooks.However, it's important to realize that (despite the title of this book) understanding of deep learning is still limited.We argue that it is surprising that deep networks are easy to train and also surprising that they generalize.Then we consider each of these topics in turn.We enumerate the factors that influence training success and discuss what is known about loss functions for deep networks.Then we consider the factors that influence generalization.We conclude with a discussion of whether networks need to be overparameterized and deep.The MNIST-1D dataset (figure 8.1) has just forty input dimensions and ten output dimensions.With enough hidden units per layer, a two-layer fully connected network classifies 10000 MNIST-1D training data points perfectly and generalizes reasonably to unseen examples (figure 8.10a).Indeed, we now take it for granted that with sufficient hidden units, deep networks will classify almost any training set near-perfectly.We also take for granted that the fitted model will generalize to new data.However, it's not at all obvious either that the training process should succeed or that the resulting model should generalize.This section argues that both these phenomena are surprising.Performance of a two-layer fully connected network on 10000 MNIST-1D training examples is perfect once there are 43 hidden units per layer (∼4000 parameters).However, finding the global minimum of an arbitrary non-convex function is NP-hard (Murty & Kabadi, 1987), and this is also true for certain neural network loss functions (Blum & Rivest, 1992).It's remarkable that the fitting algorithm doesn't get trapped in local minima or stuck near saddle points and that it can efficiently recruit spare model capacity to fit unexplained training data wherever they lie.Perhaps this success is less surprising when there are far more parameters than training data.However, it's debatable whether this is generally the case.AlexNet had ∼60 million parameters and was trained with ∼1 million data points.However, to complicate matters, each training example was augmented with 2048 transformations.GPT-3 had 175 billion parameters and was trained with 300 billion tokens.There is not a clear-cut case that either model was overparameterized, and yet they were successfully trained.In short, it's surprising that we can fit deep networks reliably and efficiently.Either the data, the models, the training algorithms, or some combination of all three must have some special properties that make this possible.If the efficient fitting of neural networks is startling, their generalization to new data is dumbfounding.First, it's not obvious a priori that typical datasets are sufficient to characterize the input/output mapping.The curse of dimensionality implies that the training dataset is tiny compared to the possible inputs; if each of the 40 inputs of the MNIST-1D data were quantized into 10 possible values, there would be 10 40 possible inputs, which is a factor of 10 35 more than the number of training examples.To summarize, it's neither obvious that we should be able to fit deep networks nor that they should generalize.A priori, deep learning shouldn't work.And yet it does.This chapter investigates why.Sections 20.2-20.3describe what we know about fitting deep networks and their loss functions.Sections 20.4-20.6 examine generalization.It's important to realize that we can't learn any function.Consider a completely random mapping from every possible 28×28 binary image to one of ten categories.Since there is no structure to this function, the only recourse is to memorize the 2 784 assignments.However, it's easy to train a model on the MNIST dataset (figures 8.10 and 15.15), which contains 60,000 examples of 28×28 images labeled with one of ten categories.One explanation for this contradiction could be that it is easy to find global minima because the real-world functions that we approximate are relatively simple. 1 This hypothesis was investigated by Zhang et al. (2017a), who trained AlexNet on the This suggests that the properties of the dataset aren't critical.Another possible explanation for the ease with which models are trained is that some regularization methods like L2 regularization (weight decay) make the loss surface flatter and more convex.However, Zhang et al. (2017a) found that neither L2 regularization nor Dropout was required to fit random data.This does not eliminate implicit regularization due to the finite step size of the fitting algorithms (section 9.2).However, this effect increases with the learning rate (equation 9.9), and model-fitting does not get easier with larger learning rates.Chapter 6 argued that the SGD algorithm potentially allows the optimization trajectory to move between "valleys" during training.However,  show that several models (including fully connected and convolutional networks) can be fit to many MNIST-1D examples with randomized labels using full-batch (i.e., non-stochastic) gradient descent.There was no explicit regularization, and the learning rate was set to a small constant value of 0.0025 to minimize implicit regularization.Here, the true map-Problem 20.3 ping from data to labels has no structure, the training is deterministic, and there is no regularization, and yet the training error still decreases to zero.This suggests that these loss functions may genuinely have no local minima.Overparameterization almost certainly is an important factor that contributes to ease of training.It implies that there is a large family of degenerate solutions, so there may always be a direction in which the parameters can be modified to decrease the loss.Sejnowski (2020) suggests that ". . . the degeneracy of solutions changes the nature of the problem from finding a needle in a haystack to a haystack of needles."In practice, networks are frequently overparameterized by one or two orders of magnitude (figure 20.3).However, data augmentation makes it difficult to make precise statements.Augmentation may increase the data by several orders of magnitude, but these are manipulations of existing examples rather than independent new data points.Moreover, figure 8.10 shows that neural networks can sometimes fit the training data well when there are the same number or fewer parameters than data points.This is presumably due to redundancy in training examples from the same underlying function.Several theoretical convergence results show that, under certain circumstances, SGD converges to a global minimum when the network is sufficiently overparameterized.For example, Du et al. (2019b) show that randomly initialized SGD converges to a global minimum for shallow fully connected ReLU networks with a least squares loss with enough hidden units.Similarly, Du et al. (2019a) consider deep, residual, and convolutional networks when the activation function is smooth and Lipschitz.Zou et al. (2020) analyzed the convergence of gradient descent on deep, fully connected networks using a hinge loss.Allen-Zhu et al. (2019) considered deep networks with ReLU functions.If a neural network is sufficiently overparameterized so that it can memorize any  et al., 2016a,b), DenseNet (Huang et al., 2017b), Xception (Chollet, 2017), Effi-cientNet (Tan & Le, 2019), Inception (Szegedy et al., 2017), ResNeXt (Xie et al., 2017), and AmoebaNet (Cubuk et al., 2019).dataset of a fixed size, then all stationary points become global minima (Livni et al., 2014;Nguyen & Hein, 2017.Other results show that if the network is wide enough, local minima where the loss is higher than the global minimum are rare (see Choromanska et al., 2015;Pascanu et al., 2014;Pennington & Bahri, 2017).Kawaguchi et al. (2019) prove that as a network becomes deeper, wider, or both, the loss at local minima becomes closer to that at the global minimum for squared loss functions.These theoretical results are intriguing but usually make unrealistic assumptions about the network structure.For example, Du et al. (2019a) show that residual networks converge to zero training loss when the width of the network D (i.e., the number of hidden units) is Ω[I 4 K 2 ] where I is the amount of training data, and K is the depth of the network.Similarly, Nguyen & Hein (2017) assume that the network's width is larger than the dataset size, which is unrealistic in most practical scenarios.Overparameterization seems to be important, but theory cannot yet explain empirical fitting performance.The activation function is also known to affect training difficulty.Networks where the activation only changes over a small part of the input range are harder to fit than ReLUs (which vary over half the input range) or Leaky ReLUs (which vary over the full range); For example, sigmoid and tanh nonlinearities (figure 3.13a) have shallow gradients in their tails; where the activation function is near-constant, the training gradient is nearzero, so there is no mechanism to improve the model.Another potential explanation is that Xavier/He initialization sets the parameters to values that are easy to optimize.Of course, for deeper networks, such initialization is necessary to avoid exploding and vanishing gradients, so in a trivial sense, initialization is critical to training success.However, for shallower networks, the initial variance of the weights is less important.Liu et al. (2023c) trained a 3-layer fully connected network with 200 hidden units per layer on 1000 MNIST data points.They found that more iterations were required to fit the training data as the variance increased from that proposed by He (figure 20.4), but this did not ultimately impede fitting.Hence, initialization doesn't shed much light on why fitting neural networks is easy, although exploding/vanishing gradients do reveal initializations that make training difficult with finite precision arithmetic.Neural networks are harder to fit when the depth becomes very large due to exploding and vanishing gradients (figure 7.7) and shattered gradients (figure 11.3).However, these are (arguably) practical numerical issues.There is no definitive evidence that the underlying loss function is fundamentally more or less convex as the network depth increases.Figure 20.2 does show that for MNIST data with randomized labels and He initialization, deeper networks train in fewer iterations.However, this might be because either (i) the gradients in deeper networks are steeper or (ii) He initialization just starts wider, shallower networks further away from the optimal parameters.Frankle & Carbin (2019) show that for small networks like VGG, you can get the same or better performance if you (i) train the network, (ii) prune the weights with the smallest magnitudes and (iii) retrain from the same initial weights.This does not work if the weights are randomly re-initialized.They concluded that the original overparameterized network contains small trainable sub-networks, which are sufficient to Notebook 20.3 Lottery tickets provide the performance.They term this the lottery ticket hypothesis and denote the sub-networks as winning tickets.This suggests that the effective number of sub-networks may have a key role to play in fitting.This (perhaps) varies with the network depth for a fixed parameter count, but a precise characterization of this idea is lacking.The previous section discussed factors that contribute to the ease with which neural networks can be trained.The number of parameters (degree of overparameterization) and the choice of activation function are both important.Surprisingly, the choice of dataset, the randomness of the fitting algorithm, and the use of regularization don't seem important.There is no definitive evidence that (for a fixed parameter count) the depth of the network matters (other than numerical problems due to exploding/vanishing/shattered gradients).This section tackles the same topic from a different angle by considering the empirical properties of loss functions.Most of this evidence comes from fully connected networks and CNNs; loss functions of transformer networks are less well understood.We expect loss functions for deep networks to have a large family of equivalent global minima.In fully connected networks, the hidden units at each layer and their associated weights can be permuted without changing the output.In convolutional networks, permuting the channels and convolution kernels appropriately doesn't change the output.We can multiply the weight before any ReLU function and divide the weight after by a positive number without changing the output.Using BatchNorm induces another set of redundancies because the mean and variance of each hidden unit or channel are reset.The above modifications all produce the same output for every input.However, the global minimum only depends on the output at the training data points.In overparameterized networks, there will also be families of solutions that behave identically at the data points but differently between them.All of these are also global minima.Goodfellow et al. (2015b) considered a straight line between the initial parameters and the final values.They show that the loss function along this line usually decreases monotonically (except for a small bump near the start sometimes).This phenomenon is observed for several different types of networks and activation functions (figure 20.5a).Of course, real optimization trajectories do not proceed in a straight line.However, Li et al. (2018b) find that they do lie in low-dimensional subspaces.They attribute this to the existence of large, nearly convex regions in the loss landscape that capture the trajectory early on and funnel it in a few important directions.Surprisingly, Li et al. (2018a) showed that networks still train well if optimization is constrained to lie in a random low-dimensional subspace (figure 20.6).Li & Liang (2018) show that the relative change in the parameters during training decreases as network width increases; for larger widths, the parameters start at smaller values, change by a smaller proportion of those values, and converge in fewer steps.Goodfellow et al. (2015b) examined the loss function along a straight line between two minima that were found independently.They saw a pronounced increase in the loss between them (figure 20.5b); good minima are not generally linearly connected.However,  A fully connected network with two hidden layers, each with 200 units was trained on MNIST.Parameters were initialized using a standard method but then constrained to lie within a random subspace.Performance reaches 90% of the unconstrained level when this subspace is 750D (termed the intrinsic dimension), which is 0.4% of the original parameters.Adapted from Li et al. (2018a).Frankle et al. (2020) showed that this increase vanishes if the networks are identically trained initially and later allowed to diverge by using different SGD noise and augmentation.This suggests that the solution is constrained early in training and that some families of minima are linearly connected.Draxler et al. (2018) found minima with good (but different) performance on the CIFAR-10 dataset.They then showed that it is possible to construct paths from one to the other, where the loss function remains low along this path.They conclude that there is a single connected manifold of low loss (figure 20.7).This seems to be increasingly true as the width and depth of the network increase.Garipov et al. (2018) and Fort & Jastrzębski (2019) present other schemes for connecting minima.Random Gaussian functions (in which points are jointly distributed with covariance given by a kernel function of their distance) have an interesting property: for points   Dauphin et al. (2014) found critical points on a neural network loss surface (i.e., points with zero gradient).They showed that the proportion of negative eigenvalues (directions that point down) decreases with the loss.The implication is that all minima (points with zero gradient where no directions point down) have low losses.Adapted from Dauphin et al. (2014) and Bahri et al. (2020).Figure 20.9 Goldilocks zone.The proportion of eigenvalues of the Hessian that are greater than zero (a measure of positive curvature/convexity) within a random subspace of dimension Ds in a twolayer fully connected network with ReLU functions applied to MNIST as a function of the squared radius r 2 of the parameters relative to Xavier initialization.There is a pronounced region of positive curvature known as the Goldilocks zone.Adapted from Fort & Scherlis (2019).Generalization of two models on the CIFAR-10 database depends on the ratio of batch size to the learning rate.As the batch size increases, generalization decreases.As the learning rate increases, generalization increases.Adapted from He et al. (2019).where the gradient is zero, the fraction of directions where the function curves down becomes smaller when these points occur at lower loss values (see Bahri et al., 2020).Dauphin et al. (2014) searched for saddle points in a neural network loss function and similarly found a correlation between the loss and the number of negative eigenvalues (figure 20.8).Baldi & Hornik (1989) analyzed the error surface of a shallow network and found that there were no local minima but only saddle points.These results suggest that there are few or no bad local minima.Fort & Scherlis (2019) measured the curvature at random points on a neural network loss surface; they showed that the curvature of the surface is unusually positive when the ℓ 2 norm of the weights lies within a certain range (figure 20.9), which they term the Goldilocks zone.He and Xavier initialization fall within this range.The last two sections considered factors that determine whether the network trains successfully and what is known about neural network loss functions.This section considers factors that determine how well the network generalizes.This complements the discussion of regularization (chapter 9), which explicitly aims to encourage generalization.Since deep networks are usually overparameterized, the details of the training process determine which of the degenerate family of minima the algorithm converges to.Some of these details reliably improve generalization.LeCun et al. (2012) show that SGD generalizes better than full-batch gradient descent.It has been argued that SGD generalizes better than Adam (e.g., Wilson et al., 2017;Keskar & Socher, 2017), but more recent studies suggest that there is little difference when the hyperparameter search is done carefully (Choi et al., 2019).show that deep nets generalize better with smaller batch-size when no other form of regularization is used.It is also well-known that larger learning rates tend to generalize better (e.g., figure 9.5).Jastrzębski et al. (2018), Goyal et al. (2018), and He  2019) argue that the batch size/learning rate ratio is important.He et al. (2019) show a significant correlation between this ratio and the degree of generalization and prove a generalization bound for neural networks, which has a positive correlation with this ratio (figure 20.10).These observations are aligned with the discovery that SGD implicitly adds regularization terms to the loss function (section 9.2), and their magnitude depends on the learning rate.The trajectory of the parameters is changed by this regularization, and they converge to a part of the loss function that generalizes well.There has been speculation dating at least to Hochreiter & Schmidhuber (1997a) that flat minima in the loss function generalize better than sharp minima (figure 20.11).Informally, if the minimum is flatter, then small errors in the estimated parameters are less important.This can also be motivated from various theoretical viewpoints.For example, minimum description length theory suggests models specified by fewer bits generalize better (Rissanen, 1983).For wide minima, the precision needed to store the weights is lower, so they should generalize better.Flatness can be measured by (i) the size of the connected region around the minimum for which training loss is similar (Hochreiter & Schmidhuber, 1997a), (ii) the secondorder curvature around the minimum (Chaudhari et al., 2019), or (iii) the maximum loss within a neighborhood of the minimum .However, caution is required; estimated flatness can be affected by trivial reparameterizations of the network due to the non-negative homogeneity property of the ReLU function (Dinh et al., 2017).Nonetheless,  varied the batch size and learning rate and showed that flatness correlates with generalization.Izmailov et al. (2018) average together weights from multiple points in a learning trajectory.This both results in flatter test and training surfaces at the minimum and improves generalization.Other regularization techniques can also be viewed through this lens.For example, averaging model outputs (ensembling) may also make the test loss surface flatter.Kleinberg et al. (2018) showed that large gradient variance during training helps avoid sharp regions.This may explain why reducing the batch size and adding noise helps generalization.The above studies consider flatness for a single model and training set.However, sharpness is not a good criterion to predict generalization between datasets; when the labels in the CIFAR dataset are randomized (making generalization impossible), there is no commensurate decrease in the flatness of the minimum (Neyshabur et al., 2017).The inductive bias of a network is determined by its architecture, and judicious choices of model can drastically improve generalization.Chapter 10 introduced convolutional networks, which are designed to process data on regular grids; they implicitly assume that the input statistics are the same across the input, so they share parameters across position.Similarly, transformers are suited for modeling data that is invariant to permutations, and graph neural networks are suited to data represented on irregular graphs.Matching the architecture to the properties of the data improves generalization over generic, fully connected architectures (see figure 10.8).Section 20.3.4 reviewed the finding of Fort & Scherlis (2019) that the curvature of the loss surface is unusually positive when the ℓ 2 norm of the weights lies within a certain range.The same authors provided evidence that generalization is also good when the ℓ 2 weight norm falls within this Goldilocks zone (figure 20.12).This is perhaps unsurprising.The norm of the weights is (indirectly) related to the Lipschitz constant of the model.If this norm is too small, then the model will not be able to change fast enough to capture the variation in the underlying function.If the norm is too large, then the model will be unnecessarily variable between training points and will not interpolate smoothly.This finding was used by Liu et al. (2023c) to explain the phenomenon of grokking (Power et al., 2022), in which a sudden improvement in generalization can occur many epochs after the training error is already zero (figure 20.13).It is proposed that grokking occurs when the norm of the weights is initially too large; the training data fits well, but the variation of the model between the data points is large.Over time, implicit or explicit regularization decreases the norm of the weights until they reach the Goldilocks zone, and generalization suddenly improves.Figure 8.10 showed that generalization performance tends to improve with the degree of overparameterization.When combined with the bias/variance trade-off curve, this results in double descent.The putative explanation for this improvement is that the network has more latitude to become smoother between the training data points when the model is overparameterized.It follows that the norm of the weights can also be used to explain double descent.The norm of the weights increases when the number of parameters is similar to the number of data points (as the model contorts itself to fit these points exactly), causing  generalization to reduce.As the network becomes wider and the number of weights increases, the overall norm of these weights decreases; the weights are initialized with a variance that is inversely proportional to the width (i.e., with He or Glorot initialization), and the weights change very little from their original values.Until this point, we have discussed how models generalize to new data that is drawn from the same distribution as the training data.This is a reasonable assumption for experimentation.However, systems deployed in the real world may encounter unexpected data due to noise, changes in the data statistics over time, or deliberate attacks.Of course, it is harder to make definite statements about this scenario, but D'Amour et al. (2020) show that the variability of identical models trained with different seeds on corrupted data can be enormous and unpredictable.Goodfellow et al. (2015a) showed that deep learning models are susceptible to adversarial attacks.Consider perturbing an image that is correctly classified by the network Notebook 20.4 Adversarial attacks as "dog" so that the probability of the correct class decreases as fast as possible until the class flips.If this image is now classified as an airplane, you might expect the perturbed image to look like a cross between a dog and an airplane.However, in practice, the perturbed image looks almost indistinguishable from the original dog image (figure 20.14).The conclusion is that there are positions that are close to but not on the data manifold that are misclassified.These are known as adversarial examples.Their existence is surprising; how can such a small change to the network input make such a drastic change to the output?The best current explanation is that adversarial examples aren't due to a lack of robustness to data from outside the training data manifold.Instead, they are exploiting a source of information that is in the training distribution but which has a small norm and is imperceptible to humans (Ilyas et al., 2019).Section 20.4 argued that models generalize better when over-parameterized.Indeed, there are almost no examples of state-of-the-art performance on complex datasets where the model has significantly fewer parameters than there were training data points.However, section 20.2 reviewed evidence that training becomes easier as the number of parameters increases.Hence, it's not clear if some fundamental property of smaller models prevents them from performing as well or whether the training algorithms can't find good solutions for small models.Pruning and distilling are two methods for reducing the size of trained models.This section examines whether these methods can produce underparameterized models which retain the performance of overparameterized ones.Pruning trained models reduces their size and hence storage requirements (figure 20.15).The simplest approach is to remove individual weights.This can be done based on the second derivatives of the loss function (LeCun et al., 1990;Hassibi & Stork, 1993) or (more practically) based on the absolute value of the weight (Han et al., 2016(Han et al., , 2015.Other work prunes hidden units (Zhou et al., 2016a;Alvarez & Salzmann, 2016), channels in convolutional networks Luo et al., 2017b;He et al., 2017;Liu et al., 2019a), or entire layers in residual nets (Huang & Wang, 2018).Often, the network is 20.5 Do we need so many parameters? Figure 20.15Pruning neural networks.The goal is to remove as many weights as possible without decreasing performance.This is often done just based on the magnitude of the weights.Typically, the network is fine-tuned after pruning.a) Example fully connected network.b) After pruning.fine-tuned after pruning, and sometimes this process is repeated.For example, Han et al. ( 2016) maintained good performance for the VGG network on ImageNet classification when 8% of the weights were retained.This significantly decreases the model size but isn't enough to show that overparameterization is not required; the VGG network has ∼100 times as many parameters as there are ImageNet training data (disregarding augmentation).Pruning is a form of architecture search.In their work on lottery tickets (see section 20.2.7), Frankle & Carbin (2019) (i) trained a network, (ii) pruned the weights with the smallest magnitudes, and (iii) retrained the remaining network from the same initial weights.By iterating this procedure, they reduced the size of the VGG-19 network (originally 138 million parameters) by 98.5% on the CIFAR-10 database (60,000 examples) while maintaining good performance.For ResNet-50 (25.6 million parameters), they reduced the parameters by 80% without reducing the performance on ImageNet (1.28 million examples).These demonstrations are impressive but (disregarding data augmentation) these networks are still over-parameterized after pruning.The parameters can also be reduced by training a smaller network (the student) to replicate the performance of a larger one (the teacher).This is known as knowledge distillation and dates back to at least Buciluǎ et al. (2006).Hinton et al. (2015) showed that the pattern of information across the output classes is important and trained a smaller network to approximate the pre-softmax logits of the larger one (figure 20.16).Zagoruyko & Komodakis (2017) further encouraged the spatial maps of the activations of the student network to be similar to the teacher network at various points.They use this attention transfer method to approximate the performance of a 34-layer residual network (∼63 million parameters) with an 18-layer residual network (∼11 million param- eters) on the ImageNet classification task.However, this is still larger than the number of training examples (∼1 million images).Modern methods (e.g.Chen et al., 2021a) can improve on this result, but distillation has not yet provided convincing evidence that under-parameterized models can perform well.Current evidence suggests that overparameterization is needed for generalization -at least for the size and complexity of datasets that are currently used.There are no demonstrations of state-of-the-art performance on complex datasets where there are significantly fewer parameters than training examples.Attempts to reduce model size by pruning or distilling trained networks have not changed this picture.Moreover, recent theory shows that there is a trade-off between the model's Lipschitz constant and overparameterization; Bubeck & Sellke (2021) proved that in D dimensions, smooth interpolation requires D times more parameters than mere interpolation.They argue that current models for large datasets (e.g., ImageNet) aren't overparameterized enough; increasing model capacity further may be key to improving performance.Chapter 3 discussed the universal approximation theorem.This states that shallow neural networks can approximate any function to arbitrary accuracy given enough hidden units.This raises the obvious question of whether networks need to be deep.First, let's consider the evidence that depth is required.Historically, there has been a definite correlation between performance and depth.For example, performance on the ImageNet benchmark initially improved as a function of network depth until training became difficult.Subsequently, residual connections and batch normalization (chapter 11) allowed training of deeper networks with commensurate gains in performance.At the time of writing, almost all state-of-the-art applications, including image classification (e.g., the vision transformer), text generation (e.g., GPT3), and text-guided image synthesis (e.g., DALL•E-2), are based on deep networks with tens or hundreds of layers.Despite this trend, there have been efforts to use shallower networks.Zagoruyko & Komodakis (2016) constructed shallower but wider residual neural networks and achieved similar performance to ResNet.More recently, Goyal et al. (2021) constructed a network that used parallel convolutional channels and achieved performance similar to deeper networks with only 12 layers.Furthermore, Veit et al. (2016) showed that it is predominantly shorter paths of 5-17 layers that drive performance in residual networks.Nonetheless, the balance of evidence suggests that depth is critical; even the shallowest networks with good image classification performance require >10 layers.However, there is no definitive explanation for why.Three possible explanations are that (i) deep networks can represent more complex functions than shallow ones, (ii) deep networks are easier to train, and (iii) deep networks impose better inductive biases.Chapter 4 showed that deep networks make functions with many more linear regions than shallow ones for the same parameter count.We also saw that "pathological" functions have been identified that require exponentially more hidden units to model with a shallow network than a deep one (e.g., Eldan & Shamir, 2016;Telgarsky, 2016).Indeed Liang & Srikant (2016) found quite general families of functions that are more efficiently modeled by deep networks.However, Nye & Saxe (2018) found that some of these functions cannot easily be fit by deep networks in practice.Moreover, there is little evidence that the real-world functions that we are approximating have these pathological properties.An alternative explanation is that shallow networks with a practical number of hidden units could support state-of-the-art performance, but it is just difficult to find a good solution that both fits the training data well and interpolates sensibly.One way to show this is to distill successful deep networks into shallower (but wider) student models and see if performance can be maintained.Urban et al. (2017) dis-tilled an ensemble of 16 convolutional networks for image classification on the CIFAR-10 dataset into student models of varying depths.They found that shallow networks could not replicate the performance of the deeper teacher and that the student performance increased as a function of depth for a constant parameter budget.Most current models rely on convolutional blocks or transformers.These networks share parameters for local regions of the input data, and often they gradually integrate this information across the whole input.These constraints mean that the functions that these networks can represent are not general.One explanation for the supremacy of deep networks, then, is that these constraints have a good inductive bias and that it is difficult to force shallow networks to obey these constraints.Multi-layer convolutional architectures seem to be inherently helpful, even without training.Ulyanov et al. (2018) demonstrated that the structure of an untrained CNN can be used as a prior in low-level tasks such as denoising and super-resolution.Frankle et al. (2021) achieved good performance in image classification by initializing the kernels randomly, fixing their values, and just training the batch normalization offset and scaling factors.Zhang et al. (2017a) show that features from randomly initialized convolutional filters can support subsequent image classification using a kernel model.Additional evidence that convolutional networks provide a useful inductive bias comes from Urban et al. (2017), who attempted to distill convolutional networks into shallower networks.They found that distilling into convolutional architectures systematically worked better than distilling into fully connected networks.This suggests that the convolutional architecture has some inherent advantages.Since the sequential local processing of convolutional networks cannot easily be replicated by shallower networks, this argues that depth is indeed important.This chapter has made the case that the success of deep learning is surprising.We discussed the challenges of optimizing high-dimensional loss functions and argued that overparameterization and the choice of activation function are the two most important factors that make this tractable in deep networks.We saw that, during training, the parameters move through a low-dimensional subspace to one of a family of connected global minima and that local minima are not apparent.Generalization of neural networks also improves with overparameterization, although other factors, such as the flatness of the minimum and the inductive bias of the architecture, are also important.It appears that both a large number of parameters and multiple network layers are required for good generalization, although we do not yet know why.Many questions remain unanswered.We do not currently have any prescriptive theory that will allow us to predict the circumstances in which training and generalization will succeed or fail.We do not know the limits of learning in deep networks or whether much more efficient models are possible.We do not know if there are parameters that would generalize better within the same model.The study of deep learning is still driven by empirical demonstrations.These are undeniably impressive, but they are not yet matched by our understanding of deep learning mechanisms.This chapter was written by Travis LaCroix and Simon J.D. Prince.AI is poised to change society for better or worse.These technologies have enormous potential for social good (Taddeo & Floridi, 2018;Tomašev et al., 2020), including important roles in healthcare (Rajpurkar et al., 2022) and the fight against climate change (Rolnick et al., 2023).However, they also have the potential for misuse and unintended harm.This has led to the emergence of the field of AI ethics.The modern era of deep learning started in 2012 with AlexNet, but sustained interest in AI ethics did not follow immediately.Indeed, a workshop on fairness in machine learning was rejected from NeurIPS 2013 for want of material.It wasn't until 2016 that AI Ethics had its "AlexNet" moment, with ProPublica's exposé on bias in the COMPAS recidivism-prediction model (Angwin et al., 2016) and Cathy O'Neil's book Weapons of Math Destruction (O'Neil, 2016).Interest has swelled ever since; submissions to the Conference on Fairness, Accountability, and Transparency (FAccT) have increased nearly ten-fold in the five years since its inception in 2018.In parallel, many organizations have proposed policy recommendations for responsible AI.Jobin et al. (2019) found 84 documents containing AI ethics principles, with 88% released since 2016.This proliferation of non-legislative policy agreements, which depend on voluntary, non-binding cooperation, calls into question their efficacy (McNamara et al., 2018;Hagendorff, 2020;LaCroix & Mohseni, 2022).In short, AI Ethics is in its infancy, and ethical considerations are often reactive rather than proactive.This chapter considers potential harms arising from the design and use of AI systems.These include algorithmic bias, lack of explainability, data privacy violations, militarization, fraud, and environmental concerns.The aim is not to provide advice on being more ethical.Instead, the goal is to express ideas and start conversations in key areas that have received attention in philosophy, political science, and the broader social sciences.When we design AI systems, we wish to ensure that their "values" (objectives) are aligned with those of humanity.This is sometimes called the value alignment problem (Russell,Problem 21.1 2019; Christian, 2020;Gabriel, 2020).This is challenging for three reasons.First, it's difficult to define our values completely and correctly.Second, it is hard to encode these values as objectives of an AI model, and third, it is hard to ensure that the model learns to carry out these objectives.In a machine learning model, the loss function is a proxy for our true objectives, and a misalignment between the two is termed the outer alignment problem (Hubinger et al., 2019).To the extent that this proxy is inadequate, there will be "loopholes" that the system can exploit to minimize its loss function while failing to satisfy the intended objective.For example, consider training an RL agent to play chess.If the agent is rewarded for capturing pieces, this may result in many drawn games rather than the desired behavior (to win the game).In contrast, the inner alignment problem is to ensure that the behavior of an AI system does not diverge from the intended objectives even when the loss function is well specified.If the learning algorithm fails to find the global minimum or the training data are unrepresentative, training can converge to a solution that is misaligned with the true objective resulting in undesirable behavior (Goldberg, 1987;Mitchell et al., 1992;Lehman & Stanley, 2008).Gabriel (2020) divides the value alignment problem into technical and normative components.The technical component concerns how we encode values into the models so that they reliably do what they should.Some concrete problems, such as avoiding reward hacking and safe exploration, may have purely technical solutions (Amodei et al., 2016).In contrast, the normative component concerns what the correct values are in the first place.There may be no single answer to this question, given the range of things that different cultures and societies value.It's important that the encoded values are representative of everyone and not just culturally dominant subsets of society.Another way to think about value alignment is as a structural problem that arises when a human principal delegates tasks to an artificial agent (LaCroix, 2022).This is similar to the principal-agent problem in economics (Laffont & Martimort, 2002), which allows that there are competing incentives inherent in any relationship where one party is expected to act in another's best interests.In the AI context, such conflicts of interest can arise when either (i) the objectives are misspecified or (ii) there is an informational asymmetry between the principal and the agent (figure 21.1).Many topics in AI ethics can be understood in terms of this structural view of value alignment.The following sections discuss problems of bias and fairness and artificial moral agency (both pertaining to specifying objectives) and transparency and explainability (both related to informational asymmetry).From a purely scientific perspective, bias refers to statistical deviation from some norm.In AI, it can be pernicious when this deviation depends on illegitimate factors that impact an output.For example, gender is irrelevant to job performance, so it is illegitimate to use gender as a basis for hiring a candidate.Similarly, race is irrelevant to criminality, so it is illegitimate to use race as a feature for recidivism prediction.Bias in AI models can be introduced in various ways (Fazelpour & Danks, 2021): • Problem specification: Choosing a model's goals requires a value judgment about what is important to us, which allows for the creation of biases (Fazelpour & Danks, 2021).Further biases may emerge if we fail to operationalize these choices successfully and the problem specification fails to capture our intended goals (Mitchell et al., 2021).• Data: Algorithmic bias can result when the dataset is unrepresentative or incomplete (Danks & London, 2017).For example, the PULSE face super-resolution algorithm (Menon et al., 2020) was trained on a database of photos of predominantly white celebrities.When applied to a low-resolution portrait of Barack Obama, it generated a photo of a white man (Vincent, 2020).If the society in which training data are generated is structurally biased against marginalized communities, even complete and representative datasets will elicit biases (Mayson, 2018).For example, Black individuals in the US have been policed and jailed more frequently than white individuals.Hence, historical data used to train recidivism prediction models are already biased against Black communities.• Modeling and validation: Choosing a mathematical definition to measure model fairness requires a value judgment.There exist distinct but equally-intuitive definitions that are logically inconsistent (Kleinberg et al., 2017;Chouldechova, 2017;Berk et al., 2017).This suggests the need to move from a purely mathematical conceptualization of fairness toward a more substantive evaluation of whether algorithms promote justice in practice (Green, 2022).• Deployment: Deployed algorithms may interact with other algorithms, structures, or institutions in society to create complex feedback loops that entrench extant biases (O'Neil, 2016).For example, large language models like GPT3 (Brown et al., 2020) are trained on web data.However, when GPT3 outputs are published  online, the training data for future models is degraded.This may exacerbate biases and generate novel societal harm (Falbo & LaCroix, 2022).Unfairness can be exacerbated by considerations of intersectionality; social categories can combine to create overlapping and interdependent systems of oppression.For example, the discrimination experienced by a queer woman of color is not merely the sum of the discrimination she might experience as queer, as gendered, or as racialized (Crenshaw, 1991).Within AI, Buolamwini & Gebru (2018) showed that face analysis algorithms trained primarily on lighter-skinned faces underperform for darker-skinned faces.However, they perform even worse on combinations of features such as skin color and gender than might be expected by considering those features independently.Of course, steps can be taken to ensure that data are diverse, representative, and complete.But if the society in which the training data are generated is structurally biased against marginalized communities, even completely accurate datasets will elicit biases.In light of the potential for algorithmic bias and the lack of representation in training datasets described above, it is also necessary to consider how failure rates for the outputs of these systems are likely to exacerbate discrimination against already-marginalized communities (Buolamwini & Gebru, 2018;Raji & Buolamwini, 2019;Raji et al., 2022).The resulting models may codify and entrench systems of power and oppression, including capitalism and classism; sexism, misogyny, and patriarchy; colonialism and imperialism; racism and white supremacy; ableism; and cis-and heteronormativity.A perspective on bias that maintains sensitivity to power dynamics requires accounting for historical inequities and labor conditions encoded in data (Micelli et al., 2022).To prevent this, we must actively ensure that our algorithms are fair.A naïve approach is fairness through unawareness which simply removes the protected attributes (e.g., race, gender) from the input features.Unfortunately, this is ineffective; the remaining features can still carry information about the protected attributes.More practical approaches first define a mathematical criterion for fairness.For example, the separation measure in binary classification requires that the prediction ŷ is conditionally independent of the protected variable a (e.g., race) given the true label y.Then they intervene in various ways to minimize the deviation from this measure (figure 21.2).A further complicating factor is that we cannot tell if an algorithm is unfair to a community or take steps to avoid this unless we can establish community membership.Most research on algorithmic bias and fairness has focused on ostensibly observable features that might be present in training data (e.g., gender).However, features of marginalized communities may be unobservable, making bias mitigation even more difficult.Examples include queerness (Tomasev et al., 2021), disability status, neurotype, class, and religion.A similar problem occurs when observable features have been excised from the training data to prevent models from exploiting them.Many decision spaces do not include actions that carry moral weight.For example, choosing the next chess move has no obvious moral consequence.However, elsewhere actions can carry moral weight.Examples include decision-making in autonomous vehicles (Awad et al., 2018;Evans et al., 2020), lethal autonomous weapons systems (Arkin, 2008a,b), and professional service robots for childcare, elderly care, and health care (Anderson & Anderson, 2008;Sharkey & Sharkey, 2012).As these systems become more autonomous, they may need to make moral decisions independent of human input.This leads to the notion of artificial moral agency.An artificial moral agent is an autonomous AI system capable of making moral judgments.Moral agency can be categorized in terms of increasing complexity (Moor, 2006):1. Ethical impact agents are agents whose actions have ethical impacts.Hence, almost any technology deployed in society might count as an ethical impact agent.2. Implicit ethical agents are ethical impact agents that include some in-built safety features.3. Explicit ethical agents can contextually follow general moral principles or rules of ethical conduct.4. Full ethical agents are agents with beliefs, desires, intentions, free will, and consciousness of their actions.The field of machine ethics seeks approaches to creating artificial moral agents.These approaches can be categorized as top-down, bottom-up, or hybrid (Allen et al., 2005).Topdown (theory-driven) methods directly implement and hierarchically arrange concrete rules based on some moral theory to guide ethical behavior.Asimov's "Three Laws of Robotics" are a trivial example of this approach.In bottom-up (learning-driven) approaches, a model learns moral regularities from data without explicit programming (Wallach et al., 2008).For example, Noothigattu et al. ( 2018) designed a voting-based system for ethical decision-making that uses data collected from human preferences in moral dilemmas to learn social preferences; the system then summarizes and aggregates the results to render an "ethical" decision.Hybrid approaches combine top-down and bottom-up approaches.Some researchers have questioned the very idea of artificial moral agency and argued that moral agency is unnecessary for ensuring safety (van Wynsberghe & Robbins, 2019).See Cervantes et al. (2019) for a recent survey of artificial moral agency and Tolmeijer et al. (2020) for a recent survey on technical approaches to artificial moral agency.A complex computational system is transparent if all of the details of its operation are known.A system is explainable if humans can understand how it makes decisions.In the absence of transparency or explainability, there is an asymmetry of information between the user and the AI system, which makes it hard to ensure value alignment.Creel ( 2020) characterizes transparency at several levels of granularity.Functional transparency refers to knowledge of the algorithmic functioning of the system (i.e., the logical rules that map inputs to outputs).The methods in this book are described at this level of detail.Structural transparency entails knowing how a program executes the algorithm.This can be obscured when commands written in high-level programming languages are executed by machine code.Finally, run transparency requires understanding how a program was executed in a particular instance.For deep networks, this includes Problem 21.4 knowledge about the hardware, input data, training data, and interactions thereof.None of these can be ascertained by scrutinizing code.For example, GPT3 is functionally transparent; its architecture is described in Brown et al. (2020).However, it does not exhibit structural transparency as we do not have access to the code, and it does not exhibit run transparency as we have no access to the learned parameters, hardware, or training data.The subsequent version GPT4 is not transparent at all.The details of how this commercial product works are unknown.Even if a system is transparent, this does not imply that we can understand how a decision is made or what information this decision is based on.Deep networks may contain billions of parameters, so there is no way we can understand how they work based on examination alone.However, in some jurisdictions, the public may have a right to an explanation.Article 22 of the EU General Data Protection Regulation suggests all data subjects should have the right to "obtain an explanation of the decision reached" in cases where a decision is based solely on automated processes. 1These difficulties have led to the sub-field of explainable AI.One moderately successful area is producing local explanations.Although we can't explain the entire system, Notebook 21.2 Explainability we can sometimes describe how a particular input was classified.For example, Local interpretable model-agnostic explanations or LIME (Ribeiro et al., 2016) samples the model output at nearby inputs and uses these samples to construct a simpler model (figure 21.3).This provides insight into the classification decision, even if the original model is neither transparent nor explainable.It remains to be seen whether it is possible to build complex decision-making systems that are fully understandable to their users or even their creators.There is also an ongoing debate about what it means for a system to be explainable, understandable, or interpretable (Erasmus et al., 2021); there is currently no concrete definition of these concepts.See Molnar (2022) for more information.The problems in the previous section arise from poorly specified objectives and informational asymmetries.However, even when a system functions correctly, it can entail unethical behavior or be intentionally misused.This section highlights some specific Problem 21.5 ethical concerns arising from the misuse of AI systems.Face recognition technologies have an especially high risk for misuse.Authoritarian states can use them to identify and silence protesters, thus risking democratic ideals of free speech and the right to protest.Smith & Miller (2022) argue that there is a mismatch between the values of liberal democracy (e.g., security, privacy, autonomy, and accountability) and the potential use cases for these technologies (e.g., border security, criminal investigation and policing, national security, and the commercializationof personal data).Thus, some researchers, activists, and policymakers have questioned whether this technology should exist (Barrett, 2020).Moreover, these technologies often do not do what they purport to (Raji et al., 2022).For example, the New York Metropolitan Transportation Authority moved forward with and expanded its use of facial recognition despite a proof-of-concept trial reporting a 100% failure rate to detect faces within acceptable parameters (Berger, 2019).Similarly, facial analysis tools often oversell their abilities (Raji & Fried, 2020), dubiously claiming to be able to infer individuals' sexual orientation (Leuner, 2019), emotions (Stark & Hoey, 2021), hireability (Fetscherin et al., 2020), or criminality .Stark & Hutson (2022) highlight that computer vision systems have created a resurgence in the "scientifically baseless, racist, and discredited pseudoscientific fields" of physiognomy and phrenology.Governments have a vested interest in funding AI research in the name of national security and state building.This risks an arms race between nation-states, which carries with it "high rates of investment, a lack of transparency, mutual suspicion and fear, and a perceived intent to deploy first" (Sisson et al., 2020).Lethal autonomous weapons systems receive significant attention because they areProblem 21.6easy to imagine, and indeed many such systems are under development (Heikkilä, 2022).However, AI also facilitates cyber-attacks and disinformation campaigns (i.e., inaccurate or misleading information that is shared with the intent to deceive).AI systems allow the creation of highly realistic fake content and facilitate the dissemination of information, often to targeted audiences (Akers et al., 2018) and at scale (Bontridder & Poullet, 2021).Kosinski et al. (2013) suggest that sensitive variables, including sexual orientation, ethnicity, religious and political views, personality traits, intelligence, happiness, use of addictive substances, parental separation, age, and gender, can be predicted by "likes" on social media alone.From this information, personality traits like "openness" can be used for manipulative purposes (e.g., to change voting behavior).Unfortunately, AI is a useful tool for automating fraudulent activities (e.g., sending mass emails or text messages that trick people into revealing sensitive information or sending money).Generative AI can be used to deceive people into thinking they are interacting with a legitimate entity or generate fake documents that mislead or deceive people.Additionally, AI could increase the sophistication of cyber-attacks, such as by generating more convincing phishing emails or adapting to the defenses of targeted organizations.This highlights the downside of calls for transparency in machine learning systems: the more open and transparent these systems are, the more vulnerable they may be to security risks or use by bad-faith actors.For example, generative language models, like Problem 21.7ChatGPT, have been used to write software and emails that could be used for espionage, ransomware, and other malware (Goodin, 2023).The tendency to anthropomorphize computer behaviors and particularly the projection of meaning onto strings of symbols is termed the ELIZA effect (Hofstadter, 1995).This leads to a false sense of security when interacting with sophisticated chatbots, making people more susceptible to text-based fraud such as romance scams or business email compromise schemes (Abrahams, 2023).Véliz (2023) highlights how emoji use in some chatbots is inherently manipulative, exploiting instinctual responses to emotive images.Modern deep learning methods rely on huge crowd-sourced datasets, which may contain sensitive or private information.Even when sensitive information is removed, auxiliary Problem 21.8 knowledge and redundant encodings can be used to de-anonymize datasets (Narayanan & Shmatikov, 2008).Indeed, this famously happened to the Governor of Massachusetts, William Weld, in 1997.After an insurance group released health records that had been stripped of obvious personal information like patient name and address, an aspiring graduate student was able to "de-anonymize" which records belonged to Governor Weld by cross-referencing with public voter rolls.Hence, privacy-first design is important for ensuring the security of individuals' information, especially when applying deep learning techniques to high-risk areas such as healthcare and finance.Differential privacy and semantic security (homomorphic encryption or secure multi-party computation) methods can be used to ensure data security during model training (see Mireshghallah et al., 2020;Boulemtafes et al., 2020).The previous section identified areas where AI can be deliberately misused.This section describes other potential side effects of the widespread adoption of AI.Intellectual property (IP) can be characterized as non-physical property that is the product of original thought (Moore & Himma, 2022).In practice, many AI models are trained on copyrighted material.Consequently, these models' deployment can pose legal and ethical risks and run afoul of intellectual property rights (Henderson et al., 2023).Sometimes, these issues are explicit.When language models are prompted with excerpts of copyrighted material, their outputs may include copyrighted text verbatim, and similar issues apply in the context of image generation in diffusion models (Henderson et al., 2023;Carlini et al., 2022Carlini et al., , 2023.Even if the training falls under "fair use," this may violate the moral rights of content creators in some cases (Weidinger et al., 2022).More subtly, generative models (chapters 12,14-18) raise novel questions regarding AIand intellectual property.Can the output of a machine learning model (e.g., art, music, code, text) be copyrighted or patented?Is it morally acceptable or legal to fine-tune a model on a particular artist's work to reproduce that artist's style?IP law is one area Problem 21.9that highlights how existing legislation was not created with machine learning models in mind.Although governments and courts may set precedents in the near future, these questions are still open at the time of writing.As society relies more on AI systems, there is an increased risk of automation bias (i.e., expectations that the model outputs are correct because they are "objective").This leads to the view that quantitative methods are better than qualitative ones.However, as we shall see in section 21.5, purportedly objective endeavors are rarely value-free.The sociological concept of deskilling refers to the redundancy and devaluation of skills in light of automation (Braverman, 1974).For example, off-loading cognitive skills like memory onto technology may cause a decrease in our capacity to remember things.Analogously, the automation of AI in morally-loaded decision-making may lead to a decrease in our moral abilities (Vallor, 2015).For example, in the context of war, the automation of weapons systems may lead to the dehumanization of victims of war (Asaro, 2012;Heyns, 2017).Similarly, care robots in elderly-, child-, or healthcare settings may reduce our ability to care for one another (Vallor, 2011).Training deep networks requires significant computational power and hence consumes a large amount of energy.Strubell et al. (2019Strubell et al. ( , 2020 estimate that training a transformer model with 213 million parameters emitted around 284 tonnes of CO 2 . 2 Luccioni et al. ( 2022) have provided similar estimates for the emissions produced from training the BLOOM language model.Unfortunately, the increasing prevalence of closed, proprietary models means that we know nothing about their environmental impacts (Luccioni, 2023).The history of technological innovation is a history of job displacement.In 2018, the McKinsey Global Institute estimated that AI may increase economic output by approximately US $13 trillion by 2030, primarily from the substitution of labor by automation (Bughin et al., 2018).Another study from the McKinsey Global Institute suggests that up to 30% of the global workforce (10-800 million people) could have their jobs displaced due to AI between 2016 and 2030 (Manyika et al., 2017;Manyika & Sneader, 2018).However, forecasting is inherently difficult, and although automation by AI may lead Problem 21.10 to short-term job losses, the concept of technological unemployment has been described as a "temporary phase of maladjustment" (Keynes, 2010).This is because gains in wealth can offset gains in productivity by creating increased demand for products and services.In addition, new technologies can create new types of jobs.Even if automation doesn't lead to a net loss of overall employment in the long term, new social programs may be required in the short term.Therefore, regardless of whether one is optimistic (Brynjolfsson & McAfee, 2016;Danaher, 2019), neutral (Metcalf et al., 2016;Calo, 2018;Frey, 2019), or pessimistic (Frey & Osborne, 2017) about the possibility of unemployment in light of AI, it is clear that society will be changed significantly.As deep networks increase in size, there is a corresponding increase in the amount of data and computing power required to train these models.In this regard, smaller companies and start-ups may not be able to compete with large, established tech companies.This may give rise to a feedback loop whereby the power and wealth become increasingly concentrated in the hands of a small number of corporations.A recent study finds an increasing discrepancy between publications at major AI venues by large tech firms and "elite" universities versus mid-or lower-tier universities (Ahmed & Wahed, 2016).In many views, such a concentration of wealth and power is incompatible with just distributions in society (Rawls, 1971).This has led to calls to democratize AI by making it possible for everyone to create Problem 21.11 such systems (Li, 2018;Knight, 2018;Kratsios, 2019;Riedl, 2020).Such a process requires making deep learning technologies more widely available and easier to use via open source and open science so that more people can benefit from them.This reduces barriers to entry and increases access to AI while cutting down costs, ensuring model accuracy, and increasing participation and inclusion (Ahmed et al., 2020).We now describe a case study that speaks to many of the issues that we have discussed in this chapter.In 2018, the popular media reported on a controversial facial analysis model-dubbed "gaydar AI" (Wang & Kosinski, 2018) (Fernandez, 2017).There are a number of problems with this work.First, the training dataset was highly biased and unrepresentative, being comprised mostly of Caucasian images.Second, modeling and validation are also questionable, given the fluidity of gender and sexuality.Third, the most obvious use case for such a model is the targeted discrimination and 21.5 The value-free ideal of science persecution of LGBTQ+ individuals in countries where queerness is criminalized.Fourth, with regard to transparency, explainability, and value alignment more generally, the "gaydar" model appears to pick up on spurious correlations due to patterns in grooming, presentation, and lifestyle rather than facial structure, as the authors claimed (Agüera y Arcas et al., 2018).Fifth, with regard to data privacy, questions arise regarding the ethics of scraping "public" photos and sexual orientation labels from a dating website.Finally, with regard to scientific communication, the researchers communicated their results in a way that was sure to generate headlines: even the title of the paper is an overstatement of the model's abilities: Deep Neural Networks Can Detect Sexual Orientation from Faces.(They cannot.)It should also be apparent that a facial-analysis model for determining sexual orientation does nothing whatsoever to benefit the LGBTQ+ community.If it is to benefit society, the most important question is whether a particular study, experiment, model, application, or technology serves the interests of the community to which it pertains.This chapter has enumerated a number of ways that the objectives of AI systems can unintentionally, or through misuse, diverge from the values of humanity.We now argue that scientists are not neutral actors; their values inevitably impinge on their work.Perhaps this is surprising.There is a broad belief that science is-or ought to beobjective.This is codified by the value-free ideal of science.Many would argue that machine learning is objective because algorithms are just mathematics.However, analogous to algorithmic bias (section 21.1.1),there are four stages at which the values of AI practitioners can affect their work (Reiss & Sprenger, 2017):1.The choice of research problem.2. Gathering evidence related to a research problem.3. Accepting a scientific hypothesis as an answer to a problem.4. Applying the results of scientific research.It is perhaps uncontroversial that values play a significant role in the first and last of these stages.The initial selection of research problems and the choice of subsequent applications are influenced by the interests of scientists, institutions, and funding agencies.However, the value-free ideal of science prescribes minimizing the influence of moral, personal, social, political, and cultural values on the intervening scientific process.This idea presupposes the value-neutrality thesis, which suggests that scientists can (at least in principle) attend to stages (2) and (3) without making these value judgments.However, whether intentional or not, values are embedded in machine learning research.Most of these values would be classed as epistemic (e.g., performance, generalization, building on past work, efficiency, novelty).But deciding the set of values is itself a value-laden decision; few papers explicitly discuss societal need, and fewer still discuss potential negative impacts (Birhane et al., 2022b).Philosophers of science have questioned whether the value-free ideal of science is attainable or desirable.For example, Longino (1990Longino ( , 1996 argues that these epistemic values are not purely epistemic.Kitcher (2011a,b) argues that scientists don't typically care about truth itself; instead, they pursue truths relevant to their goals and interests.Machine learning depends on inductive inference and is hence prone to inductive risk.Models are only constrained at the training data points, and the curse of dimensionality means this is a tiny proportion of the input space; outputs can always be wrong, regardless of how much data we use to train the model.It follows that choosing to accept or reject a model prediction requires a value judgment: that the risks if we are wrong in acceptance are lower than the risks if we are wrong in rejection.Hence, the use of inductive inference implies that machine learning models are deeply value-laden (Johnson, 2022).In fact, if they were not, they would have no application: it is precisely because they are value-laden that they are useful.Thus, accepting that algorithms are used for ranking, sorting, filtering, recommending, categorizing, labeling, predicting, etc., in the real world implies that these processes will have real-world effects.As machine learning systems become increasingly commercialized and applied, they become more entrenched in the things we care about.These insights have implications for researchers who believe that algorithms are somehow more objective than human decision-makers (and, therefore, ought to replace human decision-makers in areas where we think objectivity matters).It is easy to defer responsibility.Students and professionals who read this chapter might think their work is so far removed from the real world or a small part of a larger machine that their actions could not make a difference.However, this is a mistake.Researchers often have a choice about the projects to which they devote their time, the companies or institutions for which they work, the knowledge they seek, the social and intellectual circles in which they interact, and the way they communicate.Doing the right thing, whatever that may comprise, often takes the form of a social dilemma; the best outcomes depend upon cooperation, although it isn't necessarily in any Problem 21.12 individual's interest to cooperate: responsible AI research is a collective action problem.One positive step is to communicate responsibly.Misinformation spreads faster and persists more readily than the truth in many types of social networks (LaCroix et al., 2021;Ceylan et al., 2023).As such, it is important not to overstate machine learning systems' abilities (see case study above) and to avoid misleading anthropomorphism.It is also important to be aware of the potential for the misapplication of machine learning techniques.For example, pseudoscientific practices like phrenology and physiognomy have found a surprising resurgence in AI (Stark & Hutson, 2022).A second positive step is to encourage diversity.When social groups are homogeneous (composed mainly of similar members) or homophilous (comprising members that tend to associate with similar others), the dominant group tends to have its conventions recapitulated and stabilized (O'Connor & Bruner, 2019).One way to mitigate systems of oppression is to ensure that diverse views are considered.This might be achieved through equity, diversity, inclusion, and accessibility initiatives (at an institutional level), participatory and community-based approaches to research (at the research level), and increased awareness of social, political, and moral issues (at an individual level).The theory of standpoint epistemology (Harding, 1986) suggests that knowledge is socially situated (i.e., depends on one's social position in society).Homogeneity in tech circles can give rise to biased tech (Noble, 2018;Eubanks, 2018;Benjamin, 2019;Broussard, 2023).Lack of diversity implies that the perspectives of the individuals who create these technologies will seep into the datasets, algorithms, and code as the default perspective.Broussard (2023) argues that because much technology is developed by able-bodied, white, cisgender, American men, that technology is optimized for able-bodied, white, cisgender, American men, the perspective of whom is taken as the status quo.Ensuring technologies benefit historically marginalized communities requires researchers to understand the needs, wants, and perspectives of those communities (Birhane et al., 2022a).Design justice and participatory-and community-based approaches to AI research contend that the communities affected by technologies should be actively involved in their design (Constanza-Chock, 2020).It is undeniable that AI will radically change society for better or worse.However, optimistic visions of a future Utopian society driven by AI should be met with caution and a healthy dose of critical reflection.Many of the touted benefits of AI are beneficial only in certain contexts and only to a subset of society.For example, Green (2019) highlights that one project developed using AI to enhance police accountability and alternatives to incarceration and another developed to increase security through predictive policing are both advertised as "AI for Social Good."Assigning this label is a value judgment that lacks any grounding principles; one community's good is another's harm.When considering the potential for emerging technologies to benefit society, it is necessary to reflect on whether those benefits will be equally or equitably distributed.It is often assumed that the most technologically advanced solution is the best oneso-called technochauvinism (Broussard, 2018).However, many social issues arise from underlying social problems and do not warrant technological solutions.Some common themes emerged throughout this chapter, and we would like to impress four key points upon the reader:1. Research in machine learning cannot avoid ethics.Historically, researchers could focus on fundamental aspects of their work in a controlled laboratory set-ting.However, this luxury is dwindling due to the vast economic incentives to commercialize AI and the degree to which academic work is funded by industry (see Abdalla & Abdalla, 2021); even theoretical studies may have social impacts, so researchers must engage with the social and ethical dimensions of their work.There is still a widelyheld view that AI is fundamentally just mathematics and, therefore, it is "objective," and ethics are irrelevant.This assumption is not true when we consider the creation of AI systems or their deployment.3. We should question the structures within which AI work takes place.Much research on AI ethics focuses on specific situations rather than questioning the larger social structures within which AI will be deployed.For example, there is considerable interest in ensuring algorithmic fairness, but it may not always be possible to instantiate conceptions of fairness, justice, or equity within extant social and political structures.Therefore, technology is inherently political.Many potential ethical problems surrounding AI technologies are primarily social and structural, so technical innovation alone cannot solve these problems; if scientists are to effect positive change with new technology, they must take a political Problem 21.13and moral position.Where does this leave the average scientist?Perhaps with the following imperative: it is necessary to reflect upon the moral and social dimensions of one's work.This might require actively engaging those communities that are likely to be most affected by new technologies, thus cultivating relationships between researchers and communities and empowering those communities.Likewise, it might involve engagement with the literature beyond one's own discipline.For philosophical questions, the Stanford Encyclopedia of Philosophy is an invaluable resource.Interdisciplinary conferences are also useful in this regard.Leading work is published at both the Conference on Fairness, Accountability, and Transparency (FAccT) and the Conference on AI, Ethics, and Society (AIES).This chapter considered the ethical implications of deep learning and AI.The value alignment problem is the task of ensuring that the objectives of AI systems are aligned with human objectives.Bias, explainability, artificial moral agency, and other topics can be viewed through this lens.AI can be intentionally misused, and this chapter detailed some ways this can happen.Progress in AI has further implications in areas as diverse as IP law and climate change.Ethical AI is a collective action problem, and the chapter concludes with an appeal to scientists to consider the moral and ethical implications of their work.Every ethical issue is not within the control of every individual computer scientist.However, this does not imply that researchers have no responsibility whatsoever to consider-and mitigate where they can-the potential for misuse of the systems they create.Problem 21.1 It was suggested that the most common specification of the value alignment problem for AI is "the problem of ensuring that the values of AI systems are aligned with the values of humanity."Discuss the ways in which this statement of the problem is underspecified.Discussion Resource: LaCroix (2023).Problem 21.2 Goodhart's law states that "when a measure becomes a target, it ceases to be a good measure."Consider how this law might be reformulated to apply to value alignment for artificial intelligence, given that the loss function is a mere proxy for our true objectives.Problem 21.3 Suppose a university uses data from past students to build models for predicting "student success," where those models can support informed changes in policies and practices.Consider how biases might affect each of the four stages of the development and deployment of this model.Discussion Resource: Fazelpour & Danks (2021).Problem 21.4We might think of functional transparency, structural transparency, and run transparency as orthogonal.Provide an example of how an increase in one form of transparency may not lead to a concomitant increase in another form of transparency.Discussion Resource: Creel (2020).Problem 21.8 Some have suggested that personal data is a source of power for those who own it.Discuss the ways personal data is valuable to companies that utilize deep learning and consider the claim that losses to privacy are experienced collectively rather than individually.Discussion Resource: Véliz (2020).Problem 21.9 What are the implications of generative AI for the creative industries?How do you think IP laws should be modified to cope with this new development?Problem 21.10A good forecast must (i) be specific enough to know when it is wrong, (ii) account for possible cognitive biases, and (iii) allow for rationally updating beliefs.Consider any claim in the recent media about future AI and discuss whether it satisfies these criteria.Discussion Resource: Tetlock & Gardner (2016).Problem 21.11Some critics have argued that calls to democratize AI have focused too heavily on the participatory aspects of democracy, which can increase risks of errors in collective perception, reasoning, and agency, leading to morally-bad outcomes.Reflect on each of the following: What aspects of AI should be democratized?Why should AI be democratized?How should AI be democratized?Discussion Resource: Himmelreich (2022).Problem 21.12In March 2023, the Future of Life Institute published a letter, "Pause Giant AI Experiments," in which they called on all AI labs to immediately pause for at least six months the training of AI systems more powerful than GPT-4.Discuss the motivations of the authors in writing this letter, the public reaction, and the implications of such a pause.Relate this episode to the view that AI ethics can be considered a collective action problem (section 21.6).Discussion Resource: Gebru et al. (2023).Problem 21.13 Discuss the merits of the four points in section 21.7.Do you agree with them?Draft: please send errata to udlbookmail@gmail.com.This appendix details the notation used in this book.This mostly adheres to standard conventions in computer science, but deep learning is applicable to many different areas, so it is explained in full.In addition, there are several notational conventions that are unique to this book, including notation for functions and the systematic distinction between parameters and variables.Scalars are denoted by either small or capital letters a, A, α.Column vectors (i.e., 1D arrays of numbers) are denoted by small bold letters a, ϕ, and row vectors as the transpose of column vectors a T , ϕ T .Matrices and tensors (i.e., 2D and N D arrays of numbers, respectively) are both represented by bold capital letters B, Φ.Variables (usually the inputs and outputs of functions or intermediate calculations) are always denoted by Roman letters a, b, C. Parameters (which are internal to functions or probability distributions) are always denoted by Greek letters α, β, Γ. Generic, unspecified parameters are denoted by ϕ.This distinction is retained throughout the book except for the policy in reinforcement learning, which is denoted by π according to the usual convention.Sets are denoted by curly brackets, so {0, 1, 2} denotes the numbers 0, 1, and 2. The notation {0, 1, 2, . ..} denotes the set of non-negative integers.Sometimes, we want to specify a set of variables and {x i } I i=1 denotes the I variables x 1 , . . .x I .When it's not necessary to specify how many items are in the set, this is shortened to {x i }.The notation {x i , y i } I i=1 denotes the set of I pairs x i , y i .The convention for naming sets is to use calligraphic letters.Notably, B t is used to denote the set of indices in a batch at iteration t during training.The number of elements in a set S is denoted by |S|.The set R denotes the set of real numbers.The set R + denotes the set of non-negative real numbers.The notation R D denotes the set of D-dimensional vectors containing real numbers.The notation R D1×D2 denotes the set of matrices of dimension D 1 × D 2 .The notation R D1×D2×D3 denotes the set of tensors of size D 1 × D 2 × D 3 and so on.The notation [a, b] denotes the real numbers from a to b, including a and b themselves.When the square brackets are replaced by round brackets, this means that the adjacent value is not included in the set.For example, the set (−π, π] denotes the real numbers from −π to π, but excluding −π.Membership of sets is denoted by the ∈, so x ∈ R + means that the variable x is a non-negative real number, and the notation Σ ∈ R D×D denotes that Σ is a matrix of size D × D. Sometimes, we want to work through each element of a set systematically, and the notation ∀ {1, . . ., K} means "for all" the integers from 1 to K.Functions are expressed as a name, followed by square brackets that contain the arguments of the function.For example, log[x] returns the logarithm of the variable x.When the function returns a vector, it is written in bold and starts with a small letter.For example, the function y = mlp[x, ϕ] returns a vector y and has vector arguments x and ϕ.When a function returns a matrix or tensor, it is written in bold and starts with a capital letter.For example, the function Y = Sa[X, ϕ] returns a matrix Y and has arguments X and ϕ.When we want to leave the arguments of a function deliberately ambiguous, we use the bullet symbol (e.g., mlp[•, ϕ]).Some special functions are used repeatedly throughout the text:• The function min x [f[x]] returns the minimum value of the function f[x] over all possible values of the variable x.This notation is often used without specifying the details of how this minimum might be found.• The function argmin x [f[x]] returns the value of x that minimizes f] perform the equivalent operations for maximizing functions.Probability distributions should be written as P r(x = a), denoting that the random variable x takes the value of a.However, this notation is cumbersome.Hence, we usually simplify this and just write P r(x), where x denotes either the random variable or the value it takes according to the sense of the equation.The conditional probability of y given x is written as P r(y|x).The joint probability of y and x is written as P r(y, x).These two forms can be combined, so P r(y|x, ϕ) denotes the probability of the variable y, given that we know x and ϕ.Similarly, P r(y, x|ϕ) denotes the probability of variables y and x given that we know ϕ.When we need two probability distributions over the same variable, we write P r(x) for the first distribution and q(x) for the second.More information about probability distributions can be found in appendix C.Asymptotic notation is used to compare the amount of work done by different algorithms as the size D of the input increases.This can be done in various ways, but this book only uses big-O notation, which represents an upper bound on the growth of computation in an algorithm.A function f [n] is O[g[n]] if there exists a constant c > 0 and integer n 0 such that fThis notation provides a bound on the worst-case running time of an algorithm.For example, when we say that inversion of a D × D matrix is O[D 3 ], we mean that the computation will increase no faster than some constant times D 3 once D is large enough.This gives us an idea of how feasible it is to invert matrices of different sizes.If D = 10 3 , then it may take of the order of 10 9 operations to invert it.A small dot in a mathematical equation is intended to improve ease of reading and has no real meaning (or just implies multiplication).For example, α • f[x] is the same as αf [x] but is easier to read.To avoid ambiguity, dot products are written as a T b (see appendix B.3.4).A left arrow symbol ← denotes assignment, so x ← x + 2 means that we are adding two to the current value of x.A function defines a mapping from a set X (e.g., the set of real numbers) to another set Y.An injection is a one-to-one function where every element in the first set maps to a unique position in the second set (but there may be elements of the second set that are not mapped to).A surjection is a function where every element in the second set receives a mapping from the first (but there may be elements of the first set that are not mapped).A bijection or bijective mapping is a function that is both injective and surjective.It provides a one-to-one correspondence between all members of the two sets.A diffeomorphism is a special case of a bijection where both the forward and reverse mapping are differentiable.where β is known as the Lipschitz constant and determines the maximum gradient of the function (i.e., how fast the function can change) with respect to the distance metric.If the Lipschitz constant is less than one, the function is a contraction mapping, and we can use Banach's theorem to find the inverse for any point (see figure 16.9).Composing two functions with Lipschitz constants β 1 and β 2 creates a new Lipschitz continuous function with a constant that is less than or equal to β 1 β 2 .Adding two functions with Lipschitz constants β 1 and β 2 creates a new Lipschitz continuous function with a constant that is less than or equal to β 1 +β 2 .The Lipschitz constant of a linear transformation f[z] = Az+b with respect to a Euclidean distance measure is the maximum eigenvalue of A.A function is convex if we can draw a straight line between any two points on the function, and this line always lies above the function.Similarly, a function is concave if a straight line between any two points always lies below the function.By definition, convex (concave) functions have at most one minimum (maximum).A region of R D is convex if we can draw a line between any two points on the boundary of the region without intersecting the boundary in another place.Gradient descent guarantees to find the global minimum of any function that is both convex and defined on a convex region.The following functions are used in the main text: A dataset with N elements can be thought of as a probability distribution consisting of a sum of N delta functions centered at each data point x i and scaled by 1/N .The delta function is usually drawn as an arrow (e.g., figure 5.12).The delta function has the key property that:Binomial coefficients are written as n k and pronounced as "n choose k."They are positive integers that represent the number of ways of choosing an unordered subset of k items from a set of n items without replacement.Binomial coefficients can be computed using the simple formula:The autocorrelation r[τ ] of a continuous function f[z] is defined as:where τ is the time lag.Sometimes, this is normalized by r[0] so that the autocorrelation at time lag zero is one.The autocorrelation function is a measure of the correlation of the function with itself as a function of an offset (i.e., the time lag).If a function changes slowly and predictably, then the autocorrelation function will decrease slowly as the time lag increases from zero.If the function changes fast and unpredictably, then it will decrease quickly to zero.In machine learning, a vector x ∈ R D is a one-dimensional array of D numbers, which we will assume are organized in a column.Similarly, a matrix Y ∈ R D1×D2 is a twodimensional array of numbers with D 1 rows and D 2 columns.A tensor z ∈ R D1×D2...×D N is an N-dimensional array of numbers.Confusingly, all three of these quantities are stored in objects known as "tensors" in deep learning APIs such as PyTorch and TensorFlow.The transpose A T ∈ R D2×D1 of a matrix A ∈ R D1×D2 is formed by reflecting it around the principal diagonal so that the k th column becomes the k th row and vice-versa.If we take the transpose of a matrix product AB, then we take the transpose of the original matrices but reverse the order so thatThe transpose of a column vector a is a row vector a T and vice-versa.For a vector z, the ℓ p norm is defined as:When p = 2, this returns the length of the vector, and this is known as the Euclidean norm.It is this case that is most commonly used in deep learning, and often the exponent p is omitted, and the Euclidean norm is just written as ||z||.When p = ∞, the operator returns the maximum absolute value in the vector.Norms can be computed in a similar way for matrices.For example, the ℓ 2 norm of a matrix Z (known as the Frobenius norm) is calculated as:(B.9)The product C = AB of two matrices A ∈ R D1×D2 and B ∈ R D2×D3 is a third matrix C ∈ R D1×D3 where:A id B dj .(B.10)The dot product a T b of two vectors a ∈ R D and b ∈ R D is a scalar and is defined as:It can be shown that the dot product is proportional to the Euclidean norm of the first vector times the Euclidean norm of the second vector times the angle θ between them:(B.12)A square matrix A may or may not have an inverse A −1 such that A −1 A = AA −1 = I.If a matrix does not have an inverse, it is called singular.If we take the inverse of a matrix product AB then we can equivalently take the inverse of each matrix individually and reverse the order of multiplication.In general, it takes O[D 3 ] operations to invert a D × D matrix.However, inversion is more efficient for special types of matrices, including diagonal, orthogonal, and triangular matrices (see section B.4).Consider a matrix A ∈ R D1×D2 .If the number of columns D 2 of the matrix is fewer than the number of rows D 1 (i.e., the matrix is "portrait"), the product Ax cannot reach all When the points {xi} on the unit circle are transformed to points {x ′ i } by a linear transformation x ′ i = Axi, they are mapped to an ellipse.For example, the light blue point on the unit circle is mapped to the light blue point on the ellipse.The length of the major (longest) axis of the ellipse (long gray arrow) is the magnitude of the first eigenvalue of the matrix, and the length of the minor (shortest) axis of the ellipse (short gray arrow) is the magnitude of the second eigenvalue.possible in the D 1 -dimensional output space.This product consists of the D 2 columns of A weighted by the D 2 elements of x and can only reach the linear subspace that is spanned by these columns.This is known as the column space of the matrix.Conversely, for a landscape matrix A, the part of the input space that maps to zero (i.e., those x where Ax = 0) is termed the nullspace of the matrix.If we multiply the set of 2D points on a unit circle by a 2 × 2 matrix A, they map to an ellipse (figure B.3).The radii of the major and minor axes of this ellipse (i.e., the longest and shortest directions) correspond to the magnitude of the eigenvalues λ 1 and λ 2 of the matrix.The eigenvalues also have a sign, which relates to whether the matrix reflects the inputs about the origin.The same idea applies in higher dimensions.A D−dimensional spheroid is mapped by a D × D matrix A to a D-dimensional ellipsoid.The radii of the D principal axes of this ellipsoid determine the magnitude of the eigenvalues.The spectral norm of a square matrix is the largest absolute eigenvalue.It captures the largest possible change in magnitude when the matrix is applied to a vector of unit length.As such, it tells us about the Lipschitz constant of the transformation.The set of eigenvalues is sometimes called the eigenspectrum and tells us about the magnitude of the scaling applied by the matrix across all directions.This information can be summarized using the determinant and trace of the matrix.Every square matrix A has a scalar associated with it called the determinant and denoted by |A| or det [A], which is the product of the eigenvalues.It is hence related to the average scaling applied by the matrix for different inputs.Matrices with small absolute determinants tend to decrease the norm of vectors upon multiplication.Matrices with large absolute determinants tend to increase the norm.If a matrix is singular, the determinant will be zero, and there will be at least one direction in space that is mapped to the origin when the matrix is applied.Determinants of matrix expressions obey the following rules:The trace of a square matrix is the sum of the diagonal values (the matrix itself need not be diagonal) or the sum of the eigenvalues.Traces obey these rules: (B.15)where in the last relation, the trace is invariant for cyclic permutations only, so in general, trace[ABC] ̸ = trace[BAC].Calculating the inverse of a square matrix A ∈ R D×D has a complexity of O[D 3 ], as does the computation of the determinant.However, for some matrices with special properties, these computations can be more efficient.A diagonal matrix has zeros everywhere except on the principal diagonal.If these diagonal entries are all non-zero, the inverse is also a diagonal matrix, with each diagonal entry d ii replaced by 1/d ii .The determinant is the product of the values on the diagonal.A special case of this is the identity matrix, which has ones on the diagonal.Consequently, its inverse is also the identity matrix, and its determinant is one.A lower triangular matrix contains only non-zero values on the principal diagonal and the positions below this.An upper triangular matrix contains only non-zero values on the principal diagonal and the positions above this.In both cases, the matrix can be inverted in O [D 2 ] (see problem 16.4), and the determinant is just the product of the values on the diagonal.Orthogonal matrices represent rotations and reflections around the origin, so in figure B.3, the circle would be mapped to another circle of unit radius but rotated and possibly reflected.Accordingly, the eigenvalues must all have magnitude one, and the determinant must be either one or minus one.The inverse of an orthogonal matrix is its transpose, so A −1 = A T .A permutation matrix has exactly one non-zero entry in each row and column, and all of these entries take the value one.It is a special case of an orthogonal matrix, so its inverse is its own transpose, and its determinant is always one.As the name suggests, it has the effect of permuting the entries of a vector.For example:Linear algebra is the mathematics of linear functions, which have the form: (B.17)where ϕ 1 , . . ., ϕ D are parameters that define the function.We often add a constant term ϕ 0 to the right-hand side.This is technically an affine function but is commonly referred to as linear in machine learning.We adopt this convention throughout.Consider a collection of linear functions:These can be written in matrix form as: (B.19) or as y = ϕ 0 + Φz for short, where y i = ϕ i0 +Most readers of this book will be accustomed to the idea that if we have a function y = f[x], we can compute the derivative ∂y/∂x, and this represents how y changes when we make a small change in x.This idea extends to functions y = f[x] mapping a vector x to a scalar y, functions y = f[x] mapping a vector x to a vector y, functions y = f[X] mapping a matrix X to a vector y, and so on.The rules of matrix calculus help us compute derivatives of these quantities.The derivatives take the following forms:• For a function y = f[x] where y ∈ R and x ∈ R D , the derivative ∂y/∂x is also a D-dimensional vector, where the i th element is computed as ∂y/∂x i .• For a function y = f[x] where y ∈ R Dy and x ∈ R Dx , the derivative ∂y/∂x is a D x × D y matrix where element (i, j) contains the derivative ∂y j /∂x i .This is known as a Jacobian and is sometimes written as ∇ x y in other documents.• For a function y = f[X] where y ∈ R Dy and X ∈ R D1×D2 , the derivative ∂y/∂X is a 3D tensor containing the derivatives ∂y i /∂x jk .Often these matrix and vector derivatives have superficially similar forms to the scalar case.For example, we have:Probability is critical to deep learning.In supervised learning, deep networks implicitly rely on a probabilistic formulation of the loss function.In unsupervised learning, generative models aim to produce samples that are drawn from the same probability distribution as the training data.Reinforcement learning occurs within Markov decision processes, and these are defined in terms of probability distributions.This appendix provides a primer for probability as used in machine learning.A random variable x denotes a quantity that is uncertain.It may be discrete (take only certain values, for example integers) or continuous (take any value on a continuum, for example real numbers).If we observe several instances of a random variable x, it will take different values, and the relative propensity to take different values is described by a probability distribution P r(x).For a discrete variable, this distribution associates a probability P r(x = k) ∈ [0, 1] with each potential outcome k, and the sum of these probabilities is one.For a continuous variable, there is a non-negative probability density P r(x = a) ≥ 0 associated with each value a in the domain of x, and the integral of this probability density function (PDF) over this domain must be one.This density can be greater than one for any point a.From here on, we assume that the random variables are continuous.The ideas are exactly the same for discrete distributions but with sums replacing integrals.Consider the case where we have two random variables x and y.The joint distribution P r(x, y) tells us about the propensity that x and y take particular combinations of values (figure C.1a).Now there is a non-negative probability density P r(x = a, y = b) associated with each pair of values x = a and y = b and this must satisfy: Here, the probability density is represented by the color map, so brighter positions are more probable.For example, the combination x = 6, y = 6 is much less likely to be observed than the combination x = 5, y = 0. b) The marginal distribution P r(x) of variable x can be recovered by integrating over y. c) The marginal distribution P r(y) of variable y can be recovered by integrating over x.This idea extends to more than two variables, so the joint density of x, y, and z is written as P r(x, y, z).Sometimes, we store multiple random variables in a vector x, and we write their joint density as P r(x).Extending this, we can write the joint density of all of the variables in two vectors x and y as P r(x, y).If we know the joint distribution P r(x, y) over two variables, we can recover the marginal distributions P r(x) and P r(y) by integrating over the other variable (figure C.1b-c):This process is called marginalization and has the interpretation that we are computing the distribution of one variable regardless of the value the other one took.The idea of marginalization extends to higher dimensions, so if we have a joint distribution P r(x, y, z), we can recover the joint distribution P r(x, z) by integrating over y.The conditional probability P r(x|y) is the probability of variable x taking a certain value, assuming we know the value of y.The vertical line is read as the English word "given," When we consider the conditional probability P r(x|y) as a function of x, it must sum to one.When we consider the same quantity P r(x|y) as a function of y, it is termed the likelihood of x given y and does not have to sum to one.If the value of the random variable y tells us nothing about x and vice-versa, we say that x and y are independent, and we can write P r(x|y) = P r(x) and P r(y|x) = P r(y).It follows that all of the conditional distributions P r(y|x = •) are identical, as are the conditional distributions P r(x|y = •).Starting from the first expression for the joint probability in equation C.5, we see that the joint distribution becomes the product of the marginal distributions: Draft: please send errata to udlbookmail@gmail.com.Consider a function f[x] and a probability distribution P r(x) defined over x.The expected value of a function f[•] of a random variable x with respect to the probability distribution P r(x) is defined as:As the name suggests, this is the expected or average value of f[x] after taking into account the probability of seeing different values of x.This idea generalizes to functions f[•, •] of more than one random variable:An expectation is always taken with respect to a distribution over one or more variables.However, we don't usually make this explicit when the choice of distribution is obvious and write EIf we drew a large number I of samples {x i } I i=1 from P r(x), calculated f[x i ] for each sample and took the average of these values, the result would approximate the expectation E[f[x]] of the function:(C.10)There are four rules for manipulating expectations:where k is an arbitrary constant.These are proven below for the continuous case.Rule] of the function:] of the expectations:.Rule 4: The expectation of a product Ewhere we used the definition of independence (equation C.7) between the first two lines.The four rules generalize to the multivariate case:if x, y independent, (C.12)where now A is a constant matrix and f[x] is a function of the vector x that returns a vector, and g[y] is a function of the vector y that also returns a vector.For some choices of function f[•], the expectation is given a special name.These quantities are often used to summarize the properties of complex distributions.For example, when f[x] = x, the resulting expectation E[x] is termed the mean, µ.It is a measure of the center of a distribution.Similarly, the expected squared deviation from the mean E[(x − µ) 2 ] is termed the variance, σ 2 .This is a measure of the spread of the distribution.The standard deviation σ is the positive square root of the variance.It also measures the spread of the distribution but has the merit that it is expressed in the same units as the variable x.As the name suggests, the covariance E[(x − µ x )(y − µ y )] of two variables x and y measures the degree to which they co-vary.Here µ x and µ y represent the mean of the variables x and y, respectively.The covariance will be large when the variance of both variables is large and when the value of x tends to increase when the value of y increases.If two variables are independent, then their covariance is zero.However, a covariance of zero does not imply independence.For example, consider a distribution P r(x, y) where the probability is uniformly distributed on a circle of radius one centered on the origin of the x, y plane.There is no tendency on average for x to increase when y increases or vice-versa.However, knowing the value of x = 0 tells us that y has an equal chance of taking the values ±1, so the variables cannot be independent.The covariances of multiple random variables stored in a column vector x ∈ R D can be represented by the D×D covariance matrix E[(x − µ x )(x − µ x ) T ], where the vector µ x contains the means E[x].The element at position (i, j) of this matrix represents the covariance between variables x i and x j .The rules of expectation (appendix C.2.1) can be used to prove the following identity that allows us to write the variance in a different form:where we have used rule 3 between lines 1 and 2, rules 1 and 2 between lines 2 and 3, and the definition µ = E[x] in the remaining two lines.Setting the mean of a random variable to zero and the variance to one is known as standardization.This is achieved using the transformation:where µ is the mean of x and σ is the standard deviation.Proof: The mean of the new distribution over z is given by:where again, we have used the four rules for manipulating expectations.The variance of the new distribution is given by:By a similar argument, we can take a standardized variable z with mean zero and unit variance and convert it to a variable x with mean µ and variance σ 2 using:In the multivariate case, we can standardize a variable x with mean µ and covariance matrix Σ using:The result will have a mean E[z] = 0 and an identity covariance matrix ETo reverse this process, we use:Probability distributions used in this book include the Bernoulli distribution (figure 5.6), categorical distribution (figure 5.9), Poisson distribution (figure 5.15), von Mises distribution (figure 5.13), and mixture of Gaussians (figures 5.14 and 17.1).However, the most common distribution in machine learning is the normal or Gaussian distribution.A univariate normal distribution (figure 5.3) over scalar variable x has two parameters, the mean µ and the variance σ 2 , and is defined as:Unsurprisingly, the mean E[x] of a normally distributed variable is given by the mean parameter µ and the variance E[(x − E[x]) 2 ] by the variance parameter σ 2 .When the mean is zero and the variance is one, we refer to this as a standard normal distribution.The shape of the normal distribution can be inferred from the following argument.The term −(x−µ) 2 /2σ 2 is a quadratic function that falls away from zero when x = µ at a rate that increases when σ becomes smaller.When we pass this through the exponential function (figure B.1), we get a bell-shaped curve, which has a value of one at x = µ and falls away to either side.Dividing by the constant √ 2πσ 2 ensures that the function integrates to one and is a valid distribution.It follows from this argument that the mean µ control the position of the center of the bell curve, and the square root σ of the variance (the standard deviation) controls the width of the bell curve.The multivariate normal distribution generalizes the normal distribution to describe the probability over a vector quantity x of length D. It is defined by a D × 1 mean vector µ and a symmetric positive definite D × D covariance matrix Σ:The interpretation is similar to the univariate case.The quadratic term −(x−µ) T Σ −1 (x− µ)/2 returns a scalar that decreases as x grows further from the mean µ, at a rate that depends on the matrix Σ.This is turned into a bell-curve shape by the exponential, and dividing by (2π) D/2 |Σ| 1/2 ensures that the distribution integrates to one.The covariance matrix can take spherical, diagonal, and full forms: In two dimensions (figure C.4), spherical covariances produce circular iso-density contours, and diagonal covariances produce ellipsoidal iso-contours that are aligned with the coordinate axes.Full covariances produce general ellipsoidal iso-density contours.When the covariance is spherical or diagonal, the individual variables are independent:The product of two normal distributions is proportional to a third normal distribution according to the relation:This is easily proved by multiplying out the exponential terms and completing the square (see problem 18.5).When the mean of a multivariate normal in x is a linear function Ay + b of a second variable y, this is proportional to another normal distribution in y, where the mean is a linear function of x:At first sight, this relation is rather opaque, but figure C.5 shows the case for scalar x and y, which is easy to understand.As for the previous relation, this can be proved by expanding the quadratic product in the exponential term and completing the square to make this a distribution in y. (see problem 18.4).To sample from a univariate distribution P r(x), we first compute the cumulative distribution F[x] (the integral of P r(x)).Then we draw a sample z * from a uniform distribution over the range [0, 1] and evaluate this against the inverse of the cumulative distribution, so the sample x * is created as:The method above can be used to generate a sample x * from a univariate standard normal distribution.A sample from a normal distribution with mean µ and variance σ 2 can then be created using equation C.18.Similarly, a sample x * from a D-dimensional multivariate standard distribution can be created by independently sampling D univariate standard normal variables.A sample from a multivariate normal distribution with mean µ and covariance Σ can be then created using equation C.20.When the joint distribution can be factored into a series of conditional probabilities, we can generate samples using ancestral sampling.The basic idea is to generate a from the root variable(s) and then sample from the subsequent conditional distributions based on this instantiation.This process is known as ancestral sampling and is easiest to understand with an example.Consider a joint distribution over three variables, x, y, and z, where the distribution factors as: P r(x, y, z) = P r(x)P r(y|x)P r(z|y).To sample from this joint distribution, we first draw a sample x * from P r(x).Then we draw a sample y * from P r(y|x * ).Finally, we draw a sample z * from P r(z|y * ).Supervised learning can be framed in terms of minimizing the distance between the probability distribution implied by the model and the discrete probability distribution implied by the samples (section 5.7).Unsupervised learning can often be framed in terms of minimizing the distance between the probability distribution of real examples and the distribution of data from the model.In both cases, we need a measure of distance between two probability distributions.This section considers the properties of several different measures of distance between distributions (see also figure 15.8 for a discussion of the Wasserstein or earth mover's distance).The most common measure of distance between probability distributions p(x) and q(x) is the Kullback-Leibler or KL divergence and is defined as:This distance is always greater than or equal to zero, which is easily demonstrated by noting that − log[y] ≥ 1 − y (figure C.6) so:The KL divergence is infinite if there are places where q(x) is zero but p(x) is non-zero.This can lead to problems when we are minimizing a function based on this distance.The KL divergence is not symmetric (i.e., D KL [p(x)||q(x)]̸ = D KL [q(x)||p(x)]).The Jensen-Shannon divergence is a measure of distance that is symmetric by construction:It is the mean divergence of p(x) and q(x) to the average of the two distributions.The Fréchet distance D F R between two distributions p(x) and q(x) is given by:where π(x, y) represents the set of joint distributions that are compatible with the marginal distributions p(x) and q(y).The Fréchet distance can also be formulated as a measure of the maximum distance between the cumulative probability curves.Often we want to compute the distance between two multivariate normal distributions with means µ 1 and µ 2 and covariances Σ 1 and Σ 2 .In this case, various measures of distance can be written in closed form.The KL divergence can be computed as:where tr [•] is the trace of the matrix argument.The Fréchet/2-Wasserstein distance is given by:(C.33)Draft: please send errata to udlbookmail@gmail.com.This appendix reviews mathematical concepts that are used in the main text.Guan, S., Tai, Y., Ni, B., Zhu, F., Huang, F., & Yang, X. (2020).Collaborative learning for faster StyleGAN embedding. arXiv:2007.01758. 301 Gui, J., Sun, Z., Wen, Y., Tao, D., & Ye, J. (2021).AThis appendix reviews mathematical concepts that are used in the main text.Guan, S., Tai, Y., Ni, B., Zhu, F., Huang, F., & Yang, X. (2020).Collaborative learning for faster StyleGAN embedding. arXiv:2007.01758. 301 Gui, J., Sun, Z., Wen, Y., Tao, D., & Ye, J. (2021).A